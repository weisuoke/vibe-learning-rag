# 核心概念2：性能测试工具

> 掌握专业的性能测试工具，提高测试效率和准确性

---

## 什么是性能测试工具？

**性能测试工具是自动化执行性能测试、采集指标、生成报告的软件工具。**

类比：
- **前端开发**：Lighthouse、WebPageTest 是网站性能测试工具
- **日常生活**：体检设备（血压计、体温计）是健康测试工具

---

## 工具分类

```
性能测试工具
├── 1. 基础测试工具（pymilvus + Python脚本）
├── 2. 压力测试工具（Locust、JMeter）
├── 3. 监控工具（Prometheus + Grafana）
├── 4. 分析工具（自定义脚本、Jupyter Notebook）
└── 5. 官方工具（pymilvus-benchmark）
```

---

## 工具1：pymilvus + Python脚本

### 1.1 基础性能测试

**最简单的测试工具：pymilvus + time 模块**

```python
import time
import numpy as np
from pymilvus import connections, Collection

def basic_benchmark(collection_name, iterations=100):
    """基础性能测试"""

    # 连接
    connections.connect("default", host="localhost", port="19530")
    collection = Collection(collection_name)
    collection.load()

    # 准备测试向量
    test_vector = [[0.1] * 768]
    latencies = []

    # 预热
    for _ in range(10):
        collection.search(
            data=test_vector,
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=10
        )

    # 测试
    for _ in range(iterations):
        start = time.time()
        collection.search(
            data=test_vector,
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=10
        )
        latencies.append((time.time() - start) * 1000)

    # 统计
    print(f"平均延迟: {np.mean(latencies):.2f}ms")
    print(f"P95延迟: {np.percentile(latencies, 95):.2f}ms")
    print(f"P99延迟: {np.percentile(latencies, 99):.2f}ms")
    print(f"QPS: {iterations / (sum(latencies) / 1000):.2f}")

# 使用
basic_benchmark("my_collection")
```

**优点：**
- ✅ 简单易用，快速上手
- ✅ 灵活定制，适合各种场景
- ✅ 无需额外依赖

**缺点：**
- ❌ 功能有限，需要手动编写
- ❌ 不支持高并发测试
- ❌ 缺少可视化报告

---

### 1.2 进阶测试脚本

**封装完整的测试框架：**

```python
import time
import numpy as np
from typing import Dict, List
from pymilvus import connections, Collection
from dataclasses import dataclass

@dataclass
class BenchmarkResult:
    """测试结果数据类"""
    config_name: str
    qps: float
    avg_latency: float
    p50_latency: float
    p95_latency: float
    p99_latency: float
    recall: float

class MilvusBenchmark:
    """Milvus 性能测试框架"""

    def __init__(self, host="localhost", port="19530"):
        connections.connect("default", host=host, port=port)

    def test_search_performance(
        self,
        collection_name: str,
        search_params: Dict,
        iterations: int = 100,
        warmup: int = 10
    ) -> BenchmarkResult:
        """测试检索性能"""

        collection = Collection(collection_name)
        collection.load()

        test_vector = [[0.1] * 768]
        latencies = []

        # 预热
        for _ in range(warmup):
            collection.search(
                data=test_vector,
                anns_field="embedding",
                param=search_params,
                limit=10
            )

        # 测试
        start_time = time.time()
        for _ in range(iterations):
            start = time.time()
            collection.search(
                data=test_vector,
                anns_field="embedding",
                param=search_params,
                limit=10
            )
            latencies.append((time.time() - start) * 1000)
        total_time = time.time() - start_time

        # 计算召回率
        recall = self._calculate_recall(collection, test_vector, search_params)

        return BenchmarkResult(
            config_name=search_params.get("index_type", "unknown"),
            qps=iterations / total_time,
            avg_latency=np.mean(latencies),
            p50_latency=np.percentile(latencies, 50),
            p95_latency=np.percentile(latencies, 95),
            p99_latency=np.percentile(latencies, 99),
            recall=recall
        )

    def _calculate_recall(self, collection, test_vector, search_params):
        """计算召回率"""
        # 使用 FLAT 索引作为 ground truth
        ground_truth = collection.search(
            data=test_vector,
            anns_field="embedding",
            param={"metric_type": "L2", "params": {}},
            limit=10
        )
        ground_truth_ids = set([hit.id for hit in ground_truth[0]])

        # 测试结果
        results = collection.search(
            data=test_vector,
            anns_field="embedding",
            param=search_params,
            limit=10
        )
        result_ids = set([hit.id for hit in results[0]])

        return len(ground_truth_ids & result_ids) / len(ground_truth_ids)

    def compare_configs(
        self,
        collection_name: str,
        configs: List[Dict]
    ) -> List[BenchmarkResult]:
        """对比多个配置"""

        results = []
        for config in configs:
            print(f"\n测试配置: {config['name']}")
            result = self.test_search_performance(
                collection_name,
                config['search_params']
            )
            results.append(result)

            print(f"  QPS: {result.qps:.2f}")
            print(f"  P95延迟: {result.p95_latency:.2f}ms")
            print(f"  召回率: {result.recall:.2%}")

        return results

# 使用示例
benchmark = MilvusBenchmark()

configs = [
    {
        "name": "HNSW-ef64",
        "search_params": {"metric_type": "L2", "params": {"ef": 64}}
    },
    {
        "name": "IVF_FLAT-nprobe10",
        "search_params": {"metric_type": "L2", "params": {"nprobe": 10}}
    }
]

results = benchmark.compare_configs("my_collection", configs)
```

---

## 工具2：Locust（压力测试）

### 2.1 Locust 简介

**Locust 是一个开源的负载测试工具，用于模拟大量并发用户。**

**为什么选择 Locust？**
- ✅ Python 编写，易于集成 pymilvus
- ✅ 支持高并发（数千用户）
- ✅ 实时 Web UI 监控
- ✅ 分布式测试支持

---

### 2.2 Locust 基础用法

**安装：**
```bash
pip install locust
```

**编写测试脚本（locustfile.py）：**

```python
from locust import User, task, between
from pymilvus import connections, Collection
import random

class MilvusUser(User):
    """模拟 Milvus 用户"""

    # 每次请求间隔 0.1-0.5 秒
    wait_time = between(0.1, 0.5)

    def on_start(self):
        """初始化连接（每个用户执行一次）"""
        connections.connect("default", host="localhost", port="19530")
        self.collection = Collection("my_collection")
        self.collection.load()

    @task(weight=10)
    def search_vector(self):
        """执行向量检索（权重10，执行频率高）"""
        # 生成随机向量
        vector = [random.random() for _ in range(768)]

        # 执行检索
        self.collection.search(
            data=[vector],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=10
        )

    @task(weight=1)
    def search_with_filter(self):
        """执行混合检索（权重1，执行频率低）"""
        vector = [random.random() for _ in range(768)]

        self.collection.search(
            data=[vector],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=10,
            expr="category == 'A'"
        )
```

**运行测试：**

```bash
# 启动 Locust（Web UI 模式）
locust -f locustfile.py --host=http://localhost:19530

# 命令行模式（无 UI）
locust -f locustfile.py --host=http://localhost:19530 \
  --users 100 --spawn-rate 10 --run-time 5m --headless
```

**参数说明：**
- `--users 100`：模拟 100 个并发用户
- `--spawn-rate 10`：每秒启动 10 个用户
- `--run-time 5m`：运行 5 分钟
- `--headless`：无 UI 模式

---

### 2.3 Locust 进阶用法

**场景1：逐步增加负载**

```python
from locust import User, task, between, LoadTestShape

class StagesShape(LoadTestShape):
    """阶梯式负载测试"""

    stages = [
        {"duration": 60, "users": 10, "spawn_rate": 10},   # 1分钟，10用户
        {"duration": 120, "users": 50, "spawn_rate": 10},  # 2分钟，50用户
        {"duration": 180, "users": 100, "spawn_rate": 10}, # 3分钟，100用户
        {"duration": 240, "users": 200, "spawn_rate": 20}, # 4分钟，200用户
    ]

    def tick(self):
        run_time = self.get_run_time()

        for stage in self.stages:
            if run_time < stage["duration"]:
                return (stage["users"], stage["spawn_rate"])

        return None  # 测试结束
```

**场景2：测试不同查询参数**

```python
class MilvusUser(User):
    wait_time = between(0.1, 0.5)

    def on_start(self):
        connections.connect("default", host="localhost", port="19530")
        self.collection = Collection("my_collection")
        self.collection.load()

        # 随机选择 nprobe 值
        self.nprobe = random.choice([10, 20, 50, 100])

    @task
    def search_with_random_nprobe(self):
        """使用随机 nprobe 值测试"""
        vector = [random.random() for _ in range(768)]

        self.collection.search(
            data=[vector],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": self.nprobe}},
            limit=10
        )
```

---

## 工具3：Prometheus + Grafana（监控）

### 3.1 监控指标采集

**使用 Prometheus 采集 Milvus 指标：**

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'milvus'
    static_configs:
      - targets: ['localhost:9091']  # Milvus metrics 端口
```

**启动 Prometheus：**
```bash
docker run -d -p 9090:9090 \
  -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml \
  prom/prometheus
```

---

### 3.2 Grafana 可视化

**关键监控指标：**

```
1. QPS（每秒查询数）
   - milvus_proxy_search_vectors_count

2. 延迟（响应时间）
   - milvus_proxy_search_latency_bucket

3. CPU 使用率
   - process_cpu_seconds_total

4. 内存使用量
   - process_resident_memory_bytes

5. 磁盘 I/O
   - milvus_datanode_disk_read_bytes
   - milvus_datanode_disk_write_bytes
```

**Grafana Dashboard 配置：**

```json
{
  "dashboard": {
    "title": "Milvus Performance",
    "panels": [
      {
        "title": "QPS",
        "targets": [
          {
            "expr": "rate(milvus_proxy_search_vectors_count[1m])"
          }
        ]
      },
      {
        "title": "P95 Latency",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, milvus_proxy_search_latency_bucket)"
          }
        ]
      }
    ]
  }
}
```

---

## 工具4：自定义分析工具

### 4.1 性能报告生成器

```python
import matplotlib.pyplot as plt
import pandas as pd
from typing import List

class PerformanceReporter:
    """性能测试报告生成器"""

    def __init__(self, results: List[BenchmarkResult]):
        self.results = results
        self.df = pd.DataFrame([vars(r) for r in results])

    def generate_comparison_chart(self, output_file="comparison.png"):
        """生成对比图表"""

        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # QPS 对比
        axes[0, 0].bar(self.df['config_name'], self.df['qps'])
        axes[0, 0].set_title('QPS Comparison')
        axes[0, 0].set_ylabel('QPS')

        # 延迟对比
        axes[0, 1].bar(self.df['config_name'], self.df['p95_latency'])
        axes[0, 1].set_title('P95 Latency Comparison')
        axes[0, 1].set_ylabel('Latency (ms)')

        # 召回率对比
        axes[1, 0].bar(self.df['config_name'], self.df['recall'])
        axes[1, 0].set_title('Recall Comparison')
        axes[1, 0].set_ylabel('Recall')
        axes[1, 0].set_ylim([0, 1])

        # 综合评分（QPS * 召回率 / 延迟）
        score = self.df['qps'] * self.df['recall'] / self.df['p95_latency']
        axes[1, 1].bar(self.df['config_name'], score)
        axes[1, 1].set_title('Overall Score')
        axes[1, 1].set_ylabel('Score')

        plt.tight_layout()
        plt.savefig(output_file)
        print(f"图表已保存到: {output_file}")

    def generate_markdown_report(self, output_file="report.md"):
        """生成 Markdown 报告"""

        with open(output_file, 'w') as f:
            f.write("# Milvus 性能测试报告\n\n")
            f.write(f"测试时间: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("## 测试结果\n\n")
            f.write(self.df.to_markdown(index=False))

            f.write("\n\n## 结论\n\n")

            # 找出最佳配置
            best_qps = self.df.loc[self.df['qps'].idxmax()]
            best_latency = self.df.loc[self.df['p95_latency'].idxmin()]
            best_recall = self.df.loc[self.df['recall'].idxmax()]

            f.write(f"- **最高 QPS**: {best_qps['config_name']} ({best_qps['qps']:.2f})\n")
            f.write(f"- **最低延迟**: {best_latency['config_name']} ({best_latency['p95_latency']:.2f}ms)\n")
            f.write(f"- **最高召回率**: {best_recall['config_name']} ({best_recall['recall']:.2%})\n")

        print(f"报告已保存到: {output_file}")

# 使用
reporter = PerformanceReporter(results)
reporter.generate_comparison_chart()
reporter.generate_markdown_report()
```

---

## 工具5：pymilvus-benchmark（官方工具）

### 5.1 安装和使用

```bash
# 安装
pip install pymilvus-benchmark

# 运行基准测试
pymilvus-benchmark --host localhost --port 19530 \
  --collection my_collection \
  --index-type HNSW \
  --metric-type L2 \
  --dim 768 \
  --nq 100
```

**优点：**
- ✅ 官方维护，稳定可靠
- ✅ 开箱即用，无需编写代码
- ✅ 支持多种索引类型

**缺点：**
- ❌ 灵活性较低，难以定制
- ❌ 功能相对简单

---

## 工具选择指南

| 场景 | 推荐工具 | 原因 |
|------|----------|------|
| 快速测试 | pymilvus + Python脚本 | 简单快速，灵活定制 |
| 压力测试 | Locust | 支持高并发，实时监控 |
| 生产监控 | Prometheus + Grafana | 持续监控，可视化 |
| 对比分析 | 自定义分析工具 | 生成报告和图表 |
| 标准测试 | pymilvus-benchmark | 官方工具，标准化 |

---

## 完整测试工作流

```python
# 1. 使用 pymilvus 进行基础测试
benchmark = MilvusBenchmark()
results = benchmark.compare_configs("my_collection", configs)

# 2. 使用 Locust 进行压力测试
# locust -f locustfile.py --users 100 --spawn-rate 10

# 3. 使用 Prometheus 监控资源使用
# 查看 Grafana Dashboard

# 4. 生成分析报告
reporter = PerformanceReporter(results)
reporter.generate_comparison_chart()
reporter.generate_markdown_report()
```

---

## 关键洞察

1. **不同工具适合不同场景**
   - 基础测试 → pymilvus + Python
   - 压力测试 → Locust
   - 生产监控 → Prometheus + Grafana

2. **工具组合使用效果最好**
   - pymilvus 测试基础性能
   - Locust 测试极限性能
   - Prometheus 监控资源使用
   - 自定义工具生成报告

3. **自动化是关键**
   - 编写可重复执行的脚本
   - 集成到 CI/CD 流程
   - 定期执行性能测试

4. **可视化帮助理解**
   - 图表比数字更直观
   - 趋势比单点更重要
   - 对比帮助决策

---

## 下一步

理解了性能测试工具后，接下来学习：
- **核心概念3：性能指标分析** - 深入理解各种性能指标
- **实战代码** - 使用这些工具进行实际测试
