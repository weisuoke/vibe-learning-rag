# 实战代码 04：成本优化实践

> Token 追踪、批处理、缓存的完整实现

---

## 概述

本文提供完整的成本优化代码示例，包括 Token 追踪、批处理优化、语义缓存等。

---

## 1. Token 使用追踪

```python
"""
Token 使用追踪工具
"""
from langchain_core.callbacks import BaseCallbackHandler
from typing import Dict
from dotenv import load_dotenv

load_dotenv()

class TokenTracker(BaseCallbackHandler):
    """Token 使用追踪回调"""

    def __init__(self):
        self.total_tokens = 0
        self.total_cost = 0.0
        self.request_count = 0
        self.pricing = {
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "gpt-4o": {"input": 0.005, "output": 0.015},
            "gpt-4": {"input": 0.03, "output": 0.06}
        }

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.request_count += 1

    def on_llm_end(self, response, **kwargs):
        if hasattr(response, 'llm_output') and response.llm_output:
            usage = response.llm_output.get('token_usage', {})
            model = response.llm_output.get('model_name', 'gpt-4o-mini')

            input_tokens = usage.get('prompt_tokens', 0)
            output_tokens = usage.get('completion_tokens', 0)
            total = input_tokens + output_tokens

            self.total_tokens += total

            if model in self.pricing:
                cost = (
                    input_tokens * self.pricing[model]["input"] / 1000 +
                    output_tokens * self.pricing[model]["output"] / 1000
                )
                self.total_cost += cost

    def get_report(self) -> Dict:
        """获取成本报告"""
        return {
            "total_tokens": self.total_tokens,
            "total_cost": self.total_cost,
            "request_count": self.request_count,
            "avg_tokens_per_request": self.total_tokens / max(1, self.request_count),
            "avg_cost_per_request": self.total_cost / max(1, self.request_count)
        }

    def print_report(self):
        """打印成本报告"""
        report = self.get_report()
        print("=" * 60)
        print("成本追踪报告")
        print("=" * 60)
        print(f"总 Token 数: {report['total_tokens']:,}")
        print(f"总成本: ${report['total_cost']:.4f}")
        print(f"请求次数: {report['request_count']}")
        print(f"平均 Token/请求: {report['avg_tokens_per_request']:.1f}")
        print(f"平均成本/请求: ${report['avg_cost_per_request']:.4f}")
        print("=" * 60)

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")
    chain = prompt | llm | StrOutputParser()

    tracker = TokenTracker()

    # 执行多次请求
    for i in range(10):
        result = chain.invoke(
            {"text": f"测试文本{i}"},
            config={"callbacks": [tracker]}
        )

    # 打印报告
    tracker.print_report()
```

---

## 2. 批处理优化

```python
"""
批处理优化示例
"""
import asyncio
from langchain_core.runnables import Runnable
from typing import List
from dotenv import load_dotenv

load_dotenv()

async def batch_process_optimized(
    chain: Runnable,
    inputs: List[dict],
    batch_size: int = 10,
    max_concurrency: int = 5
) -> List:
    """
    优化的批处理

    Args:
        chain: LCEL 链
        inputs: 输入列表
        batch_size: 批次大小
        max_concurrency: 最大并发数

    Returns:
        结果列表
    """
    results = []

    # 分批处理
    for i in range(0, len(inputs), batch_size):
        batch = inputs[i:i + batch_size]

        # 使用 abatch 并发处理
        batch_results = await chain.abatch(
            batch,
            config={"max_concurrency": max_concurrency}
        )

        results.extend(batch_results)

        print(f"已处理: {len(results)}/{len(inputs)}")

    return results

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")
    chain = prompt | llm | StrOutputParser()

    # 准备大量输入
    inputs = [{"text": f"测试文本{i}"} for i in range(100)]

    # 批处理
    results = asyncio.run(batch_process_optimized(
        chain,
        inputs,
        batch_size=10,
        max_concurrency=5
    ))

    print(f"\n完成！处理了 {len(results)} 个请求")
```

---

## 3. 语义缓存

```python
"""
语义缓存实现
"""
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain_core.runnables import Runnable
import time
from dotenv import load_dotenv

load_dotenv()

def test_caching(chain: Runnable, test_inputs: list):
    """测试缓存效果"""

    print("=" * 60)
    print("缓存测试")
    print("=" * 60)

    # 启用缓存
    set_llm_cache(InMemoryCache())

    # 第一次调用（无缓存）
    print("\n第一次调用（无缓存）:")
    start = time.time()
    result1 = chain.invoke(test_inputs[0])
    time1 = time.time() - start
    print(f"耗时: {time1:.3f}s")
    print(f"结果: {result1[:50]}...")

    # 第二次调用（缓存命中）
    print("\n第二次调用（缓存命中）:")
    start = time.time()
    result2 = chain.invoke(test_inputs[0])
    time2 = time.time() - start
    print(f"耗时: {time2:.3f}s")
    print(f"结果: {result2[:50]}...")

    # 计算加速比
    speedup = time1 / time2
    print(f"\n加速比: {speedup:.1f}x")
    print(f"成本节省: {(1 - 1/speedup) * 100:.1f}%")

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")
    chain = prompt | llm | StrOutputParser()

    test_inputs = [{"text": "你好，世界！"}]
    test_caching(chain, test_inputs)
```

---

## 4. 动态模型选择

```python
"""
动态模型选择：根据任务复杂度选择模型
"""
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch
from dotenv import load_dotenv

load_dotenv()

def is_simple_task(input_dict: dict) -> bool:
    """判断是否为简单任务"""
    text = input_dict.get("text", "")
    # 简单规则：少于50字的任务用便宜模型
    return len(text) < 50

# 便宜模型（简单任务）
cheap_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
cheap_prompt = ChatPromptTemplate.from_template("翻译：{text}")
cheap_chain = cheap_prompt | cheap_llm | StrOutputParser()

# 强大模型（复杂任务）
powerful_llm = ChatOpenAI(model="gpt-4o", temperature=0)
powerful_prompt = ChatPromptTemplate.from_template("翻译：{text}")
powerful_chain = powerful_prompt | powerful_llm | StrOutputParser()

# 动态选择
smart_chain = RunnableBranch(
    (is_simple_task, cheap_chain),
    powerful_chain  # default
)

# 使用示例
if __name__ == "__main__":
    # 简单任务（使用 gpt-4o-mini）
    result1 = smart_chain.invoke({"text": "你好"})
    print(f"简单任务: {result1}")

    # 复杂任务（使用 gpt-4o）
    long_text = "这是一段很长的文本..." * 20
    result2 = smart_chain.invoke({"text": long_text})
    print(f"复杂任务: {result2[:50]}...")
```

---

## 5. 完整成本优化工具

```python
"""
完整成本优化工具
"""
import time
import asyncio
from typing import List, Dict
from langchain_core.runnables import Runnable
from langchain_core.callbacks import BaseCallbackHandler
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from dotenv import load_dotenv

load_dotenv()

class CostOptimizer:
    """成本优化工具"""

    def __init__(self, chain: Runnable):
        self.chain = chain
        self.tracker = TokenTracker()

    def enable_caching(self):
        """启用缓存"""
        set_llm_cache(InMemoryCache())
        print("✓ 缓存已启用")

    async def batch_process(
        self,
        inputs: List[dict],
        batch_size: int = 10,
        max_concurrency: int = 5
    ) -> List:
        """批处理"""
        results = []

        for i in range(0, len(inputs), batch_size):
            batch = inputs[i:i + batch_size]

            batch_results = await self.chain.abatch(
                batch,
                config={
                    "max_concurrency": max_concurrency,
                    "callbacks": [self.tracker]
                }
            )

            results.extend(batch_results)

        return results

    def get_cost_report(self) -> Dict:
        """获取成本报告"""
        return self.tracker.get_report()

    def print_cost_report(self):
        """打印成本报告"""
        self.tracker.print_report()

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")
    chain = prompt | llm | StrOutputParser()

    optimizer = CostOptimizer(chain)

    # 启用缓存
    optimizer.enable_caching()

    # 准备输入
    inputs = [{"text": f"测试文本{i}"} for i in range(50)]

    # 批处理
    print("\n开始批处理...")
    results = asyncio.run(optimizer.batch_process(
        inputs,
        batch_size=10,
        max_concurrency=5
    ))

    print(f"\n完成！处理了 {len(results)} 个请求")

    # 打印成本报告
    optimizer.print_cost_report()
```

---

## 6. langasync 批处理（2026新工具）

```python
"""
langasync 批处理示例（降低50%成本）
"""
# 注意：langasync 是 2026 年社区工具，需要单独安装
# pip install langasync

# from langasync import wrap_chain
# import asyncio
# from dotenv import load_dotenv

# load_dotenv()

# # 原始 LCEL 链
# from langchain_openai import ChatOpenAI
# from langchain.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser

# llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
# prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")
# chain = prompt | llm | StrOutputParser()

# # 包装为批处理模式
# async_chain = wrap_chain(chain, batch_size=10)

# # 批量执行（成本降低 50%）
# inputs = [{"text": f"测试{i}"} for i in range(100)]
# results = asyncio.run(async_chain.abatch(inputs))

# print(f"完成！处理了 {len(results)} 个请求")
# print("成本降低约 50%（使用批处理 API）")

# 注意：langasync 适用于非实时场景
# - 批量评估和测试
# - 数据标注任务
# - 离线分析和报告生成
```

---

## 总结

### 成本优化策略总结

| 策略 | 成本降低 | 适用场景 |
|------|----------|----------|
| Token 追踪 | 0%（监控） | 所有场景 |
| 批处理 | 10-20% | 大量请求 |
| 语义缓存 | 50-90% | 重复查询 |
| 动态模型选择 | 30-50% | 混合任务 |
| langasync | 50% | 非实时场景 |

### 关键要点

1. **Token 追踪**：监控成本，识别优化机会
2. **批处理**：提升吞吐量，降低成本
3. **缓存**：避免重复调用，大幅降低成本
4. **动态选择**：根据任务复杂度选择模型

---

**下一步**：阅读 `07_实战代码_05_生产部署模式.md` 学习生产部署
