# 30字核心

**Transformer 是一种通过注意力机制让模型同时"看到"所有输入的架构，是现代大语言模型的基础。**

---

## 拆解理解

| 关键词 | 含义 |
|-------|------|
| **注意力机制** | 模型能决定关注输入的哪些部分 |
| **同时"看到"** | 并行处理，不是按顺序一个个看 |
| **所有输入** | 每个词都能直接关注其他任何词 |
| **现代大语言模型的基础** | GPT、Claude、Llama 都基于此 |

---

## 为什么这句话重要？

理解这句话，你就理解了：

1. **为什么 LLM 能理解上下文** → 注意力机制让它能"看到"所有相关信息
2. **为什么 RAG 能工作** → 注入的上下文会被模型"看到"并关注
3. **为什么 Prompt 设计很重要** → 你给的信息会影响模型关注什么

---

**下一步：** [02_第一性原理](./02_第一性原理.md) - 从根本理解为什么需要 Transformer
