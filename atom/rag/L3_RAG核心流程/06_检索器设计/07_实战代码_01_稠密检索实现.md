# 实战代码1：稠密检索实现

> 完整的Dense Retrieval实现，包含Embedding生成、向量存储和检索

---

## 代码概述

本示例展示如何实现一个完整的稠密检索系统，包括：
1. 使用OpenAI Embedding生成向量
2. 使用ChromaDB存储向量
3. 实现相似度检索
4. 性能优化和缓存策略

---

## 完整代码

```python
"""
稠密检索完整实现
演示：从文档加载到检索的完整流程
"""

import os
from typing import List, Dict, Optional
from dotenv import load_dotenv
from openai import OpenAI
import chromadb
from chromadb.config import Settings
import numpy as np
import time
import hashlib
import json

# 加载环境变量
load_dotenv()

# ===== 1. Embedding生成器 =====
class EmbeddingGenerator:
    """OpenAI Embedding生成器（带缓存）"""

    def __init__(self, model: str = "text-embedding-3-small", cache_dir: str = "./embedding_cache"):
        self.client = OpenAI()
        self.model = model
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def _get_cache_key(self, text: str) -> str:
        """生成缓存键"""
        content = f"{self.model}:{text}"
        return hashlib.md5(content.encode()).hexdigest()

    def _load_from_cache(self, cache_key: str) -> Optional[List[float]]:
        """从缓存加载"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        if os.path.exists(cache_file):
            with open(cache_file, 'r') as f:
                return json.load(f)
        return None

    def _save_to_cache(self, cache_key: str, embedding: List[float]):
        """保存到缓存"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")
        with open(cache_file, 'w') as f:
            json.dump(embedding, f)

    def generate(self, text: str) -> List[float]:
        """
        生成单个文本的embedding

        Args:
            text: 输入文本

        Returns:
            embedding向量
        """
        # 检查缓存
        cache_key = self._get_cache_key(text)
        cached = self._load_from_cache(cache_key)
        if cached is not None:
            print(f"✓ 从缓存加载: {text[:50]}...")
            return cached

        # 生成embedding
        print(f"→ 生成embedding: {text[:50]}...")
        response = self.client.embeddings.create(
            model=self.model,
            input=text
        )
        embedding = response.data[0].embedding

        # 保存到缓存
        self._save_to_cache(cache_key, embedding)

        return embedding

    def generate_batch(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:
        """
        批量生成embedding

        Args:
            texts: 文本列表
            batch_size: 批次大小

        Returns:
            embedding向量列表
        """
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]

            # 检查缓存
            batch_embeddings = []
            uncached_texts = []
            uncached_indices = []

            for j, text in enumerate(batch):
                cache_key = self._get_cache_key(text)
                cached = self._load_from_cache(cache_key)
                if cached is not None:
                    batch_embeddings.append((j, cached))
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(j)

            # 生成未缓存的embedding
            if uncached_texts:
                print(f"→ 批量生成embedding: {len(uncached_texts)}个文本")
                response = self.client.embeddings.create(
                    model=self.model,
                    input=uncached_texts
                )

                for idx, item in zip(uncached_indices, response.data):
                    embedding = item.embedding
                    # 保存到缓存
                    cache_key = self._get_cache_key(uncached_texts[uncached_indices.index(idx)])
                    self._save_to_cache(cache_key, embedding)
                    batch_embeddings.append((idx, embedding))

            # 按原始顺序排序
            batch_embeddings.sort(key=lambda x: x[0])
            embeddings.extend([emb for _, emb in batch_embeddings])

        return embeddings


# ===== 2. 向量存储 =====
class VectorStore:
    """ChromaDB向量存储"""

    def __init__(self, collection_name: str = "documents", persist_directory: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )

        # 创建或获取集合
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={
                "hnsw:space": "cosine",           # 余弦相似度
                "hnsw:construction_ef": 200,      # 构建参数
                "hnsw:M": 16,                     # 连接数
                "hnsw:search_ef": 100             # 查询参数
            }
        )

    def add_documents(self, documents: List[str], embeddings: List[List[float]], metadatas: Optional[List[Dict]] = None):
        """
        添加文档到向量存储

        Args:
            documents: 文档文本列表
            embeddings: embedding向量列表
            metadatas: 元数据列表（可选）
        """
        # 生成ID
        ids = [f"doc_{i}" for i in range(len(documents))]

        # 添加到集合
        self.collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas or [{} for _ in documents],
            ids=ids
        )

        print(f"✓ 添加{len(documents)}个文档到向量存储")

    def search(self, query_embedding: List[float], k: int = 5) -> Dict:
        """
        检索相似文档

        Args:
            query_embedding: 查询向量
            k: 返回结果数量

        Returns:
            检索结果
        """
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )

        return {
            "documents": results["documents"][0],
            "distances": results["distances"][0],
            "metadatas": results["metadatas"][0],
            "ids": results["ids"][0]
        }

    def get_stats(self) -> Dict:
        """获取统计信息"""
        count = self.collection.count()
        return {
            "total_documents": count,
            "collection_name": self.collection.name
        }


# ===== 3. 稠密检索器 =====
class DenseRetriever:
    """完整的稠密检索器"""

    def __init__(self, embedding_model: str = "text-embedding-3-small"):
        self.embedding_generator = EmbeddingGenerator(model=embedding_model)
        self.vector_store = VectorStore()

    def index_documents(self, documents: List[str], metadatas: Optional[List[Dict]] = None):
        """
        索引文档

        Args:
            documents: 文档列表
            metadatas: 元数据列表
        """
        print(f"\n=== 索引{len(documents)}个文档 ===")

        # 生成embeddings
        start = time.time()
        embeddings = self.embedding_generator.generate_batch(documents)
        embedding_time = time.time() - start

        print(f"✓ Embedding生成耗时: {embedding_time:.2f}s")

        # 添加到向量存储
        start = time.time()
        self.vector_store.add_documents(documents, embeddings, metadatas)
        index_time = time.time() - start

        print(f"✓ 索引构建耗时: {index_time:.2f}s")
        print(f"✓ 总耗时: {embedding_time + index_time:.2f}s")

    def search(self, query: str, k: int = 5) -> List[Dict]:
        """
        检索相似文档

        Args:
            query: 查询文本
            k: 返回结果数量

        Returns:
            检索结果列表
        """
        print(f"\n=== 检索: {query} ===")

        # 生成查询embedding
        start = time.time()
        query_embedding = self.embedding_generator.generate(query)
        embedding_time = time.time() - start

        # 检索
        start = time.time()
        results = self.vector_store.search(query_embedding, k=k)
        search_time = time.time() - start

        print(f"✓ 查询embedding耗时: {embedding_time:.3f}s")
        print(f"✓ 检索耗时: {search_time:.3f}s")

        # 格式化结果
        formatted_results = []
        for i in range(len(results["documents"])):
            formatted_results.append({
                "rank": i + 1,
                "document": results["documents"][i],
                "similarity": 1 - results["distances"][i],  # 转换为相似度
                "metadata": results["metadatas"][i],
                "id": results["ids"][i]
            })

        return formatted_results

    def get_stats(self) -> Dict:
        """获取统计信息"""
        return self.vector_store.get_stats()


# ===== 4. 使用示例 =====
def main():
    """主函数"""

    # 创建检索器
    retriever = DenseRetriever(embedding_model="text-embedding-3-small")

    # 准备文档
    documents = [
        "Python是一种高级编程语言，由Guido van Rossum于1991年创建。",
        "JavaScript是一种用于Web开发的编程语言，主要用于前端开发。",
        "机器学习是人工智能的一个子领域，专注于让计算机从数据中学习。",
        "深度学习使用神经网络进行特征学习，是机器学习的一个分支。",
        "RAG系统结合检索和生成技术，用于构建智能问答系统。",
        "FastAPI是一个现代、快速的Python Web框架，用于构建API。",
        "Docker是一个容器化平台，用于打包和部署应用程序。",
        "Kubernetes是一个容器编排系统，用于管理容器化应用。",
        "PostgreSQL是一个强大的开源关系型数据库系统。",
        "Redis是一个内存数据库，常用于缓存和消息队列。"
    ]

    # 元数据
    metadatas = [
        {"category": "编程语言", "topic": "Python"},
        {"category": "编程语言", "topic": "JavaScript"},
        {"category": "AI", "topic": "机器学习"},
        {"category": "AI", "topic": "深度学习"},
        {"category": "AI", "topic": "RAG"},
        {"category": "Web框架", "topic": "FastAPI"},
        {"category": "DevOps", "topic": "Docker"},
        {"category": "DevOps", "topic": "Kubernetes"},
        {"category": "数据库", "topic": "PostgreSQL"},
        {"category": "数据库", "topic": "Redis"}
    ]

    # 索引文档
    retriever.index_documents(documents, metadatas)

    # 统计信息
    stats = retriever.get_stats()
    print(f"\n=== 统计信息 ===")
    print(f"总文档数: {stats['total_documents']}")
    print(f"集合名称: {stats['collection_name']}")

    # 测试查询
    test_queries = [
        "如何学习编程",
        "人工智能技术",
        "容器化部署",
        "数据库选择"
    ]

    for query in test_queries:
        results = retriever.search(query, k=3)

        print(f"\n=== Top-3结果 ===")
        for result in results:
            print(f"\n{result['rank']}. [相似度: {result['similarity']:.3f}]")
            print(f"   文档: {result['document']}")
            print(f"   分类: {result['metadata'].get('category', 'N/A')}")
            print(f"   主题: {result['metadata'].get('topic', 'N/A')}")


# ===== 5. 性能测试 =====
def benchmark():
    """性能基准测试"""

    print("\n" + "="*50)
    print("性能基准测试")
    print("="*50)

    retriever = DenseRetriever()

    # 生成测试文档
    n_docs = 1000
    documents = [f"这是第{i}个测试文档，内容关于主题{i%10}" for i in range(n_docs)]

    # 测试索引性能
    print(f"\n测试索引{n_docs}个文档...")
    start = time.time()
    retriever.index_documents(documents)
    index_time = time.time() - start

    print(f"\n索引性能:")
    print(f"  总耗时: {index_time:.2f}s")
    print(f"  平均每文档: {index_time/n_docs*1000:.2f}ms")

    # 测试检索性能
    print(f"\n测试检索性能...")
    query = "测试查询"
    n_queries = 100

    start = time.time()
    for _ in range(n_queries):
        retriever.search(query, k=5)
    search_time = time.time() - start

    print(f"\n检索性能:")
    print(f"  总耗时: {search_time:.2f}s")
    print(f"  平均每查询: {search_time/n_queries*1000:.2f}ms")
    print(f"  QPS: {n_queries/search_time:.1f}")


if __name__ == "__main__":
    # 运行主示例
    main()

    # 运行性能测试（可选）
    # benchmark()
```

---

## 运行输出示例

```
=== 索引10个文档 ===
→ 批量生成embedding: 10个文本
✓ Embedding生成耗时: 0.45s
✓ 添加10个文档到向量存储
✓ 索引构建耗时: 0.02s
✓ 总耗时: 0.47s

=== 统计信息 ===
总文档数: 10
集合名称: documents

=== 检索: 如何学习编程 ===
✓ 查询embedding耗时: 0.123s
✓ 检索耗时: 0.003s

=== Top-3结果 ===

1. [相似度: 0.782]
   文档: Python是一种高级编程语言，由Guido van Rossum于1991年创建。
   分类: 编程语言
   主题: Python

2. [相似度: 0.745]
   文档: JavaScript是一种用于Web开发的编程语言，主要用于前端开发。
   分类: 编程语言
   主题: JavaScript

3. [相似度: 0.623]
   文档: FastAPI是一个现代、快速的Python Web框架，用于构建API。
   分类: Web框架
   主题: FastAPI
```

---

## 代码说明

### 1. EmbeddingGenerator类

**功能**：
- 生成OpenAI Embedding
- 实现缓存机制（避免重复计算）
- 支持批量生成（提升效率）

**关键方法**：
- `generate()`: 生成单个embedding
- `generate_batch()`: 批量生成embedding
- `_load_from_cache()`: 从缓存加载
- `_save_to_cache()`: 保存到缓存

### 2. VectorStore类

**功能**：
- 使用ChromaDB存储向量
- 配置HNSW索引参数
- 实现相似度检索

**关键方法**：
- `add_documents()`: 添加文档
- `search()`: 检索相似文档
- `get_stats()`: 获取统计信息

### 3. DenseRetriever类

**功能**：
- 整合Embedding生成和向量存储
- 提供简单的API接口
- 性能监控和统计

**关键方法**：
- `index_documents()`: 索引文档
- `search()`: 检索文档
- `get_stats()`: 获取统计信息

---

## 优化要点

### 1. 缓存策略

```python
# 使用MD5哈希作为缓存键
cache_key = hashlib.md5(f"{model}:{text}".encode()).hexdigest()

# 缓存命中率监控
cache_hits = 0
total_requests = 0
cache_hit_rate = cache_hits / total_requests
```

### 2. 批量处理

```python
# 批量生成embedding（最多2048个文本）
batch_size = 100
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    response = client.embeddings.create(model=model, input=batch)
```

### 3. HNSW参数配置

```python
# 生产级配置
metadata = {
    "hnsw:space": "cosine",           # 余弦相似度
    "hnsw:construction_ef": 200,      # 构建质量
    "hnsw:M": 16,                     # 连接数
    "hnsw:search_ef": 100             # 查询召回
}
```

---

## 扩展功能

### 1. 增量索引

```python
def add_new_documents(self, new_documents: List[str]):
    """增量添加新文档"""
    # 生成新文档的embedding
    new_embeddings = self.embedding_generator.generate_batch(new_documents)

    # 添加到现有集合
    self.vector_store.add_documents(new_documents, new_embeddings)
```

### 2. 文档更新

```python
def update_document(self, doc_id: str, new_content: str):
    """更新文档"""
    # 删除旧文档
    self.vector_store.collection.delete(ids=[doc_id])

    # 添加新文档
    new_embedding = self.embedding_generator.generate(new_content)
    self.vector_store.collection.add(
        documents=[new_content],
        embeddings=[new_embedding],
        ids=[doc_id]
    )
```

### 3. 过滤检索

```python
def search_with_filter(self, query: str, category: str, k: int = 5):
    """带过滤条件的检索"""
    query_embedding = self.embedding_generator.generate(query)

    results = self.vector_store.collection.query(
        query_embeddings=[query_embedding],
        n_results=k,
        where={"category": category}  # 过滤条件
    )

    return results
```

---

## 常见问题

### Q1: 如何选择Embedding模型？

**A**:
- 通用场景：`text-embedding-3-small`（性价比最优）
- 高精度需求：`text-embedding-3-large`
- 本地部署：`sentence-transformers/all-mpnet-base-v2`

### Q2: 缓存占用空间过大怎么办？

**A**:
```python
# 定期清理缓存
import shutil
shutil.rmtree(cache_dir)
os.makedirs(cache_dir)

# 或使用LRU缓存
from functools import lru_cache
@lru_cache(maxsize=1000)
def generate_cached(text):
    return generate(text)
```

### Q3: 如何提升检索速度？

**A**:
1. 增加`hnsw:search_ef`参数
2. 使用批量检索
3. 预热索引（提前加载到内存）
4. 使用更快的Embedding模型

---

## 总结

本示例展示了完整的稠密检索实现，包括：
- ✅ Embedding生成（带缓存）
- ✅ 向量存储（ChromaDB + HNSW）
- ✅ 相似度检索
- ✅ 性能优化
- ✅ 可扩展架构

**下一步**：学习【实战代码2：稀疏检索BM25】，实现基于词法匹配的检索。
