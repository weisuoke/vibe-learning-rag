# 核心概念 04：可配置字段

> **使用 configurable_fields() 和 configurable_alternatives() 实现运行时动态参数调整**

---

## 概述

可配置字段允许在不重建链的情况下，运行时动态调整参数（如 temperature、model_name）。这是 LangChain 实现 A/B 测试、成本优化、降级策略的核心机制。

**引用**：
> "Configurable fields enable runtime parameter adjustment without rebuilding chains, supporting dynamic model switching and A/B testing."
> — [LangChain Configurable Runnables](https://python.langchain.com/docs/how_to/configure/)

---

## configurable_fields() 方法

### 基本用法

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField

# 1. 声明可配置字段
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
).configurable_fields(
    model=ConfigurableField(
        id="model_name",
        name="Model Name",
        description="要使用的模型名称"
    ),
    temperature=ConfigurableField(
        id="temp",
        name="Temperature",
        description="控制输出随机性 (0-2)"
    )
)

# 2. 运行时动态调整
config = RunnableConfig(
    configurable={
        "model_name": "gpt-4",
        "temp": 0.3
    }
)

result = llm.invoke("你好", config=config)
```

### ConfigurableField 参数

```python
ConfigurableField(
    id: str,              # 唯一标识符（必需）
    name: str = None,     # 可读名称
    description: str = None,  # 字段描述
    annotation: Type = None   # 类型注解
)
```

---

## 应用场景

### 场景 1：动态模型切换

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig

# 声明可配置模型
llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_fields(
    model=ConfigurableField(id="model")
)

# 简单查询用 GPT-3.5（便宜）
simple_config = RunnableConfig(
    configurable={"model": "gpt-3.5-turbo"}
)
llm.invoke("1+1=?", config=simple_config)

# 复杂查询用 GPT-4（质量高）
complex_config = RunnableConfig(
    configurable={"model": "gpt-4"}
)
llm.invoke("解释量子纠缠", config=complex_config)
```

### 场景 2：A/B 测试

```python
def ab_test(query: str):
    """对比不同 temperature 的效果"""
    llm = ChatOpenAI(temperature=0.7).configurable_fields(
        temperature=ConfigurableField(id="temp")
    )

    configs = [
        RunnableConfig(
            configurable={"temp": 0.3},
            tags=["variant_A"],
            metadata={"variant": "A"}
        ),
        RunnableConfig(
            configurable={"temp": 0.9},
            tags=["variant_B"],
            metadata={"variant": "B"}
        )
    ]

    results = []
    for config in configs:
        result = llm.invoke(query, config=config)
        results.append({
            "variant": config["metadata"]["variant"],
            "output": result.content
        })

    return results

# 使用
results = ab_test("写一首诗")
for r in results:
    print(f"变体 {r['variant']}: {r['output'][:50]}...")
```

### 场景 3：成本优化

```python
class CostOptimizer:
    """根据查询复杂度选择模型"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_fields(
            model=ConfigurableField(id="model")
        )

    def invoke(self, query: str):
        # 简单启发式：根据查询长度选择模型
        if len(query) < 50:
            model = "gpt-3.5-turbo"  # $0.0005/1K tokens
        else:
            model = "gpt-4"  # $0.03/1K tokens

        config = RunnableConfig(
            configurable={"model": model},
            metadata={"selected_model": model}
        )

        return self.llm.invoke(query, config=config)

# 使用
optimizer = CostOptimizer()
optimizer.invoke("你好")  # 使用 GPT-3.5
optimizer.invoke("请详细解释量子力学的基本原理..." * 10)  # 使用 GPT-4
```

### 场景 4：降级策略

```python
class FallbackChain:
    """主模型失败时自动降级"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4").configurable_fields(
            model=ConfigurableField(id="model")
        )

    def invoke_with_fallback(self, query: str):
        models = ["gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-16k"]

        for model in models:
            try:
                config = RunnableConfig(
                    configurable={"model": model}
                )
                result = self.llm.invoke(query, config=config)
                logger.info(f"成功使用模型: {model}")
                return result
            except Exception as e:
                logger.warning(f"模型 {model} 失败: {e}")
                continue

        raise Exception("所有模型都失败了")

# 使用
chain = FallbackChain()
result = chain.invoke_with_fallback("你好")
```

---

## configurable_alternatives() 方法

### 基本用法

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import ConfigurableField

# 定义多个模型选项
llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_alternatives(
    ConfigurableField(id="model"),
    default_key="gpt35",
    gpt4=ChatOpenAI(model="gpt-4"),
    claude=ChatAnthropic(model="claude-3-opus-20240229"),
    claude_sonnet=ChatAnthropic(model="claude-3-sonnet-20240229")
)

# 运行时切换
config_gpt4 = RunnableConfig(configurable={"model": "gpt4"})
config_claude = RunnableConfig(configurable={"model": "claude"})

llm.invoke("你好", config=config_gpt4)  # 使用 GPT-4
llm.invoke("你好", config=config_claude)  # 使用 Claude
```

### 应用场景：多模型对比

```python
def compare_models(query: str):
    """对比不同模型的输出"""
    llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_alternatives(
        ConfigurableField(id="model"),
        default_key="gpt35",
        gpt4=ChatOpenAI(model="gpt-4"),
        claude=ChatAnthropic(model="claude-3-opus-20240229")
    )

    models = ["gpt35", "gpt4", "claude"]
    results = []

    for model_key in models:
        config = RunnableConfig(
            configurable={"model": model_key},
            metadata={"model": model_key}
        )
        result = llm.invoke(query, config=config)
        results.append({
            "model": model_key,
            "output": result.content
        })

    return results

# 使用
results = compare_models("解释量子纠缠")
for r in results:
    print(f"{r['model']}: {r['output'][:100]}...")
```

---

## 配置优先级

### 优先级规则

```
invoke 时配置 > with_config 配置 > 构造时配置
```

### 示例

```python
# 1. 构造时配置（优先级 3）
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
).configurable_fields(
    model=ConfigurableField(id="model"),
    temperature=ConfigurableField(id="temp")
)

# 2. with_config 配置（优先级 2）
llm_with_config = llm.with_config(
    configurable={"model": "gpt-4", "temp": 0.5}
)

# 3. invoke 时配置（优先级 1，最高）
result = llm_with_config.invoke(
    "你好",
    config=RunnableConfig(
        configurable={"temp": 0.3}  # 只覆盖 temp
    )
)

# 最终配置：
# model: "gpt-4"（来自 with_config）
# temp: 0.3（来自 invoke，优先级最高）
```

---

## 完整生产示例

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import ConfigurableField, RunnableConfig
from langchain_core.callbacks import BaseCallbackHandler
import logging

logger = logging.getLogger(__name__)

class ModelSelector:
    """智能模型选择器"""

    def __init__(self):
        # 配置多个模型选项
        self.llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_alternatives(
            ConfigurableField(id="model"),
            default_key="gpt35",
            gpt4=ChatOpenAI(model="gpt-4"),
            claude_opus=ChatAnthropic(model="claude-3-opus-20240229"),
            claude_sonnet=ChatAnthropic(model="claude-3-sonnet-20240229")
        ).configurable_fields(
            temperature=ConfigurableField(id="temp")
        )

        # 模型价格表（$/1K tokens）
        self.prices = {
            "gpt35": {"input": 0.0005, "output": 0.0015},
            "gpt4": {"input": 0.03, "output": 0.06},
            "claude_opus": {"input": 0.015, "output": 0.075},
            "claude_sonnet": {"input": 0.003, "output": 0.015}
        }

    def select_model(self, query: str, user_tier: str = "free"):
        """根据用户等级和查询复杂度选择模型"""
        query_length = len(query)

        if user_tier == "premium":
            # 付费用户：根据查询复杂度选择
            if query_length > 200:
                return "gpt4", 0.3
            else:
                return "claude_sonnet", 0.5
        else:
            # 免费用户：只用便宜模型
            if query_length > 200:
                return "claude_sonnet", 0.5
            else:
                return "gpt35", 0.7

    def invoke(self, query: str, user_id: str, user_tier: str = "free"):
        """执行查询"""
        # 选择模型
        model, temp = self.select_model(query, user_tier)

        # 创建配置
        config = RunnableConfig(
            configurable={
                "model": model,
                "temp": temp
            },
            metadata={
                "user_id": user_id,
                "user_tier": user_tier,
                "selected_model": model,
                "selected_temp": temp
            },
            tags=["production", user_tier]
        )

        # 执行
        logger.info(f"用户 {user_id} 使用模型 {model}")
        result = self.llm.invoke(query, config=config)

        return result

# 使用
selector = ModelSelector()

# 免费用户
result1 = selector.invoke("你好", user_id="user_001", user_tier="free")

# 付费用户
result2 = selector.invoke(
    "请详细解释量子力学的基本原理...",
    user_id="user_002",
    user_tier="premium"
)
```

---

## 最佳实践

### 1. 字段命名规范

```python
# ✅ 好的命名
llm.configurable_fields(
    model=ConfigurableField(id="model_name"),
    temperature=ConfigurableField(id="temp"),
    max_tokens=ConfigurableField(id="max_tokens")
)

# ❌ 不好的命名
llm.configurable_fields(
    model=ConfigurableField(id="m"),  # 太短
    temperature=ConfigurableField(id="temperature_parameter_for_llm")  # 太长
)
```

### 2. 类型安全

```python
from typing import Literal

# 使用类型注解限制可选值
llm.configurable_fields(
    model=ConfigurableField(
        id="model",
        annotation=Literal["gpt-3.5-turbo", "gpt-4"]
    )
)
```

### 3. 默认值设置

```python
# 构造时设置合理的默认值
llm = ChatOpenAI(
    model="gpt-3.5-turbo",  # 默认使用便宜模型
    temperature=0.7         # 默认中等随机性
).configurable_fields(
    model=ConfigurableField(id="model"),
    temperature=ConfigurableField(id="temp")
)
```

### 4. 配置验证

```python
def validate_config(config: dict):
    """验证配置有效性"""
    if "temp" in config:
        temp = config["temp"]
        if not 0 <= temp <= 2:
            raise ValueError(f"temperature 必须在 0-2 之间，当前值: {temp}")

    if "model" in config:
        valid_models = ["gpt-3.5-turbo", "gpt-4"]
        if config["model"] not in valid_models:
            raise ValueError(f"无效的模型: {config['model']}")

# 使用
config = {"model": "gpt-4", "temp": 0.5}
validate_config(config)
llm.invoke("你好", config=RunnableConfig(configurable=config))
```

---

## 总结

### configurable_fields vs configurable_alternatives

| 特性 | configurable_fields | configurable_alternatives |
|------|---------------------|---------------------------|
| **用途** | 调整参数值 | 切换整个组件 |
| **示例** | temperature: 0.3 → 0.9 | gpt-3.5 → gpt-4 |
| **灵活性** | 高（任意值） | 中（预定义选项） |
| **类型安全** | 需要手动验证 | 编译时检查 |

### 应用场景速查

| 场景 | 方法 | 示例 |
|------|------|------|
| **动态调整参数** | `configurable_fields` | temperature, max_tokens |
| **模型切换** | `configurable_alternatives` | GPT-3.5 ↔ GPT-4 |
| **A/B 测试** | `configurable_fields` | 对比不同参数效果 |
| **成本优化** | `configurable_alternatives` | 根据复杂度选模型 |
| **降级策略** | `configurable_alternatives` | 主模型失败时切换 |

---

## 参考资料

- [LangChain Configurable Runnables](https://python.langchain.com/docs/how_to/configure/)
- [ConfigurableField API](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.config.ConfigurableField.html)
