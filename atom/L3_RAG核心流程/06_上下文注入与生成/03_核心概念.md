# 核心概念

掌握三个核心概念，覆盖 RAG 生成阶段 90% 的优化空间。

---

## 核心概念1：Prompt 模板设计

**Prompt 模板是将 System 指令、检索上下文、用户问题组装成完整提示词的结构化方式。**

### 标准 RAG Prompt 结构

```
┌─────────────────────────────────────────────────────────────┐
│                    RAG Prompt 三层结构                       │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  System Prompt（系统指令）                            │   │
│  │  - 角色定义："你是一个专业的客服助手"                  │   │
│  │  - 行为约束："只基于提供的资料回答"                    │   │
│  │  - 输出格式："用 Markdown 格式回答"                   │   │
│  └─────────────────────────────────────────────────────┘   │
│                          ↓                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  Context（检索上下文）                                │   │
│  │  - 检索到的 Top-K 文档块                              │   │
│  │  - 格式化后的参考资料                                 │   │
│  │  - 可选：来源标注、相关度分数                         │   │
│  └─────────────────────────────────────────────────────┘   │
│                          ↓                                  │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  User Query（用户问题）                               │   │
│  │  - 用户的原始问题                                     │   │
│  │  - 可选：问题改写后的版本                             │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 代码示例：基础 Prompt 模板

```python
from langchain_core.prompts import ChatPromptTemplate

# 定义 RAG Prompt 模板
RAG_PROMPT_TEMPLATE = """你是一个专业的知识库助手。请基于以下参考资料回答用户问题。

【行为准则】
1. 只基于提供的参考资料回答问题
2. 如果资料中没有相关信息，请如实告知"根据现有资料无法回答"
3. 回答时引用具体来源（如"根据文档A..."）
4. 保持回答简洁、准确

【参考资料】
{context}

【用户问题】
{question}

【回答】"""

# 创建 LangChain Prompt 模板
prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

# 使用模板
formatted_prompt = prompt.format(
    context="根据《员工手册》第3章：入职满1年享有5天年假...",
    question="公司的年假政策是什么？"
)
```

### System Prompt 设计要点

```python
# 好的 System Prompt 包含四个要素
SYSTEM_PROMPT = """
【角色定义】
你是{company_name}的智能客服助手，专门回答关于{domain}的问题。

【行为约束】
1. 只基于【参考资料】中的内容回答
2. 不要编造或推测资料中没有的信息
3. 如果不确定，明确说明不确定
4. 如果资料不足以回答，建议用户咨询人工客服

【输出格式】
- 使用简洁的中文回答
- 重要信息用**加粗**标注
- 如有多个要点，使用编号列表

【引用规范】
回答时标注信息来源，格式：[来源：文档名称]
"""
```

### Context 格式化策略

```python
def format_context(documents: list, include_metadata: bool = True) -> str:
    """
    将检索到的文档格式化为上下文字符串

    Args:
        documents: 检索到的文档列表
        include_metadata: 是否包含元数据（来源、页码等）
    """
    formatted_docs = []

    for i, doc in enumerate(documents, 1):
        if include_metadata:
            source = doc.metadata.get("source", "未知来源")
            page = doc.metadata.get("page", "")
            header = f"【文档{i}】来源：{source}"
            if page:
                header += f"，第{page}页"
        else:
            header = f"【文档{i}】"

        formatted_docs.append(f"{header}\n{doc.page_content}")

    return "\n\n---\n\n".join(formatted_docs)


# 使用示例
context = format_context(retrieved_docs)
# 输出：
# 【文档1】来源：员工手册.pdf，第15页
# 入职满1年的员工享有5天带薪年假...
#
# ---
#
# 【文档2】来源：HR政策.docx，第3页
# 年假申请需提前3个工作日提交...
```

### 在 RAG 开发中的应用

| 场景 | System Prompt 重点 | Context 格式 |
|------|-------------------|--------------|
| 客服问答 | 礼貌、准确、引导人工 | 包含来源，便于追溯 |
| 技术文档 | 专业、代码示例 | 包含代码块、版本号 |
| 法律咨询 | 严谨、免责声明 | 包含法条编号、生效日期 |
| 医疗问答 | 谨慎、建议就医 | 包含参考文献、置信度 |

---

## 核心概念2：Lost in the Middle 现象

**Lost in the Middle 是指 LLM 对长上下文中间部分的注意力显著下降的现象。**

### 现象解释

```
┌─────────────────────────────────────────────────────────────┐
│                Lost in the Middle 现象                       │
│                                                             │
│  上下文位置：  开头        中间        结尾                   │
│                                                             │
│  注意力强度：  ████████   ████       ████████               │
│               (高)       (低)        (高)                   │
│                                                             │
│  信息利用率：  ~90%       ~50%       ~85%                    │
│                                                             │
│  结论：LLM 更容易"记住"开头和结尾的内容                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 实验数据

斯坦福大学 2023 年的研究表明：

```
当关键信息放在不同位置时，LLM 的准确率：

位置          | GPT-3.5 | Claude | GPT-4
--------------|---------|--------|-------
开头 (前10%)  | 85%     | 88%    | 92%
中间 (40-60%) | 52%     | 58%    | 71%
结尾 (后10%)  | 78%     | 82%    | 89%

结论：即使是最强的模型，中间位置的信息利用率也明显下降
```

### 为什么会这样？

```
1. Transformer 的注意力机制特性
   - 自注意力对位置有隐式偏好
   - 开头：建立上下文基调
   - 结尾：最近的信息，记忆更清晰

2. 训练数据的分布
   - 大多数文本的重要信息在开头（标题、摘要）
   - 结尾通常是总结或结论
   - 模型学会了"关注首尾"的模式

3. 上下文长度的影响
   - 上下文越长，中间部分的"稀释"越严重
   - 4K tokens 时影响较小
   - 32K+ tokens 时影响显著
```

### 应对策略

#### 策略1：相关性优先排序

```python
def reorder_by_relevance(documents: list, query: str) -> list:
    """
    将最相关的文档放在开头和结尾

    排序策略：
    - 最相关 → 开头（第1位）
    - 次相关 → 结尾（最后1位）
    - 其他 → 中间
    """
    # 假设 documents 已按相关性降序排列
    if len(documents) <= 2:
        return documents

    # 最相关放开头
    reordered = [documents[0]]

    # 中等相关放中间
    middle = documents[2:-1] if len(documents) > 3 else []
    reordered.extend(middle)

    # 次相关放结尾
    if len(documents) > 1:
        reordered.append(documents[1])

    # 第三相关放倒数第二
    if len(documents) > 2:
        reordered.insert(-1, documents[2])

    return reordered
```

#### 策略2：首尾强化法

```python
def format_context_with_emphasis(documents: list) -> str:
    """
    在开头和结尾添加强调标记
    """
    context_parts = []

    for i, doc in enumerate(documents):
        if i == 0:
            # 开头强调
            context_parts.append(f"【最相关】\n{doc.page_content}")
        elif i == len(documents) - 1:
            # 结尾强调
            context_parts.append(f"【重要补充】\n{doc.page_content}")
        else:
            context_parts.append(doc.page_content)

    return "\n\n---\n\n".join(context_parts)
```

#### 策略3：分块处理（Map-Reduce）

```python
from langchain.chains import MapReduceDocumentsChain

# 当上下文过长时，分块处理
# 1. Map：对每个文档块单独提问
# 2. Reduce：汇总所有答案

# 这样每个文档块都在"开头"位置，避免 Lost in the Middle
```

### 在 RAG 开发中的应用

```
┌─────────────────────────────────────────────────────────────┐
│                    最佳实践                                  │
│                                                             │
│  1. 控制上下文长度                                           │
│     - 尽量保持在 4K tokens 以内                              │
│     - 超过时考虑分块处理                                     │
│                                                             │
│  2. 优化文档排序                                             │
│     - 最相关的放开头                                         │
│     - 次相关的放结尾                                         │
│     - 相关性较低的放中间（或直接丢弃）                        │
│                                                             │
│  3. 使用结构化标记                                           │
│     - 用【】或 ### 标记重要内容                              │
│     - 帮助 LLM 识别关键信息                                  │
│                                                             │
│  4. 在结尾重复关键问题                                       │
│     - "基于以上资料，请回答：{question}"                     │
│     - 强化 LLM 对问题的关注                                  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 核心概念3：Token 预算管理

**Token 预算管理是在有限的 Context Window 内，合理分配 System、Context、Query 各部分 Token 的策略。**

### 为什么需要 Token 预算？

```
┌─────────────────────────────────────────────────────────────┐
│                    Context Window 限制                       │
│                                                             │
│  模型              Context Window    实际可用（留给输出）     │
│  ─────────────────────────────────────────────────────────  │
│  GPT-3.5-turbo     16K tokens       ~12K tokens            │
│  GPT-4             128K tokens      ~100K tokens           │
│  Claude 3          200K tokens      ~150K tokens           │
│  Claude 3.5        200K tokens      ~150K tokens           │
│                                                             │
│  注意：Context Window = 输入 + 输出                          │
│  需要为输出预留空间（通常 2K-4K tokens）                      │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Token 预算分配策略

```
推荐的 Token 分配比例：

┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  总预算：12K tokens（以 GPT-3.5 为例）                        │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  System Prompt    │  500-1000 tokens  │  ~8%       │   │
│  ├───────────────────┼───────────────────┼────────────┤   │
│  │  Context (RAG)    │  6000-8000 tokens │  ~65%      │   │
│  ├───────────────────┼───────────────────┼────────────┤   │
│  │  User Query       │  100-500 tokens   │  ~4%       │   │
│  ├───────────────────┼───────────────────┼────────────┤   │
│  │  预留输出         │  2000-4000 tokens │  ~23%      │   │
│  └───────────────────┴───────────────────┴────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 代码示例：Token 计数与预算管理

```python
import tiktoken

class TokenBudgetManager:
    """Token 预算管理器"""

    def __init__(
        self,
        model: str = "gpt-3.5-turbo",
        max_context_tokens: int = 12000,
        reserved_output_tokens: int = 2000
    ):
        self.encoding = tiktoken.encoding_for_model(model)
        self.max_context_tokens = max_context_tokens
        self.reserved_output_tokens = reserved_output_tokens
        self.available_tokens = max_context_tokens - reserved_output_tokens

    def count_tokens(self, text: str) -> int:
        """计算文本的 token 数量"""
        return len(self.encoding.encode(text))

    def allocate_budget(
        self,
        system_prompt: str,
        query: str,
        documents: list
    ) -> list:
        """
        分配 Token 预算，返回能放入的文档列表

        Args:
            system_prompt: 系统提示词
            query: 用户问题
            documents: 检索到的文档列表（按相关性排序）

        Returns:
            能放入预算的文档列表
        """
        # 计算固定部分的 token
        system_tokens = self.count_tokens(system_prompt)
        query_tokens = self.count_tokens(query)
        fixed_tokens = system_tokens + query_tokens

        # 剩余给 Context 的预算
        context_budget = self.available_tokens - fixed_tokens

        print(f"Token 预算分配：")
        print(f"  - System: {system_tokens} tokens")
        print(f"  - Query: {query_tokens} tokens")
        print(f"  - Context 预算: {context_budget} tokens")

        # 贪心选择文档
        selected_docs = []
        used_tokens = 0

        for doc in documents:
            doc_tokens = self.count_tokens(doc.page_content)
            if used_tokens + doc_tokens <= context_budget:
                selected_docs.append(doc)
                used_tokens += doc_tokens
            else:
                # 预算不足，停止添加
                print(f"  - 已选择 {len(selected_docs)} 个文档，使用 {used_tokens} tokens")
                break

        return selected_docs


# 使用示例
budget_manager = TokenBudgetManager(
    model="gpt-3.5-turbo",
    max_context_tokens=12000,
    reserved_output_tokens=2000
)

# 分配预算
selected_docs = budget_manager.allocate_budget(
    system_prompt=SYSTEM_PROMPT,
    query="公司的年假政策是什么？",
    documents=retrieved_docs
)
```

### 动态截断策略

```python
def truncate_document(doc: str, max_tokens: int, encoding) -> str:
    """
    智能截断文档，保留开头和结尾

    策略：保留前 70% + 后 30%（应对 Lost in the Middle）
    """
    tokens = encoding.encode(doc)

    if len(tokens) <= max_tokens:
        return doc

    # 计算保留的 token 数
    head_tokens = int(max_tokens * 0.7)
    tail_tokens = max_tokens - head_tokens

    # 截断
    truncated_tokens = tokens[:head_tokens] + tokens[-tail_tokens:]

    return encoding.decode(truncated_tokens)


def smart_context_builder(
    documents: list,
    max_tokens: int,
    encoding
) -> str:
    """
    智能构建上下文，确保不超过 Token 预算
    """
    context_parts = []
    remaining_tokens = max_tokens

    for i, doc in enumerate(documents):
        doc_tokens = len(encoding.encode(doc.page_content))

        if doc_tokens <= remaining_tokens:
            # 完整添加
            context_parts.append(doc.page_content)
            remaining_tokens -= doc_tokens
        elif remaining_tokens > 200:  # 至少保留 200 tokens 才截断
            # 截断添加
            truncated = truncate_document(
                doc.page_content,
                remaining_tokens,
                encoding
            )
            context_parts.append(truncated + "\n[...内容已截断...]")
            break
        else:
            # 预算不足，停止
            break

    return "\n\n---\n\n".join(context_parts)
```

### Token 预算管理最佳实践

| 策略 | 说明 | 适用场景 |
|------|------|----------|
| 固定预算 | 为每部分分配固定比例 | 简单场景，问题长度稳定 |
| 动态预算 | 根据问题复杂度调整 | 问题长度变化大 |
| 优先级截断 | 按相关性截断低优先级文档 | 检索结果多 |
| 压缩摘要 | 用 LLM 先压缩长文档 | 单文档过长 |

---

## 三个概念的关系

```
┌─────────────────────────────────────────────────────────────┐
│                上下文注入与生成的核心概念关系                   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Token 预算管理                           │   │
│  │              (决定能放多少内容)                        │   │
│  └──────────────────────┬──────────────────────────────┘   │
│                         │                                   │
│                         ▼                                   │
│  ┌─────────────────────────────────────────────────────┐   │
│  │           Lost in the Middle 优化                    │   │
│  │           (决定内容如何排列)                          │   │
│  └──────────────────────┬──────────────────────────────┘   │
│                         │                                   │
│                         ▼                                   │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Prompt 模板设计                          │   │
│  │              (决定内容如何组装)                        │   │
│  └──────────────────────┬──────────────────────────────┘   │
│                         │                                   │
│                         ▼                                   │
│                    最终 Prompt                              │
│                         │                                   │
│                         ▼                                   │
│                    LLM 生成答案                             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 概念速查表

| 概念 | 核心作用 | 关键参数/策略 |
|------|----------|--------------|
| Prompt 模板设计 | 结构化组装提示词 | System + Context + Query |
| Lost in the Middle | 优化信息位置 | 重要内容放首尾 |
| Token 预算管理 | 控制上下文长度 | 预留输出空间，动态截断 |

---

**下一步：** [04_最小可用](./04_最小可用.md) - 掌握 20% 核心知识解决 80% 问题
