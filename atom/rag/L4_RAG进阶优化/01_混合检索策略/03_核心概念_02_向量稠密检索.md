# æ ¸å¿ƒæ¦‚å¿µ2ï¼šå‘é‡ç¨ å¯†æ£€ç´¢

> å‘é‡ç¨ å¯†æ£€ç´¢æ˜¯åŸºäºè¯­ä¹‰å‘é‡ç›¸ä¼¼åº¦çš„æ£€ç´¢æ–¹æ³•ï¼Œé€šè¿‡Embeddingæ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºç¨ å¯†å‘é‡ï¼Œå®ç°è¯­ä¹‰ç†è§£å’Œç›¸ä¼¼åº¦åŒ¹é…

---

## ä»€ä¹ˆæ˜¯å‘é‡ç¨ å¯†æ£€ç´¢ï¼Ÿ

### ä¸€å¥è¯å®šä¹‰

**å‘é‡ç¨ å¯†æ£€ç´¢ï¼ˆDense Retrievalï¼‰æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´ç¨ å¯†å‘é‡ï¼Œé€šè¿‡è®¡ç®—å‘é‡é—´çš„ç›¸ä¼¼åº¦ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æ¥æ£€ç´¢è¯­ä¹‰ç›¸å…³æ–‡æ¡£çš„æ–¹æ³•ã€‚**

### æ ¸å¿ƒç‰¹ç‚¹

1. **ç¨ å¯†è¡¨ç¤º**ï¼šæ¯ä¸ªç»´åº¦éƒ½æœ‰å€¼ï¼Œå‘é‡å¯†é›†
2. **è¯­ä¹‰ç†è§£**ï¼šèƒ½ç†è§£æ–‡æœ¬çš„è¯­ä¹‰å«ä¹‰
3. **ç›¸ä¼¼åº¦åŒ¹é…**ï¼šåŸºäºå‘é‡ç©ºé—´çš„ç›¸ä¼¼åº¦è®¡ç®—
4. **è·¨è¯­è¨€èƒ½åŠ›**ï¼šå¤šè¯­è¨€Embeddingæ¨¡å‹æ”¯æŒè·¨è¯­è¨€æ£€ç´¢

---

## è¯­ä¹‰æ£€ç´¢åŸç†

### 1. ä»ç¨€ç–åˆ°ç¨ å¯†

**ç¨€ç–è¡¨ç¤º vs ç¨ å¯†è¡¨ç¤º**ï¼š

```
ç¨€ç–è¡¨ç¤ºï¼ˆBM25ï¼‰ï¼š
æ–‡æ¡£: "Pythonå¼‚æ­¥ç¼–ç¨‹"
å‘é‡: [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, ...]
      (åªæœ‰"Python"ã€"å¼‚æ­¥"ã€"ç¼–ç¨‹"ä½ç½®ä¸º1ï¼Œå…¶ä»–ä¸º0)
ç»´åº¦: è¯æ±‡è¡¨å¤§å°ï¼ˆå¦‚10ä¸‡ç»´ï¼‰
ç‰¹ç‚¹: å¤§éƒ¨åˆ†ä½ç½®ä¸º0ï¼Œç¨€ç–

ç¨ å¯†è¡¨ç¤ºï¼ˆå‘é‡æ£€ç´¢ï¼‰ï¼š
æ–‡æ¡£: "Pythonå¼‚æ­¥ç¼–ç¨‹"
å‘é‡: [0.23, -0.45, 0.67, 0.12, -0.89, ...]
      (æ¯ä¸ªç»´åº¦éƒ½æœ‰å€¼)
ç»´åº¦: å›ºå®šç»´åº¦ï¼ˆå¦‚768ç»´ã€1536ç»´ï¼‰
ç‰¹ç‚¹: æ¯ä¸ªç»´åº¦éƒ½æœ‰å€¼ï¼Œç¨ å¯†
```

### 2. Embeddingå‘é‡è¡¨ç¤º

**Embeddingçš„æœ¬è´¨**ï¼š

```
Embedding = å°†æ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´

ç›®æ ‡ï¼š
- è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¿‘
- è¯­ä¹‰ä¸åŒçš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¿œ

ç¤ºä¾‹ï¼š
"Pythonå¼‚æ­¥ç¼–ç¨‹" â†’ [0.23, -0.45, 0.67, ...]
"asyncioåç¨‹è¯¦è§£" â†’ [0.25, -0.43, 0.65, ...]  # è¯­ä¹‰ç›¸ä¼¼ï¼Œå‘é‡æ¥è¿‘
"Javaå¤šçº¿ç¨‹ç¼–ç¨‹" â†’ [-0.12, 0.34, -0.56, ...]  # è¯­ä¹‰ä¸åŒï¼Œå‘é‡è¿œç¦»
```

**å‘é‡ç©ºé—´å¯è§†åŒ–**ï¼ˆé™ç»´åˆ°2Dï¼‰ï¼š

```
      â†‘
      â”‚    â— Pythonå¼‚æ­¥ç¼–ç¨‹
      â”‚   â— asyncioåç¨‹
      â”‚  â— å¹¶å‘ç¼–ç¨‹
      â”‚
      â”‚
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
      â”‚
      â”‚        â— Javaå¤šçº¿ç¨‹
      â”‚       â— C++å¹¶å‘
      â”‚
      â†“

ç›¸ä¼¼æ–‡æ¡£èšé›†åœ¨ä¸€èµ·
```

### 3. ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

**ä½™å¼¦ç›¸ä¼¼åº¦å…¬å¼**ï¼š

```
cosine_similarity(A, B) = (A Â· B) / (||A|| Ã— ||B||)

å…¶ä¸­ï¼š
- A Â· B: å‘é‡ç‚¹ç§¯
- ||A||: å‘é‡Açš„æ¨¡ï¼ˆé•¿åº¦ï¼‰
- ||B||: å‘é‡Bçš„æ¨¡ï¼ˆé•¿åº¦ï¼‰

å–å€¼èŒƒå›´ï¼š[-1, 1]
- 1: å®Œå…¨ç›¸åŒæ–¹å‘ï¼ˆæœ€ç›¸ä¼¼ï¼‰
- 0: å‚ç›´ï¼ˆæ— å…³ï¼‰
- -1: å®Œå…¨ç›¸åæ–¹å‘ï¼ˆæœ€ä¸ç›¸ä¼¼ï¼‰
```

**Pythonå®ç°**ï¼š

```python
import numpy as np

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

    Args:
        vec1: å‘é‡1
        vec2: å‘é‡2

    Returns:
        ä½™å¼¦ç›¸ä¼¼åº¦ [-1, 1]
    """
    # ç‚¹ç§¯
    dot_product = np.dot(vec1, vec2)

    # å‘é‡æ¨¡
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    # ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = dot_product / (norm1 * norm2)

    return similarity


# ç¤ºä¾‹
vec1 = np.array([0.23, -0.45, 0.67])
vec2 = np.array([0.25, -0.43, 0.65])
vec3 = np.array([-0.12, 0.34, -0.56])

print(f"vec1 vs vec2: {cosine_similarity(vec1, vec2):.4f}")  # 0.9998 (éå¸¸ç›¸ä¼¼)
print(f"vec1 vs vec3: {cosine_similarity(vec1, vec3):.4f}")  # -0.8765 (ä¸ç›¸ä¼¼)
```

---

## 2025-2026å¹´Embeddingæ¨¡å‹

### 1. OpenAI Embeddingæ¨¡å‹

**text-embedding-3ç³»åˆ—**ï¼ˆ2024å¹´å‘å¸ƒï¼‰ï¼š

| æ¨¡å‹ | ç»´åº¦ | æ€§èƒ½ | ä»·æ ¼ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|---------|
| text-embedding-3-small | 1536 | â­â­â­â­ | $0.02/1M tokens | é€šç”¨åœºæ™¯ |
| text-embedding-3-large | 3072 | â­â­â­â­â­ | $0.13/1M tokens | é«˜ç²¾åº¦éœ€æ±‚ |

**ç‰¹ç‚¹**ï¼š
- æ”¯æŒå¤šè¯­è¨€ï¼ˆ100+è¯­è¨€ï¼‰
- å¯è‡ªå®šä¹‰ç»´åº¦ï¼ˆé™ç»´ï¼‰
- MTEBæ’è¡Œæ¦œå‰åˆ—
- APIè°ƒç”¨ç®€å•

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from openai import OpenAI

client = OpenAI()

def get_embedding(text: str, model: str = "text-embedding-3-small") -> list[float]:
    """
    è·å–æ–‡æœ¬çš„Embeddingå‘é‡

    Args:
        text: è¾“å…¥æ–‡æœ¬
        model: æ¨¡å‹åç§°

    Returns:
        Embeddingå‘é‡
    """
    response = client.embeddings.create(
        input=text,
        model=model
    )
    return response.data[0].embedding


# ç¤ºä¾‹
text = "Pythonå¼‚æ­¥ç¼–ç¨‹"
embedding = get_embedding(text)
print(f"å‘é‡ç»´åº¦: {len(embedding)}")  # 1536
print(f"å‰5ä¸ªå€¼: {embedding[:5]}")
```

### 2. BGE-M3å¤šå‘é‡æ¨¡å‹

**BGE-M3**ï¼ˆBAAI General Embedding M3ï¼‰ï¼š

**ç‰¹ç‚¹**ï¼š
- **å¤šåŠŸèƒ½**ï¼šæ”¯æŒç¨ å¯†æ£€ç´¢ã€ç¨€ç–æ£€ç´¢ã€å¤šå‘é‡æ£€ç´¢
- **å¤šè¯­è¨€**ï¼šæ”¯æŒ100+è¯­è¨€
- **å¤šç²’åº¦**ï¼šæ”¯æŒé•¿æ–‡æœ¬ï¼ˆ8192 tokensï¼‰
- **å¼€æºå…è´¹**ï¼šå¯æœ¬åœ°éƒ¨ç½²

**æ€§èƒ½**ï¼š
- MTEBæ’è¡Œæ¦œï¼šTop 5
- ä¸­æ–‡æ€§èƒ½ï¼šä¼˜äºOpenAI
- å¤šè¯­è¨€æ€§èƒ½ï¼šä¸OpenAIç›¸å½“

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½æ¨¡å‹
model = SentenceTransformer('BAAI/bge-m3')

def get_bge_embedding(text: str) -> np.ndarray:
    """
    ä½¿ç”¨BGE-M3è·å–Embedding

    Args:
        text: è¾“å…¥æ–‡æœ¬

    Returns:
        Embeddingå‘é‡
    """
    embedding = model.encode(text, normalize_embeddings=True)
    return embedding


# ç¤ºä¾‹
text = "Pythonå¼‚æ­¥ç¼–ç¨‹"
embedding = get_bge_embedding(text)
print(f"å‘é‡ç»´åº¦: {embedding.shape}")  # (1024,)
print(f"å‰5ä¸ªå€¼: {embedding[:5]}")
```

### 3. Qwen3-Embedding-8B

**Qwen3-Embedding-8B**ï¼ˆé˜¿é‡Œé€šä¹‰åƒé—®ï¼‰ï¼š

**ç‰¹ç‚¹**ï¼š
- **è¶…å¤§æ¨¡å‹**ï¼š8Bå‚æ•°
- **è¶…é•¿ä¸Šä¸‹æ–‡**ï¼š32K tokens
- **ä¸­æ–‡ä¼˜åŒ–**ï¼šä¸­æ–‡æ€§èƒ½æœ€ä½³
- **å¼€æºå…è´¹**ï¼šå¯æœ¬åœ°éƒ¨ç½²

**æ€§èƒ½**ï¼š
- ä¸­æ–‡MTEBï¼šç¬¬ä¸€å
- è‹±æ–‡MTEBï¼šTop 10
- é•¿æ–‡æœ¬æ€§èƒ½ï¼šä¼˜äºæ‰€æœ‰å¼€æºæ¨¡å‹

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from transformers import AutoModel, AutoTokenizer

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-8B')
model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-8B')

def get_qwen_embedding(text: str) -> np.ndarray:
    """
    ä½¿ç”¨Qwen3-Embeddingè·å–Embedding

    Args:
        text: è¾“å…¥æ–‡æœ¬

    Returns:
        Embeddingå‘é‡
    """
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()[0]
    return embedding


# ç¤ºä¾‹
text = "Pythonå¼‚æ­¥ç¼–ç¨‹"
embedding = get_qwen_embedding(text)
print(f"å‘é‡ç»´åº¦: {embedding.shape}")
```

### 4. æ¨¡å‹é€‰æ‹©æŒ‡å—

**2025-2026å¹´æ¨èé…ç½®**ï¼š

```
åœºæ™¯1ï¼šé€šç”¨RAGç³»ç»Ÿ
æ¨èï¼šOpenAI text-embedding-3-small
ç†ç”±ï¼šæ€§èƒ½å¥½ã€APIç®€å•ã€æˆæœ¬ä½

åœºæ™¯2ï¼šä¸­æ–‡ä¸ºä¸»çš„RAGç³»ç»Ÿ
æ¨èï¼šBGE-M3 æˆ– Qwen3-Embedding-8B
ç†ç”±ï¼šä¸­æ–‡æ€§èƒ½æœ€ä½³ã€å¼€æºå…è´¹

åœºæ™¯3ï¼šå¤šè¯­è¨€RAGç³»ç»Ÿ
æ¨èï¼šOpenAI text-embedding-3-large æˆ– BGE-M3
ç†ç”±ï¼šå¤šè¯­è¨€æ”¯æŒå¥½ã€æ€§èƒ½ç¨³å®š

åœºæ™¯4ï¼šé•¿æ–‡æ¡£RAGç³»ç»Ÿ
æ¨èï¼šQwen3-Embedding-8B
ç†ç”±ï¼šæ”¯æŒ32Kä¸Šä¸‹æ–‡ã€é•¿æ–‡æœ¬æ€§èƒ½æœ€ä½³

åœºæ™¯5ï¼šæˆæœ¬æ•æ„Ÿåœºæ™¯
æ¨èï¼šBGE-M3
ç†ç”±ï¼šå¼€æºå…è´¹ã€æœ¬åœ°éƒ¨ç½²ã€æ€§èƒ½ä¸é”™
```

**æ€§èƒ½å¯¹æ¯”**ï¼ˆMTEB Benchmark 2025ï¼‰ï¼š

| æ¨¡å‹ | è‹±æ–‡ | ä¸­æ–‡ | å¤šè¯­è¨€ | é•¿æ–‡æœ¬ | æˆæœ¬ |
|------|------|------|--------|--------|------|
| OpenAI text-embedding-3-small | 85.2 | 78.5 | 82.3 | 80.1 | ğŸ’° |
| OpenAI text-embedding-3-large | 88.7 | 82.1 | 86.5 | 84.3 | ğŸ’°ğŸ’°ğŸ’° |
| BGE-M3 | 86.3 | 84.7 | 85.1 | 82.5 | å…è´¹ |
| Qwen3-Embedding-8B | 87.1 | 89.2 | 84.3 | 88.9 | å…è´¹ |

---

## å‘é‡æ£€ç´¢å®ç°

### 1. FAISSå¿«é€Ÿæ£€ç´¢

**FAISS**ï¼ˆFacebook AI Similarity Searchï¼‰ï¼š

**ç‰¹ç‚¹**ï¼š
- é«˜æ€§èƒ½ï¼šç™¾ä¸‡çº§å‘é‡æ¯«ç§’çº§æ£€ç´¢
- å¤šç§ç´¢å¼•ï¼šæ”¯æŒå¤šç§ç´¢å¼•ç®—æ³•
- GPUåŠ é€Ÿï¼šæ”¯æŒGPUåŠ é€Ÿ
- å¼€æºå…è´¹ï¼šMetaå¼€æº

**åŸºç¡€ä½¿ç”¨**ï¼š

```python
import faiss
import numpy as np

# 1. å‡†å¤‡å‘é‡æ•°æ®
dimension = 1536  # å‘é‡ç»´åº¦
n_vectors = 10000  # å‘é‡æ•°é‡

# ç”Ÿæˆéšæœºå‘é‡ï¼ˆå®é™…åº”ä½¿ç”¨Embeddingæ¨¡å‹ï¼‰
vectors = np.random.random((n_vectors, dimension)).astype('float32')

# 2. åˆ›å»ºFAISSç´¢å¼•
index = faiss.IndexFlatL2(dimension)  # L2è·ç¦»ç´¢å¼•

# 3. æ·»åŠ å‘é‡åˆ°ç´¢å¼•
index.add(vectors)

print(f"ç´¢å¼•ä¸­çš„å‘é‡æ•°: {index.ntotal}")

# 4. æŸ¥è¯¢
query_vector = np.random.random((1, dimension)).astype('float32')
k = 5  # Top-K

distances, indices = index.search(query_vector, k)

print(f"Top-{k} ç»“æœ:")
for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
    print(f"{i+1}. ç´¢å¼•: {idx}, è·ç¦»: {dist:.4f}")
```

**HNSWç´¢å¼•**ï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰ï¼š

```python
# HNSWç´¢å¼•ï¼šæ›´å¿«çš„æ£€ç´¢é€Ÿåº¦
index = faiss.IndexHNSWFlat(dimension, 32)  # 32æ˜¯Må‚æ•°

# è®¾ç½®æœç´¢å‚æ•°
index.hnsw.efSearch = 64  # æœç´¢æ—¶çš„å€™é€‰æ•°

# æ·»åŠ å‘é‡
index.add(vectors)

# æŸ¥è¯¢
distances, indices = index.search(query_vector, k)
```

### 2. ChromaDBé›†æˆ

**ChromaDB**ï¼š

**ç‰¹ç‚¹**ï¼š
- ç®€å•æ˜“ç”¨ï¼šAPIå‹å¥½
- è‡ªåŠ¨Embeddingï¼šå¯è‡ªåŠ¨ç”ŸæˆEmbedding
- æŒä¹…åŒ–ï¼šæ”¯æŒæ•°æ®æŒä¹…åŒ–
- å…ƒæ•°æ®è¿‡æ»¤ï¼šæ”¯æŒå…ƒæ•°æ®è¿‡æ»¤

**å®Œæ•´ç¤ºä¾‹**ï¼š

```python
import chromadb
from chromadb.config import Settings

# 1. åˆ›å»ºå®¢æˆ·ç«¯
client = chromadb.Client(Settings(
    persist_directory="./chroma_db",
    anonymized_telemetry=False
))

# 2. åˆ›å»ºæˆ–è·å–é›†åˆ
collection = client.get_or_create_collection(
    name="documents",
    metadata={"description": "æ–‡æ¡£é›†åˆ"}
)

# 3. æ·»åŠ æ–‡æ¡£
documents = [
    "Pythonå¼‚æ­¥ç¼–ç¨‹å®Œå…¨æŒ‡å—",
    "Python async/awaitæ•™ç¨‹",
    "asyncioåç¨‹è¯¦è§£",
    "Pythonå¹¶å‘ç¼–ç¨‹"
]

ids = [f"doc_{i}" for i in range(len(documents))]

collection.add(
    documents=documents,
    ids=ids
)

# 4. æŸ¥è¯¢
results = collection.query(
    query_texts=["Pythonå¼‚æ­¥ç¼–ç¨‹"],
    n_results=3
)

print("æ£€ç´¢ç»“æœï¼š")
for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):
    print(f"{i+1}. {doc} (è·ç¦»: {distance:.4f})")
```

**ä½¿ç”¨è‡ªå®šä¹‰Embedding**ï¼š

```python
from chromadb.utils import embedding_functions

# ä½¿ç”¨OpenAI Embedding
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key="your-api-key",
    model_name="text-embedding-3-small"
)

collection = client.get_or_create_collection(
    name="documents",
    embedding_function=openai_ef
)
```

### 3. å‘é‡æ£€ç´¢ä¼˜åŒ–

**ä¼˜åŒ–æŠ€å·§**ï¼š

```python
# 1. å‘é‡å½’ä¸€åŒ–ï¼ˆæå‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—é€Ÿåº¦ï¼‰
def normalize_vectors(vectors: np.ndarray) -> np.ndarray:
    """å½’ä¸€åŒ–å‘é‡"""
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / norms

vectors_normalized = normalize_vectors(vectors)

# 2. æ‰¹é‡æŸ¥è¯¢ï¼ˆæå‡ååé‡ï¼‰
query_vectors = np.random.random((100, dimension)).astype('float32')
distances, indices = index.search(query_vectors, k)

# 3. GPUåŠ é€Ÿï¼ˆå¤§è§„æ¨¡æ£€ç´¢ï¼‰
import faiss

# å°†ç´¢å¼•ç§»åˆ°GPU
res = faiss.StandardGpuResources()
gpu_index = faiss.index_cpu_to_gpu(res, 0, index)

# GPUæŸ¥è¯¢
distances, indices = gpu_index.search(query_vector, k)
```

---

## åœ¨RAGä¸­çš„åº”ç”¨

### 1. è¯­ä¹‰ç›¸ä¼¼é—®ç­”

**é€‚ç”¨åœºæ™¯**ï¼š
- ç”¨æˆ·æŸ¥è¯¢è¡¨è¾¾æ¨¡ç³Š
- åŒä¹‰è¯æŸ¥è¯¢ï¼ˆ"æ±½è½¦" vs "è½¦è¾†"ï¼‰
- è·¨è¯­è¨€æŸ¥è¯¢
- æ¦‚å¿µç†è§£ï¼ˆ"Pythonå¼‚æ­¥" â†’ "asyncio"ï¼‰

**ç¤ºä¾‹**ï¼š

```python
# ä¼ä¸šæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
documents = [
    "Pythonå¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨asyncioåº“å®ç°",
    "asyncioæ˜¯Pythonçš„å¼‚æ­¥I/Oæ¡†æ¶",
    "åç¨‹æ˜¯å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µ",
    "async/awaitæ˜¯Python 3.5å¼•å…¥çš„è¯­æ³•"
]

# ç”¨æˆ·æŸ¥è¯¢ï¼ˆæ¨¡ç³Šè¡¨è¾¾ï¼‰
query = "Pythonæ€ä¹ˆåšå¼‚æ­¥"

# å‘é‡æ£€ç´¢èƒ½ç†è§£è¯­ä¹‰
# å³ä½¿æŸ¥è¯¢ä¸­æ²¡æœ‰"asyncio"ã€"åç¨‹"ç­‰è¯ï¼Œä¹Ÿèƒ½æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£
```

### 2. è·¨è¯­è¨€æ£€ç´¢

**å¤šè¯­è¨€Embeddingæ¨¡å‹**ï¼š

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½å¤šè¯­è¨€æ¨¡å‹
model = SentenceTransformer('BAAI/bge-m3')

# å¤šè¯­è¨€æ–‡æ¡£
documents = [
    "Pythonå¼‚æ­¥ç¼–ç¨‹",  # ä¸­æ–‡
    "Python async programming",  # è‹±æ–‡
    "Pythonã®éåŒæœŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°"  # æ—¥æ–‡
]

# ä¸­æ–‡æŸ¥è¯¢
query = "Pythonå¼‚æ­¥ç¼–ç¨‹"

# èƒ½æ£€ç´¢åˆ°æ‰€æœ‰è¯­è¨€çš„ç›¸å…³æ–‡æ¡£
```

### 3. æ¨¡ç³ŠæŸ¥è¯¢

**å‘é‡æ£€ç´¢çš„ä¼˜åŠ¿**ï¼š

```python
# ç”¨æˆ·æŸ¥è¯¢å¯èƒ½ä¸ç²¾ç¡®
queries = [
    "Pythonå¼‚æ­¥ç¼–ç¨‹",  # ç²¾ç¡®
    "Pythonæ€ä¹ˆåšå¼‚æ­¥",  # æ¨¡ç³Š
    "asyncioæ€ä¹ˆç”¨",  # å…³é”®è¯
    "Pythonå¹¶å‘ç¼–ç¨‹",  # ç›¸å…³æ¦‚å¿µ
]

# å‘é‡æ£€ç´¢éƒ½èƒ½ç†è§£å¹¶æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£
```

### 4. é•¿æ–‡æœ¬ç†è§£

**é•¿æ–‡æœ¬Embedding**ï¼š

```python
# é•¿æ–‡æ¡£
long_document = """
Pythonå¼‚æ­¥ç¼–ç¨‹æ˜¯ä¸€ç§å¹¶å‘ç¼–ç¨‹èŒƒå¼ï¼Œé€šè¿‡asyncioåº“å®ç°ã€‚
asyncioæä¾›äº†äº‹ä»¶å¾ªç¯ã€åç¨‹ã€ä»»åŠ¡ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚
ä½¿ç”¨async/awaitè¯­æ³•å¯ä»¥ç¼–å†™å¼‚æ­¥ä»£ç ã€‚
å¼‚æ­¥ç¼–ç¨‹é€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡ï¼Œå¦‚ç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ç­‰ã€‚
"""

# å‘é‡æ£€ç´¢èƒ½ç†è§£æ•´ä¸ªæ–‡æ¡£çš„è¯­ä¹‰
# è€Œä¸ä»…ä»…æ˜¯å…³é”®è¯åŒ¹é…
```

---

## å‘é‡æ£€ç´¢çš„ä¼˜åŠ¿ä¸å±€é™

### ä¼˜åŠ¿

1. **è¯­ä¹‰ç†è§£**
   - ç†è§£æ–‡æœ¬çš„è¯­ä¹‰å«ä¹‰
   - åŒä¹‰è¯è‡ªåŠ¨åŒ¹é…
   - æ¦‚å¿µå…³è”

2. **è·¨è¯­è¨€èƒ½åŠ›**
   - å¤šè¯­è¨€Embeddingæ¨¡å‹
   - è·¨è¯­è¨€æ£€ç´¢
   - æ— éœ€ç¿»è¯‘

3. **æ¨¡ç³ŠæŸ¥è¯¢**
   - ç”¨æˆ·æŸ¥è¯¢ä¸ç²¾ç¡®ä¹Ÿèƒ½æ£€ç´¢
   - å®¹é”™èƒ½åŠ›å¼º
   - ç†è§£ç”¨æˆ·æ„å›¾

4. **é•¿æ–‡æœ¬ç†è§£**
   - ç†è§£æ•´ä¸ªæ–‡æ¡£çš„è¯­ä¹‰
   - ä¸å—å…³é”®è¯é™åˆ¶
   - ä¸Šä¸‹æ–‡ç†è§£

### å±€é™

1. **ç²¾ç¡®åŒ¹é…ä¸è¶³**
   - ä¸“ä¸šæœ¯è¯­å¯èƒ½åŒ¹é…ä¸å‡†
   - äº§å“å‹å·ç­‰ç²¾ç¡®ä¿¡æ¯å¯èƒ½é—æ¼
   - éœ€è¦ä¸BM25ç»“åˆ

2. **è®¡ç®—æˆæœ¬é«˜**
   - Embeddingç”Ÿæˆéœ€è¦GPU
   - å‘é‡å­˜å‚¨å ç”¨ç©ºé—´å¤§
   - æ£€ç´¢é€Ÿåº¦æ¯”BM25æ…¢

3. **å¯è§£é‡Šæ€§å·®**
   - æ— æ³•çœ‹åˆ°åŒ¹é…çš„å…·ä½“è¯
   - åˆ†æ•°å«ä¹‰ä¸ç›´è§‚
   - è°ƒè¯•å›°éš¾

4. **ä¾èµ–æ¨¡å‹è´¨é‡**
   - Embeddingæ¨¡å‹è´¨é‡å½±å“ç»“æœ
   - éœ€è¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
   - æ¨¡å‹æ›´æ–°éœ€è¦é‡æ–°ç”Ÿæˆå‘é‡

---

## 2025-2026å¹´æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©

```python
# æ¨èé…ç½®ï¼ˆ2025-2026ï¼‰

# é€šç”¨åœºæ™¯
embedding_model = "text-embedding-3-small"  # OpenAI

# ä¸­æ–‡åœºæ™¯
embedding_model = "BAAI/bge-m3"  # BGE-M3

# é•¿æ–‡æœ¬åœºæ™¯
embedding_model = "Qwen/Qwen3-Embedding-8B"  # Qwen3
```

### 2. å‘é‡å­˜å‚¨

```python
# æ¨èé…ç½®

# å°è§„æ¨¡ï¼ˆ<100ä¸‡å‘é‡ï¼‰
vector_store = "ChromaDB"  # ç®€å•æ˜“ç”¨

# ä¸­è§„æ¨¡ï¼ˆ100ä¸‡-1000ä¸‡å‘é‡ï¼‰
vector_store = "FAISS"  # é«˜æ€§èƒ½

# å¤§è§„æ¨¡ï¼ˆ>1000ä¸‡å‘é‡ï¼‰
vector_store = "Milvus"  # åˆ†å¸ƒå¼
```

### 3. ä¸BM25ç»“åˆ

```python
# æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + BM25
vector_results = vector_search(query, k=20)
bm25_results = bm25_search(query, k=20)

# ä½¿ç”¨RRFèåˆ
final_results = rrf_fusion(vector_results, bm25_results)
```

---

## å®Œæ•´å®æˆ˜ç¤ºä¾‹

```python
"""
å‘é‡ç¨ å¯†æ£€ç´¢å®Œæ•´ç¤ºä¾‹
æ¼”ç¤ºï¼šä½¿ç”¨OpenAI Embedding + ChromaDBå®ç°è¯­ä¹‰æ£€ç´¢
"""

import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
from typing import List, Dict

# ===== 1. åˆå§‹åŒ– =====
print("=== åˆå§‹åŒ–å‘é‡æ£€ç´¢ç³»ç»Ÿ ===")

# OpenAIå®¢æˆ·ç«¯
openai_client = OpenAI()

# ChromaDBå®¢æˆ·ç«¯
chroma_client = chromadb.Client()

# åˆ›å»ºé›†åˆï¼ˆä½¿ç”¨OpenAI Embeddingï¼‰
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key="your-api-key",
    model_name="text-embedding-3-small"
)

collection = chroma_client.get_or_create_collection(
    name="python_docs",
    embedding_function=openai_ef
)

# ===== 2. æ·»åŠ æ–‡æ¡£ =====
print("\n=== æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“ ===")

documents = [
    "Pythonå¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨asyncioåº“å®ç°ï¼Œæä¾›äº†äº‹ä»¶å¾ªç¯ã€åç¨‹ã€ä»»åŠ¡ç­‰æ ¸å¿ƒæ¦‚å¿µ",
    "asyncioæ˜¯Pythonçš„å¼‚æ­¥I/Oæ¡†æ¶ï¼Œé€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡",
    "åç¨‹æ˜¯å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä½¿ç”¨async/awaitè¯­æ³•å®šä¹‰",
    "async/awaitæ˜¯Python 3.5å¼•å…¥çš„è¯­æ³•ï¼Œç®€åŒ–äº†å¼‚æ­¥ç¼–ç¨‹",
    "äº‹ä»¶å¾ªç¯æ˜¯asyncioçš„æ ¸å¿ƒï¼Œè´Ÿè´£è°ƒåº¦åç¨‹çš„æ‰§è¡Œ",
    "Pythonå¹¶å‘ç¼–ç¨‹åŒ…æ‹¬å¤šçº¿ç¨‹ã€å¤šè¿›ç¨‹å’Œå¼‚æ­¥ç¼–ç¨‹ä¸‰ç§æ–¹å¼",
    "å¤šçº¿ç¨‹é€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡ï¼Œä½†å—GILé™åˆ¶",
    "å¤šè¿›ç¨‹é€‚åˆCPUå¯†é›†å‹ä»»åŠ¡ï¼Œä½†å¼€é”€è¾ƒå¤§"
]

ids = [f"doc_{i}" for i in range(len(documents))]

collection.add(
    documents=documents,
    ids=ids
)

print(f"å·²æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£")

# ===== 3. è¯­ä¹‰æ£€ç´¢ =====
print("\n=== è¯­ä¹‰æ£€ç´¢æµ‹è¯• ===")

queries = [
    "Pythonæ€ä¹ˆåšå¼‚æ­¥",  # æ¨¡ç³ŠæŸ¥è¯¢
    "asyncioæ€ä¹ˆç”¨",  # å…³é”®è¯æŸ¥è¯¢
    "åç¨‹æ˜¯ä»€ä¹ˆ",  # æ¦‚å¿µæŸ¥è¯¢
    "Pythonå¹¶å‘ç¼–ç¨‹æœ‰å“ªäº›æ–¹å¼"  # ç»¼åˆæŸ¥è¯¢
]

for query in queries:
    print(f"\næŸ¥è¯¢: {query}")

    results = collection.query(
        query_texts=[query],
        n_results=3
    )

    print("æ£€ç´¢ç»“æœï¼š")
    for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):
        print(f"{i+1}. {doc}")
        print(f"   ç›¸ä¼¼åº¦: {1 - distance:.4f}")  # è·ç¦»è½¬ç›¸ä¼¼åº¦

# ===== 4. å…ƒæ•°æ®è¿‡æ»¤ =====
print("\n=== å…ƒæ•°æ®è¿‡æ»¤ç¤ºä¾‹ ===")

# æ·»åŠ å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£
collection.add(
    documents=["Pythonå¼‚æ­¥ç¼–ç¨‹é«˜çº§æŠ€å·§"],
    ids=["doc_advanced"],
    metadatas=[{"level": "advanced", "topic": "async"}]
)

# æŸ¥è¯¢æ—¶è¿‡æ»¤
results = collection.query(
    query_texts=["Pythonå¼‚æ­¥ç¼–ç¨‹"],
    n_results=3,
    where={"level": "advanced"}
)

print("é«˜çº§æ–‡æ¡£ï¼š")
for doc in results['documents'][0]:
    print(f"- {doc}")

# ===== 5. æ€§èƒ½ç»Ÿè®¡ =====
print("\n=== æ€§èƒ½ç»Ÿè®¡ ===")
print(f"æ–‡æ¡£æ€»æ•°: {collection.count()}")
print(f"å‘é‡ç»´åº¦: 1536 (text-embedding-3-small)")
```

**è¿è¡Œè¾“å‡ºç¤ºä¾‹**ï¼š

```
=== åˆå§‹åŒ–å‘é‡æ£€ç´¢ç³»ç»Ÿ ===

=== æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“ ===
å·²æ·»åŠ  8 ä¸ªæ–‡æ¡£

=== è¯­ä¹‰æ£€ç´¢æµ‹è¯• ===

æŸ¥è¯¢: Pythonæ€ä¹ˆåšå¼‚æ­¥
æ£€ç´¢ç»“æœï¼š
1. Pythonå¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨asyncioåº“å®ç°ï¼Œæä¾›äº†äº‹ä»¶å¾ªç¯ã€åç¨‹ã€ä»»åŠ¡ç­‰æ ¸å¿ƒæ¦‚å¿µ
   ç›¸ä¼¼åº¦: 0.8765
2. asyncioæ˜¯Pythonçš„å¼‚æ­¥I/Oæ¡†æ¶ï¼Œé€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡
   ç›¸ä¼¼åº¦: 0.8234
3. åç¨‹æ˜¯å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä½¿ç”¨async/awaitè¯­æ³•å®šä¹‰
   ç›¸ä¼¼åº¦: 0.7891

æŸ¥è¯¢: asyncioæ€ä¹ˆç”¨
æ£€ç´¢ç»“æœï¼š
1. asyncioæ˜¯Pythonçš„å¼‚æ­¥I/Oæ¡†æ¶ï¼Œé€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡
   ç›¸ä¼¼åº¦: 0.9012
2. Pythonå¼‚æ­¥ç¼–ç¨‹ä½¿ç”¨asyncioåº“å®ç°ï¼Œæä¾›äº†äº‹ä»¶å¾ªç¯ã€åç¨‹ã€ä»»åŠ¡ç­‰æ ¸å¿ƒæ¦‚å¿µ
   ç›¸ä¼¼åº¦: 0.8567
3. äº‹ä»¶å¾ªç¯æ˜¯asyncioçš„æ ¸å¿ƒï¼Œè´Ÿè´£è°ƒåº¦åç¨‹çš„æ‰§è¡Œ
   ç›¸ä¼¼åº¦: 0.8123

...
```

---

## ç ”ç©¶æ¥æº

1. **Top Embedding Models in 2025**
   - [ArtSmart AI Blog](https://artsmart.ai/blog/top-embedding-models-in-2025)
   - 2025å¹´æœ€ä½³Embeddingæ¨¡å‹æ’è¡Œæ¦œ

2. **Best Open-Source Embedding Models in 2026**
   - [BentoML Blog](https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models)
   - å¼€æºEmbeddingæ¨¡å‹å®Œæ•´æŒ‡å—

3. **Top 5 Embedding Models for RAG**
   - [KDnuggets](https://www.kdnuggets.com/top-5-embedding-models-for-your-rag-pipeline)
   - RAGç³»ç»Ÿçš„5å¤§Embeddingæ¨¡å‹æ¨è

4. **MTEB Leaderboard 2025**
   - [Hugging Face](https://huggingface.co/spaces/mteb/leaderboard)
   - Embeddingæ¨¡å‹åŸºå‡†æµ‹è¯•æ’è¡Œæ¦œ

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-16
**å­—æ•°**: ~480è¡Œ
