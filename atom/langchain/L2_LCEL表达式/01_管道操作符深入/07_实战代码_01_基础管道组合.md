# å®æˆ˜ä»£ç 1ï¼šåŸºç¡€ç®¡é“ç»„åˆ

> é€šè¿‡å¯è¿è¡Œçš„ä»£ç ç¤ºä¾‹æŒæ¡åŸºç¡€çš„ LCEL ç®¡é“ç»„åˆ

---

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›å®Œæ•´å¯è¿è¡Œçš„ä»£ç ç¤ºä¾‹ï¼Œæ¶µç›–ï¼š
- åŸºç¡€ä¸‰æ®µå¼ç®¡é“ï¼ˆPrompt â†’ LLM â†’ Parserï¼‰
- å¤šå˜é‡ Prompt ç»„åˆ
- é”™è¯¯å¤„ç†
- æ‰¹å¤„ç†
- æµå¼æ‰§è¡Œ

**æ‰€æœ‰ä»£ç éƒ½å¯ä»¥ç›´æ¥è¿è¡Œ**ï¼ˆéœ€è¦é…ç½® API å¯†é’¥ï¼‰

---

## ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–

```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd /Users/wuxiao/Documents/codeWithFelix/vibe-learning/vibe-learning-rag

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# å·²å®‰è£…çš„ä¾èµ–
# - langchain
# - langchain-core
# - langchain-openai
# - python-dotenv
```

### é…ç½® API å¯†é’¥

```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶
# OPENAI_API_KEY=your_key_here
```

---

## ç¤ºä¾‹1ï¼šæœ€ç®€å•çš„ç®¡é“

### ä»£ç 

```python
"""
ç¤ºä¾‹1ï¼šæœ€ç®€å•çš„ LCEL ç®¡é“
æ¼”ç¤ºï¼šPrompt â†’ LLM â†’ Parser ä¸‰æ®µå¼
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ===== 1. å®šä¹‰ç»„ä»¶ =====
print("=== å®šä¹‰ç»„ä»¶ ===")

# æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
print(f"Prompt: {prompt}")

# å¤§æ¨¡å‹
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
print(f"LLM: {llm.model_name}")

# è¾“å‡ºè§£æå™¨
parser = StrOutputParser()
print(f"Parser: {parser}")

# ===== 2. ç»„åˆç®¡é“ =====
print("\n=== ç»„åˆç®¡é“ ===")

chain = prompt | llm | parser
print(f"Chain: {chain}")
print(f"Chain type: {type(chain)}")

# ===== 3. æ‰§è¡Œç®¡é“ =====
print("\n=== æ‰§è¡Œç®¡é“ ===")

result = chain.invoke({"topic": "ç¨‹åºå‘˜"})
print(f"\nç»“æœ:\n{result}")

# ===== 4. æµ‹è¯•ä¸åŒè¾“å…¥ =====
print("\n=== æµ‹è¯•ä¸åŒè¾“å…¥ ===")

topics = ["äººå·¥æ™ºèƒ½", "Python", "å’–å•¡"]
for topic in topics:
    result = chain.invoke({"topic": topic})
    print(f"\nä¸»é¢˜: {topic}")
    print(f"ç¬‘è¯: {result[:100]}...")  # åªæ˜¾ç¤ºå‰100å­—ç¬¦
```

### è¿è¡Œè¾“å‡º

```
=== å®šä¹‰ç»„ä»¶ ===
Prompt: ChatPromptTemplate(...)
LLM: gpt-4o-mini
Parser: StrOutputParser()

=== ç»„åˆç®¡é“ ===
Chain: RunnableSequence(...)
Chain type: <class 'langchain_core.runnables.base.RunnableSequence'>

=== æ‰§è¡Œç®¡é“ ===

ç»“æœ:
ä¸ºä»€ä¹ˆç¨‹åºå‘˜æ€»æ˜¯æ··æ·†åœ£è¯èŠ‚å’Œä¸‡åœ£èŠ‚ï¼Ÿ
å› ä¸º Oct 31 == Dec 25ï¼

=== æµ‹è¯•ä¸åŒè¾“å…¥ ===

ä¸»é¢˜: äººå·¥æ™ºèƒ½
ç¬‘è¯: ä¸ºä»€ä¹ˆAIä¸ä¼šè®²ç¬‘è¯ï¼Ÿå› ä¸ºå®ƒæ€»æ˜¯è¿‡åº¦æ‹Ÿåˆï¼...

ä¸»é¢˜: Python
ç¬‘è¯: ä¸ºä»€ä¹ˆPythonç¨‹åºå‘˜å–œæ¬¢å¤§è‡ªç„¶ï¼Ÿå› ä¸ºä»–ä»¬å–œæ¬¢èŸ’è›‡ï¼ˆPythonï¼‰ï¼...

ä¸»é¢˜: å’–å•¡
ç¬‘è¯: ç¨‹åºå‘˜ä¸ºä»€ä¹ˆå–œæ¬¢å’–å•¡ï¼Ÿå› ä¸ºJavaï¼...
```

---

## ç¤ºä¾‹2ï¼šå¤šå˜é‡ Prompt

### ä»£ç 

```python
"""
ç¤ºä¾‹2ï¼šå¤šå˜é‡ Prompt
æ¼”ç¤ºï¼šä½¿ç”¨å¤šä¸ªå˜é‡çš„æç¤ºæ¨¡æ¿
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from dotenv import load_dotenv

load_dotenv()

# ===== 1. å®šä¹‰å¤šå˜é‡ Prompt =====
print("=== å¤šå˜é‡ Prompt ===")

prompt = ChatPromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä¸ª{role}ã€‚\n\n"
    "è¯·ç”¨{style}çš„é£æ ¼å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n\n"
    "{question}"
)

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
parser = StrOutputParser()

# ç»„åˆ
chain = prompt | llm | parser

# ===== 2. æµ‹è¯•ä¸åŒè§’è‰²å’Œé£æ ¼ =====
print("\n=== æµ‹è¯•ä¸åŒç»„åˆ ===")

test_cases = [
    {
        "role": "èµ„æ·±Pythonå·¥ç¨‹å¸ˆ",
        "style": "æŠ€æœ¯ä¸“ä¸š",
        "question": "ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
    },
    {
        "role": "å¹¼å„¿å›­è€å¸ˆ",
        "style": "ç®€å•æ˜“æ‡‚",
        "question": "ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
    },
    {
        "role": "è¯—äºº",
        "style": "è¯—æ„æµªæ¼«",
        "question": "ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
    }
]

for i, test_case in enumerate(test_cases, 1):
    print(f"\n--- æµ‹è¯• {i} ---")
    print(f"è§’è‰²: {test_case['role']}")
    print(f"é£æ ¼: {test_case['style']}")
    print(f"é—®é¢˜: {test_case['question']}")

    result = chain.invoke(test_case)
    print(f"\nå›ç­”:\n{result}\n")
```

### è¿è¡Œè¾“å‡º

```
=== å¤šå˜é‡ Prompt ===

=== æµ‹è¯•ä¸åŒç»„åˆ ===

--- æµ‹è¯• 1 ---
è§’è‰²: èµ„æ·±Pythonå·¥ç¨‹å¸ˆ
é£æ ¼: æŠ€æœ¯ä¸“ä¸š
é—®é¢˜: ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ

å›ç­”:
è£…é¥°å™¨æ˜¯Pythonä¸­çš„ä¸€ç§è®¾è®¡æ¨¡å¼ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ¥å—å‡½æ•°ä½œä¸ºå‚æ•°å¹¶è¿”å›æ–°å‡½æ•°çš„é«˜é˜¶å‡½æ•°ã€‚
å®ƒä½¿ç”¨@è¯­æ³•ç³–ï¼Œå¯ä»¥åœ¨ä¸ä¿®æ”¹åŸå‡½æ•°ä»£ç çš„æƒ…å†µä¸‹å¢å¼ºå‡½æ•°åŠŸèƒ½...

--- æµ‹è¯• 2 ---
è§’è‰²: å¹¼å„¿å›­è€å¸ˆ
é£æ ¼: ç®€å•æ˜“æ‡‚
é—®é¢˜: ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ

å›ç­”:
è£…é¥°å™¨å°±åƒç»™ç¤¼ç‰©åŒ…è£…çº¸ä¸€æ ·ï¼ä½ æœ‰ä¸€ä¸ªç¤¼ç‰©ï¼ˆå‡½æ•°ï¼‰ï¼Œç„¶åç”¨æ¼‚äº®çš„åŒ…è£…çº¸ï¼ˆè£…é¥°å™¨ï¼‰
æŠŠå®ƒåŒ…èµ·æ¥ï¼Œç¤¼ç‰©è¿˜æ˜¯é‚£ä¸ªç¤¼ç‰©ï¼Œä½†æ˜¯çœ‹èµ·æ¥æ›´æ¼‚äº®äº†...

--- æµ‹è¯• 3 ---
è§’è‰²: è¯—äºº
é£æ ¼: è¯—æ„æµªæ¼«
é—®é¢˜: ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ

å›ç­”:
è£…é¥°å™¨å¦‚æ˜¥é£æ‹‚é¢ï¼Œ
ä¸æ”¹å‡½æ•°ä¹‹æœ¬è´¨ï¼Œ
å´æ·»åŠŸèƒ½ä¹‹åå½©...
```

---

## ç¤ºä¾‹3ï¼šé”™è¯¯å¤„ç†

### ä»£ç 

```python
"""
ç¤ºä¾‹3ï¼šé”™è¯¯å¤„ç†
æ¼”ç¤ºï¼šå¤„ç†ç®¡é“æ‰§è¡Œä¸­çš„é”™è¯¯
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_core.runnables import Runnable
from dotenv import load_dotenv

load_dotenv()

# ===== 1. åŸºç¡€é”™è¯¯å¤„ç† =====
print("=== åŸºç¡€é”™è¯¯å¤„ç† ===")

prompt = ChatPromptTemplate.from_template("å›ç­”ï¼š{question}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | llm | parser

# æµ‹è¯•æ­£å¸¸è¾“å…¥
try:
    result = chain.invoke({"question": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"})
    print(f"âœ… æˆåŠŸ: {result[:50]}...")
except Exception as e:
    print(f"âŒ å¤±è´¥: {e}")

# æµ‹è¯•é”™è¯¯è¾“å…¥ï¼ˆç¼ºå°‘å¿…éœ€å­—æ®µï¼‰
try:
    result = chain.invoke({"wrong_key": "value"})
    print(f"âœ… æˆåŠŸ: {result}")
except Exception as e:
    print(f"âŒ å¤±è´¥: {type(e).__name__}: {e}")

# ===== 2. è‡ªå®šä¹‰é”™è¯¯å¤„ç† Runnable =====
print("\n=== è‡ªå®šä¹‰é”™è¯¯å¤„ç† ===")

class SafeRunnable(Runnable):
    """å¸¦é”™è¯¯å¤„ç†çš„ Runnable åŒ…è£…å™¨"""

    def __init__(self, wrapped: Runnable, fallback_value=None):
        self.wrapped = wrapped
        self.fallback_value = fallback_value

    def invoke(self, input, config=None):
        try:
            return self.wrapped.invoke(input, config)
        except Exception as e:
            print(f"âš ï¸  é”™è¯¯: {type(e).__name__}: {e}")
            if self.fallback_value is not None:
                print(f"ğŸ”„ ä½¿ç”¨é™çº§å€¼: {self.fallback_value}")
                return self.fallback_value
            raise

# ä½¿ç”¨å®‰å…¨åŒ…è£…å™¨
safe_chain = SafeRunnable(chain, fallback_value="æŠ±æ­‰ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚")

# æµ‹è¯•é”™è¯¯è¾“å…¥
result = safe_chain.invoke({"wrong_key": "value"})
print(f"ç»“æœ: {result}")

# ===== 3. é€æ­¥è°ƒè¯• =====
print("\n=== é€æ­¥è°ƒè¯• ===")

def debug_chain(chain, input_data):
    """é€æ­¥æ‰§è¡Œç®¡é“å¹¶æ‰“å°ä¸­é—´ç»“æœ"""
    print(f"è¾“å…¥: {input_data}")

    try:
        # è·å–ç®¡é“çš„æ­¥éª¤
        if hasattr(chain, 'steps'):
            steps = chain.steps
        else:
            steps = [chain]

        current_input = input_data
        for i, step in enumerate(steps, 1):
            print(f"\næ­¥éª¤ {i}: {type(step).__name__}")
            print(f"  è¾“å…¥: {current_input}")

            try:
                current_output = step.invoke(current_input)
                print(f"  è¾“å‡º: {current_output}")
                current_input = current_output
            except Exception as e:
                print(f"  âŒ é”™è¯¯: {type(e).__name__}: {e}")
                raise

        return current_input

    except Exception as e:
        print(f"\nğŸ’¥ ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
        return None

# è°ƒè¯•æ­£å¸¸è¾“å…¥
result = debug_chain(chain, {"question": "ä»€ä¹ˆæ˜¯AIï¼Ÿ"})

# è°ƒè¯•é”™è¯¯è¾“å…¥
result = debug_chain(chain, {"wrong_key": "value"})
```

### è¿è¡Œè¾“å‡º

```
=== åŸºç¡€é”™è¯¯å¤„ç† ===
âœ… æˆåŠŸ: Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»å...
âŒ å¤±è´¥: KeyError: 'question'

=== è‡ªå®šä¹‰é”™è¯¯å¤„ç† ===
âš ï¸  é”™è¯¯: KeyError: 'question'
ğŸ”„ ä½¿ç”¨é™çº§å€¼: æŠ±æ­‰ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚
ç»“æœ: æŠ±æ­‰ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚

=== é€æ­¥è°ƒè¯• ===
è¾“å…¥: {'question': 'ä»€ä¹ˆæ˜¯AIï¼Ÿ'}

æ­¥éª¤ 1: ChatPromptTemplate
  è¾“å…¥: {'question': 'ä»€ä¹ˆæ˜¯AIï¼Ÿ'}
  è¾“å‡º: ChatPromptValue(...)

æ­¥éª¤ 2: ChatOpenAI
  è¾“å…¥: ChatPromptValue(...)
  è¾“å‡º: AIMessage(content='AIæ˜¯äººå·¥æ™ºèƒ½...')

æ­¥éª¤ 3: StrOutputParser
  è¾“å…¥: AIMessage(content='AIæ˜¯äººå·¥æ™ºèƒ½...')
  è¾“å‡º: AIæ˜¯äººå·¥æ™ºèƒ½...

è¾“å…¥: {'wrong_key': 'value'}

æ­¥éª¤ 1: ChatPromptTemplate
  è¾“å…¥: {'wrong_key': 'value'}
  âŒ é”™è¯¯: KeyError: 'question'

ğŸ’¥ ç®¡é“æ‰§è¡Œå¤±è´¥: 'question'
```

---

## ç¤ºä¾‹4ï¼šæ‰¹å¤„ç†

### ä»£ç 

```python
"""
ç¤ºä¾‹4ï¼šæ‰¹å¤„ç†
æ¼”ç¤ºï¼šä¸€æ¬¡å¤„ç†å¤šä¸ªè¾“å…¥
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from dotenv import load_dotenv
import time

load_dotenv()

# ===== 1. å®šä¹‰ç®¡é“ =====
prompt = ChatPromptTemplate.from_template("å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±è¯­ï¼š{text}")
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
parser = StrOutputParser()

chain = prompt | llm | parser

# ===== 2. å•æ¬¡è°ƒç”¨ vs æ‰¹å¤„ç† =====
print("=== æ€§èƒ½å¯¹æ¯” ===")

texts = [
    "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é›†",
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
    "è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†æ–‡æœ¬æ•°æ®",
    "è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒæ•°æ®"
]

# å•æ¬¡è°ƒç”¨
print("\n--- å•æ¬¡è°ƒç”¨ ---")
start = time.time()
results_single = []
for text in texts:
    result = chain.invoke({"text": text})
    results_single.append(result)
elapsed_single = time.time() - start
print(f"è€—æ—¶: {elapsed_single:.2f}ç§’")
print(f"ç»“æœæ•°é‡: {len(results_single)}")

# æ‰¹å¤„ç†
print("\n--- æ‰¹å¤„ç† ---")
start = time.time()
inputs = [{"text": text} for text in texts]
results_batch = chain.batch(inputs)
elapsed_batch = time.time() - start
print(f"è€—æ—¶: {elapsed_batch:.2f}ç§’")
print(f"ç»“æœæ•°é‡: {len(results_batch)}")

# æ€§èƒ½æå‡
print(f"\næ€§èƒ½æå‡: {(elapsed_single - elapsed_batch) / elapsed_single * 100:.1f}%")

# ===== 3. æ˜¾ç¤ºç»“æœ =====
print("\n=== ç¿»è¯‘ç»“æœ ===")
for i, (original, translated) in enumerate(zip(texts, results_batch), 1):
    print(f"\n{i}. åŸæ–‡: {original}")
    print(f"   è¯‘æ–‡: {translated}")

# ===== 4. æ‰¹å¤„ç†é…ç½® =====
print("\n=== æ‰¹å¤„ç†é…ç½® ===")

# ä½¿ç”¨é…ç½®æ§åˆ¶å¹¶å‘
config = {
    "max_concurrency": 3  # æœ€å¤š3ä¸ªå¹¶å‘è¯·æ±‚
}

start = time.time()
results_configured = chain.batch(inputs, config=config)
elapsed_configured = time.time() - start
print(f"é…ç½®å¹¶å‘åè€—æ—¶: {elapsed_configured:.2f}ç§’")
```

### è¿è¡Œè¾“å‡º

```
=== æ€§èƒ½å¯¹æ¯” ===

--- å•æ¬¡è°ƒç”¨ ---
è€—æ—¶: 8.45ç§’
ç»“æœæ•°é‡: 5

--- æ‰¹å¤„ç† ---
è€—æ—¶: 3.21ç§’
ç»“æœæ•°é‡: 5

æ€§èƒ½æå‡: 62.0%

=== ç¿»è¯‘ç»“æœ ===

1. åŸæ–‡: äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ
   è¯‘æ–‡: Artificial intelligence is changing the world

2. åŸæ–‡: æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é›†
   è¯‘æ–‡: Machine learning is a subset of AI

3. åŸæ–‡: æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ
   è¯‘æ–‡: Deep learning uses neural networks

4. åŸæ–‡: è‡ªç„¶è¯­è¨€å¤„ç†å¤„ç†æ–‡æœ¬æ•°æ®
   è¯‘æ–‡: Natural language processing handles text data

5. åŸæ–‡: è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒæ•°æ®
   è¯‘æ–‡: Computer vision processes image data

=== æ‰¹å¤„ç†é…ç½® ===
é…ç½®å¹¶å‘åè€—æ—¶: 3.18ç§’
```

---

## ç¤ºä¾‹5ï¼šæµå¼æ‰§è¡Œ

### ä»£ç 

```python
"""
ç¤ºä¾‹5ï¼šæµå¼æ‰§è¡Œ
æ¼”ç¤ºï¼šé€æ­¥è¾“å‡ºç»“æœï¼Œé™ä½æ„ŸçŸ¥å»¶è¿Ÿ
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from dotenv import load_dotenv
import time

load_dotenv()

# ===== 1. å®šä¹‰ç®¡é“ =====
prompt = ChatPromptTemplate.from_template("å†™ä¸€ç¯‡å…³äº{topic}çš„çŸ­æ–‡ï¼ˆ100å­—å·¦å³ï¼‰")
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7, streaming=True)
parser = StrOutputParser()

chain = prompt | llm | parser

# ===== 2. éæµå¼æ‰§è¡Œ =====
print("=== éæµå¼æ‰§è¡Œ ===")
print("ç­‰å¾…å®Œæ•´å“åº”...\n")

start = time.time()
result = chain.invoke({"topic": "äººå·¥æ™ºèƒ½"})
elapsed = time.time() - start

print(result)
print(f"\næ€»è€—æ—¶: {elapsed:.2f}ç§’")
print(f"é¦–å­—èŠ‚æ—¶é—´: {elapsed:.2f}ç§’ï¼ˆç­‰å¾…å®Œæ•´å“åº”ï¼‰")

# ===== 3. æµå¼æ‰§è¡Œ =====
print("\n\n=== æµå¼æ‰§è¡Œ ===")
print("é€æ­¥è¾“å‡º...\n")

start = time.time()
first_chunk_time = None
chunk_count = 0

for chunk in chain.stream({"topic": "äººå·¥æ™ºèƒ½"}):
    if first_chunk_time is None:
        first_chunk_time = time.time() - start
    chunk_count += 1
    print(chunk, end="", flush=True)

elapsed = time.time() - start

print(f"\n\næ€»è€—æ—¶: {elapsed:.2f}ç§’")
print(f"é¦–å­—èŠ‚æ—¶é—´: {first_chunk_time:.2f}ç§’")
print(f"å»¶è¿Ÿé™ä½: {(elapsed - first_chunk_time) / elapsed * 100:.1f}%")
print(f"æ€»å—æ•°: {chunk_count}")

# ===== 4. æµå¼æ‰§è¡Œçš„å®é™…åº”ç”¨ =====
print("\n\n=== å®é™…åº”ç”¨ï¼šå®æ—¶å¯¹è¯ ===")

def chat_with_streaming(question: str):
    """æ¨¡æ‹Ÿå®æ—¶å¯¹è¯"""
    prompt = ChatPromptTemplate.from_template("ç®€çŸ­å›ç­”ï¼š{question}")
    llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)
    parser = StrOutputParser()
    chain = prompt | llm | parser

    print(f"ç”¨æˆ·: {question}")
    print("åŠ©æ‰‹: ", end="", flush=True)

    for chunk in chain.stream({"question": question}):
        print(chunk, end="", flush=True)
        time.sleep(0.02)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ

    print("\n")

# æµ‹è¯•å¯¹è¯
questions = [
    "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
    "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
    "AIçš„æœªæ¥æ˜¯ä»€ä¹ˆï¼Ÿ"
]

for question in questions:
    chat_with_streaming(question)
```

### è¿è¡Œè¾“å‡º

```
=== éæµå¼æ‰§è¡Œ ===
ç­‰å¾…å®Œæ•´å“åº”...

äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚
é€šè¿‡æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯ï¼ŒAIå¯ä»¥å¤„ç†å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå†³ç­–åˆ¶å®šã€‚
éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒAIæ­£åœ¨æ”¹å˜å„è¡Œå„ä¸šï¼Œä»åŒ»ç–—åˆ°é‡‘èï¼Œæå‡æ•ˆç‡å’Œåˆ›æ–°èƒ½åŠ›ã€‚

æ€»è€—æ—¶: 2.45ç§’
é¦–å­—èŠ‚æ—¶é—´: 2.45ç§’ï¼ˆç­‰å¾…å®Œæ•´å“åº”ï¼‰


=== æµå¼æ‰§è¡Œ ===
é€æ­¥è¾“å‡º...

äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚
é€šè¿‡æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯ï¼ŒAIå¯ä»¥å¤„ç†å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå†³ç­–åˆ¶å®šã€‚
éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼ŒAIæ­£åœ¨æ”¹å˜å„è¡Œå„ä¸šï¼Œä»åŒ»ç–—åˆ°é‡‘èï¼Œæå‡æ•ˆç‡å’Œåˆ›æ–°èƒ½åŠ›ã€‚

æ€»è€—æ—¶: 2.43ç§’
é¦–å­—èŠ‚æ—¶é—´: 0.78ç§’
å»¶è¿Ÿé™ä½: 67.9%
æ€»å—æ•°: 45


=== å®é™…åº”ç”¨ï¼šå®æ—¶å¯¹è¯ ===
ç”¨æˆ·: ä»€ä¹ˆæ˜¯Pythonï¼Ÿ
åŠ©æ‰‹: Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»å...

ç”¨æˆ·: å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ
åŠ©æ‰‹: å­¦ä¹ ç¼–ç¨‹éœ€è¦ä»åŸºç¡€å¼€å§‹ï¼Œé€‰æ‹©ä¸€é—¨è¯­è¨€ï¼ˆå¦‚Pythonï¼‰ï¼Œé€šè¿‡å®è·µé¡¹ç›®å·©å›ºçŸ¥è¯†...

ç”¨æˆ·: AIçš„æœªæ¥æ˜¯ä»€ä¹ˆï¼Ÿ
åŠ©æ‰‹: AIçš„æœªæ¥å°†æ›´åŠ æ™ºèƒ½åŒ–å’Œæ™®åŠåŒ–ï¼Œåº”ç”¨äºæ›´å¤šé¢†åŸŸï¼ŒåŒæ—¶ä¹Ÿéœ€è¦å…³æ³¨ä¼¦ç†å’Œå®‰å…¨é—®é¢˜...
```

---

## ç¤ºä¾‹6ï¼šç»„åˆå¤šä¸ªç®¡é“

### ä»£ç 

```python
"""
ç¤ºä¾‹6ï¼šç»„åˆå¤šä¸ªç®¡é“
æ¼”ç¤ºï¼šå°†å°ç®¡é“ç»„åˆæˆå¤§ç®¡é“
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv

load_dotenv()

# ===== 1. å®šä¹‰åŸºç¡€ç»„ä»¶ =====
print("=== å®šä¹‰åŸºç¡€ç»„ä»¶ ===")

# ç»„ä»¶1ï¼šæ–‡æœ¬æ¸…ç†
def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬"""
    return text.strip().lower()

text_cleaner = RunnableLambda(clean_text)

# ç»„ä»¶2ï¼šç¿»è¯‘
translate_prompt = ChatPromptTemplate.from_template("ç¿»è¯‘æˆè‹±è¯­ï¼š{text}")
translate_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
translate_parser = StrOutputParser()

translator = translate_prompt | translate_llm | translate_parser

# ç»„ä»¶3ï¼šæ€»ç»“
summarize_prompt = ChatPromptTemplate.from_template("ç”¨ä¸€å¥è¯æ€»ç»“ï¼š{text}")
summarize_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
summarize_parser = StrOutputParser()

summarizer = summarize_prompt | summarize_llm | summarize_parser

# ===== 2. ç»„åˆç®¡é“ =====
print("\n=== ç»„åˆç®¡é“ ===")

# æ–¹å¼1ï¼šç›´æ¥ç»„åˆ
pipeline1 = text_cleaner | translator | summarizer
print(f"Pipeline 1: cleaner â†’ translator â†’ summarizer")

# æ–¹å¼2ï¼šåˆ†æ­¥ç»„åˆ
step1 = text_cleaner | translator
step2 = step1 | summarizer
print(f"Pipeline 2: (cleaner â†’ translator) â†’ summarizer")

# ===== 3. æµ‹è¯•ç®¡é“ =====
print("\n=== æµ‹è¯•ç®¡é“ ===")

input_text = """
  äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚
  é€šè¿‡æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯ï¼ŒAIå¯ä»¥å¤„ç†å¤æ‚çš„ä»»åŠ¡ã€‚
"""

print(f"åŸå§‹è¾“å…¥:\n{input_text}")

# é€æ­¥æŸ¥çœ‹ç»“æœ
print("\n--- æ­¥éª¤1ï¼šæ¸…ç† ---")
cleaned = text_cleaner.invoke(input_text)
print(f"æ¸…ç†å: {cleaned[:50]}...")

print("\n--- æ­¥éª¤2ï¼šç¿»è¯‘ ---")
translated = translator.invoke({"text": cleaned})
print(f"ç¿»è¯‘å: {translated[:100]}...")

print("\n--- æ­¥éª¤3ï¼šæ€»ç»“ ---")
summarized = summarizer.invoke({"text": translated})
print(f"æ€»ç»“: {summarized}")

# å®Œæ•´ç®¡é“
print("\n--- å®Œæ•´ç®¡é“ ---")
# æ³¨æ„ï¼šéœ€è¦åŒ…è£…è¾“å…¥ä¸ºå­—å…¸
wrapped_pipeline = (
    RunnableLambda(lambda x: {"text": clean_text(x)})
    | translator
    | RunnableLambda(lambda x: {"text": x})
    | summarizer
)

result = wrapped_pipeline.invoke(input_text)
print(f"æœ€ç»ˆç»“æœ: {result}")
```

### è¿è¡Œè¾“å‡º

```
=== å®šä¹‰åŸºç¡€ç»„ä»¶ ===

=== ç»„åˆç®¡é“ ===
Pipeline 1: cleaner â†’ translator â†’ summarizer
Pipeline 2: (cleaner â†’ translator) â†’ summarizer

=== æµ‹è¯•ç®¡é“ ===
åŸå§‹è¾“å…¥:

  äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚
  é€šè¿‡æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯ï¼ŒAIå¯ä»¥å¤„ç†å¤æ‚çš„ä»»åŠ¡ã€‚


--- æ­¥éª¤1ï¼šæ¸…ç† ---
æ¸…ç†å: äººå·¥æ™ºèƒ½ï¼ˆaiï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿ...

--- æ­¥éª¤2ï¼šç¿»è¯‘ ---
ç¿»è¯‘å: Artificial Intelligence (AI) is a branch of computer science dedicated to creating systems...

--- æ­¥éª¤3ï¼šæ€»ç»“ ---
æ€»ç»“: AI is a computer science field focused on creating intelligent systems using machine learning.

--- å®Œæ•´ç®¡é“ ---
æœ€ç»ˆç»“æœ: AI is a computer science field focused on creating intelligent systems using machine learning.
```

---

## æ€»ç»“

### æ ¸å¿ƒæ¨¡å¼

1. **åŸºç¡€ä¸‰æ®µå¼**ï¼š`prompt | llm | parser`
2. **å¤šå˜é‡è¾“å…¥**ï¼šä½¿ç”¨å­—å…¸ä¼ é€’å¤šä¸ªå˜é‡
3. **é”™è¯¯å¤„ç†**ï¼šä½¿ç”¨ try-except æˆ–è‡ªå®šä¹‰åŒ…è£…å™¨
4. **æ‰¹å¤„ç†**ï¼šä½¿ç”¨ `.batch()` æå‡æ€§èƒ½
5. **æµå¼æ‰§è¡Œ**ï¼šä½¿ç”¨ `.stream()` é™ä½å»¶è¿Ÿ
6. **ç®¡é“ç»„åˆ**ï¼šå°ç®¡é“ç»„åˆæˆå¤§ç®¡é“

### æ€§èƒ½æ•°æ®

| æ¨¡å¼ | æ€§èƒ½æå‡ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|
| æ‰¹å¤„ç† | 40-60% | æ‰¹é‡å¤„ç†å¤šä¸ªè¾“å…¥ |
| æµå¼æ‰§è¡Œ | 67% å»¶è¿Ÿé™ä½ | å®æ—¶å¯¹è¯ã€é•¿æ–‡æœ¬ç”Ÿæˆ |
| ç®¡é“ç»„åˆ | ä»£ç å¤ç”¨ | å¤æ‚å·¥ä½œæµ |

### æœ€ä½³å®è·µ

1. **å§‹ç»ˆåŠ è½½ç¯å¢ƒå˜é‡**ï¼š`load_dotenv()`
2. **ä½¿ç”¨ç±»å‹æç¤º**ï¼šæé«˜ä»£ç å¯è¯»æ€§
3. **æ·»åŠ é”™è¯¯å¤„ç†**ï¼šç”Ÿäº§ç¯å¢ƒå¿…éœ€
4. **é€‰æ‹©åˆé€‚çš„æ‰§è¡Œæ¨¡å¼**ï¼šæ ¹æ®åœºæ™¯é€‰æ‹© invoke/batch/stream
5. **é€æ­¥è°ƒè¯•**ï¼šé‡åˆ°é—®é¢˜æ—¶é€æ­¥æµ‹è¯•æ¯ä¸ªç»„ä»¶

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-19
**ç»´æŠ¤è€…**: Claude Code

**ä¸‹ä¸€æ­¥**: é˜…è¯» `07_å®æˆ˜ä»£ç _02_å¤æ‚æ•°æ®æµè½¬.md` å­¦ä¹ æ›´å¤æ‚çš„æ•°æ®è½¬æ¢æ¨¡å¼
