# 核心概念3：生产环境最佳实践

> 掌握后台任务在生产环境中的错误处理、监控、性能优化和可靠性保障

---

## 1. 错误处理策略

### 1.1 基础错误处理

**问题：** 后台任务失败不会影响 HTTP 响应，但会导致操作未完成

```python
# ❌ 没有错误处理
def send_email(email: str):
    # 如果这里抛出异常，任务静默失败
    smtp_client.send(email, "欢迎")

@app.post("/register")
async def register(email: str, bg: BackgroundTasks):
    bg.add_task(send_email, email)
    return {"status": "ok"}
    # 用户以为邮件发送了，实际可能失败了
```

**解决方案：** 添加 try-except 和日志

```python
import logging

logger = logging.getLogger(__name__)

# ✅ 有错误处理
def send_email(email: str):
    try:
        smtp_client.send(email, "欢迎")
        logger.info(f"邮件发送成功: {email}")
    except Exception as e:
        logger.error(f"邮件发送失败: {email}, 错误: {e}")
        # 可选：记录到数据库，稍后重试
```

### 1.2 装饰器模式

**创建通用的错误处理装饰器：**

```python
from functools import wraps
import asyncio
import logging

logger = logging.getLogger(__name__)

def safe_background_task(func):
    """装饰器：为后台任务添加错误处理"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            logger.error(
                f"后台任务失败: {func.__name__}",
                extra={
                    "function": func.__name__,
                    "args": args,
                    "kwargs": kwargs,
                    "error": str(e)
                },
                exc_info=True
            )
            # 可选：记录失败任务
            await save_failed_task(func.__name__, args, kwargs, str(e))

    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(
                f"后台任务失败: {func.__name__}",
                extra={
                    "function": func.__name__,
                    "args": args,
                    "kwargs": kwargs,
                    "error": str(e)
                },
                exc_info=True
            )

    # 根据函数类型返回对应的包装器
    if asyncio.iscoroutinefunction(func):
        return async_wrapper
    else:
        return sync_wrapper

# 使用装饰器
@safe_background_task
async def send_email(email: str):
    # 任务逻辑
    await smtp_client.send(email, "欢迎")

@safe_background_task
def write_log(message: str):
    # 任务逻辑
    with open("log.txt", "a") as f:
        f.write(f"{message}\n")
```

### 1.3 失败任务记录和重试

**记录失败任务到数据库：**

```python
from sqlalchemy import Column, Integer, String, DateTime, Text
from datetime import datetime

class FailedTask(Base):
    __tablename__ = "failed_tasks"

    id = Column(Integer, primary_key=True)
    function_name = Column(String(255))
    args = Column(Text)  # JSON 序列化的参数
    kwargs = Column(Text)  # JSON 序列化的参数
    error_message = Column(Text)
    retry_count = Column(Integer, default=0)
    created_at = Column(DateTime, default=datetime.now)
    last_retry_at = Column(DateTime, nullable=True)

async def save_failed_task(
    function_name: str,
    args: tuple,
    kwargs: dict,
    error: str
):
    """保存失败的任务到数据库"""
    db = SessionLocal()
    try:
        import json
        failed_task = FailedTask(
            function_name=function_name,
            args=json.dumps(args),
            kwargs=json.dumps(kwargs),
            error_message=error
        )
        db.add(failed_task)
        db.commit()
    finally:
        db.close()
```

**定期重试失败任务：**

```python
import json

async def retry_failed_tasks():
    """定期重试失败的任务（可以用定时任务调用）"""
    db = SessionLocal()
    try:
        # 获取重试次数 < 3 的失败任务
        failed_tasks = db.query(FailedTask).filter(
            FailedTask.retry_count < 3
        ).all()

        for task in failed_tasks:
            try:
                # 反序列化参数
                args = json.loads(task.args)
                kwargs = json.loads(task.kwargs)

                # 获取函数
                func = globals()[task.function_name]

                # 执行任务
                if asyncio.iscoroutinefunction(func):
                    await func(*args, **kwargs)
                else:
                    func(*args, **kwargs)

                # 成功：删除记录
                db.delete(task)
                db.commit()
                logger.info(f"重试成功: {task.function_name}")

            except Exception as e:
                # 失败：增加重试次数
                task.retry_count += 1
                task.last_retry_at = datetime.now()
                db.commit()
                logger.error(f"重试失败: {task.function_name}, 错误: {e}")
    finally:
        db.close()
```

---

## 2. 结构化日志

### 2.1 使用 structlog

**安装：**
```bash
uv add structlog
```

**配置：**
```python
import structlog

# 配置 structlog
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()
```

**在后台任务中使用：**
```python
@safe_background_task
async def send_email(email: str, subject: str):
    logger.info(
        "开始发送邮件",
        email=email,
        subject=subject,
        task="send_email"
    )

    try:
        await smtp_client.send(email, subject)

        logger.info(
            "邮件发送成功",
            email=email,
            subject=subject,
            task="send_email"
        )
    except Exception as e:
        logger.error(
            "邮件发送失败",
            email=email,
            subject=subject,
            error=str(e),
            task="send_email"
        )
        raise
```

**日志输出示例：**
```json
{
  "event": "开始发送邮件",
  "email": "user@example.com",
  "subject": "欢迎",
  "task": "send_email",
  "timestamp": "2026-02-11T10:30:00.123456Z",
  "level": "info"
}
```

### 2.2 日志最佳实践

**记录关键信息：**
```python
@safe_background_task
async def process_document(file_path: str, user_id: int):
    start_time = time.time()

    logger.info(
        "开始处理文档",
        file_path=file_path,
        user_id=user_id,
        task="process_document"
    )

    try:
        # 1. 解析文档
        text = extract_text(file_path)
        logger.info(
            "文档解析完成",
            file_path=file_path,
            text_length=len(text)
        )

        # 2. 生成 Embedding
        embedding = await generate_embedding(text)
        logger.info(
            "Embedding 生成完成",
            file_path=file_path,
            embedding_dim=len(embedding)
        )

        # 3. 存储到向量数据库
        await store_to_vectordb(user_id, embedding)

        duration = time.time() - start_time
        logger.info(
            "文档处理完成",
            file_path=file_path,
            user_id=user_id,
            duration=duration,
            task="process_document"
        )

    except Exception as e:
        duration = time.time() - start_time
        logger.error(
            "文档处理失败",
            file_path=file_path,
            user_id=user_id,
            duration=duration,
            error=str(e),
            task="process_document"
        )
        raise
```

---

## 3. 监控和指标

### 3.1 任务执行时间监控

**使用装饰器记录执行时间：**
```python
import time
from functools import wraps

def monitor_task_duration(func):
    """监控任务执行时间"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time

            logger.info(
                "任务执行完成",
                function=func.__name__,
                duration=duration,
                status="success"
            )

            # 可选：发送到监控系统（如 Prometheus）
            # metrics.task_duration.labels(
            #     task=func.__name__,
            #     status="success"
            # ).observe(duration)

            return result
        except Exception as e:
            duration = time.time() - start_time

            logger.error(
                "任务执行失败",
                function=func.__name__,
                duration=duration,
                status="failed",
                error=str(e)
            )

            # metrics.task_duration.labels(
            #     task=func.__name__,
            #     status="failed"
            # ).observe(duration)

            raise

    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time

            logger.info(
                "任务执行完成",
                function=func.__name__,
                duration=duration,
                status="success"
            )

            return result
        except Exception as e:
            duration = time.time() - start_time

            logger.error(
                "任务执行失败",
                function=func.__name__,
                duration=duration,
                status="failed",
                error=str(e)
            )

            raise

    if asyncio.iscoroutinefunction(func):
        return async_wrapper
    else:
        return sync_wrapper

# 使用
@monitor_task_duration
@safe_background_task
async def send_email(email: str):
    await smtp_client.send(email, "欢迎")
```

### 3.2 任务成功率监控

**记录任务执行统计：**
```python
from collections import defaultdict
from datetime import datetime, timedelta

class TaskMetrics:
    """任务指标收集器"""
    def __init__(self):
        self.success_count = defaultdict(int)
        self.failure_count = defaultdict(int)
        self.total_duration = defaultdict(float)

    def record_success(self, task_name: str, duration: float):
        self.success_count[task_name] += 1
        self.total_duration[task_name] += duration

    def record_failure(self, task_name: str, duration: float):
        self.failure_count[task_name] += 1
        self.total_duration[task_name] += duration

    def get_stats(self, task_name: str):
        total = self.success_count[task_name] + self.failure_count[task_name]
        if total == 0:
            return None

        return {
            "task": task_name,
            "total": total,
            "success": self.success_count[task_name],
            "failure": self.failure_count[task_name],
            "success_rate": self.success_count[task_name] / total,
            "avg_duration": self.total_duration[task_name] / total
        }

# 全局指标收集器
task_metrics = TaskMetrics()

def monitored_task(func):
    """监控任务的装饰器"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time
            task_metrics.record_success(func.__name__, duration)
            return result
        except Exception as e:
            duration = time.time() - start_time
            task_metrics.record_failure(func.__name__, duration)
            raise

    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            task_metrics.record_success(func.__name__, duration)
            return result
        except Exception as e:
            duration = time.time() - start_time
            task_metrics.record_failure(func.__name__, duration)
            raise

    if asyncio.iscoroutinefunction(func):
        return async_wrapper
    else:
        return sync_wrapper

# 提供监控端点
@app.get("/metrics/tasks")
async def get_task_metrics():
    """获取任务执行统计"""
    stats = {}
    for task_name in set(
        list(task_metrics.success_count.keys()) +
        list(task_metrics.failure_count.keys())
    ):
        stats[task_name] = task_metrics.get_stats(task_name)
    return stats
```

---

## 4. 性能优化

### 4.1 避免阻塞操作

**问题：** 同步阻塞操作会影响性能

```python
# ❌ 同步阻塞
def send_email(email: str):
    time.sleep(5)  # 阻塞5秒
    smtp_client.send(email, "欢迎")

# ✅ 异步非阻塞
async def send_email(email: str):
    await asyncio.sleep(5)  # 不阻塞
    await smtp_client.send_async(email, "欢迎")
```

### 4.2 批量处理

**场景：** 需要处理多个相似任务

```python
# ❌ 低效：每个任务单独处理
@app.post("/send-notifications")
async def send_notifications(
    user_ids: List[int],
    bg: BackgroundTasks
):
    for user_id in user_ids:
        bg.add_task(send_notification, user_id)
    return {"message": "通知已发送"}
    # 问题：100个用户 = 100个任务 = 顺序执行

# ✅ 高效：批量处理
async def send_notifications_batch(user_ids: List[int]):
    """批量发送通知"""
    # 并发发送
    await asyncio.gather(*[
        send_notification(user_id)
        for user_id in user_ids
    ])

@app.post("/send-notifications")
async def send_notifications(
    user_ids: List[int],
    bg: BackgroundTasks
):
    bg.add_task(send_notifications_batch, user_ids)
    return {"message": "通知已发送"}
    # 优势：1个任务 = 并发执行
```

### 4.3 资源池管理

**数据库连接池：**
```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# 配置连接池
engine = create_engine(
    "postgresql://user:password@localhost/dbname",
    pool_size=10,          # 连接池大小
    max_overflow=20,       # 最大溢出连接数
    pool_timeout=30,       # 获取连接超时
    pool_recycle=3600,     # 连接回收时间
)

SessionLocal = sessionmaker(bind=engine)

# 在后台任务中使用
@safe_background_task
async def process_data(data_id: int):
    db = SessionLocal()
    try:
        # 使用数据库连接
        data = db.query(Data).filter(Data.id == data_id).first()
        # 处理数据
    finally:
        db.close()  # 归还连接到池
```

---

## 5. 超时控制

### 5.1 任务超时

**使用 asyncio.wait_for：**
```python
import asyncio

async def long_running_task():
    # 可能很慢的操作
    await asyncio.sleep(100)

@safe_background_task
async def task_with_timeout():
    try:
        await asyncio.wait_for(
            long_running_task(),
            timeout=30.0  # 30秒超时
        )
    except asyncio.TimeoutError:
        logger.error("任务超时")
        # 处理超时情况
```

### 5.2 外部 API 调用超时

**使用 httpx 的超时配置：**
```python
import httpx

@safe_background_task
async def call_external_api(url: str):
    async with httpx.AsyncClient(timeout=10.0) as client:
        try:
            response = await client.post(url, json={"data": "value"})
            return response.json()
        except httpx.TimeoutException:
            logger.error(f"API 调用超时: {url}")
            raise
```

---

## 6. 资源清理

### 6.1 临时文件清理

```python
import os
import tempfile

@safe_background_task
async def process_uploaded_file(file_path: str):
    try:
        # 处理文件
        text = extract_text(file_path)
        embedding = await generate_embedding(text)
        await store_to_vectordb(embedding)

    finally:
        # 确保清理临时文件
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"临时文件已删除: {file_path}")
```

### 6.2 使用上下文管理器

```python
from contextlib import asynccontextmanager

@asynccontextmanager
async def temp_file_context(file_path: str):
    """临时文件上下文管理器"""
    try:
        yield file_path
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

@safe_background_task
async def process_file(file_path: str):
    async with temp_file_context(file_path) as path:
        # 处理文件
        text = extract_text(path)
        # 文件会自动清理
```

---

## 7. 告警机制

### 7.1 失败率告警

```python
from datetime import datetime, timedelta

class AlertManager:
    """告警管理器"""
    def __init__(self):
        self.failure_threshold = 0.1  # 失败率阈值 10%
        self.check_interval = timedelta(minutes=5)
        self.last_check = datetime.now()

    async def check_and_alert(self):
        """检查并发送告警"""
        now = datetime.now()
        if now - self.last_check < self.check_interval:
            return

        self.last_check = now

        # 检查各任务的失败率
        for task_name in task_metrics.success_count.keys():
            stats = task_metrics.get_stats(task_name)
            if stats and stats["success_rate"] < (1 - self.failure_threshold):
                await self.send_alert(
                    f"任务 {task_name} 失败率过高: {stats['success_rate']:.2%}"
                )

    async def send_alert(self, message: str):
        """发送告警（邮件/Slack/钉钉等）"""
        logger.critical(f"告警: {message}")
        # 实际发送告警的逻辑
        # await send_slack_message(message)

alert_manager = AlertManager()

# 在后台任务中定期检查
@app.on_event("startup")
async def start_alert_checker():
    async def check_loop():
        while True:
            await alert_manager.check_and_alert()
            await asyncio.sleep(60)  # 每分钟检查一次

    asyncio.create_task(check_loop())
```

---

## 8. 完整示例：生产级后台任务

```python
from fastapi import FastAPI, BackgroundTasks, UploadFile
from sqlalchemy.orm import Session
import structlog
import time
import asyncio
from functools import wraps

app = FastAPI()
logger = structlog.get_logger()

# ===== 1. 装饰器：错误处理 + 监控 =====
def production_task(func):
    """生产级后台任务装饰器"""
    @wraps(func)
    async def async_wrapper(*args, **kwargs):
        start_time = time.time()
        task_name = func.__name__

        logger.info(
            "任务开始",
            task=task_name,
            args=args,
            kwargs=kwargs
        )

        try:
            # 执行任务（带超时）
            result = await asyncio.wait_for(
                func(*args, **kwargs),
                timeout=300.0  # 5分钟超时
            )

            duration = time.time() - start_time

            logger.info(
                "任务成功",
                task=task_name,
                duration=duration
            )

            task_metrics.record_success(task_name, duration)
            return result

        except asyncio.TimeoutError:
            duration = time.time() - start_time

            logger.error(
                "任务超时",
                task=task_name,
                duration=duration
            )

            task_metrics.record_failure(task_name, duration)
            await save_failed_task(task_name, args, kwargs, "超时")

        except Exception as e:
            duration = time.time() - start_time

            logger.error(
                "任务失败",
                task=task_name,
                duration=duration,
                error=str(e),
                exc_info=True
            )

            task_metrics.record_failure(task_name, duration)
            await save_failed_task(task_name, args, kwargs, str(e))

    @wraps(func)
    def sync_wrapper(*args, **kwargs):
        # 同步版本（类似逻辑）
        pass

    if asyncio.iscoroutinefunction(func):
        return async_wrapper
    else:
        return sync_wrapper

# ===== 2. 生产级任务示例 =====
@production_task
async def process_document(file_path: str, user_id: int):
    """处理文档：解析 + 生成 Embedding + 存储"""
    try:
        # 1. 解析文档
        logger.info("开始解析文档", file_path=file_path)
        text = extract_text(file_path)

        # 2. 生成 Embedding
        logger.info("开始生成 Embedding", text_length=len(text))
        embedding = await generate_embedding(text)

        # 3. 存储到向量数据库
        logger.info("开始存储到向量数据库")
        await store_to_vectordb(user_id, embedding)

        logger.info("文档处理完成", file_path=file_path)

    finally:
        # 清理临时文件
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info("临时文件已删除", file_path=file_path)

# ===== 3. API 端点 =====
@app.post("/upload")
async def upload_document(
    file: UploadFile,
    user_id: int,
    background_tasks: BackgroundTasks
):
    # 保存文件
    file_path = f"/tmp/{file.filename}"
    with open(file_path, "wb") as f:
        f.write(await file.read())

    # 添加后台任务
    background_tasks.add_task(process_document, file_path, user_id)

    return {
        "message": "文档上传成功，正在后台处理",
        "filename": file.filename
    }

# ===== 4. 监控端点 =====
@app.get("/metrics/tasks")
async def get_task_metrics():
    """获取任务执行统计"""
    stats = {}
    for task_name in set(
        list(task_metrics.success_count.keys()) +
        list(task_metrics.failure_count.keys())
    ):
        stats[task_name] = task_metrics.get_stats(task_name)
    return stats

@app.get("/health")
async def health_check():
    """健康检查"""
    return {
        "status": "healthy",
        "tasks": {
            "total": sum(task_metrics.success_count.values()) +
                    sum(task_metrics.failure_count.values()),
            "success": sum(task_metrics.success_count.values()),
            "failure": sum(task_metrics.failure_count.values())
        }
    }
```

---

## 9. 部署检查清单

### 9.1 代码层面

- [ ] 所有后台任务都有错误处理
- [ ] 使用结构化日志（structlog）
- [ ] 添加任务执行时间监控
- [ ] 实现失败任务记录和重试
- [ ] 设置合理的超时时间
- [ ] 确保资源正确清理（文件、连接等）
- [ ] 任务是自包含的（不依赖请求上下文）

### 9.2 基础设施层面

- [ ] 配置日志收集（ELK/Loki）
- [ ] 设置监控告警（Prometheus/Grafana）
- [ ] 配置错误追踪（Sentry）
- [ ] 数据库连接池配置合理
- [ ] 设置合理的 Worker 数量
- [ ] 配置健康检查端点

### 9.3 运维层面

- [ ] 定期检查失败任务
- [ ] 监控任务执行时间趋势
- [ ] 设置失败率告警阈值
- [ ] 定期清理过期日志
- [ ] 备份失败任务数据

---

## 10. 总结

### 10.1 核心原则

1. **错误处理**：所有任务都要有 try-except
2. **日志记录**：使用结构化日志，记录关键信息
3. **监控指标**：记录执行时间、成功率
4. **超时控制**：设置合理的超时时间
5. **资源清理**：确保资源正确释放
6. **告警机制**：失败率过高时及时告警

### 10.2 生产环境模板

```python
# 生产级后台任务模板
@production_task  # 错误处理 + 监控 + 超时
async def my_task(param: str):
    """任务描述"""
    logger.info("任务开始", param=param)

    try:
        # 任务逻辑
        result = await do_something(param)

        logger.info("任务完成", result=result)
        return result

    finally:
        # 资源清理
        cleanup_resources()
```

### 10.3 关键指标

监控以下指标：
- 任务执行时间（P50, P95, P99）
- 任务成功率
- 任务失败原因分布
- 任务队列长度（如果使用 Celery）
- 资源使用情况（CPU、内存）

---

**版本：** v1.0
**最后更新：** 2026-02-11
