# 决策树原理 - 核心概念详解

> 深入理解决策树：分类树、CART算法、信息增益、剪枝

---

## 概述

决策树是一种可解释的机器学习模型，通过树形结构表示分类规则。在 AI Agent 中广泛应用于意图分类、决策推理等场景。

**学习目标：**
- 理解决策树的基本原理
- 掌握信息增益和基尼系数
- 理解 CART 算法
- 掌握剪枝技术
- 能够使用 sklearn 训练决策树

---

## 1. 决策树基础

### 1.1 定义

**决策树**：通过树形结构表示分类规则的监督学习模型

```python
# 决策树示例
#           [年龄 > 30?]
#           /          \
#         是            否
#        /              \
#  [收入 > 50K?]    [学历 = 本科?]
#    /      \          /        \
#  是        否       是         否
#  /          \       /           \
# 批准      拒绝    批准         拒绝
```

### 1.2 核心组成

**三种节点类型：**
1. **根节点**：第一个判断条件
2. **内部节点**：中间的判断条件
3. **叶子节点**：最终的分类结果

```python
class DecisionNode:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature      # 特征索引
        self.threshold = threshold  # 分裂阈值
        self.left = left           # 左子树（满足条件）
        self.right = right         # 右子树（不满足条件）
        self.value = value         # 叶子节点的类别
```

---

## 2. 信息增益

### 2.1 信息熵

**信息熵**：衡量数据的混乱程度

```python
import math

def entropy(labels):
    """计算信息熵"""
    n = len(labels)
    if n == 0:
        return 0
    
    # 统计每个类别的数量
    counts = {}
    for label in labels:
        counts[label] = counts.get(label, 0) + 1
    
    # 计算熵：H = -Σ p_i * log2(p_i)
    ent = 0
    for count in counts.values():
        p = count / n
        if p > 0:
            ent -= p * math.log2(p)
    
    return ent

# 示例
labels = [0, 0, 1, 1, 1]  # 混乱
print(f"熵: {entropy(labels):.3f}")  # 0.971

labels = [0, 0, 0, 0, 0]  # 纯净
print(f"熵: {entropy(labels):.3f}")  # 0.000
```

### 2.2 信息增益

**信息增益**：分裂前后熵的减少

```python
def information_gain(parent_labels, left_labels, right_labels):
    """计算信息增益"""
    n = len(parent_labels)
    n_left = len(left_labels)
    n_right = len(right_labels)
    
    # 分裂前的熵
    parent_entropy = entropy(parent_labels)
    
    # 分裂后的加权熵
    child_entropy = (n_left / n) * entropy(left_labels) + \
                    (n_right / n) * entropy(right_labels)
    
    # 信息增益 = 熵的减少
    return parent_entropy - child_entropy

# 示例
parent = [0, 0, 1, 1, 1]
left = [0, 0]
right = [1, 1, 1]

gain = information_gain(parent, left, right)
print(f"信息增益: {gain:.3f}")  # 0.971
```

---

## 3. CART 算法

### 3.1 基尼系数

**基尼系数**：衡量数据不纯度的另一种方法

```python
def gini(labels):
    """计算基尼系数"""
    n = len(labels)
    if n == 0:
        return 0
    
    counts = {}
    for label in labels:
        counts[label] = counts.get(label, 0) + 1
    
    # Gini = 1 - Σ p_i^2
    gini_val = 1.0
    for count in counts.values():
        p = count / n
        gini_val -= p * p
    
    return gini_val

# 示例
labels = [0, 0, 1, 1, 1]
print(f"基尼系数: {gini(labels):.3f}")  # 0.480
```

### 3.2 最佳分裂点

```python
def find_best_split(X, y):
    """找到最佳分裂点"""
    best_gain = 0
    best_feature = None
    best_threshold = None
    
    n_features = X.shape[1]
    
    for feature in range(n_features):
        # 获取该特征的所有唯一值
        thresholds = sorted(set(X[:, feature]))
        
        for threshold in thresholds:
            # 分裂数据
            left_mask = X[:, feature] <= threshold
            right_mask = ~left_mask
            
            left_y = y[left_mask]
            right_y = y[right_mask]
            
            # 计算信息增益
            gain = information_gain(y, left_y, right_y)
            
            if gain > best_gain:
                best_gain = gain
                best_feature = feature
                best_threshold = threshold
    
    return best_feature, best_threshold, best_gain
```

---

## 4. sklearn 决策树

### 4.1 基本使用

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 1. 加载数据
iris = load_iris()
X, y = iris.data, iris.target

# 2. 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. 训练决策树
clf = DecisionTreeClassifier(
    max_depth=3,           # 最大深度
    min_samples_split=2,   # 分裂所需的最小样本数
    min_samples_leaf=1,    # 叶子节点的最小样本数
    criterion='gini'       # 分裂标准：'gini' 或 'entropy'
)
clf.fit(X_train, y_train)

# 4. 预测
y_pred = clf.predict(X_test)

# 5. 评估
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print(f"准确率: {accuracy:.3f}")
```

### 4.2 可视化决策树

```python
from sklearn.tree import export_text, plot_tree
import matplotlib.pyplot as plt

# 文本形式
tree_rules = export_text(clf, feature_names=iris.feature_names)
print(tree_rules)

# 图形形式
plt.figure(figsize=(20, 10))
plot_tree(clf, feature_names=iris.feature_names, 
          class_names=iris.target_names, filled=True)
plt.show()
```

---

## 5. 剪枝技术

### 5.1 预剪枝

**预剪枝**：在构建树的过程中提前停止

```python
clf = DecisionTreeClassifier(
    max_depth=5,              # 限制最大深度
    min_samples_split=10,     # 分裂所需的最小样本数
    min_samples_leaf=5,       # 叶子节点的最小样本数
    max_leaf_nodes=20         # 最大叶子节点数
)
```

### 5.2 后剪枝

**后剪枝**：先构建完整的树，再剪掉不必要的分支

```python
# 使用 cost_complexity_pruning
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas

# 训练不同 alpha 的树
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

# 选择最佳 alpha
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]

# 找到测试集上表现最好的 alpha
best_idx = test_scores.index(max(test_scores))
best_alpha = ccp_alphas[best_idx]
print(f"最佳 alpha: {best_alpha:.4f}")
```

---

## 6. AI Agent 应用

### 6.1 意图分类

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# 训练数据
texts = [
    "查询天气", "今天天气怎么样", "明天会下雨吗",
    "设置闹钟", "明天早上7点叫我", "提醒我开会",
    "播放音乐", "放一首周杰伦的歌", "我想听音乐"
]
labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]  # 0=天气, 1=闹钟, 2=音乐

# 特征提取
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# 训练决策树
clf = DecisionTreeClassifier(max_depth=5)
clf.fit(X, labels)

# 预测
def predict_intent(text):
    X_new = vectorizer.transform([text])
    intent_id = clf.predict(X_new)[0]
    intents = ["天气查询", "闹钟设置", "音乐播放"]
    return intents[intent_id]

print(predict_intent("后天天气如何"))  # 天气查询
```

### 6.2 LLM 推理轨迹转决策树（PCE 2026）

```python
# PCE 框架：将 LLM 推理轨迹转化为决策树
# 参考：https://arxiv.org/html/2602.04326v1

class PCEConverter:
    """将 LLM 推理轨迹转换为决策树"""
    
    def __init__(self, llm_traces):
        self.traces = llm_traces
    
    def extract_decision_points(self):
        """提取决策点"""
        decision_points = []
        for trace in self.traces:
            # 分析 LLM 的推理步骤
            # 提取关键决策点
            pass
        return decision_points
    
    def build_tree(self, decision_points):
        """构建决策树"""
        # 使用决策点构建树
        pass
```

---

## 7. 2025-2026 最新进展

### 7.1 PCE 框架

**论文**：Converting LLM Reasoning Traces into Decision Trees (arxiv 2026)

**核心思想**：
- 将 LLM 的推理轨迹转化为可解释的决策树
- 提高 AI Agent 的可解释性
- 减少推理成本

### 7.2 ACT 框架

**论文**：Agentic Classification Tree (arxiv 2025)

**核心思想**：
- 结合决策树的可解释性和 LLM 的语义推理
- 适用于需要解释的分类任务

---

## 学习检查清单

- [ ] 理解决策树的基本原理
- [ ] 掌握信息熵和信息增益的计算
- [ ] 理解基尼系数
- [ ] 能够使用 sklearn 训练决策树
- [ ] 理解剪枝技术
- [ ] 能够应用决策树解决实际问题

---

## 参考资源

### 教程
- [Coursera - Machine Learning Algorithms](https://www.coursera.org/articles/machine-learning-algorithms) (2026)
- [GeeksforGeeks - Decision Tree](https://www.geeksforgeeks.org/decision-tree/)

### 最新研究
- [PCE: Converting LLM Reasoning Traces into Decision Trees](https://arxiv.org/html/2602.04326v1) (2026)
- [ACT: Agentic Classification Tree](https://arxiv.org/html/2509.26433v3) (2025)

---

**版本**: v1.0
**最后更新**: 2026-02-13
**适用于**: Python 3.13+, AI Agent 开发, RAG 系统
**学习时间**: 30-40 分钟
