# 核心概念12：RAG系统集成实践

> 将向量存储集成到完整RAG系统的最佳实践

---

## 概述

RAG系统的核心是向量存储，本文介绍端到端集成实践。

**核心流程**：
```
文档加载 → Chunking → Embedding → 向量存储 → 检索 → LLM生成
```

---

## 1. Embedding模型选择

### 1.1 主流模型对比

| 模型 | 维度 | 性能 | 成本 | 适用场景 |
|------|------|------|------|---------|
| **text-embedding-3-small** | 1536 | 高 | 低 | 通用推荐 |
| **text-embedding-3-large** | 3072 | 最高 | 中 | 高精度需求 |
| **all-MiniLM-L6-v2** | 384 | 中 | 免费 | 资源受限 |
| **multilingual-e5-large** | 1024 | 高 | 免费 | 多语言 |

---

### 1.2 选择建议

```python
from openai import OpenAI

client = OpenAI()

# 场景1：通用RAG（推荐）
def embed_text(text):
    return client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    ).data[0].embedding

# 场景2：多语言
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('intfloat/multilingual-e5-large')

# 场景3：资源受限
model = SentenceTransformer('all-MiniLM-L6-v2')
```

---

## 2. Chunking策略

### 2.1 固定大小分块

```python
def fixed_size_chunking(text, chunk_size=512, overlap=50):
    """固定大小分块"""
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap  # 重叠

    return chunks

# 优点：简单
# 缺点：可能切断语义
```

---

### 2.2 语义分块

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def semantic_chunking(text):
    """语义分块"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=50,
        separators=["\n\n", "\n", "。", ".", " "]
    )

    chunks = splitter.split_text(text)
    return chunks

# 优点：保持语义完整
# 缺点：chunk大小不均匀
```

---

### 2.3 推荐配置

```python
# 通用配置
chunk_config = {
    "chunk_size": 512,      # Token数量
    "chunk_overlap": 50,    # 10%重叠
    "separators": ["\n\n", "\n", "。", "."]
}

# 效果：召回率提升15%
```

---

## 3. 完整RAG流程

### 3.1 离线构建

```python
from openai import OpenAI
import chromadb

client = OpenAI()
chroma_client = chromadb.PersistentClient(path="./rag_db")
collection = chroma_client.get_or_create_collection("knowledge_base")

def build_knowledge_base(documents):
    """构建知识库"""

    for doc in documents:
        # 1. 分块
        chunks = semantic_chunking(doc['text'])

        # 2. Embedding
        embeddings = []
        for chunk in chunks:
            emb = client.embeddings.create(
                input=chunk,
                model="text-embedding-3-small"
            ).data[0].embedding
            embeddings.append(emb)

        # 3. 存储
        collection.add(
            documents=chunks,
            embeddings=embeddings,
            metadatas=[{"source": doc['source']} for _ in chunks],
            ids=[f"{doc['id']}_{i}" for i in range(len(chunks))]
        )

    print(f"知识库构建完成: {len(documents)}个文档")
```

---

### 3.2 在线检索

```python
def rag_query(question, top_k=3):
    """RAG查询"""

    # 1. 问题Embedding
    query_emb = client.embeddings.create(
        input=question,
        model="text-embedding-3-small"
    ).data[0].embedding

    # 2. 向量检索
    results = collection.query(
        query_embeddings=[query_emb],
        n_results=top_k
    )

    # 3. 构建上下文
    context = "\n\n".join(results['documents'][0])

    # 4. LLM生成
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "你是一个helpful助手，根据提供的上下文回答问题。"},
            {"role": "user", "content": f"上下文：\n{context}\n\n问题：{question}"}
        ]
    )

    return response.choices[0].message.content

# 使用
answer = rag_query("如何优化RAG系统？")
print(answer)
```

---

## 4. LangChain集成

### 4.1 完整示例

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. 加载文档
from langchain.document_loaders import TextLoader
loader = TextLoader("documents.txt")
documents = loader.load()

# 2. 分块
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=50
)
chunks = text_splitter.split_documents(documents)

# 3. 创建向量存储
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 4. 创建RAG链
llm = ChatOpenAI(model="gpt-4", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)

# 5. 查询
answer = qa_chain.run("如何优化RAG系统？")
print(answer)
```

---

## 5. 检索评估

### 5.1 关键指标

```python
def evaluate_retrieval(queries, ground_truth):
    """评估检索性能"""

    recalls = []
    precisions = []

    for query, truth in zip(queries, ground_truth):
        # 检索
        results = collection.query(
            query_texts=[query],
            n_results=10
        )

        retrieved_ids = set(results['ids'][0])
        truth_ids = set(truth)

        # 召回率
        recall = len(retrieved_ids & truth_ids) / len(truth_ids)
        recalls.append(recall)

        # 精确率
        precision = len(retrieved_ids & truth_ids) / len(retrieved_ids)
        precisions.append(precision)

    return {
        "recall@10": np.mean(recalls),
        "precision@10": np.mean(precisions)
    }
```

---

### 5.2 端到端评估

```python
def evaluate_rag(test_cases):
    """评估RAG系统"""

    scores = []

    for case in test_cases:
        question = case['question']
        expected_answer = case['answer']

        # RAG生成
        generated_answer = rag_query(question)

        # 评分（使用LLM）
        score_prompt = f"""
        问题：{question}
        期望答案：{expected_answer}
        生成答案：{generated_answer}

        评分（1-5）：
        """

        score = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": score_prompt}]
        ).choices[0].message.content

        scores.append(int(score))

    return np.mean(scores)
```

---

## 6. 生产优化

### 6.1 缓存策略

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_embedding(text):
    """缓存Embedding"""
    return client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    ).data[0].embedding

@lru_cache(maxsize=100)
def cached_retrieval(query_hash):
    """缓存检索结果"""
    return collection.query(query_texts=[query], n_results=3)
```

---

### 6.2 批量处理

```python
def batch_embed(texts, batch_size=100):
    """批量Embedding"""
    embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]

        response = client.embeddings.create(
            input=batch,
            model="text-embedding-3-small"
        )

        embeddings.extend([d.embedding for d in response.data])

    return embeddings
```

---

## 7. 常见问题

### Q1：如何处理长文档？

**A**：分层检索
```python
# 1. 粗粒度检索（章节级）
chapter_results = retrieve_chapters(query)

# 2. 细粒度检索（段落级）
paragraph_results = retrieve_paragraphs(query, chapters=chapter_results)
```

---

### Q2：如何提升召回率？

**A**：混合检索
```python
# Vector + BM25
from langchain.retrievers import EnsembleRetriever

ensemble = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.6, 0.4]
)
```

---

### Q3：如何减少幻觉？

**A**：引用来源
```python
prompt = f"""
根据以下上下文回答问题，并引用来源。

上下文：
{context}

问题：{question}

要求：
1. 只使用上下文中的信息
2. 引用具体段落
3. 如果上下文中没有答案，明确说明
"""
```

---

## 8. 最佳实践

### 8.1 Chunking

- ✅ chunk_size=512（经验最优）
- ✅ overlap=50（10%重叠）
- ✅ 使用语义分块

### 8.2 Embedding

- ✅ text-embedding-3-small（通用）
- ✅ 批量处理（降低成本）
- ✅ 缓存常见查询

### 8.3 检索

- ✅ top_k=3-5（平衡）
- ✅ 混合检索（Vector+BM25）
- ✅ 重排序（ReRank）

### 8.4 生成

- ✅ 明确指令（只用上下文）
- ✅ 引用来源
- ✅ 温度=0（减少幻觉）

---

## 总结

### 核心要点

1. **Embedding**：text-embedding-3-small
2. **Chunking**：512 tokens，10%重叠
3. **检索**：混合检索，top_k=3
4. **生成**：明确指令，引用来源
5. **评估**：召回率、端到端质量

### 完整流程

```
文档 → 分块(512) → Embedding → 向量存储
                                    ↓
查询 → Embedding → 混合检索(top_k=3) → 上下文
                                    ↓
                              LLM生成 → 答案
```

### 下一步

学习实战代码场景，动手实现完整RAG系统。
