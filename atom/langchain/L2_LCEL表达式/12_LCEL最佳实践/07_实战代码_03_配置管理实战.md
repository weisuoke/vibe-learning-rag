# 实战代码：配置管理实战

> **本节目标**：通过多环境部署项目，掌握RunnableConfig、configurable_fields和configurable_alternatives的实战应用。

---

## 一、项目场景

**多环境RAG系统**：
- 开发环境（dev）：详细日志、快速模型
- 测试环境（staging）：中等日志、标准模型
- 生产环境（prod）：最小日志、高质量模型
- A/B测试：动态切换Prompt版本
- 多租户：租户级配置隔离

**配置需求**：
- 环境变量管理
- 运行时配置覆盖
- 动态模型切换
- 租户配置隔离

---

## 二、环境准备

### 2.1 安装依赖

```bash
# 安装依赖
uv add langchain langchain-openai python-dotenv pydantic

# 创建环境配置文件
mkdir -p config
touch config/.env.dev
touch config/.env.staging
touch config/.env.prod
```

### 2.2 项目结构

```
config_management/
├── config/
│   ├── __init__.py
│   ├── base.py              # 基础配置
│   ├── environments.py      # 环境配置
│   ├── .env.dev            # 开发环境
│   ├── .env.staging        # 测试环境
│   └── .env.prod           # 生产环境
├── chains/
│   ├── __init__.py
│   ├── configurable.py     # 可配置链
│   └── factory.py          # 链工厂
├── main.py                 # 主程序
└── .env                    # 当前环境
```

---

## 三、基础配置管理

### 3.1 配置模型

```python
# config/base.py
from pydantic import BaseModel, Field
from typing import Literal
import os

class LLMConfig(BaseModel):
    """LLM配置"""
    model: str = Field(default="gpt-4o-mini", description="模型名称")
    temperature: float = Field(default=0, ge=0, le=2, description="温度")
    max_tokens: int = Field(default=2000, gt=0, le=8000, description="最大Token数")
    max_retries: int = Field(default=3, ge=0, le=10, description="最大重试次数")

class RetrievalConfig(BaseModel):
    """检索配置"""
    k: int = Field(default=5, gt=0, le=20, description="检索文档数")
    score_threshold: float = Field(default=0.7, ge=0, le=1, description="相似度阈值")

class LogConfig(BaseModel):
    """日志配置"""
    level: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = "INFO"
    format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

class AppConfig(BaseModel):
    """应用配置"""
    environment: Literal["dev", "staging", "prod"] = "dev"
    llm: LLMConfig = Field(default_factory=LLMConfig)
    retrieval: RetrievalConfig = Field(default_factory=RetrievalConfig)
    log: LogConfig = Field(default_factory=LogConfig)
    enable_cache: bool = True
    max_concurrency: int = Field(default=10, gt=0, le=100)

    class Config:
        """Pydantic配置"""
        validate_assignment = True


# 示例使用
if __name__ == "__main__":
    # 创建配置
    config = AppConfig(
        environment="prod",
        llm=LLMConfig(model="gpt-4o", temperature=0.3),
        retrieval=RetrievalConfig(k=3)
    )

    print(f"环境: {config.environment}")
    print(f"模型: {config.llm.model}")
    print(f"温度: {config.llm.temperature}")
    print(f"检索数: {config.retrieval.k}")
```

### 3.2 环境配置

```python
# config/environments.py
from dotenv import load_dotenv
from pathlib import Path
from config.base import AppConfig, LLMConfig, RetrievalConfig, LogConfig
import os

class EnvironmentConfig:
    """环境配置管理"""

    CONFIGS = {
        "dev": AppConfig(
            environment="dev",
            llm=LLMConfig(
                model="gpt-4o-mini",
                temperature=0,
                max_tokens=1000,
                max_retries=2
            ),
            retrieval=RetrievalConfig(k=3, score_threshold=0.6),
            log=LogConfig(level="DEBUG"),
            enable_cache=False,
            max_concurrency=5
        ),
        "staging": AppConfig(
            environment="staging",
            llm=LLMConfig(
                model="gpt-4o-mini",
                temperature=0,
                max_tokens=2000,
                max_retries=3
            ),
            retrieval=RetrievalConfig(k=5, score_threshold=0.7),
            log=LogConfig(level="INFO"),
            enable_cache=True,
            max_concurrency=10
        ),
        "prod": AppConfig(
            environment="prod",
            llm=LLMConfig(
                model="gpt-4o",
                temperature=0,
                max_tokens=4000,
                max_retries=5
            ),
            retrieval=RetrievalConfig(k=5, score_threshold=0.8),
            log=LogConfig(level="WARNING"),
            enable_cache=True,
            max_concurrency=50
        )
    }

    @classmethod
    def load(cls, environment: str = None) -> AppConfig:
        """加载环境配置

        Args:
            environment: 环境名称（dev/staging/prod）
                        如果为None，从环境变量ENV读取

        Returns:
            AppConfig: 应用配置
        """
        # 确定环境
        if environment is None:
            environment = os.getenv("ENV", "dev")

        # 加载环境变量文件
        env_file = Path(f"config/.env.{environment}")
        if env_file.exists():
            load_dotenv(env_file)

        # 获取配置
        config = cls.CONFIGS.get(environment)
        if not config:
            raise ValueError(f"未知环境: {environment}")

        return config


# 示例使用
if __name__ == "__main__":
    # 加载开发环境配置
    dev_config = EnvironmentConfig.load("dev")
    print(f"开发环境: {dev_config.llm.model}")

    # 加载生产环境配置
    prod_config = EnvironmentConfig.load("prod")
    print(f"生产环境: {prod_config.llm.model}")
```

---

## 四、可配置链实现

### 4.1 configurable_fields

```python
# chains/configurable.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import ConfigurableField

def create_configurable_chain():
    """创建可配置链

    可配置字段:
    - llm_model: 模型名称
    - llm_temperature: 温度
    - max_tokens: 最大Token数
    """
    # 定义可配置的LLM
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        max_tokens=2000
    ).configurable_fields(
        model=ConfigurableField(
            id="llm_model",
            name="LLM Model",
            description="The LLM model to use"
        ),
        temperature=ConfigurableField(
            id="llm_temperature",
            name="LLM Temperature",
            description="Controls randomness (0-2)"
        ),
        max_tokens=ConfigurableField(
            id="max_tokens",
            name="Max Tokens",
            description="Maximum tokens to generate"
        )
    )

    # 创建链
    prompt = ChatPromptTemplate.from_template("回答问题: {question}")
    chain = prompt | llm | StrOutputParser()

    return chain


# 示例使用
if __name__ == "__main__":
    chain = create_configurable_chain()

    # 使用默认配置
    result1 = chain.invoke({"question": "什么是LCEL?"})
    print(f"默认配置: {result1}")

    # 运行时覆盖配置
    result2 = chain.invoke(
        {"question": "什么是LCEL?"},
        config={
            "configurable": {
                "llm_model": "gpt-4o",
                "llm_temperature": 0.7,
                "max_tokens": 1000
            }
        }
    )
    print(f"自定义配置: {result2}")
```

### 4.2 configurable_alternatives

```python
# 模型切换
from langchain_anthropic import ChatAnthropic
from langchain_community.llms import Ollama

def create_multi_model_chain():
    """创建多模型可切换链

    支持的模型:
    - openai: GPT-4o (默认)
    - anthropic: Claude-3-Sonnet
    - local: Llama2 (本地)
    """
    # 定义可切换的LLM
    llm = ChatOpenAI(model="gpt-4o").configurable_alternatives(
        ConfigurableField(id="llm_provider"),
        default_key="openai",
        anthropic=ChatAnthropic(model="claude-3-sonnet-20240229"),
        local=Ollama(model="llama2")
    )

    # 创建链
    prompt = ChatPromptTemplate.from_template("回答: {question}")
    chain = prompt | llm | StrOutputParser()

    return chain


# 示例使用
if __name__ == "__main__":
    chain = create_multi_model_chain()

    # 使用OpenAI (默认)
    result1 = chain.invoke({"question": "什么是LCEL?"})
    print(f"OpenAI: {result1}")

    # 切换到Anthropic
    result2 = chain.invoke(
        {"question": "什么是LCEL?"},
        config={"configurable": {"llm_provider": "anthropic"}}
    )
    print(f"Anthropic: {result2}")

    # 切换到本地模型
    result3 = chain.invoke(
        {"question": "什么是LCEL?"},
        config={"configurable": {"llm_provider": "local"}}
    )
    print(f"Local: {result3}")
```

---

## 五、链工厂模式

### 5.1 工厂实现

```python
# chains/factory.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import ConfigurableField
from config.base import AppConfig

class ChainFactory:
    """链工厂"""

    def __init__(self, config: AppConfig):
        """初始化工厂

        Args:
            config: 应用配置
        """
        self.config = config

    def create_qa_chain(self):
        """创建问答链"""
        # 使用配置创建LLM
        llm = ChatOpenAI(
            model=self.config.llm.model,
            temperature=self.config.llm.temperature,
            max_tokens=self.config.llm.max_tokens,
            max_retries=self.config.llm.max_retries
        ).configurable_fields(
            model=ConfigurableField(id="llm_model"),
            temperature=ConfigurableField(id="llm_temperature")
        )

        # 创建链
        prompt = ChatPromptTemplate.from_template(
            "回答问题: {question}"
        )
        chain = prompt | llm | StrOutputParser()

        # 添加配置标记
        chain = chain.with_config(
            run_name=f"qa_chain_{self.config.environment}",
            tags=[self.config.environment, "qa"],
            metadata={"environment": self.config.environment}
        )

        return chain

    def create_rag_chain(self, vectorstore):
        """创建RAG链"""
        from langchain_core.runnables import RunnablePassthrough

        # 检索器
        retriever = vectorstore.as_retriever(
            search_kwargs={
                "k": self.config.retrieval.k,
                "score_threshold": self.config.retrieval.score_threshold
            }
        )

        # LLM
        llm = ChatOpenAI(
            model=self.config.llm.model,
            temperature=self.config.llm.temperature,
            max_tokens=self.config.llm.max_tokens
        )

        # Prompt
        prompt = ChatPromptTemplate.from_template(
            """基于以下上下文回答问题:

上下文: {context}

问题: {question}

回答:"""
        )

        # 组合链
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        # 添加配置
        chain = chain.with_config(
            run_name=f"rag_chain_{self.config.environment}",
            tags=[self.config.environment, "rag"],
            max_concurrency=self.config.max_concurrency
        )

        return chain


# 示例使用
if __name__ == "__main__":
    from config.environments import EnvironmentConfig

    # 开发环境
    dev_config = EnvironmentConfig.load("dev")
    dev_factory = ChainFactory(dev_config)
    dev_chain = dev_factory.create_qa_chain()

    # 生产环境
    prod_config = EnvironmentConfig.load("prod")
    prod_factory = ChainFactory(prod_config)
    prod_chain = prod_factory.create_qa_chain()

    # 测试
    question = {"question": "什么是LCEL?"}
    print(f"开发环境: {dev_chain.invoke(question)}")
    print(f"生产环境: {prod_chain.invoke(question)}")
```

---

## 六、A/B测试实现

### 6.1 Prompt版本管理

```python
# A/B测试配置
class ABTestConfig:
    """A/B测试配置"""

    PROMPT_VERSIONS = {
        "v1": ChatPromptTemplate.from_template(
            "简洁回答: {question}"
        ),
        "v2": ChatPromptTemplate.from_template(
            """请详细回答以下问题:

问题: {question}

要求:
- 结构清晰
- 包含示例
- 易于理解"""
        )
    }

    def __init__(self):
        self.user_variants = {}  # user_id -> variant

    def get_variant(self, user_id: str) -> str:
        """获取用户的变体

        使用一致性哈希分配
        """
        if user_id not in self.user_variants:
            # 基于user_id哈希分配
            variant = "v1" if hash(user_id) % 2 == 0 else "v2"
            self.user_variants[user_id] = variant

        return self.user_variants[user_id]

    def create_chain(self, user_id: str):
        """为用户创建链"""
        variant = self.get_variant(user_id)
        prompt = self.PROMPT_VERSIONS[variant]

        llm = ChatOpenAI(model="gpt-4o-mini")
        chain = prompt | llm | StrOutputParser()

        # 添加变体标记
        chain = chain.with_config(
            tags=["ab_test", f"variant_{variant}"],
            metadata={"variant": variant, "user_id": user_id}
        )

        return chain


# 示例使用
if __name__ == "__main__":
    ab_test = ABTestConfig()

    # 用户A
    chain_a = ab_test.create_chain("user_123")
    result_a = chain_a.invoke({"question": "什么是LCEL?"})
    print(f"用户A (变体: {ab_test.get_variant('user_123')}): {result_a}")

    # 用户B
    chain_b = ab_test.create_chain("user_456")
    result_b = chain_b.invoke({"question": "什么是LCEL?"})
    print(f"用户B (变体: {ab_test.get_variant('user_456')}): {result_b}")
```

---

## 七、多租户配置

### 7.1 租户配置管理

```python
# 多租户配置
from typing import Dict
from config.base import AppConfig, LLMConfig

class TenantConfigManager:
    """租户配置管理器"""

    def __init__(self):
        self.tenant_configs: Dict[str, AppConfig] = {
            "tenant_a": AppConfig(
                environment="prod",
                llm=LLMConfig(
                    model="gpt-4o",
                    temperature=0,
                    max_tokens=4000
                ),
                max_concurrency=50
            ),
            "tenant_b": AppConfig(
                environment="prod",
                llm=LLMConfig(
                    model="gpt-4o-mini",
                    temperature=0.3,
                    max_tokens=2000
                ),
                max_concurrency=20
            ),
            "tenant_c": AppConfig(
                environment="prod",
                llm=LLMConfig(
                    model="gpt-3.5-turbo",
                    temperature=0,
                    max_tokens=1000
                ),
                max_concurrency=10
            )
        }

    def get_config(self, tenant_id: str) -> AppConfig:
        """获取租户配置"""
        config = self.tenant_configs.get(tenant_id)
        if not config:
            # 默认配置
            config = AppConfig(environment="prod")

        return config

    def create_chain(self, tenant_id: str):
        """为租户创建链"""
        config = self.get_config(tenant_id)

        # 使用租户配置创建LLM
        llm = ChatOpenAI(
            model=config.llm.model,
            temperature=config.llm.temperature,
            max_tokens=config.llm.max_tokens
        )

        # 创建链
        prompt = ChatPromptTemplate.from_template("回答: {question}")
        chain = prompt | llm | StrOutputParser()

        # 添加租户标记
        chain = chain.with_config(
            run_name=f"chain_{tenant_id}",
            tags=["multi_tenant", tenant_id],
            metadata={"tenant_id": tenant_id},
            max_concurrency=config.max_concurrency
        )

        return chain


# 示例使用
if __name__ == "__main__":
    manager = TenantConfigManager()

    # 租户A (高级)
    chain_a = manager.create_chain("tenant_a")
    result_a = chain_a.invoke({"question": "什么是LCEL?"})
    print(f"租户A: {result_a}")

    # 租户B (标准)
    chain_b = manager.create_chain("tenant_b")
    result_b = chain_b.invoke({"question": "什么是LCEL?"})
    print(f"租户B: {result_b}")

    # 租户C (基础)
    chain_c = manager.create_chain("tenant_c")
    result_c = chain_c.invoke({"question": "什么是LCEL?"})
    print(f"租户C: {result_c}")
```

---

## 八、完整系统实现

### 8.1 主程序

```python
# main.py
from dotenv import load_dotenv
from config.environments import EnvironmentConfig
from chains.factory import ChainFactory
import logging
import os

# 加载环境变量
load_dotenv()

class ConfigurableRAGSystem:
    """可配置RAG系统"""

    def __init__(self, environment: str = None):
        """初始化系统

        Args:
            environment: 环境名称 (dev/staging/prod)
        """
        # 加载配置
        self.config = EnvironmentConfig.load(environment)

        # 配置日志
        logging.basicConfig(
            level=self.config.log.level,
            format=self.config.log.format
        )
        self.logger = logging.getLogger(__name__)

        # 创建工厂
        self.factory = ChainFactory(self.config)

        self.logger.info(f"系统初始化完成 (环境: {self.config.environment})")

    def create_chain(self, chain_type: str = "qa"):
        """创建链

        Args:
            chain_type: 链类型 (qa/rag)
        """
        if chain_type == "qa":
            return self.factory.create_qa_chain()
        elif chain_type == "rag":
            # 需要传入vectorstore
            raise NotImplementedError("RAG链需要vectorstore")
        else:
            raise ValueError(f"未知链类型: {chain_type}")

    def ask(self, question: str, **config_overrides) -> str:
        """问答

        Args:
            question: 问题
            **config_overrides: 配置覆盖

        Returns:
            str: 答案
        """
        chain = self.create_chain("qa")

        # 运行时配置覆盖
        if config_overrides:
            result = chain.invoke(
                {"question": question},
                config={"configurable": config_overrides}
            )
        else:
            result = chain.invoke({"question": question})

        return result


def main():
    """主程序"""
    # 从环境变量读取环境
    environment = os.getenv("ENV", "dev")

    print("=" * 60)
    print(f"可配置RAG系统 (环境: {environment})")
    print("=" * 60)

    # 创建系统
    system = ConfigurableRAGSystem(environment)

    # 测试1: 使用默认配置
    print("\n[测试1] 默认配置:")
    answer1 = system.ask("什么是LCEL?")
    print(f"答案: {answer1}")

    # 测试2: 运行时覆盖配置
    print("\n[测试2] 覆盖配置 (使用GPT-4o, 温度0.7):")
    answer2 = system.ask(
        "什么是LCEL?",
        llm_model="gpt-4o",
        llm_temperature=0.7
    )
    print(f"答案: {answer2}")

    # 测试3: 不同环境对比
    print("\n[测试3] 多环境对比:")
    for env in ["dev", "staging", "prod"]:
        sys = ConfigurableRAGSystem(env)
        answer = sys.ask("什么是LCEL?")
        print(f"{env}: {answer[:50]}...")


if __name__ == "__main__":
    main()
```

---

## 九、配置文件示例

### 9.1 环境变量文件

```bash
# config/.env.dev
ENV=dev
OPENAI_API_KEY=sk-dev-...
LOG_LEVEL=DEBUG
ENABLE_CACHE=false
MAX_CONCURRENCY=5

# config/.env.staging
ENV=staging
OPENAI_API_KEY=sk-staging-...
LOG_LEVEL=INFO
ENABLE_CACHE=true
MAX_CONCURRENCY=10

# config/.env.prod
ENV=prod
OPENAI_API_KEY=sk-prod-...
LOG_LEVEL=WARNING
ENABLE_CACHE=true
MAX_CONCURRENCY=50
```

---

## 十、运行与测试

### 10.1 运行不同环境

```bash
# 开发环境
ENV=dev python main.py

# 测试环境
ENV=staging python main.py

# 生产环境
ENV=prod python main.py
```

### 10.2 测试脚本

```python
# test_config.py
import pytest
from config.environments import EnvironmentConfig
from chains.factory import ChainFactory

def test_dev_config():
    """测试开发环境配置"""
    config = EnvironmentConfig.load("dev")

    assert config.environment == "dev"
    assert config.llm.model == "gpt-4o-mini"
    assert config.log.level == "DEBUG"
    assert config.enable_cache == False

def test_prod_config():
    """测试生产环境配置"""
    config = EnvironmentConfig.load("prod")

    assert config.environment == "prod"
    assert config.llm.model == "gpt-4o"
    assert config.log.level == "WARNING"
    assert config.enable_cache == True

def test_chain_factory():
    """测试链工厂"""
    config = EnvironmentConfig.load("dev")
    factory = ChainFactory(config)

    chain = factory.create_qa_chain()
    result = chain.invoke({"question": "测试"})

    assert isinstance(result, str)
    assert len(result) > 0

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

---

## 十一、总结

### 核心要点

1. **环境配置**: dev/staging/prod三环境隔离
2. **configurable_fields**: 运行时参数覆盖
3. **configurable_alternatives**: 动态模型切换
4. **工厂模式**: 统一配置管理
5. **A/B测试**: Prompt版本管理
6. **多租户**: 租户级配置隔离

### 配置层次

```
全局配置 (base.py)
  ↓
环境配置 (dev/staging/prod)
  ↓
租户配置 (tenant_a/b/c)
  ↓
运行时覆盖 (configurable)
```

### 实战收获

- ✅ 多环境部署能力
- ✅ 灵活的配置管理
- ✅ A/B测试框架
- ✅ 多租户支持
- ✅ 运行时动态调整

---

**下一步**: 学习【并发批处理实战】,实现高吞吐量系统。
