# 实战代码 - 场景4：大文件流式传输

> 完整可运行的大文件流式传输示例

---

## 场景描述

实现大文件的流式下载和上传，包括断点续传、进度跟踪、分块传输等功能。

---

## 完整代码

```python
"""
场景4：大文件流式传输
演示：大文件的流式下载和上传
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Header
from fastapi.responses import StreamingResponse, FileResponse
from typing import Optional, AsyncGenerator
import os
import aiofiles
import hashlib
from pathlib import Path

app = FastAPI(title="大文件流式传输示例")

# 配置
UPLOAD_DIR = Path("uploads")
DOWNLOAD_DIR = Path("downloads")
CHUNK_SIZE = 8192  # 8KB

# 确保目录存在
UPLOAD_DIR.mkdir(exist_ok=True)
DOWNLOAD_DIR.mkdir(exist_ok=True)

# ===== 1. 基础文件流式下载 =====
print("=== 场景1：基础文件流式下载 ===")

async def stream_file(file_path: Path) -> AsyncGenerator[bytes, None]:
    """流式读取文件"""
    async with aiofiles.open(file_path, mode='rb') as f:
        while True:
            chunk = await f.read(CHUNK_SIZE)
            if not chunk:
                break
            yield chunk

@app.get("/download/{filename}")
async def download_file(filename: str):
    """基础文件流式下载"""
    file_path = DOWNLOAD_DIR / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="文件不存在")

    return StreamingResponse(
        stream_file(file_path),
        media_type="application/octet-stream",
        headers={
            "Content-Disposition": f"attachment; filename={filename}"
        }
    )

# ===== 2. 带进度的文件下载 =====
print("=== 场景2：带进度的文件下载 ===")

async def stream_file_with_progress(
    file_path: Path,
    total_size: int
) -> AsyncGenerator[bytes, None]:
    """带进度的文件流式下载"""
    downloaded = 0

    async with aiofiles.open(file_path, mode='rb') as f:
        while True:
            chunk = await f.read(CHUNK_SIZE)
            if not chunk:
                break

            downloaded += len(chunk)
            progress = (downloaded / total_size) * 100

            # 在响应头中添加进度信息（实际应该用 SSE）
            print(f"下载进度: {progress:.2f}% ({downloaded}/{total_size} 字节)")

            yield chunk

@app.get("/download-progress/{filename}")
async def download_file_with_progress(filename: str):
    """带进度的文件下载"""
    file_path = DOWNLOAD_DIR / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="文件不存在")

    file_size = file_path.stat().st_size

    return StreamingResponse(
        stream_file_with_progress(file_path, file_size),
        media_type="application/octet-stream",
        headers={
            "Content-Disposition": f"attachment; filename={filename}",
            "Content-Length": str(file_size)
        }
    )

# ===== 3. 断点续传下载（Range 请求） =====
print("=== 场景3：断点续传下载 ===")

async def stream_file_range(
    file_path: Path,
    start: int,
    end: int
) -> AsyncGenerator[bytes, None]:
    """流式读取文件的指定范围"""
    async with aiofiles.open(file_path, mode='rb') as f:
        await f.seek(start)
        remaining = end - start + 1

        while remaining > 0:
            chunk_size = min(CHUNK_SIZE, remaining)
            chunk = await f.read(chunk_size)
            if not chunk:
                break

            remaining -= len(chunk)
            yield chunk

@app.get("/download-range/{filename}")
async def download_file_range(
    filename: str,
    range: Optional[str] = Header(None)
):
    """支持断点续传的文件下载"""
    file_path = DOWNLOAD_DIR / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="文件不存在")

    file_size = file_path.stat().st_size

    # 解析 Range 头
    if range:
        # Range: bytes=0-1023
        range_match = range.replace("bytes=", "").split("-")
        start = int(range_match[0]) if range_match[0] else 0
        end = int(range_match[1]) if range_match[1] else file_size - 1
    else:
        start = 0
        end = file_size - 1

    # 验证范围
    if start >= file_size or end >= file_size or start > end:
        raise HTTPException(status_code=416, detail="请求范围无效")

    content_length = end - start + 1

    return StreamingResponse(
        stream_file_range(file_path, start, end),
        status_code=206,  # Partial Content
        media_type="application/octet-stream",
        headers={
            "Content-Disposition": f"attachment; filename={filename}",
            "Content-Range": f"bytes {start}-{end}/{file_size}",
            "Content-Length": str(content_length),
            "Accept-Ranges": "bytes"
        }
    )

# ===== 4. 流式文件上传 =====
print("=== 场景4：流式文件上传 ===")

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """流式文件上传"""
    file_path = UPLOAD_DIR / file.filename

    try:
        # 流式写入文件
        async with aiofiles.open(file_path, mode='wb') as f:
            while True:
                chunk = await file.read(CHUNK_SIZE)
                if not chunk:
                    break
                await f.write(chunk)

        file_size = file_path.stat().st_size

        return {
            "filename": file.filename,
            "size": file_size,
            "message": "上传成功"
        }

    except Exception as e:
        # 清理失败的文件
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=f"上传失败: {e}")

# ===== 5. 分块上传（大文件） =====
print("=== 场景5：分块上传 ===")

# 存储上传会话
upload_sessions = {}

@app.post("/upload-chunk/init")
async def init_chunk_upload(filename: str, total_size: int):
    """初始化分块上传"""
    session_id = hashlib.md5(f"{filename}{total_size}".encode()).hexdigest()

    upload_sessions[session_id] = {
        "filename": filename,
        "total_size": total_size,
        "uploaded_chunks": set(),
        "file_path": UPLOAD_DIR / f"{session_id}_{filename}"
    }

    return {
        "session_id": session_id,
        "chunk_size": CHUNK_SIZE
    }

@app.post("/upload-chunk/{session_id}/{chunk_index}")
async def upload_chunk(
    session_id: str,
    chunk_index: int,
    file: UploadFile = File(...)
):
    """上传单个分块"""
    if session_id not in upload_sessions:
        raise HTTPException(status_code=404, detail="会话不存在")

    session = upload_sessions[session_id]
    file_path = session["file_path"]

    try:
        # 写入分块
        async with aiofiles.open(file_path, mode='ab') as f:
            chunk = await file.read()
            await f.write(chunk)

        session["uploaded_chunks"].add(chunk_index)

        return {
            "chunk_index": chunk_index,
            "uploaded_chunks": len(session["uploaded_chunks"]),
            "message": "分块上传成功"
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"分块上传失败: {e}")

@app.post("/upload-chunk/{session_id}/complete")
async def complete_chunk_upload(session_id: str):
    """完成分块上传"""
    if session_id not in upload_sessions:
        raise HTTPException(status_code=404, detail="会话不存在")

    session = upload_sessions[session_id]
    temp_file = session["file_path"]
    final_file = UPLOAD_DIR / session["filename"]

    # 重命名临时文件
    temp_file.rename(final_file)

    # 清理会话
    del upload_sessions[session_id]

    return {
        "filename": session["filename"],
        "size": final_file.stat().st_size,
        "message": "上传完成"
    }

# ===== 6. 文件校验（MD5） =====
print("=== 场景6：文件校验 ===")

async def calculate_md5(file_path: Path) -> str:
    """流式计算文件 MD5"""
    md5_hash = hashlib.md5()

    async with aiofiles.open(file_path, mode='rb') as f:
        while True:
            chunk = await f.read(CHUNK_SIZE)
            if not chunk:
                break
            md5_hash.update(chunk)

    return md5_hash.hexdigest()

@app.get("/file-info/{filename}")
async def get_file_info(filename: str):
    """获取文件信息（包含 MD5）"""
    file_path = DOWNLOAD_DIR / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="文件不存在")

    file_size = file_path.stat().st_size
    md5 = await calculate_md5(file_path)

    return {
        "filename": filename,
        "size": file_size,
        "md5": md5
    }

# ===== 7. 压缩文件流式传输 =====
print("=== 场景7：压缩文件流式传输 ===")

import gzip

async def stream_compressed_file(file_path: Path) -> AsyncGenerator[bytes, None]:
    """流式压缩并传输文件"""
    async with aiofiles.open(file_path, mode='rb') as f:
        while True:
            chunk = await f.read(CHUNK_SIZE)
            if not chunk:
                break

            # 压缩数据块
            compressed = gzip.compress(chunk)
            yield compressed

@app.get("/download-compressed/{filename}")
async def download_compressed_file(filename: str):
    """下载压缩文件"""
    file_path = DOWNLOAD_DIR / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="文件不存在")

    return StreamingResponse(
        stream_compressed_file(file_path),
        media_type="application/gzip",
        headers={
            "Content-Disposition": f"attachment; filename={filename}.gz",
            "Content-Encoding": "gzip"
        }
    )

# ===== 运行服务器 =====
if __name__ == "__main__":
    import uvicorn

    # 创建测试文件
    test_file = DOWNLOAD_DIR / "test.txt"
    if not test_file.exists():
        with open(test_file, "w") as f:
            f.write("这是一个测试文件\n" * 1000)
        print(f"创建测试文件: {test_file}")

    print("\n" + "="*50)
    print("启动 FastAPI 服务器")
    print("="*50)
    print("\n可用端点：")
    print("  GET  http://localhost:8000/download/test.txt")
    print("  GET  http://localhost:8000/download-progress/test.txt")
    print("  GET  http://localhost:8000/download-range/test.txt")
    print("  POST http://localhost:8000/upload")
    print("  POST http://localhost:8000/upload-chunk/init")
    print("  GET  http://localhost:8000/file-info/test.txt")
    print("  GET  http://localhost:8000/download-compressed/test.txt")
    print("\n测试命令：")
    print("  # 下载文件")
    print("  curl -O http://localhost:8000/download/test.txt")
    print("\n  # 断点续传（下载前1024字节）")
    print("  curl -H 'Range: bytes=0-1023' http://localhost:8000/download-range/test.txt")
    print("\n  # 上传文件")
    print("  curl -F 'file=@test.txt' http://localhost:8000/upload")
    print("\n按 Ctrl+C 停止服务器")
    print("="*50 + "\n")

    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 测试示例

### 1. 基础文件下载

```bash
# 下载文件
curl -O http://localhost:8000/download/test.txt

# 查看文件
cat test.txt
```

### 2. 断点续传下载

```bash
# 下载前1024字节
curl -H "Range: bytes=0-1023" \
  http://localhost:8000/download-range/test.txt \
  -o test_part1.txt

# 下载剩余部分
curl -H "Range: bytes=1024-" \
  http://localhost:8000/download-range/test.txt \
  -o test_part2.txt

# 合并文件
cat test_part1.txt test_part2.txt > test_complete.txt
```

### 3. 文件上传

```bash
# 上传文件
curl -F "file=@test.txt" http://localhost:8000/upload

# 查看上传的文件
ls uploads/
```

### 4. 分块上传

```python
import requests
import os

def upload_large_file(file_path: str, chunk_size: int = 8192):
    """分块上传大文件"""
    filename = os.path.basename(file_path)
    file_size = os.path.getsize(file_path)

    # 1. 初始化上传
    response = requests.post(
        "http://localhost:8000/upload-chunk/init",
        params={"filename": filename, "total_size": file_size}
    )
    session_id = response.json()["session_id"]
    print(f"会话ID: {session_id}")

    # 2. 分块上传
    with open(file_path, "rb") as f:
        chunk_index = 0
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break

            files = {"file": (filename, chunk)}
            response = requests.post(
                f"http://localhost:8000/upload-chunk/{session_id}/{chunk_index}",
                files=files
            )
            print(f"上传分块 {chunk_index}: {response.json()}")
            chunk_index += 1

    # 3. 完成上传
    response = requests.post(
        f"http://localhost:8000/upload-chunk/{session_id}/complete"
    )
    print(f"上传完成: {response.json()}")

# 使用
upload_large_file("large_file.zip")
```

### 5. 文件校验

```bash
# 获取文件信息（包含 MD5）
curl http://localhost:8000/file-info/test.txt

# 输出：
# {
#   "filename": "test.txt",
#   "size": 24000,
#   "md5": "5d41402abc4b2a76b9719d911017c592"
# }
```

### 6. 压缩下载

```bash
# 下载压缩文件
curl http://localhost:8000/download-compressed/test.txt -o test.txt.gz

# 解压
gunzip test.txt.gz
```

---

## Python 客户端示例

```python
import httpx
import asyncio
from pathlib import Path

async def download_file_streaming(url: str, output_path: Path):
    """流式下载文件"""
    async with httpx.AsyncClient() as client:
        async with client.stream('GET', url) as response:
            response.raise_for_status()

            total_size = int(response.headers.get('Content-Length', 0))
            downloaded = 0

            async with aiofiles.open(output_path, mode='wb') as f:
                async for chunk in response.aiter_bytes(chunk_size=8192):
                    await f.write(chunk)
                    downloaded += len(chunk)

                    if total_size:
                        progress = (downloaded / total_size) * 100
                        print(f"\r下载进度: {progress:.2f}%", end='')

            print(f"\n下载完成: {output_path}")

async def download_file_with_resume(url: str, output_path: Path):
    """支持断点续传的下载"""
    # 检查已下载的大小
    if output_path.exists():
        downloaded = output_path.stat().st_size
        headers = {"Range": f"bytes={downloaded}-"}
        mode = "ab"  # 追加模式
    else:
        downloaded = 0
        headers = {}
        mode = "wb"  # 写入模式

    async with httpx.AsyncClient() as client:
        async with client.stream('GET', url, headers=headers) as response:
            if response.status_code not in (200, 206):
                raise Exception(f"下载失败: {response.status_code}")

            total_size = int(response.headers.get('Content-Length', 0)) + downloaded

            async with aiofiles.open(output_path, mode=mode) as f:
                async for chunk in response.aiter_bytes(chunk_size=8192):
                    await f.write(chunk)
                    downloaded += len(chunk)

                    progress = (downloaded / total_size) * 100
                    print(f"\r下载进度: {progress:.2f}%", end='')

            print(f"\n下载完成: {output_path}")

# 使用
asyncio.run(download_file_streaming(
    "http://localhost:8000/download/test.txt",
    Path("test.txt")
))

asyncio.run(download_file_with_resume(
    "http://localhost:8000/download-range/test.txt",
    Path("test_resume.txt")
))
```

---

## 关键知识点

### 1. 流式读取文件

```python
async with aiofiles.open(file_path, mode='rb') as f:
    while True:
        chunk = await f.read(CHUNK_SIZE)
        if not chunk:
            break
        yield chunk
```

### 2. Range 请求处理

```python
# 客户端请求
Range: bytes=0-1023

# 服务器响应
HTTP/1.1 206 Partial Content
Content-Range: bytes 0-1023/10000
Content-Length: 1024
```

### 3. 分块上传

```python
# 1. 初始化会话
session_id = init_upload()

# 2. 上传分块
for chunk in file_chunks:
    upload_chunk(session_id, chunk)

# 3. 完成上传
complete_upload(session_id)
```

### 4. 文件校验

```python
md5_hash = hashlib.md5()
while chunk := f.read(CHUNK_SIZE):
    md5_hash.update(chunk)
return md5_hash.hexdigest()
```

---

## 性能优化

### 1. 调整块大小

```python
# 小文件：4KB
CHUNK_SIZE = 4096

# 中等文件：8KB（推荐）
CHUNK_SIZE = 8192

# 大文件：64KB
CHUNK_SIZE = 65536
```

### 2. 使用缓冲

```python
async with aiofiles.open(file_path, mode='rb', buffering=65536) as f:
    # 使用 64KB 缓冲区
    pass
```

### 3. 并发上传

```python
import asyncio

async def upload_chunks_parallel(chunks):
    """并发上传多个分块"""
    tasks = [upload_chunk(chunk) for chunk in chunks]
    await asyncio.gather(*tasks)
```

---

## 常见问题

### Q1: 如何处理大文件（>1GB）？

**A:** 使用分块传输：
1. 设置合理的块大小（64KB-1MB）
2. 使用断点续传
3. 实现分块上传
4. 添加进度跟踪

### Q2: 如何防止内存溢出？

**A:** 使用流式处理：
- 不要一次性读取整个文件
- 使用生成器逐块处理
- 设置合理的块大小

### Q3: 如何实现下载限速？

**A:** 在生成器中添加延迟：
```python
async def stream_with_rate_limit(file_path, rate_limit_kbps):
    async with aiofiles.open(file_path, 'rb') as f:
        while chunk := await f.read(CHUNK_SIZE):
            yield chunk
            # 限速：等待时间 = 块大小 / 速率
            await asyncio.sleep(len(chunk) / (rate_limit_kbps * 1024))
```

---

## 下一步

完成本场景后，继续学习：
- **场景5**：生产级流式响应系统
- **化骨绵掌**：10个2分钟知识卡片
