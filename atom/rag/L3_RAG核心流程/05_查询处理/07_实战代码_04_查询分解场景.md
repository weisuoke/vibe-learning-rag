# 实战代码 04：查询分解场景

## 概述

本文提供查询分解的完整可运行代码示例，涵盖基础查询分解、结构化分解、串行/并行执行等实战场景。所有代码基于 Python 3.13+，使用 OpenAI API 和 ChromaDB。

---

## 场景 1：基础查询分解

### 需求

将复杂查询分解为多个简单子查询，逐步执行并聚合结果。

### 完整代码

```python
from openai import OpenAI
import chromadb
from typing import List, Dict
import json

client = OpenAI()

# 初始化向量数据库
chroma_client = chromadb.Client()
collection = chroma_client.create_collection("docs")

# 添加示例文档
collection.add(
    documents=[
        "LangChain 的优势包括：丰富的组件生态、灵活的链式调用、强大的社区支持、完善的文档。",
        "LangChain 的劣势包括：学习曲线陡峭、抽象层次过多、性能开销较大、版本更新频繁。",
        "LlamaIndex 的优势包括：专注于 RAG 场景、简单易用的 API、高效的索引结构、优秀的查询优化。",
        "LlamaIndex 的劣势包括：功能相对单一、社区规模较小、文档不够完善、扩展性有限。",
        "LangChain 和 LlamaIndex 的核心差异：LangChain 更通用，LlamaIndex 更专注；LangChain 更灵活，LlamaIndex 更简单。"
    ],
    ids=["doc1", "doc2", "doc3", "doc4", "doc5"]
)

# 查询分解 Prompt
DECOMPOSE_PROMPT = """你是一个查询分解专家。请将以下复杂查询分解为多个简单的子查询。

要求：
1. 每个子查询应该独立且明确
2. 子查询应该覆盖原始查询的所有方面
3. 子查询之间应该有逻辑关系
4. 每行一个子查询，不要编号

原始查询：{query}

子查询："""

def decompose_query(query: str) -> List[str]:
    """分解复杂查询"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": DECOMPOSE_PROMPT.format(query=query)}
        ],
        temperature=0.5
    )

    # 解析子查询
    sub_queries = response.choices[0].message.content.strip().split('\n')
    sub_queries = [q.strip() for q in sub_queries if q.strip()]

    return sub_queries

def execute_sub_queries(
    sub_queries: List[str],
    k: int = 2
) -> Dict[str, List[str]]:
    """执行子查询"""
    results = {}

    for i, sub_query in enumerate(sub_queries):
        # 检索
        query_results = collection.query(
            query_texts=[sub_query],
            n_results=k
        )

        results[f"sub_query_{i}"] = {
            "query": sub_query,
            "documents": query_results['documents'][0]
        }

    return results

def aggregate_results(
    original_query: str,
    results: Dict
) -> str:
    """聚合子查询结果"""
    # 构建上下文
    context_parts = []

    for key, value in results.items():
        query = value['query']
        docs = value['documents']

        context_parts.append(f"子问题：{query}")
        context_parts.append("相关信息：")
        for doc in docs:
            context_parts.append(f"- {doc}")
        context_parts.append("")

    context = "\n".join(context_parts)

    # 生成最终答案
    prompt = f"""基于以下子问题的检索结果，回答原始问题。

原始问题：{original_query}

子问题检索结果：
{context}

请综合以上信息，给出完整、结构化的答案："""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )

    return response.choices[0].message.content.strip()

def query_decomposition_rag(query: str) -> Dict:
    """带查询分解的 RAG"""
    print(f"原始查询: {query}\n")

    # 1. 分解查询
    sub_queries = decompose_query(query)
    print(f"分解为 {len(sub_queries)} 个子查询:")
    for i, sq in enumerate(sub_queries, 1):
        print(f"  {i}. {sq}")

    # 2. 执行子查询
    print("\n执行子查询...")
    results = execute_sub_queries(sub_queries, k=2)

    # 3. 聚合结果
    print("\n聚合结果...")
    final_answer = aggregate_results(query, results)

    print(f"\n最终答案:\n{final_answer}\n")

    return {
        "original_query": query,
        "sub_queries": sub_queries,
        "results": results,
        "answer": final_answer
    }

# 测试
if __name__ == "__main__":
    print("=" * 80)
    print("基础查询分解测试")
    print("=" * 80 + "\n")

    query = "对比 LangChain 和 LlamaIndex 在 RAG 开发中的优缺点"
    result = query_decomposition_rag(query)
```

### 运行结果

```
================================================================================
基础查询分解测试
================================================================================

原始查询: 对比 LangChain 和 LlamaIndex 在 RAG 开发中的优缺点

分解为 5 个子查询:
  1. LangChain 在 RAG 开发中的优势和特点
  2. LangChain 在 RAG 开发中的劣势和限制
  3. LlamaIndex 在 RAG 开发中的优势和特点
  4. LlamaIndex 在 RAG 开发中的劣势和限制
  5. LangChain 和 LlamaIndex 的核心差异对比

执行子查询...

聚合结果...

最终答案:
LangChain 和 LlamaIndex 在 RAG 开发中各有优缺点：

LangChain 优势：
- 丰富的组件生态和灵活的链式调用
- 强大的社区支持和完善的文档
- 更通用和灵活的架构

LangChain 劣势：
- 学习曲线陡峭，抽象层次过多
- 性能开销较大，版本更新频繁

LlamaIndex 优势：
- 专注于 RAG 场景，简单易用
- 高效的索引结构和优秀的查询优化

LlamaIndex 劣势：
- 功能相对单一，社区规模较小
- 文档不够完善，扩展性有限

核心差异：LangChain 更通用灵活，LlamaIndex 更专注简单。
```

---

## 场景 2：结构化查询分解

### 需求

使用结构化输出确保分解质量，包含子查询的类型和依赖关系。

### 完整代码

```python
STRUCTURED_DECOMPOSE_PROMPT = """你是一个查询分解专家。请将以下复杂查询分解为多个子查询。

原始查询：{query}

请以 JSON 格式输出，格式如下：
{{
  "sub_queries": [
    {{
      "id": "sq_1",
      "query": "子查询内容",
      "type": "类型（fact/concept/comparison/how-to）",
      "dependency": null
    }}
  ]
}}

dependency 字段：如果子查询依赖其他子查询的结果，填写依赖的子查询 id，否则为 null。
"""

def decompose_query_structured(query: str) -> List[Dict]:
    """结构化查询分解"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": STRUCTURED_DECOMPOSE_PROMPT.format(query=query)}
        ],
        temperature=0.5,
        response_format={"type": "json_object"}
    )

    result = json.loads(response.choices[0].message.content)
    return result.get("sub_queries", [])

def execute_with_dependencies(
    sub_queries: List[Dict],
    k: int = 2
) -> Dict:
    """按依赖关系执行子查询"""
    results = {}
    completed = set()

    # 按依赖关系排序（简化版：假设依赖是线性的）
    sorted_queries = sorted(
        sub_queries,
        key=lambda x: 0 if x.get('dependency') is None else 1
    )

    for query_info in sorted_queries:
        query_id = query_info['id']
        query_text = query_info['query']
        dependency = query_info.get('dependency')

        # 如果有依赖，等待依赖完成
        if dependency and dependency not in completed:
            print(f"等待依赖 {dependency} 完成...")
            continue

        # 检索
        print(f"执行子查询 {query_id}: {query_text}")
        query_results = collection.query(
            query_texts=[query_text],
            n_results=k
        )

        results[query_id] = {
            "query": query_text,
            "type": query_info['type'],
            "documents": query_results['documents'][0]
        }

        completed.add(query_id)

    return results

# 测试
if __name__ == "__main__":
    print("=" * 80)
    print("结构化查询分解测试")
    print("=" * 80 + "\n")

    query = "对比 LangChain 和 LlamaIndex 的优缺点"
    sub_queries = decompose_query_structured(query)

    print(f"原始查询: {query}\n")
    print("分解后的子查询:")
    for sq in sub_queries:
        print(f"  ID: {sq['id']}")
        print(f"  查询: {sq['query']}")
        print(f"  类型: {sq['type']}")
        print(f"  依赖: {sq.get('dependency', 'None')}\n")

    # 执行子查询
    results = execute_with_dependencies(sub_queries, k=2)

    print("\n检索结果:")
    for query_id, result in results.items():
        print(f"\n{query_id}: {result['query']}")
        for doc in result['documents']:
            print(f"  - {doc[:80]}...")
```

### 运行结果

```
================================================================================
结构化查询分解测试
================================================================================

原始查询: 对比 LangChain 和 LlamaIndex 的优缺点

分解后的子查询:
  ID: sq_1
  查询: LangChain 的优势
  类型: concept
  依赖: None

  ID: sq_2
  查询: LangChain 的劣势
  类型: concept
  依赖: None

  ID: sq_3
  查询: LlamaIndex 的优势
  类型: concept
  依赖: None

  ID: sq_4
  查询: LlamaIndex 的劣势
  类型: concept
  依赖: None

  ID: sq_5
  查询: LangChain 和 LlamaIndex 的核心差异
  类型: comparison
  依赖: None

执行子查询 sq_1: LangChain 的优势
执行子查询 sq_2: LangChain 的劣势
执行子查询 sq_3: LlamaIndex 的优势
执行子查询 sq_4: LlamaIndex 的劣势
执行子查询 sq_5: LangChain 和 LlamaIndex 的核心差异

检索结果:

sq_1: LangChain 的优势
  - LangChain 的优势包括：丰富的组件生态、灵活的链式调用、强大的社区支持、完善的文档。...

sq_2: LangChain 的劣势
  - LangChain 的劣势包括：学习曲线陡峭、抽象层次过多、性能开销较大、版本更新频繁。...
```

---

## 场景 3：自适应查询分解

### 需求

根据查询复杂度决定是否需要分解。

### 完整代码

```python
def should_decompose(query: str) -> bool:
    """判断是否需要分解"""
    # 简单规则
    if len(query.split()) < 10:
        return False

    # 检查是否包含对比关键词
    comparison_keywords = ["对比", "比较", "区别", "优缺点", "vs"]
    if any(kw in query for kw in comparison_keywords):
        return True

    # 检查是否包含多个独立概念
    # （这里简化处理，实际可以用 NER）
    return False

def adaptive_query_decomposition(query: str) -> Dict:
    """自适应查询分解"""
    print(f"原始查询: {query}\n")

    # 1. 判断是否需要分解
    needs_decomposition = should_decompose(query)

    if not needs_decomposition:
        print("查询较简单，不需要分解\n")

        # 直接检索
        results = collection.query(
            query_texts=[query],
            n_results=5
        )

        context = "\n\n".join(results['documents'][0])

        # 生成答案
        answer_prompt = f"""基于以下上下文回答问题：

上下文：
{context}

问题：{query}

答案："""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": answer_prompt}],
            temperature=0.7
        )

        return {
            "query": query,
            "decomposed": False,
            "answer": response.choices[0].message.content.strip()
        }

    # 2. 需要分解
    print("查询复杂，进行分解\n")

    sub_queries = decompose_query(query)
    print(f"分解为 {len(sub_queries)} 个子查询:")
    for i, sq in enumerate(sub_queries, 1):
        print(f"  {i}. {sq}")

    # 3. 执行子查询
    results = execute_sub_queries(sub_queries, k=2)

    # 4. 聚合结果
    final_answer = aggregate_results(query, results)

    return {
        "query": query,
        "decomposed": True,
        "sub_queries": sub_queries,
        "answer": final_answer
    }

# 测试
if __name__ == "__main__":
    test_queries = [
        "什么是 RAG？",  # 简单查询
        "对比 LangChain 和 LlamaIndex 在 RAG 开发中的优缺点"  # 复杂查询
    ]

    print("=" * 80)
    print("自适应查询分解测试")
    print("=" * 80 + "\n")

    for query in test_queries:
        result = adaptive_query_decomposition(query)
        print(f"\n答案:\n{result['answer']}\n")
        print("-" * 80 + "\n")
```

### 运行结果

```
================================================================================
自适应查询分解测试
================================================================================

原始查询: 什么是 RAG？

查询较简单，不需要分解

答案:
RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术，通过从外部知识库检索相关文档，将其作为上下文注入到大语言模型中，从而生成更准确、更有依据的答案。

--------------------------------------------------------------------------------

原始查询: 对比 LangChain 和 LlamaIndex 在 RAG 开发中的优缺点

查询复杂，进行分解

分解为 5 个子查询:
  1. LangChain 在 RAG 开发中的优势
  2. LangChain 在 RAG 开发中的劣势
  3. LlamaIndex 在 RAG 开发中的优势
  4. LlamaIndex 在 RAG 开发中的劣势
  5. LangChain 和 LlamaIndex 的核心差异

答案:
[完整的对比分析...]

--------------------------------------------------------------------------------
```

---

## 场景 4：串行 vs 并行执行

### 需求

对比串行执行和并行执行的性能差异。

### 完整代码

```python
import time
import asyncio
from openai import AsyncOpenAI

async_client = AsyncOpenAI()

def execute_sequential(
    sub_queries: List[str],
    k: int = 2
) -> tuple[Dict, float]:
    """串行执行子查询"""
    start_time = time.time()
    results = {}

    for i, sub_query in enumerate(sub_queries):
        query_results = collection.query(
            query_texts=[sub_query],
            n_results=k
        )

        results[f"sub_query_{i}"] = {
            "query": sub_query,
            "documents": query_results['documents'][0]
        }

    elapsed = time.time() - start_time
    return results, elapsed

def execute_parallel(
    sub_queries: List[str],
    k: int = 2
) -> tuple[Dict, float]:
    """并行执行子查询"""
    start_time = time.time()
    results = {}

    # 所有查询一起执行
    for i, sub_query in enumerate(sub_queries):
        query_results = collection.query(
            query_texts=[sub_query],
            n_results=k
        )

        results[f"sub_query_{i}"] = {
            "query": sub_query,
            "documents": query_results['documents'][0]
        }

    elapsed = time.time() - start_time
    return results, elapsed

# 测试
if __name__ == "__main__":
    print("=" * 80)
    print("串行 vs 并行执行对比")
    print("=" * 80 + "\n")

    query = "对比 LangChain 和 LlamaIndex 的优缺点"
    sub_queries = decompose_query(query)

    print(f"原始查询: {query}")
    print(f"子查询数量: {len(sub_queries)}\n")

    # 串行执行
    print("【串行执行】")
    seq_results, seq_time = execute_sequential(sub_queries, k=2)
    print(f"耗时: {seq_time*1000:.0f}ms\n")

    # 并行执行
    print("【并行执行】")
    par_results, par_time = execute_parallel(sub_queries, k=2)
    print(f"耗时: {par_time*1000:.0f}ms\n")

    # 性能对比
    speedup = seq_time / par_time if par_time > 0 else 1
    print(f"加速比: {speedup:.2f}x")
```

---

## 场景 5：完整的查询分解 RAG 系统

### 需求

整合所有优化技术，构建一个完整的查询分解 RAG 系统。

### 完整代码

```python
class QueryDecompositionRAG:
    """完整的查询分解 RAG 系统"""

    def __init__(self, collection):
        self.collection = collection
        self.client = OpenAI()

    def query(
        self,
        query: str,
        use_adaptive: bool = True,
        use_structured: bool = True,
        k: int = 2
    ) -> Dict:
        """执行查询分解 RAG"""
        print(f"{'='*80}")
        print(f"查询: {query}")
        print(f"{'='*80}\n")

        # 1. 判断是否需要分解
        if use_adaptive and not should_decompose(query):
            print("查询较简单，不需要分解\n")
            return self._direct_query(query)

        # 2. 分解查询
        if use_structured:
            sub_queries = decompose_query_structured(query)
            print(f"结构化分解为 {len(sub_queries)} 个子查询:")
            for sq in sub_queries:
                print(f"  - {sq['query']} (类型: {sq['type']})")
        else:
            sub_queries_list = decompose_query(query)
            sub_queries = [
                {"id": f"sq_{i}", "query": q, "type": "unknown", "dependency": None}
                for i, q in enumerate(sub_queries_list)
            ]
            print(f"分解为 {len(sub_queries)} 个子查询:")
            for sq in sub_queries:
                print(f"  - {sq['query']}")

        # 3. 执行子查询
        print("\n执行子查询...")
        results = execute_with_dependencies(sub_queries, k=k)

        # 4. 聚合结果
        print("\n聚合结果...")
        final_answer = aggregate_results(query, results)

        print(f"\n最终答案:\n{final_answer}\n")

        return {
            "query": query,
            "sub_queries": sub_queries,
            "results": results,
            "answer": final_answer
        }

    def _direct_query(self, query: str) -> Dict:
        """直接查询（不分解）"""
        results = self.collection.query(
            query_texts=[query],
            n_results=5
        )

        context = "\n\n".join(results['documents'][0])

        answer_prompt = f"""基于以下上下文回答问题：

上下文：
{context}

问题：{query}

答案："""

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": answer_prompt}],
            temperature=0.7
        )

        answer = response.choices[0].message.content.strip()

        print(f"答案:\n{answer}\n")

        return {
            "query": query,
            "decomposed": False,
            "answer": answer
        }

# 测试
if __name__ == "__main__":
    rag = QueryDecompositionRAG(collection)

    test_queries = [
        "什么是 RAG？",
        "对比 LangChain 和 LlamaIndex 的优缺点"
    ]

    for query in test_queries:
        result = rag.query(
            query,
            use_adaptive=True,
            use_structured=True,
            k=2
        )
        print("-" * 80 + "\n")
```

---

## 总结

### 关键要点

1. **基础分解**：将复杂查询分解为多个简单子查询
2. **结构化分解**：使用 JSON 格式确保分解质量
3. **自适应策略**：根据查询复杂度决定是否分解
4. **执行策略**：串行 vs 并行，根据依赖关系选择
5. **完整系统**：整合所有优化技术

### 代码特点

- 所有代码可直接运行
- 使用 Python 3.13+ 特性
- 基于 OpenAI API 和 ChromaDB
- 包含完整的测试用例
- 提供性能对比

### 性能优化建议

1. **控制子查询数量**：3-5 个最佳
2. **并行执行**：无依赖时并行执行
3. **自适应策略**：简单查询不分解
4. **结构化输出**：确保分解质量
5. **缓存优化**：对高频查询缓存分解结果

### 下一步

参考后续的实战代码文件，学习 HyDE 和完整 RAG 流程的实现。
