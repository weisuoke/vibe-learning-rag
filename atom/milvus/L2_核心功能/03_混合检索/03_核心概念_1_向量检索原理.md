# 核心概念1：向量检索原理

## 一句话定义

**向量检索是在高维向量空间中寻找与查询向量最相似的 K 个向量的过程，Milvus 使用 ANN（近似最近邻）算法在百万级数据中实现毫秒级检索。**

---

## 1. 向量检索的本质

### 1.1 什么是向量检索？

**核心思想**：将"相似度匹配"转化为"距离计算"

```python
# 传统关键词检索
query = "Python 编程"
results = database.search(keywords=["Python", "编程"])  # 精确匹配

# 向量检索
query = "Python 编程"
query_vector = embed(query)  # [0.2, 0.5, 0.8, ..., 0.3]  # 768维
results = milvus.search(vector=query_vector)  # 语义相似度匹配
```

**关键区别**：

| 维度 | 关键词检索 | 向量检索 |
|------|-----------|---------|
| 匹配方式 | 精确匹配 | 语义相似 |
| 理解能力 | 无语义理解 | 理解语义 |
| 召回率 | 低（漏掉同义词） | 高（理解语义） |
| 示例 | "Python" ≠ "python" | "Python" ≈ "python" ≈ "蟒蛇语言" |

---

### 1.2 向量空间的直觉理解

**类比：城市地图上的距离**

```
想象一个二维平面（实际是768维）：

    y轴
    ↑
    |  ● "Python编程"
    |
    |      ● "Java开发"
    |
    |            ● "机器学习"
    |
    +------------------------→ x轴

距离越近 = 语义越相似
```

**在 Milvus 中**：

```python
# 向量表示（简化为3维）
vector_python = [0.8, 0.2, 0.1]   # "Python编程"
vector_java = [0.7, 0.3, 0.1]     # "Java开发"
vector_ml = [0.3, 0.1, 0.9]       # "机器学习"

# 计算距离
distance(vector_python, vector_java) = 0.14    # 很近（相似）
distance(vector_python, vector_ml) = 0.85      # 较远（不太相似）
```

---

## 2. ANN 算法：近似最近邻

### 2.1 为什么需要 ANN？

**问题：暴力搜索太慢**

```python
# 暴力搜索（Brute Force）
def brute_force_search(query_vector, all_vectors, k=10):
    distances = []
    for vector in all_vectors:  # 遍历所有向量
        dist = calculate_distance(query_vector, vector)
        distances.append((vector, dist))

    # 排序并返回 Top-K
    distances.sort(key=lambda x: x[1])
    return distances[:k]

# 时间复杂度：O(N × D)
# N = 数据量（如100万）
# D = 向量维度（如768）
# 计算量 = 100万 × 768 = 7.68亿次乘法
# 耗时：~1秒（不可接受）
```

**解决方案：ANN 算法**

```python
# ANN 算法（如 HNSW）
def ann_search(query_vector, index, k=10):
    # 只访问部分向量（如1000个）
    candidates = index.search_approximate(query_vector)
    return candidates[:k]

# 时间复杂度：O(log N × D)
# 计算量 = log(100万) × 768 ≈ 15,000次乘法
# 耗时：~1毫秒（可接受）
# 加速：1000倍
```

**权衡**：

- ✅ 速度快：毫秒级响应
- ⚠️ 精度略降：召回率 95-99%（可接受）

---

### 2.2 Milvus 支持的 ANN 算法

#### 算法1：HNSW（推荐）

**全称**：Hierarchical Navigable Small World（层次化可导航小世界图）

**原理**：构建多层图结构，快速导航到目标区域

```
层级结构（类比：高速公路 + 省道 + 县道）

Layer 2 (高速公路):  ●--------●--------●
                     |        |        |
Layer 1 (省道):      ●--●--●--●--●--●--●
                     |  |  |  |  |  |  |
Layer 0 (县道):      ●-●-●-●-●-●-●-●-●-●

检索过程：
1. 从顶层（高速公路）快速接近目标区域
2. 逐层下降到底层（县道）
3. 在底层精确查找
```

**特点**：

- ✅ 召回率高（95-99%）
- ✅ 查询速度快（毫秒级）
- ⚠️ 内存占用大（需要存储图结构）
- ⚠️ 构建索引慢（适合静态数据）

**适用场景**：

- 高召回率要求（如 RAG 系统）
- 数据更新不频繁
- 内存充足

**Milvus 配置**：

```python
index_params = {
    "index_type": "HNSW",
    "metric_type": "L2",  # 或 "IP"（内积）
    "params": {
        "M": 16,              # 每个节点的连接数（8-64）
        "efConstruction": 200 # 构建时的搜索深度（100-500）
    }
}

search_params = {
    "metric_type": "L2",
    "params": {
        "ef": 64  # 查询时的搜索深度（top_k 到 512）
    }
}
```

**参数调优**：

| 参数 | 值越大 | 召回率 | 速度 | 内存 |
|------|--------|--------|------|------|
| M | 增加 | ↑ | ↓ | ↑ |
| efConstruction | 增加 | ↑ | ↓（构建） | - |
| ef | 增加 | ↑ | ↓（查询） | - |

---

#### 算法2：IVF_FLAT

**全称**：Inverted File with Flat（倒排文件 + 暴力搜索）

**原理**：先聚类分区，再在分区内暴力搜索

```
数据聚类（类比：图书馆分类）

Cluster 1 (计算机):  ●●●●●
Cluster 2 (文学):    ●●●●●
Cluster 3 (历史):    ●●●●●

检索过程：
1. 找到最近的 nprobe 个 Cluster（如 Cluster 1）
2. 在这些 Cluster 内暴力搜索
3. 返回 Top-K
```

**特点**：

- ✅ 内存占用小
- ✅ 构建索引快
- ⚠️ 召回率中等（85-95%）
- ⚠️ 查询速度中等

**适用场景**：

- 内存受限
- 数据更新频繁
- 对召回率要求不极致

**Milvus 配置**：

```python
index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "L2",
    "params": {
        "nlist": 1024  # 聚类中心数量（推荐：sqrt(N) 到 4*sqrt(N)）
    }
}

search_params = {
    "metric_type": "L2",
    "params": {
        "nprobe": 16  # 搜索的聚类数量（1 到 nlist）
    }
}
```

**参数调优**：

| 参数 | 值越大 | 召回率 | 速度 | 内存 |
|------|--------|--------|------|------|
| nlist | 增加 | - | ↑（构建） | ↓ |
| nprobe | 增加 | ↑ | ↓（查询） | - |

---

#### 算法3：IVF_SQ8（量化压缩）

**全称**：IVF + Scalar Quantization 8-bit

**原理**：IVF_FLAT + 向量量化（压缩）

```python
# 原始向量（32位浮点数）
vector_original = [0.123456, 0.789012, ...]  # 每个元素 4 字节

# 量化后（8位整数）
vector_quantized = [31, 201, ...]  # 每个元素 1 字节

# 压缩比：4倍
```

**特点**：

- ✅ 内存占用极小（压缩 4 倍）
- ✅ 查询速度快
- ⚠️ 精度略降（召回率 90-95%）

**适用场景**：

- 大规模数据（千万级以上）
- 内存严重受限
- 可接受轻微精度损失

---

### 2.3 算法选择指南

**决策树**：

```
数据量 < 10万？
├─ 是 → FLAT（暴力搜索，精度100%）
└─ 否 → 内存充足？
    ├─ 是 → 召回率要求高？
    │   ├─ 是 → HNSW（推荐）
    │   └─ 否 → IVF_FLAT
    └─ 否 → IVF_SQ8（量化压缩）
```

**性能对比**（100万条数据，768维）：

| 算法 | 召回率 | QPS | 内存 | 构建时间 |
|------|--------|-----|------|---------|
| FLAT | 100% | 10 | 3GB | 0s |
| HNSW | 98% | 5000 | 6GB | 10min |
| IVF_FLAT | 92% | 2000 | 3GB | 2min |
| IVF_SQ8 | 90% | 3000 | 0.8GB | 3min |

---

## 3. 相似度度量

### 3.1 L2 距离（欧氏距离）

**定义**：两点之间的直线距离

```python
# 数学公式
L2(a, b) = sqrt(Σ(aᵢ - bᵢ)²)

# Python 实现
import numpy as np

def l2_distance(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# 示例
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
distance = l2_distance(a, b)  # 5.196
```

**特点**：

- 距离越小 = 越相似
- 范围：[0, +∞)
- 适用：通用场景

**Milvus 配置**：

```python
search_params = {
    "metric_type": "L2",
    "params": {"ef": 64}
}
```

---

### 3.2 内积（IP）

**定义**：向量点积

```python
# 数学公式
IP(a, b) = Σ(aᵢ × bᵢ)

# Python 实现
def inner_product(a, b):
    return np.dot(a, b)

# 示例
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
ip = inner_product(a, b)  # 32
```

**特点**：

- 值越大 = 越相似
- 范围：(-∞, +∞)
- 适用：归一化向量（如 OpenAI Embedding）

**Milvus 配置**：

```python
search_params = {
    "metric_type": "IP",
    "params": {"ef": 64}
}
```

---

### 3.3 余弦相似度

**定义**：向量夹角的余弦值

```python
# 数学公式
Cosine(a, b) = (a · b) / (||a|| × ||b||)

# Python 实现
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# 示例
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
cos = cosine_similarity(a, b)  # 0.974
```

**特点**：

- 值越大 = 越相似
- 范围：[-1, 1]
- 适用：文本相似度

**Milvus 实现**：

```python
# Milvus 不直接支持 Cosine，但可以通过归一化 + IP 实现
# 如果向量已归一化，IP = Cosine
search_params = {
    "metric_type": "IP",  # 对归一化向量，IP 等价于 Cosine
    "params": {"ef": 64}
}
```

---

### 3.4 度量选择指南

| 场景 | 推荐度量 | 原因 |
|------|---------|------|
| OpenAI Embedding | IP | 向量已归一化 |
| 自定义 Embedding | L2 | 通用，无需归一化 |
| 文本相似度 | IP（归一化后） | 关注方向，不关注长度 |
| 图像检索 | L2 | 关注绝对距离 |

---

## 4. 在混合检索中的应用

### 4.1 向量检索的作用

**在混合检索中**：

```python
# 混合检索 = 向量检索（语义相似） + 标量过滤（精确条件）

results = collection.search(
    data=[query_vector],        # 向量检索：找到语义相似的文档
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"ef": 64}},
    limit=100,
    expr="year == 2024"         # 标量过滤：只要2024年的文档
)
```

**向量检索负责**：

1. 理解查询的语义
2. 召回语义相关的候选集
3. 提供相似度排序

---

### 4.2 性能优化策略

#### 策略1：调整索引参数

```python
# 高召回率场景（RAG 系统）
index_params = {
    "index_type": "HNSW",
    "params": {
        "M": 32,              # 增加连接数
        "efConstruction": 400 # 增加构建深度
    }
}

search_params = {
    "params": {"ef": 128}  # 增加查询深度
}
```

#### 策略2：选择合适的度量

```python
# OpenAI Embedding（已归一化）
search_params = {"metric_type": "IP"}  # 更快

# 自定义 Embedding（未归一化）
search_params = {"metric_type": "L2"}  # 更准确
```

#### 策略3：控制 Top-K

```python
# 不要过大的 Top-K
results = collection.search(
    data=[query_vector],
    limit=10,  # ✅ 合理（10-100）
    # limit=10000,  # ❌ 过大（影响性能）
)
```

---

## 5. 实战示例

### 5.1 基础向量检索

```python
from pymilvus import connections, Collection
import numpy as np

# 连接 Milvus
connections.connect("default", host="localhost", port="19530")

# 加载 Collection
collection = Collection("documents")
collection.load()

# 准备查询向量
query_text = "Python 编程教程"
query_vector = embed(query_text)  # 假设已有 embed 函数

# 向量检索
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"ef": 64}},
    limit=10,
    output_fields=["title", "content"]
)

# 输出结果
for hits in results:
    for hit in hits:
        print(f"ID: {hit.id}, Distance: {hit.distance:.4f}")
        print(f"Title: {hit.entity.get('title')}")
        print(f"Content: {hit.entity.get('content')[:100]}...")
        print("-" * 50)
```

---

### 5.2 性能对比测试

```python
import time

# 测试不同索引的性能
index_types = ["HNSW", "IVF_FLAT", "IVF_SQ8"]

for index_type in index_types:
    # 创建索引
    collection.create_index(
        field_name="embedding",
        index_params={"index_type": index_type, "metric_type": "L2", "params": {...}}
    )
    collection.load()

    # 测试查询性能
    start = time.time()
    for _ in range(100):
        collection.search(data=[query_vector], anns_field="embedding", param={...}, limit=10)
    end = time.time()

    qps = 100 / (end - start)
    print(f"{index_type}: QPS = {qps:.2f}")
```

---

## 6. 关键要点总结

### 核心概念

1. **向量检索 = 在高维空间中找最近邻**
2. **ANN 算法 = 用速度换精度（95-99%召回率）**
3. **HNSW = 高召回率，适合 RAG**
4. **IVF = 低内存，适合大规模数据**
5. **L2/IP = 根据向量是否归一化选择**

### 在混合检索中的作用

- 向量检索：语义理解 + 召回候选集
- 标量过滤：精确条件 + 业务规则
- 组合：精准召回 + 高性能

### 性能优化

- 选择合适的索引算法
- 调整索引参数（M, ef, nprobe）
- 使用正确的度量方式
- 控制 Top-K 大小

---

**下一步**：学习 [04_核心概念_2_标量过滤机制.md](./04_核心概念_2_标量过滤机制.md)
