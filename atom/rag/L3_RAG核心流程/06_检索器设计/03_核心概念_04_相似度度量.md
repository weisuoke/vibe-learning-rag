# 核心概念4：相似度度量 (Similarity Metrics)

> 向量检索的核心：如何量化"相似"

---

## 一句话定义

**相似度度量是计算两个向量之间相似程度的数学方法，主要包括余弦相似度（Cosine）、欧氏距离（Euclidean）和内积（Dot Product），选择合适的度量方式直接影响检索质量和性能。**

---

## 三种主要度量

### 1. 余弦相似度 (Cosine Similarity)

**定义**：计算两个向量夹角的余弦值

```
cosine_similarity(A, B) = (A · B) / (||A|| × ||B||)

其中：
- A · B: 向量内积
- ||A||: 向量A的模长
- ||B||: 向量B的模长
```

**特点**：
- 值域：[-1, 1]（通常归一化到[0, 1]）
- 只关注方向，不关注长度
- 最常用的相似度度量

**参考**: [Vector Similarity Metrics - Redis](https://redis.io/blog/vector-similarity)

### 2. 欧氏距离 (Euclidean Distance)

**定义**：两个向量在空间中的直线距离

```
euclidean_distance(A, B) = √(Σ(Ai - Bi)²)
```

**特点**：
- 值域：[0, +∞)
- 距离越小越相似
- 受向量长度影响

### 3. 内积 (Dot Product)

**定义**：两个向量对应元素乘积之和

```
dot_product(A, B) = Σ(Ai × Bi)
```

**特点**：
- 值域：(-∞, +∞)
- 值越大越相似
- 计算最快，但需要向量归一化

---

## Python实现

```python
"""
三种相似度度量实现
"""
import numpy as np

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """
    余弦相似度

    Args:
        a, b: 向量

    Returns:
        相似度 [0, 1]
    """
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)

    if norm_a == 0 or norm_b == 0:
        return 0.0

    return dot_product / (norm_a * norm_b)


def euclidean_distance(a: np.ndarray, b: np.ndarray) -> float:
    """
    欧氏距离

    Args:
        a, b: 向量

    Returns:
        距离 [0, +∞)
    """
    return np.linalg.norm(a - b)


def dot_product(a: np.ndarray, b: np.ndarray) -> float:
    """
    内积

    Args:
        a, b: 向量

    Returns:
        内积值 (-∞, +∞)
    """
    return np.dot(a, b)


# 示例
vec1 = np.array([1, 2, 3])
vec2 = np.array([2, 4, 6])
vec3 = np.array([1, 0, 0])

print("=== 向量1 vs 向量2（方向相同，长度不同）===")
print(f"Cosine: {cosine_similarity(vec1, vec2):.3f}")  # 1.000（方向完全相同）
print(f"Euclidean: {euclidean_distance(vec1, vec2):.3f}")  # 3.742（距离较远）
print(f"Dot Product: {dot_product(vec1, vec2):.3f}")  # 28.000（较大）

print("\n=== 向量1 vs 向量3（方向不同）===")
print(f"Cosine: {cosine_similarity(vec1, vec3):.3f}")  # 0.267（方向差异大）
print(f"Euclidean: {euclidean_distance(vec1, vec3):.3f}")  # 3.606（距离较远）
print(f"Dot Product: {dot_product(vec1, vec3):.3f}")  # 1.000（较小）
```

---

## 直觉理解

### Cosine：方向相似度

```python
"""
Cosine只看方向，不看长度
"""
# 场景：文档长度不同，但主题相同

doc1 = "Python"  # 短文档
doc1_vec = np.array([1, 0, 0])

doc2 = "Python Python Python"  # 长文档（重复3次）
doc2_vec = np.array([3, 0, 0])

# Cosine相似度
cos_sim = cosine_similarity(doc1_vec, doc2_vec)
print(f"Cosine: {cos_sim:.3f}")  # 1.000（方向完全相同）

# 欧氏距离
euc_dist = euclidean_distance(doc1_vec, doc2_vec)
print(f"Euclidean: {euc_dist:.3f}")  # 2.000（距离较远）

# 结论：Cosine认为两个文档完全相似（主题相同）
#       Euclidean认为两个文档有差异（长度不同）
```

### Euclidean：空间距离

```python
"""
Euclidean考虑向量长度
"""
# 场景：聚类分析

points = np.array([
    [1, 1],  # 点A
    [1, 2],  # 点B（靠近A）
    [5, 5]   # 点C（远离A和B）
])

# 计算距离
dist_AB = euclidean_distance(points[0], points[1])
dist_AC = euclidean_distance(points[0], points[2])

print(f"A到B的距离: {dist_AB:.3f}")  # 1.000
print(f"A到C的距离: {dist_AC:.3f}")  # 5.657

# 结论：B离A更近，适合聚类
```

### Dot Product：综合匹配度

```python
"""
Dot Product考虑方向和长度
"""
# 场景：推荐系统

user_preference = np.array([5, 3, 1])  # 用户偏好（运动:5, 音乐:3, 阅读:1）
item1 = np.array([4, 2, 0])  # 商品1（运动相关）
item2 = np.array([1, 5, 2])  # 商品2（音乐相关）

# 计算匹配度
score1 = dot_product(user_preference, item1)
score2 = dot_product(user_preference, item2)

print(f"商品1匹配度: {score1:.3f}")  # 26.000
print(f"商品2匹配度: {score2:.3f}")  # 22.000

# 结论：商品1更匹配用户偏好
```

**参考**: [Embeddings in Practice 2026](https://medium.com/@adnanmasood/embeddings-in-practice-a-research-implementation-guide-9dbf20961590)

---

## 选择指南

### 快速决策表

| 度量 | 适用场景 | 优点 | 缺点 |
|------|----------|------|------|
| **Cosine** | 文本检索、推荐系统 | 不受向量长度影响 | 计算稍慢 |
| **Euclidean** | 聚类、异常检测 | 直观、易理解 | 受向量长度影响 |
| **Dot Product** | 归一化向量检索 | 计算最快 | 需要向量归一化 |

### 详细选择标准

```python
"""
根据场景选择度量
"""
def choose_metric(scenario: str) -> str:
    """
    根据场景选择相似度度量

    Args:
        scenario: 应用场景

    Returns:
        推荐的度量方式
    """
    if scenario in ["文本检索", "语义搜索", "RAG系统"]:
        return "Cosine"  # 不受文档长度影响

    elif scenario in ["聚类", "异常检测", "图像识别"]:
        return "Euclidean"  # 需要考虑向量长度

    elif scenario in ["归一化向量检索", "高性能场景"]:
        return "Dot Product"  # 计算最快

    else:
        return "Cosine"  # 默认推荐


# 测试
scenarios = ["文本检索", "聚类", "归一化向量检索"]
for s in scenarios:
    metric = choose_metric(s)
    print(f"{s} → {metric}")

# 输出:
# 文本检索 → Cosine
# 聚类 → Euclidean
# 归一化向量检索 → Dot Product
```

---

## 性能对比

### 计算复杂度

```python
"""
性能对比实验
"""
import time

# 生成测试数据
n_vectors = 10000
dim = 1536
vectors = np.random.randn(n_vectors, dim).astype(np.float32)
query = np.random.randn(dim).astype(np.float32)

# 1. Cosine相似度
start = time.time()
for vec in vectors:
    _ = cosine_similarity(query, vec)
cosine_time = time.time() - start

# 2. 欧氏距离
start = time.time()
for vec in vectors:
    _ = euclidean_distance(query, vec)
euclidean_time = time.time() - start

# 3. 内积
start = time.time()
for vec in vectors:
    _ = dot_product(query, vec)
dot_time = time.time() - start

print(f"Cosine: {cosine_time:.3f}s")
print(f"Euclidean: {euclidean_time:.3f}s")
print(f"Dot Product: {dot_time:.3f}s")

# 输出示例:
# Cosine: 0.245s
# Euclidean: 0.123s
# Dot Product: 0.089s

print(f"\nDot Product是Cosine的 {cosine_time/dot_time:.1f}倍快")
# 输出: Dot Product是Cosine的 2.8倍快
```

### 优化技巧

```python
"""
向量归一化优化
"""
# 如果向量已归一化，Cosine = Dot Product
def normalize_vector(vec: np.ndarray) -> np.ndarray:
    """归一化向量"""
    norm = np.linalg.norm(vec)
    if norm == 0:
        return vec
    return vec / norm


# 预先归一化所有向量
normalized_vectors = np.array([normalize_vector(v) for v in vectors])
normalized_query = normalize_vector(query)

# 使用Dot Product代替Cosine（更快）
start = time.time()
similarities = np.dot(normalized_vectors, normalized_query)
optimized_time = time.time() - start

print(f"优化后: {optimized_time:.3f}s")
print(f"加速比: {cosine_time/optimized_time:.1f}x")

# 输出:
# 优化后: 0.012s
# 加速比: 20.4x
```

**参考**: [Vector Similarity Search 2025: RAG Engineer's Guide](https://www.linkedin.com/pulse/vector-similarity-search-2025-rag-engineers-field-guide-gadiraju-gqucc)

---

## 在RAG中的应用

### ChromaDB配置

```python
"""
ChromaDB中配置相似度度量
"""
import chromadb

client = chromadb.Client()

# 1. Cosine相似度（默认，推荐）
collection_cosine = client.create_collection(
    name="docs_cosine",
    metadata={"hnsw:space": "cosine"}
)

# 2. 欧氏距离
collection_euclidean = client.create_collection(
    name="docs_euclidean",
    metadata={"hnsw:space": "l2"}  # L2 = Euclidean
)

# 3. 内积
collection_ip = client.create_collection(
    name="docs_ip",
    metadata={"hnsw:space": "ip"}  # Inner Product
)

# 添加文档
documents = ["Python教程", "JavaScript指南", "机器学习入门"]
collection_cosine.add(
    documents=documents,
    ids=["doc1", "doc2", "doc3"]
)

# 查询
results = collection_cosine.query(
    query_texts=["编程语言"],
    n_results=2
)

print("=== Cosine检索结果 ===")
for doc in results['documents'][0]:
    print(f"- {doc}")
```

### FAISS配置

```python
"""
FAISS中配置相似度度量
"""
import faiss

dim = 1536
n_vectors = 10000

# 生成测试数据
vectors = np.random.randn(n_vectors, dim).astype(np.float32)

# 1. 内积索引（最快）
index_ip = faiss.IndexFlatIP(dim)
index_ip.add(vectors)

# 2. L2距离索引（欧氏距离）
index_l2 = faiss.IndexFlatL2(dim)
index_l2.add(vectors)

# 查询
query = np.random.randn(1, dim).astype(np.float32)
k = 5

# 内积检索
D_ip, I_ip = index_ip.search(query, k)
print("=== 内积检索 ===")
print(f"Top-5得分: {D_ip[0]}")

# L2检索
D_l2, I_l2 = index_l2.search(query, k)
print("\n=== L2检索 ===")
print(f"Top-5距离: {D_l2[0]}")
```

### LangChain配置

```python
"""
LangChain中配置相似度度量
"""
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# Cosine相似度（默认）
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    collection_metadata={"hnsw:space": "cosine"}
)

# 检索
results = vectorstore.similarity_search(
    query="编程语言",
    k=3
)

for doc in results:
    print(f"- {doc.page_content}")
```

---

## 2025-2026最佳实践

### 1. 默认使用Cosine

```python
"""
90%场景用Cosine
"""
# ✅ 推荐：Cosine相似度
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    collection_metadata={"hnsw:space": "cosine"}
)

# 原因：
# 1. 不受文档长度影响
# 2. 适合文本检索
# 3. 行业标准
```

### 2. 归一化向量用Dot Product

```python
"""
向量已归一化时用Dot Product（更快）
"""
# OpenAI Embedding已归一化
from openai import OpenAI

client = OpenAI()
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="测试文本"
)
embedding = response.data[0].embedding

# 检查是否归一化
norm = np.linalg.norm(embedding)
print(f"向量模长: {norm:.6f}")  # 接近1.0

# 如果归一化，使用Dot Product
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    collection_metadata={"hnsw:space": "ip"}  # Inner Product
)
```

### 3. 监控度量质量

```python
"""
监控相似度分布
"""
def monitor_similarity_distribution(vectorstore, test_queries):
    """
    监控相似度分布

    Args:
        vectorstore: 向量存储
        test_queries: 测试查询列表
    """
    similarities = []

    for query in test_queries:
        results = vectorstore.similarity_search_with_score(query, k=10)
        scores = [score for _, score in results]
        similarities.extend(scores)

    # 统计
    mean_sim = np.mean(similarities)
    std_sim = np.std(similarities)
    min_sim = np.min(similarities)
    max_sim = np.max(similarities)

    print(f"相似度统计:")
    print(f"  均值: {mean_sim:.3f}")
    print(f"  标准差: {std_sim:.3f}")
    print(f"  最小值: {min_sim:.3f}")
    print(f"  最大值: {max_sim:.3f}")

    # 异常检测
    if std_sim < 0.1:
        print("⚠️ 警告: 相似度方差过小，可能缺乏区分度")

    if mean_sim > 0.9:
        print("⚠️ 警告: 平均相似度过高，可能所有文档都很相似")

# 使用
test_queries = ["Python教程", "机器学习", "Web开发"]
monitor_similarity_distribution(vectorstore, test_queries)
```

---

## 常见问题

### Q1: Cosine和Dot Product有什么区别？

**A**:
- Cosine只看方向，不看长度：`cosine(A, B) = (A·B) / (||A||×||B||)`
- Dot Product考虑方向和长度：`dot(A, B) = A·B`
- 如果向量已归一化（||A||=||B||=1），两者等价
- OpenAI Embedding已归一化，可以用Dot Product代替Cosine（更快）

### Q2: 为什么RAG系统通常用Cosine？

**A**:
1. 不受文档长度影响（长文档和短文档公平比较）
2. 值域固定[0, 1]，易于理解和设置阈值
3. 行业标准，大部分向量数据库默认支持
4. 适合文本语义相似度计算

### Q3: 什么时候用Euclidean距离？

**A**:
- 聚类分析（K-means等）
- 异常检测（离群点检测）
- 图像识别（像素距离）
- 需要考虑向量长度的场景

### Q4: 如何选择相似度阈值？

**A**:
```python
# 在测试集上实验找到最优阈值
thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]
best_threshold = 0.7  # 默认

for threshold in thresholds:
    precision, recall = evaluate_with_threshold(threshold)
    print(f"阈值{threshold}: Precision={precision:.3f}, Recall={recall:.3f}")

# 根据业务需求选择：
# - 精准问答：阈值0.8（高精度）
# - 通用检索：阈值0.7（平衡）
# - 探索性搜索：阈值0.6（高召回）
```

---

## 总结

**相似度度量的核心价值**：
1. ✅ 量化"相似"：将主观的"相似"转化为客观的数值
2. ✅ 影响检索质量：选择合适的度量直接影响检索效果
3. ✅ 影响性能：不同度量计算复杂度不同

**选择建议**：
- **90%场景**：Cosine相似度（推荐）
- **归一化向量**：Dot Product（更快）
- **聚类/异常检测**：Euclidean距离

**2025-2026标准实践**：
- 默认使用Cosine
- 向量归一化后用Dot Product优化性能
- 监控相似度分布，及时发现问题
- 在测试集上实验找到最优阈值

**下一步**：学习【核心概念5：Top-K与MMR】，理解结果数量控制和多样性优化。
