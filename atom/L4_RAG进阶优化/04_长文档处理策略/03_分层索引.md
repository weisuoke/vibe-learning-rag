# 核心概念1：分层索引（Hierarchical Indexing）

**分层索引是构建多层次索引结构的技术，通过从粗到细的逐层检索，快速定位长文档中的相关内容。**

---

## 什么是分层索引？

### 直觉理解

想象你在图书馆找一本书：

```
第1步：找到正确的楼层（文学区 vs 科技区）
第2步：找到正确的书架（编程类 vs 数据库类）
第3步：找到正确的书（Python vs Java）
第4步：翻到正确的章节（第3章：函数）
第5步：找到正确的段落（闭包的定义）
```

**分层索引就是这样的过程**：不是一次性扫描所有内容，而是逐层缩小范围。

### 形式化定义

```
分层索引 = 树形结构 + 逐层检索

树形结构：
- 根节点：全文摘要
- 中间节点：章节摘要
- 叶节点：原文片段

逐层检索：
1. 在根节点检索，找到相关的章节
2. 在章节节点检索，找到相关的段落
3. 在段落节点检索，找到相关的句子
```

---

## 为什么需要分层索引？

### 问题：平铺检索的效率问题

假设一篇 500 页的论文，直接分块：

```python
# 普通分块方式
chunk_size = 500  # 每块 500 字
total_chars = 500 * 1000  # 500 页 × 1000 字/页 = 500,000 字
num_chunks = total_chars / chunk_size  # 1000 个块

# 检索时需要：
# 1. 计算 1000 个块的 Embedding（如果用向量检索）
# 2. 计算 1000 个相似度分数
# 3. 排序并返回 Top-K
```

**问题**：
- 检索效率低（需要扫描 1000 个块）
- 容易丢失上下文（块之间没有层次关系）
- 难以定位（不知道相关内容在哪个章节）

### 解决：分层索引

```python
# 分层索引方式
层级1：全文摘要（1 个节点）
层级2：章节摘要（10 个节点，假设 10 章）
层级3：段落摘要（100 个节点，每章 10 段）
层级4：原文片段（1000 个节点，每段 10 块）

# 检索时：
# 第1步：在 10 个章节中检索，找到相关章节（如第 3 章）
# 第2步：在第 3 章的 10 个段落中检索，找到相关段落（如第 2 段）
# 第3步：在第 2 段的 10 个块中检索，找到相关块
# 总共只需要检索：10 + 10 + 10 = 30 个节点（而不是 1000 个）
```

**优势**：
- ✅ 检索效率高（30 vs 1000）
- ✅ 保留上下文（知道内容在哪个章节）
- ✅ 快速定位（逐层缩小范围）

---

## 分层索引的核心原理

### 原理1：树形结构

```
                    [全文摘要]
                        |
        +---------------+---------------+
        |               |               |
    [第1章摘要]     [第2章摘要]     [第3章摘要]
        |               |               |
    +---+---+       +---+---+       +---+---+
    |   |   |       |   |   |       |   |   |
  [1.1][1.2][1.3] [2.1][2.2][2.3] [3.1][3.2][3.3]
    |   |   |       |   |   |       |   |   |
   原文 原文 原文    原文 原文 原文    原文 原文 原文
```

**关键特性**：
- 每个节点都有摘要（描述子节点的内容）
- 叶节点是原文片段
- 中间节点是摘要

### 原理2：逐层检索

```python
def hierarchical_search(query, tree, top_k=3):
    """分层检索"""

    # 第1层：在根节点的子节点（章节）中检索
    chapter_scores = []
    for chapter in tree.root.children:
        score = similarity(query, chapter.summary)
        chapter_scores.append((chapter, score))

    # 选择 Top-K 章节
    top_chapters = sorted(chapter_scores, key=lambda x: x[1], reverse=True)[:top_k]

    # 第2层：在选中章节的子节点（段落）中检索
    paragraph_scores = []
    for chapter, _ in top_chapters:
        for paragraph in chapter.children:
            score = similarity(query, paragraph.summary)
            paragraph_scores.append((paragraph, score))

    # 选择 Top-K 段落
    top_paragraphs = sorted(paragraph_scores, key=lambda x: x[1], reverse=True)[:top_k]

    # 第3层：在选中段落的子节点（原文块）中检索
    chunk_scores = []
    for paragraph, _ in top_paragraphs:
        for chunk in paragraph.children:
            score = similarity(query, chunk.text)
            chunk_scores.append((chunk, score))

    # 返回 Top-K 块
    return sorted(chunk_scores, key=lambda x: x[1], reverse=True)[:top_k]
```

### 原理3：剪枝优化

**核心思想**：如果父节点不相关，子节点也不需要检索

```python
def hierarchical_search_with_pruning(query, tree, threshold=0.5):
    """带剪枝的分层检索"""

    # 第1层：检索章节
    relevant_chapters = []
    for chapter in tree.root.children:
        score = similarity(query, chapter.summary)
        if score > threshold:  # 剪枝：只保留相关章节
            relevant_chapters.append((chapter, score))

    # 如果没有相关章节，直接返回
    if not relevant_chapters:
        return []

    # 第2层：只在相关章节中检索段落
    relevant_paragraphs = []
    for chapter, _ in relevant_chapters:
        for paragraph in chapter.children:
            score = similarity(query, paragraph.summary)
            if score > threshold:  # 剪枝
                relevant_paragraphs.append((paragraph, score))

    # 第3层：只在相关段落中检索原文块
    relevant_chunks = []
    for paragraph, _ in relevant_paragraphs:
        for chunk in paragraph.children:
            score = similarity(query, chunk.text)
            relevant_chunks.append((chunk, score))

    return sorted(relevant_chunks, key=lambda x: x[1], reverse=True)
```

---

## Python 手写实现

### 完整实现：分层索引系统

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
import numpy as np

@dataclass
class TreeNode:
    """树节点"""
    id: str
    text: str  # 原文或摘要
    embedding: Optional[np.ndarray] = None
    children: List['TreeNode'] = None
    parent: Optional['TreeNode'] = None
    level: int = 0  # 层级（0=根，1=章节，2=段落，3=原文块）

    def __post_init__(self):
        if self.children is None:
            self.children = []

class HierarchicalIndex:
    """分层索引"""

    def __init__(self, embedding_func):
        """
        Args:
            embedding_func: 文本转向量的函数
        """
        self.embedding_func = embedding_func
        self.root = None

    def build_from_document(self, document: str, structure: Dict) -> TreeNode:
        """
        从文档构建分层索引

        Args:
            document: 原文
            structure: 文档结构，如：
                {
                    "title": "全文标题",
                    "chapters": [
                        {
                            "title": "第1章",
                            "paragraphs": ["段落1", "段落2", ...]
                        },
                        ...
                    ]
                }
        """
        # 创建根节点（全文摘要）
        full_summary = self._summarize(document)
        root = TreeNode(
            id="root",
            text=full_summary,
            embedding=self.embedding_func(full_summary),
            level=0
        )

        # 创建章节节点
        for i, chapter in enumerate(structure["chapters"]):
            chapter_text = "\n".join(chapter["paragraphs"])
            chapter_summary = self._summarize(chapter_text)

            chapter_node = TreeNode(
                id=f"chapter_{i}",
                text=chapter_summary,
                embedding=self.embedding_func(chapter_summary),
                parent=root,
                level=1
            )
            root.children.append(chapter_node)

            # 创建段落节点
            for j, paragraph in enumerate(chapter["paragraphs"]):
                paragraph_node = TreeNode(
                    id=f"chapter_{i}_para_{j}",
                    text=paragraph,
                    embedding=self.embedding_func(paragraph),
                    parent=chapter_node,
                    level=2
                )
                chapter_node.children.append(paragraph_node)

        self.root = root
        return root

    def search(self, query: str, top_k: int = 3, threshold: float = 0.5) -> List[TreeNode]:
        """
        分层检索

        Args:
            query: 查询文本
            top_k: 每层返回的 Top-K 结果
            threshold: 相似度阈值（用于剪枝）

        Returns:
            相关的叶节点列表
        """
        query_embedding = self.embedding_func(query)

        # 第1层：检索章节
        chapter_scores = []
        for chapter in self.root.children:
            score = self._cosine_similarity(query_embedding, chapter.embedding)
            if score > threshold:
                chapter_scores.append((chapter, score))

        if not chapter_scores:
            return []

        # 选择 Top-K 章节
        top_chapters = sorted(chapter_scores, key=lambda x: x[1], reverse=True)[:top_k]

        # 第2层：在选中章节中检索段落
        paragraph_scores = []
        for chapter, _ in top_chapters:
            for paragraph in chapter.children:
                score = self._cosine_similarity(query_embedding, paragraph.embedding)
                if score > threshold:
                    paragraph_scores.append((paragraph, score))

        # 返回 Top-K 段落
        top_paragraphs = sorted(paragraph_scores, key=lambda x: x[1], reverse=True)[:top_k]
        return [node for node, _ in top_paragraphs]

    def _summarize(self, text: str, max_length: int = 200) -> str:
        """
        生成摘要（简化版，实际应使用 LLM）
        """
        if len(text) <= max_length:
            return text
        return text[:max_length] + "..."

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """计算余弦相似度"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))


# ===== 使用示例 =====

# 模拟 Embedding 函数（实际应使用 OpenAI API）
def mock_embedding(text: str) -> np.ndarray:
    """模拟 Embedding（实际应使用真实模型）"""
    np.random.seed(hash(text) % 2**32)
    return np.random.rand(384)  # 384 维向量

# 准备文档结构
document_structure = {
    "title": "深度学习入门",
    "chapters": [
        {
            "title": "第1章：神经网络基础",
            "paragraphs": [
                "神经网络是由大量神经元组成的计算模型，每个神经元接收输入，进行加权求和，然后通过激活函数输出。",
                "常见的激活函数包括 Sigmoid、ReLU、Tanh 等，它们引入非线性，使神经网络能够拟合复杂函数。",
                "反向传播算法是训练神经网络的核心，通过梯度下降更新权重，最小化损失函数。"
            ]
        },
        {
            "title": "第2章：卷积神经网络",
            "paragraphs": [
                "卷积神经网络（CNN）专门用于处理图像数据，通过卷积层提取局部特征。",
                "池化层用于降低特征图的维度，减少计算量，同时保留重要特征。",
                "经典的 CNN 架构包括 LeNet、AlexNet、VGG、ResNet 等。"
            ]
        },
        {
            "title": "第3章：循环神经网络",
            "paragraphs": [
                "循环神经网络（RNN）用于处理序列数据，如文本、时间序列等。",
                "LSTM 和 GRU 是 RNN 的改进版本，解决了梯度消失问题，能够捕捉长期依赖。",
                "Transformer 架构通过自注意力机制，彻底取代了 RNN，成为 NLP 的主流模型。"
            ]
        }
    ]
}

# 构建分层索引
print("=== 构建分层索引 ===")
index = HierarchicalIndex(embedding_func=mock_embedding)
full_document = "\n\n".join([
    "\n".join(chapter["paragraphs"])
    for chapter in document_structure["chapters"]
])
root = index.build_from_document(full_document, document_structure)

print(f"根节点: {root.text[:50]}...")
print(f"章节数: {len(root.children)}")
for i, chapter in enumerate(root.children):
    print(f"  第{i+1}章: {chapter.text[:30]}... (段落数: {len(chapter.children)})")

# 执行检索
print("\n=== 执行分层检索 ===")
query = "什么是 LSTM？"
print(f"查询: {query}")

results = index.search(query, top_k=2, threshold=0.3)
print(f"\n检索结果 (Top-{len(results)}):")
for i, node in enumerate(results):
    print(f"\n结果 {i+1}:")
    print(f"  ID: {node.id}")
    print(f"  层级: {node.level}")
    print(f"  内容: {node.text}")
    print(f"  路径: {node.parent.parent.id} → {node.parent.id} → {node.id}")
```

---

## 分层索引的优缺点

### 优点

| 优点 | 说明 |
|------|------|
| ✅ 检索效率高 | 逐层缩小范围，避免扫描所有块 |
| ✅ 保留文档结构 | 知道内容在哪个章节、段落 |
| ✅ 支持多粒度检索 | 可以返回章节摘要或具体段落 |
| ✅ 易于可视化 | 树形结构直观，便于调试 |

### 缺点

| 缺点 | 说明 |
|------|------|
| ❌ 依赖文档结构 | 需要文档有清晰的章节结构 |
| ❌ 构建成本高 | 需要为每层生成摘要 |
| ❌ 可能漏检 | 如果父节点不相关，子节点不会被检索 |
| ❌ 不适合无结构文档 | 如聊天记录、社交媒体内容 |

---

## 在 RAG 开发中的应用

### 应用场景1：学术论文问答

```python
# 场景：用户问"这篇论文的实验结果如何？"
#
# 分层检索流程：
# 1. 在章节层检索 → 找到"第5章：实验结果"
# 2. 在段落层检索 → 找到"5.2 主要实验结果"
# 3. 返回相关段落给 LLM 生成回答
```

### 应用场景2：技术文档检索

```python
# 场景：用户问"如何配置 Kubernetes 的网络策略？"
#
# 分层检索流程：
# 1. 在大类层检索 → 找到"网络配置"
# 2. 在小类层检索 → 找到"网络策略"
# 3. 在具体步骤层检索 → 找到配置示例
```

### 应用场景3：多文档问答

```python
# 场景：用户问"对比 3 篇论文的方法"
#
# 分层检索流程：
# 1. 为每篇论文构建分层索引
# 2. 在每篇论文的章节层检索 → 找到"方法"章节
# 3. 提取每篇论文的方法描述
# 4. 让 LLM 对比
```

---

## 分层索引 vs 平铺检索

| 维度 | 平铺检索 | 分层索引 |
|------|---------|---------|
| **检索效率** | 需要扫描所有块 | 逐层缩小范围，效率高 |
| **上下文保留** | 块之间独立，无上下文 | 保留文档结构，有层次感 |
| **适用文档** | 任何文档 | 需要有结构的文档 |
| **构建成本** | 低（只需分块） | 高（需要生成摘要） |
| **检索精度** | 可能返回无关块 | 更精准（有层次过滤） |
| **可解释性** | 低（不知道块的位置） | 高（知道内容在哪个章节） |

---

## 实战技巧

### 技巧1：自动提取文档结构

```python
def extract_structure_from_markdown(markdown_text: str) -> Dict:
    """从 Markdown 文档提取结构"""
    lines = markdown_text.split("\n")
    structure = {"title": "", "chapters": []}
    current_chapter = None
    current_paragraph = []

    for line in lines:
        if line.startswith("# "):  # 一级标题 = 文档标题
            structure["title"] = line[2:].strip()
        elif line.startswith("## "):  # 二级标题 = 章节
            if current_chapter and current_paragraph:
                current_chapter["paragraphs"].append("\n".join(current_paragraph))
                current_paragraph = []
            current_chapter = {
                "title": line[3:].strip(),
                "paragraphs": []
            }
            structure["chapters"].append(current_chapter)
        elif line.strip() and current_chapter:  # 非空行 = 段落内容
            current_paragraph.append(line)
        elif not line.strip() and current_paragraph:  # 空行 = 段落结束
            current_chapter["paragraphs"].append("\n".join(current_paragraph))
            current_paragraph = []

    # 处理最后一个段落
    if current_chapter and current_paragraph:
        current_chapter["paragraphs"].append("\n".join(current_paragraph))

    return structure
```

### 技巧2：动态调整检索深度

```python
def adaptive_search(index, query, max_results=5):
    """自适应检索深度"""

    # 先在章节层检索
    chapter_results = index.search(query, top_k=3, threshold=0.6)

    # 如果章节层结果足够好（高相似度），直接返回
    if chapter_results and chapter_results[0].score > 0.8:
        return chapter_results

    # 否则，深入到段落层检索
    paragraph_results = []
    for chapter in chapter_results:
        for paragraph in chapter.children:
            score = similarity(query, paragraph.text)
            paragraph_results.append((paragraph, score))

    return sorted(paragraph_results, key=lambda x: x[1], reverse=True)[:max_results]
```

### 技巧3：混合检索（分层 + 向量）

```python
def hybrid_hierarchical_search(index, query, alpha=0.5):
    """
    混合检索：分层索引 + 向量检索

    Args:
        alpha: 分层索引的权重（1-alpha 为向量检索的权重）
    """
    # 分层检索得分
    hierarchical_results = index.search(query, top_k=10)

    # 向量检索得分（在所有叶节点中检索）
    all_leaves = index.get_all_leaves()
    vector_results = vector_search(query, all_leaves, top_k=10)

    # 融合得分
    final_scores = {}
    for node, score in hierarchical_results:
        final_scores[node.id] = alpha * score

    for node, score in vector_results:
        if node.id in final_scores:
            final_scores[node.id] += (1 - alpha) * score
        else:
            final_scores[node.id] = (1 - alpha) * score

    # 排序返回
    return sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
```

---

## 总结

**分层索引的核心**：
1. **树形结构**：根节点 → 章节 → 段落 → 原文块
2. **逐层检索**：从粗到细，逐层缩小范围
3. **剪枝优化**：父节点不相关，子节点不检索

**适用场景**：
- ✅ 有清晰结构的长文档（学术论文、技术文档、书籍）
- ✅ 需要快速定位的场景
- ✅ 需要保留文档结构的场景

**不适用场景**：
- ❌ 无结构的文档（聊天记录、社交媒体）
- ❌ 需要全文理解的场景（用摘要链更好）
- ❌ 文档结构不清晰的场景

---

**下一步：** [04_摘要链](./04_摘要链.md) - 学习如何通过逐层总结压缩长文档
