# 实战代码 - 场景2：AI 流式响应

> 使用生成器实现 AI 流式输出，提升用户体验

---

## 场景描述

**问题：** 调用 LLM API 时，如果等待全部生成完成再返回，用户需要等待很久（10-30秒）

**挑战：**
- 用户体验差：长时间等待，没有反馈
- 无法提前展示部分结果
- 不能实时看到 AI 的"思考过程"

**解决方案：** 使用生成器实现流式响应，逐 token 返回

---

## 完整可运行代码

```python
"""
场景2：AI 流式响应
演示：使用生成器实现 LLM 流式输出，提升用户体验
"""

import time
from typing import Generator, AsyncGenerator
import asyncio

# ===== 1. 模拟 LLM 流式响应 =====
print("=== 1. 模拟 LLM 流式响应 ===\n")

def simulate_llm_stream(prompt: str) -> Generator[str, None, None]:
    """
    模拟 LLM 流式响应（用于演示）

    实际使用时，这里会调用 OpenAI/Anthropic API
    """
    response = f"这是对 '{prompt}' 的回答：Python 生成器是一种强大的工具，可以实现惰性求值和流式处理。"

    # 逐字返回（模拟流式输出）
    for char in response:
        yield char
        time.sleep(0.05)  # 模拟网络延迟

# 使用
print("流式输出演示:")
for token in simulate_llm_stream("什么是生成器？"):
    print(token, end='', flush=True)

print("\n\n")

# ===== 2. 对比：非流式 vs 流式 =====
print("=== 2. 对比：非流式 vs 流式 ===\n")

def non_stream_response(prompt: str) -> str:
    """非流式：等待全部生成完成"""
    response = f"这是对 '{prompt}' 的回答：Python 生成器是一种强大的工具。"
    time.sleep(2)  # 模拟生成时间
    return response

def stream_response(prompt: str) -> Generator[str, None, None]:
    """流式：边生成边返回"""
    response = f"这是对 '{prompt}' 的回答：Python 生成器是一种强大的工具。"
    for char in response:
        yield char
        time.sleep(0.02)

# 非流式
print("非流式响应:")
start = time.time()
result = non_stream_response("什么是生成器？")
print(f"等待 {time.time() - start:.1f}s 后显示:")
print(result)

print("\n流式响应:")
start = time.time()
print("立即开始显示:")
for token in stream_response("什么是生成器？"):
    print(token, end='', flush=True)
print(f"\n总耗时: {time.time() - start:.1f}s")

print("\n")

# ===== 3. FastAPI 流式响应端点 =====
print("=== 3. FastAPI 流式响应端点 ===\n")

from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

def generate_stream(prompt: str) -> Generator[str, None, None]:
    """生成流式数据"""
    response = f"回答 '{prompt}'：生成器是 Python 的核心特性之一。"

    for char in response:
        yield char
        time.sleep(0.01)

@app.get("/chat")
async def chat_endpoint(prompt: str):
    """
    流式聊天端点

    返回格式：text/plain（逐字返回）
    """
    return StreamingResponse(
        generate_stream(prompt),
        media_type="text/plain"
    )

@app.get("/chat-sse")
async def chat_sse_endpoint(prompt: str):
    """
    流式聊天端点（Server-Sent Events 格式）

    返回格式：text/event-stream
    """
    def generate_sse():
        response = f"回答 '{prompt}'：生成器是 Python 的核心特性。"
        for i, char in enumerate(response):
            # SSE 格式：data: {content}\n\n
            yield f"data: {char}\n\n"
            time.sleep(0.01)

    return StreamingResponse(
        generate_sse(),
        media_type="text/event-stream"
    )

print("FastAPI 端点定义完成")
print("  GET /chat?prompt=你的问题")
print("  GET /chat-sse?prompt=你的问题")

print("\n")

# ===== 4. 使用 OpenAI API（真实示例）=====
print("=== 4. 使用 OpenAI API（真实示例）===\n")

# 注意：需要安装 openai 库并设置 API key
# pip install openai
# export OPENAI_API_KEY=your_key_here

def stream_openai_response(prompt: str) -> Generator[str, None, None]:
    """
    使用 OpenAI API 流式响应

    注意：这是真实的 API 调用示例
    """
    try:
        from openai import OpenAI
        client = OpenAI()

        # 调用 OpenAI API，启用流式响应
        stream = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            stream=True  # 关键：启用流式
        )

        # 逐个 token 返回
        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    except ImportError:
        yield "（需要安装 openai 库）"
    except Exception as e:
        yield f"（API 调用失败: {e}）"

print("OpenAI 流式响应函数定义完成")
print("使用示例:")
print("  for token in stream_openai_response('什么是生成器？'):")
print("      print(token, end='', flush=True)")

print("\n")

# ===== 5. 异步生成器（推荐）=====
print("=== 5. 异步生成器（推荐）===\n")

async def async_stream_response(prompt: str) -> AsyncGenerator[str, None]:
    """
    异步流式响应（推荐用于生产环境）

    优势：
    - 不阻塞事件循环
    - 可以同时处理多个请求
    - 性能更好
    """
    response = f"回答 '{prompt}'：异步生成器更适合生产环境。"

    for char in response:
        yield char
        await asyncio.sleep(0.01)  # 异步等待

# FastAPI 异步端点
@app.get("/chat-async")
async def chat_async_endpoint(prompt: str):
    """异步流式聊天端点"""
    return StreamingResponse(
        async_stream_response(prompt),
        media_type="text/plain"
    )

print("异步生成器定义完成")
print("  async for token in async_stream_response('问题'):")
print("      print(token, end='')")

print("\n")

# ===== 6. 带状态的流式响应 =====
print("=== 6. 带状态的流式响应 ===\n")

class StreamingChat:
    """
    带状态的流式聊天

    功能：
    - 记录对话历史
    - 流式返回响应
    - 支持多轮对话
    """

    def __init__(self):
        self.history = []

    def chat(self, prompt: str) -> Generator[str, None, None]:
        """流式聊天"""
        # 添加用户消息到历史
        self.history.append({"role": "user", "content": prompt})

        # 生成响应
        response = f"基于历史对话（{len(self.history)}轮），回答：{prompt[:20]}..."
        full_response = ""

        for char in response:
            full_response += char
            yield char
            time.sleep(0.01)

        # 添加助手响应到历史
        self.history.append({"role": "assistant", "content": full_response})

    def get_history(self):
        """获取对话历史"""
        return self.history

# 使用
chat = StreamingChat()

print("第一轮对话:")
for token in chat.chat("什么是生成器？"):
    print(token, end='', flush=True)

print("\n\n第二轮对话:")
for token in chat.chat("它有什么优势？"):
    print(token, end='', flush=True)

print(f"\n\n对话历史: {len(chat.get_history())} 条消息")

print("\n")

# ===== 7. 流式响应 + 错误处理 =====
print("=== 7. 流式响应 + 错误处理 ===\n")

def safe_stream_response(prompt: str) -> Generator[str, None, None]:
    """
    安全的流式响应（带错误处理）
    """
    try:
        # 验证输入
        if not prompt or len(prompt) > 1000:
            yield "错误：输入无效"
            return

        # 生成响应
        response = f"回答 '{prompt}'：这是一个安全的流式响应。"

        for char in response:
            yield char
            time.sleep(0.01)

    except Exception as e:
        yield f"\n错误：{str(e)}"

# 使用
print("正常情况:")
for token in safe_stream_response("测试问题"):
    print(token, end='', flush=True)

print("\n\n异常情况:")
for token in safe_stream_response(""):
    print(token, end='', flush=True)

print("\n\n")

# ===== 8. 流式响应 + 进度提示 =====
print("=== 8. 流式响应 + 进度提示 ===\n")

def stream_with_progress(prompt: str) -> Generator[str, None, None]:
    """
    带进度提示的流式响应
    """
    # 阶段1：思考中
    yield "[思考中...]\n"
    time.sleep(0.5)

    # 阶段2：生成回答
    yield "[开始回答]\n"
    response = f"关于 '{prompt}'：生成器是 Python 的重要特性。"

    for char in response:
        yield char
        time.sleep(0.02)

    # 阶段3：完成
    yield "\n[回答完成]"

# 使用
print("带进度提示的流式响应:")
for token in stream_with_progress("什么是生成器？"):
    print(token, end='', flush=True)

print("\n\n")

# ===== 9. 批量流式响应 =====
print("=== 9. 批量流式响应 ===\n")

def batch_stream_response(prompts: list) -> Generator[dict, None, None]:
    """
    批量处理多个问题，流式返回结果
    """
    for i, prompt in enumerate(prompts):
        # 返回问题信息
        yield {
            "index": i,
            "prompt": prompt,
            "status": "processing"
        }

        # 生成回答
        response = f"回答 {i+1}：{prompt[:20]}..."

        for char in response:
            yield {
                "index": i,
                "token": char,
                "status": "streaming"
            }
            time.sleep(0.01)

        # 完成标记
        yield {
            "index": i,
            "status": "completed"
        }

# 使用
prompts = ["什么是生成器？", "什么是迭代器？"]

print("批量流式响应:")
for result in batch_stream_response(prompts):
    if result["status"] == "processing":
        print(f"\n处理问题 {result['index']+1}: {result['prompt']}")
    elif result["status"] == "streaming":
        print(result["token"], end='', flush=True)
    elif result["status"] == "completed":
        print(f" [完成]")

print("\n")

# ===== 10. 实际应用：完整的 AI 聊天服务 =====
print("=== 10. 实际应用：完整的 AI 聊天服务 ===\n")

class AIStreamingService:
    """
    AI 流式聊天服务

    功能：
    - 流式响应
    - 对话历史管理
    - 错误处理
    - 超时控制
    """

    def __init__(self, timeout: float = 30.0):
        self.timeout = timeout
        self.history = []

    def chat_stream(self, prompt: str) -> Generator[str, None, None]:
        """
        流式聊天

        Args:
            prompt: 用户输入

        Yields:
            str: 响应的每个 token
        """
        try:
            # 验证输入
            if not prompt:
                yield "错误：输入不能为空"
                return

            # 添加到历史
            self.history.append({"role": "user", "content": prompt})

            # 模拟 LLM 调用
            response = self._generate_response(prompt)
            full_response = ""

            # 流式返回
            for token in response:
                full_response += token
                yield token

            # 保存响应到历史
            self.history.append({
                "role": "assistant",
                "content": full_response
            })

        except Exception as e:
            yield f"\n错误：{str(e)}"

    def _generate_response(self, prompt: str) -> Generator[str, None, None]:
        """生成响应（模拟）"""
        response = f"基于 {len(self.history)} 轮对话，回答 '{prompt}'：生成器是 Python 的核心特性。"

        for char in response:
            yield char
            time.sleep(0.01)

    def get_history(self):
        """获取对话历史"""
        return self.history

    def clear_history(self):
        """清空对话历史"""
        self.history = []

# 使用
service = AIStreamingService()

print("AI 流式聊天服务演示:")
print("\n问题1: 什么是生成器？")
for token in service.chat_stream("什么是生成器？"):
    print(token, end='', flush=True)

print("\n\n问题2: 它有什么用？")
for token in service.chat_stream("它有什么用？"):
    print(token, end='', flush=True)

print(f"\n\n对话历史: {len(service.get_history())} 条消息")

print("\n")

# ===== 总结 =====
print("=== 总结 ===")
print("\n生成器实现流式响应的优势:")
print("  1. 用户体验好：立即开始显示，不需要等待")
print("  2. 内存高效：不需要存储完整响应")
print("  3. 可中断：用户可以随时停止")
print("  4. 实时反馈：看到 AI 的'思考过程'")
print("\n适用场景:")
print("  - AI 聊天应用")
print("  - 代码生成")
print("  - 长文本生成")
print("  - 实时翻译")
print("\n技术要点:")
print("  - 使用 yield 逐 token 返回")
print("  - FastAPI 用 StreamingResponse")
print("  - 推荐使用异步生成器")
print("  - 注意错误处理和超时控制")
```

---

## 运行输出示例

```
=== 1. 模拟 LLM 流式响应 ===

流式输出演示:
这是对 '什么是生成器？' 的回答：Python 生成器是一种强大的工具，可以实现惰性求值和流式处理。

=== 2. 对比：非流式 vs 流式 ===

非流式响应:
等待 2.0s 后显示:
这是对 '什么是生成器？' 的回答：Python 生成器是一种强大的工具。

流式响应:
立即开始显示:
这是对 '什么是生成器？' 的回答：Python 生成器是一种强大的工具。
总耗时: 1.4s

=== 3. FastAPI 流式响应端点 ===

FastAPI 端点定义完成
  GET /chat?prompt=你的问题
  GET /chat-sse?prompt=你的问题

=== 4. 使用 OpenAI API（真实示例）===

OpenAI 流式响应函数定义完成
使用示例:
  for token in stream_openai_response('什么是生成器？'):
      print(token, end='', flush=True)

=== 5. 异步生成器（推荐）===

异步生成器定义完成
  async for token in async_stream_response('问题'):
      print(token, end='')

=== 6. 带状态的流式响应 ===

第一轮对话:
基于历史对话（1轮），回答：什么是生成器？...

第二轮对话:
基于历史对话（3轮），回答：它有什么优势？...

对话历史: 4 条消息

=== 7. 流式响应 + 错误处理 ===

正常情况:
回答 '测试问题'：这是一个安全的流式响应。

异常情况:
错误：输入无效

=== 8. 流式响应 + 进度提示 ===

带进度提示的流式响应:
[思考中...]
[开始回答]
关于 '什么是生成器？'：生成器是 Python 的重要特性。
[回答完成]

=== 9. 批量流式响应 ===

批量流式响应:

处理问题 1: 什么是生成器？
回答 1：什么是生成器？... [完成]

处理问题 2: 什么是迭代器？
回答 2：什么是迭代器？... [完成]

=== 10. 实际应用：完整的 AI 聊天服务 ===

AI 流式聊天服务演示:

问题1: 什么是生成器？
基于 1 轮对话，回答 '什么是生成器？'：生成器是 Python 的核心特性。

问题2: 它有什么用？
基于 3 轮对话，回答 '它有什么用？'：生成器是 Python 的核心特性。

对话历史: 4 条消息

=== 总结 ===

生成器实现流式响应的优势:
  1. 用户体验好：立即开始显示，不需要等待
  2. 内存高效：不需要存储完整响应
  3. 可中断：用户可以随时停止
  4. 实时反馈：看到 AI 的'思考过程'

适用场景:
  - AI 聊天应用
  - 代码生成
  - 长文本生成
  - 实时翻译

技术要点:
  - 使用 yield 逐 token 返回
  - FastAPI 用 StreamingResponse
  - 推荐使用异步生成器
  - 注意错误处理和超时控制
```

---

## 关键技术点

### 1. FastAPI StreamingResponse

```python
from fastapi.responses import StreamingResponse

@app.get("/stream")
async def stream_endpoint():
    def generate():
        for item in data:
            yield item

    return StreamingResponse(
        generate(),
        media_type="text/plain"
    )
```

### 2. Server-Sent Events (SSE) 格式

```python
def generate_sse():
    for item in data:
        yield f"data: {item}\n\n"  # SSE 格式

return StreamingResponse(
    generate_sse(),
    media_type="text/event-stream"
)
```

### 3. 异步生成器（推荐）

```python
async def async_generator():
    for item in data:
        yield item
        await asyncio.sleep(0.01)

# 使用
async for item in async_generator():
    print(item)
```

### 4. OpenAI 流式调用

```python
stream = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    stream=True  # 启用流式
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        yield chunk.choices[0].delta.content
```

---

## 用户体验对比

### 非流式响应

```
用户: 什么是生成器？
[等待 10 秒...]
AI: 生成器是 Python 中一种特殊的迭代器...（突然显示全部内容）
```

**问题：**
- 等待时间长
- 没有反馈
- 用户不知道是否在处理

### 流式响应

```
用户: 什么是生成器？
AI: 生（立即开始）
AI: 生成（逐字显示）
AI: 生成器（像打字机）
AI: 生成器是（实时反馈）
AI: 生成器是 Python 中一种特殊的迭代器...（逐字完成）
```

**优势：**
- 立即开始显示
- 实时反馈
- 用户体验好

---

## 在 AI Agent 开发中的应用

### 应用1：聊天机器人

```python
@app.post("/chat")
async def chat(request: ChatRequest):
    """AI 聊天端点"""
    return StreamingResponse(
        stream_ai_response(request.message),
        media_type="text/plain"
    )
```

### 应用2：代码生成

```python
def generate_code_stream(prompt: str):
    """流式生成代码"""
    for chunk in llm_stream(f"生成代码：{prompt}"):
        yield chunk

# 用户可以实时看到代码生成过程
```

### 应用3：文档生成

```python
def generate_doc_stream(topic: str):
    """流式生成文档"""
    for section in ["简介", "原理", "示例", "总结"]:
        yield f"\n## {section}\n"
        for chunk in llm_stream(f"写{section}"):
            yield chunk
```

---

## 最佳实践

### 1. 使用异步生成器

```python
# ✅ 推荐：异步生成器
async def async_stream():
    async for chunk in async_source():
        yield chunk

# ❌ 不推荐：同步生成器（阻塞事件循环）
def sync_stream():
    for chunk in source():
        yield chunk
```

### 2. 错误处理

```python
def safe_stream():
    try:
        for chunk in source():
            yield chunk
    except Exception as e:
        yield f"\n错误：{str(e)}"
```

### 3. 超时控制

```python
import asyncio

async def stream_with_timeout(timeout=30):
    try:
        async with asyncio.timeout(timeout):
            async for chunk in source():
                yield chunk
    except asyncio.TimeoutError:
        yield "\n超时"
```

### 4. 进度提示

```python
def stream_with_status():
    yield "[开始]\n"
    for chunk in source():
        yield chunk
    yield "\n[完成]"
```

---

## 常见问题

### Q1: 如何中断流式响应？

```python
# 客户端关闭连接时，生成器会自动停止
# 服务端可以检测客户端断开
```

### Q2: 如何限制流式响应的速度？

```python
async def rate_limited_stream():
    for chunk in source():
        yield chunk
        await asyncio.sleep(0.01)  # 限速
```

### Q3: 如何缓存流式响应？

```python
def cached_stream():
    cache = []
    for chunk in source():
        cache.append(chunk)
        yield chunk
    # cache 包含完整响应
```

---

**版本：** v1.0
**最后更新：** 2026-02-11
