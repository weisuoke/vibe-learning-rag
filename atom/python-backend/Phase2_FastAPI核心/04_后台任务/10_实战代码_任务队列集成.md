# 实战代码：任务队列集成

> 完整可运行的 Celery 和 RQ 集成示例，展示如何从 BackgroundTasks 迁移到任务队列

---

## 示例1：Celery 基础集成

### 场景描述
将 BackgroundTasks 迁移到 Celery，处理长时间任务。

### 完整代码

```python
"""
示例1：Celery 基础集成
演示：使用 Celery 处理长时间任务
"""

# ===== 文件1: celery_app.py =====
from celery import Celery
import os

# 创建 Celery 应用
celery_app = Celery(
    'tasks',
    broker=os.getenv('CELERY_BROKER_URL', 'redis://localhost:6379/0'),
    backend=os.getenv('CELERY_RESULT_BACKEND', 'redis://localhost:6379/1')
)

# 配置 Celery
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=300,  # 5分钟超时
)

# ===== 文件2: tasks.py =====
from celery_app import celery_app
import time
import logging

logger = logging.getLogger(__name__)

@celery_app.task(bind=True, max_retries=3)
def process_large_file(self, file_path: str):
    """处理大文件（长时间任务）"""
    try:
        logger.info(f"开始处理文件: {file_path}")

        # 模拟长时间处理
        for i in range(10):
            time.sleep(1)
            # 更新任务进度
            self.update_state(
                state='PROGRESS',
                meta={'current': i + 1, 'total': 10}
            )

        logger.info(f"文件处理完成: {file_path}")
        return {"status": "success", "file": file_path}

    except Exception as e:
        logger.error(f"文件处理失败: {e}")
        # 重试任务
        raise self.retry(exc=e, countdown=60)

@celery_app.task(bind=True, max_retries=3)
def send_email(self, email: str, subject: str, body: str):
    """发送邮件（带重试）"""
    try:
        logger.info(f"发送邮件到: {email}")

        # 模拟发送邮件
        time.sleep(2)

        logger.info(f"邮件发送成功: {email}")
        return {"status": "success", "email": email}

    except Exception as e:
        logger.error(f"邮件发送失败: {e}")
        raise self.retry(exc=e, countdown=60)

# ===== 文件3: main.py =====
from fastapi import FastAPI
from pydantic import BaseModel, EmailStr
from tasks import process_large_file, send_email
from celery_app import celery_app
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

class FileProcessRequest(BaseModel):
    file_path: str

class EmailRequest(BaseModel):
    email: EmailStr
    subject: str
    body: str

# ===== API 端点 =====
@app.post("/process-file")
async def process_file(request: FileProcessRequest):
    """提交文件处理任务"""
    # 提交任务到 Celery
    task = process_large_file.delay(request.file_path)

    return {
        "message": "任务已提交",
        "task_id": task.id,
        "status": "pending"
    }

@app.post("/send-email")
async def send_email_endpoint(request: EmailRequest):
    """提交邮件发送任务"""
    task = send_email.delay(
        request.email,
        request.subject,
        request.body
    )

    return {
        "message": "邮件任务已提交",
        "task_id": task.id
    }

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    """查询任务状态"""
    task = celery_app.AsyncResult(task_id)

    if task.state == 'PENDING':
        response = {
            "state": task.state,
            "status": "任务等待中"
        }
    elif task.state == 'PROGRESS':
        response = {
            "state": task.state,
            "current": task.info.get('current', 0),
            "total": task.info.get('total', 1),
            "status": "任务进行中"
        }
    elif task.state == 'SUCCESS':
        response = {
            "state": task.state,
            "result": task.result,
            "status": "任务完成"
        }
    elif task.state == 'FAILURE':
        response = {
            "state": task.state,
            "error": str(task.info),
            "status": "任务失败"
        }
    else:
        response = {
            "state": task.state,
            "status": "未知状态"
        }

    return response

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 部署步骤

```bash
# 1. 安装依赖
uv add celery redis fastapi uvicorn[standard]

# 2. 启动 Redis
docker run -d -p 6379:6379 redis:latest

# 3. 启动 Celery Worker
celery -A celery_app worker --loglevel=info

# 4. 启动 FastAPI 应用
python main.py

# 5. 测试 API
# 提交任务
curl -X POST "http://localhost:8000/process-file" \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/tmp/large_file.txt"}'

# 查询任务状态
curl "http://localhost:8000/task/{task_id}"
```

---

## 示例2：RQ 集成

### 场景描述
使用 RQ（Redis Queue）作为轻量级任务队列。

### 完整代码

```python
"""
示例2：RQ 集成
演示：使用 RQ 处理后台任务
"""

# ===== 文件1: tasks.py =====
import time
import logging

logger = logging.getLogger(__name__)

def process_document(file_path: str):
    """处理文档"""
    logger.info(f"开始处理文档: {file_path}")

    # 模拟处理
    time.sleep(5)

    logger.info(f"文档处理完成: {file_path}")
    return {"status": "success", "file": file_path}

def generate_report(user_id: int, report_type: str):
    """生成报告"""
    logger.info(f"开始生成报告: user_id={user_id}, type={report_type}")

    # 模拟生成报告
    time.sleep(10)

    logger.info(f"报告生成完成: user_id={user_id}")
    return {
        "status": "success",
        "user_id": user_id,
        "report_type": report_type,
        "url": f"/reports/{user_id}/{report_type}.pdf"
    }

# ===== 文件2: main.py =====
from fastapi import FastAPI
from pydantic import BaseModel
from redis import Redis
from rq import Queue
from rq.job import Job
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# 连接 Redis
redis_conn = Redis(host='localhost', port=6379, db=0)

# 创建队列
default_queue = Queue('default', connection=redis_conn)
high_priority_queue = Queue('high', connection=redis_conn)

class DocumentRequest(BaseModel):
    file_path: str

class ReportRequest(BaseModel):
    user_id: int
    report_type: str

# ===== API 端点 =====
@app.post("/process-document")
async def process_document_endpoint(request: DocumentRequest):
    """提交文档处理任务"""
    from tasks import process_document

    # 提交任务到 RQ
    job = default_queue.enqueue(
        process_document,
        request.file_path,
        job_timeout='5m'  # 5分钟超时
    )

    return {
        "message": "任务已提交",
        "job_id": job.id,
        "status": "queued"
    }

@app.post("/generate-report")
async def generate_report_endpoint(request: ReportRequest):
    """提交报告生成任务（高优先级）"""
    from tasks import generate_report

    # 提交到高优先级队列
    job = high_priority_queue.enqueue(
        generate_report,
        request.user_id,
        request.report_type,
        job_timeout='10m'
    )

    return {
        "message": "报告生成任务已提交",
        "job_id": job.id
    }

@app.get("/job/{job_id}")
async def get_job_status(job_id: str):
    """查询任务状态"""
    try:
        job = Job.fetch(job_id, connection=redis_conn)

        response = {
            "job_id": job.id,
            "status": job.get_status(),
            "created_at": job.created_at.isoformat() if job.created_at else None,
            "started_at": job.started_at.isoformat() if job.started_at else None,
            "ended_at": job.ended_at.isoformat() if job.ended_at else None,
        }

        if job.is_finished:
            response["result"] = job.result
        elif job.is_failed:
            response["error"] = job.exc_info

        return response

    except Exception as e:
        return {
            "error": f"任务不存在: {str(e)}"
        }

@app.get("/queues")
async def get_queue_stats():
    """获取队列统计"""
    return {
        "default": {
            "count": len(default_queue),
            "name": default_queue.name
        },
        "high": {
            "count": len(high_priority_queue),
            "name": high_priority_queue.name
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 部署步骤

```bash
# 1. 安装依赖
uv add rq redis fastapi uvicorn[standard]

# 2. 启动 Redis
docker run -d -p 6379:6379 redis:latest

# 3. 启动 RQ Worker（默认队列）
rq worker default --url redis://localhost:6379

# 4. 启动 RQ Worker（高优先级队列）
rq worker high --url redis://localhost:6379

# 5. 启动 FastAPI 应用
python main.py

# 6. 测试 API
curl -X POST "http://localhost:8000/process-document" \
  -H "Content-Type: application/json" \
  -d '{"file_path": "/tmp/document.pdf"}'

# 查询任务状态
curl "http://localhost:8000/job/{job_id}"

# 查询队列统计
curl "http://localhost:8000/queues"
```

---

## 示例3：混合使用 BackgroundTasks 和 Celery

### 场景描述
根据任务类型选择合适的执行方式。

### 完整代码

```python
"""
示例3：混合使用 BackgroundTasks 和 Celery
演示：根据任务类型选择执行方式
"""

from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from celery import Celery
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# 配置 Celery
celery_app = Celery(
    'tasks',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

# ===== Celery 任务（长时间任务） =====
@celery_app.task(bind=True, max_retries=3)
def process_video(self, video_path: str):
    """处理视频（长时间任务）"""
    try:
        logger.info(f"开始处理视频: {video_path}")

        # 模拟视频处理（可能需要几分钟）
        for i in range(60):
            time.sleep(1)
            self.update_state(
                state='PROGRESS',
                meta={'current': i + 1, 'total': 60}
            )

        logger.info(f"视频处理完成: {video_path}")
        return {"status": "success", "video": video_path}

    except Exception as e:
        logger.error(f"视频处理失败: {e}")
        raise self.retry(exc=e, countdown=300)

# ===== BackgroundTasks 任务（短时间任务） =====
def log_user_action(user_id: int, action: str):
    """记录用户操作（短时间任务）"""
    logger.info(f"记录用户操作: user_id={user_id}, action={action}")

    # 写入日志文件
    with open("user_actions.log", "a") as f:
        f.write(f"{user_id},{action},{time.time()}\n")

    logger.info("操作记录完成")

def send_notification(user_id: int, message: str):
    """发送通知（短时间任务）"""
    logger.info(f"发送通知: user_id={user_id}")

    # 模拟发送通知
    time.sleep(2)

    logger.info("通知发送完成")

# ===== 请求模型 =====
class VideoProcessRequest(BaseModel):
    video_path: str
    user_id: int

class UserActionRequest(BaseModel):
    user_id: int
    action: str

# ===== API 端点 =====
@app.post("/process-video")
async def process_video_endpoint(
    request: VideoProcessRequest,
    background_tasks: BackgroundTasks
):
    """处理视频（混合使用）"""
    # 长时间任务：用 Celery
    task = process_video.delay(request.video_path)

    # 短时间任务：用 BackgroundTasks
    background_tasks.add_task(
        log_user_action,
        request.user_id,
        "process_video"
    )
    background_tasks.add_task(
        send_notification,
        request.user_id,
        "视频处理已开始"
    )

    return {
        "message": "视频处理已开始",
        "task_id": task.id
    }

@app.post("/user-action")
async def user_action_endpoint(
    request: UserActionRequest,
    background_tasks: BackgroundTasks
):
    """记录用户操作（只用 BackgroundTasks）"""
    # 短时间任务：用 BackgroundTasks
    background_tasks.add_task(
        log_user_action,
        request.user_id,
        request.action
    )

    return {"message": "操作已记录"}

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    """查询 Celery 任务状态"""
    task = celery_app.AsyncResult(task_id)

    if task.state == 'PROGRESS':
        return {
            "state": task.state,
            "current": task.info.get('current', 0),
            "total": task.info.get('total', 1),
            "progress": f"{task.info.get('current', 0) / task.info.get('total', 1) * 100:.1f}%"
        }
    elif task.state == 'SUCCESS':
        return {
            "state": task.state,
            "result": task.result
        }
    else:
        return {
            "state": task.state
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 使用建议

**选择标准：**

```python
# 决策函数
def choose_task_executor(task_duration: int, is_critical: bool):
    """选择任务执行器"""
    if task_duration < 30 and not is_critical:
        return "BackgroundTasks"
    else:
        return "Celery"

# 示例
choose_task_executor(5, False)   # → BackgroundTasks
choose_task_executor(120, True)  # → Celery
choose_task_executor(10, True)   # → Celery（关键任务）
```

---

## 示例4：Celery 定时任务

### 场景描述
使用 Celery Beat 实现定时任务。

### 完整代码

```python
"""
示例4：Celery 定时任务
演示：使用 Celery Beat 实现定时任务
"""

# ===== 文件1: celery_app.py =====
from celery import Celery
from celery.schedules import crontab

celery_app = Celery(
    'tasks',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

# 配置定时任务
celery_app.conf.beat_schedule = {
    # 每分钟执行一次
    'cleanup-temp-files-every-minute': {
        'task': 'tasks.cleanup_temp_files',
        'schedule': 60.0,  # 60秒
    },
    # 每天凌晨2点执行
    'generate-daily-report': {
        'task': 'tasks.generate_daily_report',
        'schedule': crontab(hour=2, minute=0),
    },
    # 每小时执行一次
    'sync-data': {
        'task': 'tasks.sync_data',
        'schedule': crontab(minute=0),
    },
}

celery_app.conf.timezone = 'UTC'

# ===== 文件2: tasks.py =====
from celery_app import celery_app
import logging
import os
import time

logger = logging.getLogger(__name__)

@celery_app.task
def cleanup_temp_files():
    """清理临时文件"""
    logger.info("开始清理临时文件")

    temp_dir = "/tmp"
    count = 0

    # 删除超过1小时的临时文件
    current_time = time.time()
    for filename in os.listdir(temp_dir):
        file_path = os.path.join(temp_dir, filename)
        if os.path.isfile(file_path):
            file_age = current_time - os.path.getmtime(file_path)
            if file_age > 3600:  # 1小时
                try:
                    os.remove(file_path)
                    count += 1
                except Exception as e:
                    logger.error(f"删除文件失败: {file_path}, {e}")

    logger.info(f"清理完成，删除了 {count} 个文件")
    return {"deleted": count}

@celery_app.task
def generate_daily_report():
    """生成每日报告"""
    logger.info("开始生成每日报告")

    # 模拟生成报告
    time.sleep(10)

    logger.info("每日报告生成完成")
    return {"status": "success"}

@celery_app.task
def sync_data():
    """同步数据"""
    logger.info("开始同步数据")

    # 模拟数据同步
    time.sleep(5)

    logger.info("数据同步完成")
    return {"status": "success"}
```

### 启动方式

```bash
# 1. 启动 Celery Worker
celery -A celery_app worker --loglevel=info

# 2. 启动 Celery Beat（定时任务调度器）
celery -A celery_app beat --loglevel=info

# 3. 或者同时启动 Worker 和 Beat
celery -A celery_app worker --beat --loglevel=info
```

---

## 对比总结

### BackgroundTasks vs Celery vs RQ

| 特性 | BackgroundTasks | Celery | RQ |
|------|----------------|--------|-----|
| **部署** | 无需额外服务 | 需要 Broker + Worker | 需要 Redis + Worker |
| **持久化** | ❌ | ✅ | ✅ |
| **重试** | ❌ | ✅ | ✅ |
| **定时任务** | ❌ | ✅ (Beat) | ❌ |
| **任务状态** | ❌ | ✅ | ✅ |
| **优先级** | ❌ | ✅ | ✅ |
| **监控** | ❌ | ✅ (Flower) | ✅ (Dashboard) |
| **学习曲线** | 简单 | 复杂 | 中等 |
| **适用场景** | < 30秒 | 长任务、定时任务 | 中等任务 |

### 选择建议

```python
# 场景1：发送邮件（5秒）
→ BackgroundTasks ✅

# 场景2：生成缩略图（10秒）
→ BackgroundTasks 或 RQ

# 场景3：处理大文件（2分钟）
→ RQ 或 Celery ✅

# 场景4：训练模型（1小时）
→ Celery ✅

# 场景5：定时任务
→ Celery Beat ✅

# 场景6：对话日志记录（1秒）
→ BackgroundTasks ✅
```

---

**版本：** v1.0
**最后更新：** 2026-02-11
