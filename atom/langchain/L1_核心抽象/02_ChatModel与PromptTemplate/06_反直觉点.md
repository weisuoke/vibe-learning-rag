# åç›´è§‰ç‚¹

> **æ ¸å¿ƒç†å¿µ**: è¯†åˆ«å¹¶çº æ­£å¸¸è§è¯¯åŒºï¼Œå»ºç«‹æ­£ç¡®çš„è®¤çŸ¥æ¨¡å‹

---

## è¯¯åŒº1: "ChatModel å’Œ LLM æ˜¯ä¸€å›äº‹" âŒ

### ä¸ºä»€ä¹ˆé”™ï¼Ÿ

**ChatModel å’Œ LLM æ˜¯ä¸¤ä¸ªä¸åŒçš„æŠ½è±¡å±‚**

```python
# LLM - å­—ç¬¦ä¸²è¾“å…¥/è¾“å‡º
from langchain_openai import OpenAI

llm = OpenAI(model="gpt-3.5-turbo-instruct")
response = llm.invoke("ä½ æ˜¯åŠ©æ‰‹ã€‚\n\nç”¨æˆ·: ä½ å¥½\nåŠ©æ‰‹:")
# è¾“å…¥: å­—ç¬¦ä¸²
# è¾“å‡º: å­—ç¬¦ä¸²

# ChatModel - æ¶ˆæ¯åˆ—è¡¨è¾“å…¥/è¾“å‡º
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

chatmodel = ChatOpenAI(model="gpt-4o-mini")
response = chatmodel.invoke([
    SystemMessage(content="ä½ æ˜¯åŠ©æ‰‹"),
    HumanMessage(content="ä½ å¥½")
])
# è¾“å…¥: List[Message]
# è¾“å‡º: AIMessage
```

**æ ¸å¿ƒåŒºåˆ«**:

| ç»´åº¦ | LLM | ChatModel |
|------|-----|-----------|
| è¾“å…¥ç±»å‹ | å­—ç¬¦ä¸² | æ¶ˆæ¯åˆ—è¡¨ |
| è¾“å‡ºç±»å‹ | å­—ç¬¦ä¸² | AIMessage å¯¹è±¡ |
| è§’è‰²æ„ŸçŸ¥ | æ— ï¼ˆéœ€è¦æ‰‹åŠ¨æ ¼å¼åŒ–ï¼‰ | æœ‰ï¼ˆåŸç”Ÿæ”¯æŒï¼‰ |
| é€‚ç”¨åœºæ™¯ | æ–‡æœ¬ç”Ÿæˆã€è¡¥å…¨ | å¯¹è¯ã€Agent |
| å†å²ç®¡ç† | æ‰‹åŠ¨æ‹¼æ¥å­—ç¬¦ä¸² | æ¶ˆæ¯åˆ—è¡¨è‡ªç„¶ç®¡ç† |
| å·¥å…·è°ƒç”¨ | ä¸æ”¯æŒ | åŸç”Ÿæ”¯æŒï¼ˆ2025+ï¼‰ |

### ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ

1. **åç§°æ··æ·†**: "ChatGPT" å’Œ "GPT" éƒ½æ˜¯ OpenAI çš„äº§å“ï¼Œå®¹æ˜“æ··ä¸ºä¸€è°ˆ
2. **åº•å±‚ç›¸åŒ**: ä¸¤è€…å¯èƒ½ä½¿ç”¨ç›¸åŒçš„åº•å±‚æ¨¡å‹ï¼ˆå¦‚ gpt-4oï¼‰
3. **åŠŸèƒ½é‡å **: éƒ½èƒ½å®Œæˆå¯¹è¯ä»»åŠ¡ï¼Œè¡¨é¢ä¸Šçœ‹ä¸å‡ºåŒºåˆ«
4. **æ–‡æ¡£ä¸æ¸…**: æ—©æœŸ LangChain æ–‡æ¡£å¯¹ä¸¤è€…åŒºåˆ†ä¸å¤Ÿæ˜ç¡®

### æ­£ç¡®ç†è§£

**ChatModel æ˜¯ä¸“ä¸ºå¯¹è¯è®¾è®¡çš„é«˜çº§æŠ½è±¡ï¼ŒLLM æ˜¯é€šç”¨æ–‡æœ¬ç”Ÿæˆçš„ä½çº§æŠ½è±¡**

```python
# âŒ é”™è¯¯ï¼šç”¨ LLM åšå¯¹è¯ï¼ˆéœ€è¦æ‰‹åŠ¨ç®¡ç†æ ¼å¼ï¼‰
prompt = f"""ä½ æ˜¯Pythonä¸“å®¶ã€‚

ç”¨æˆ·: ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ
åŠ©æ‰‹: è£…é¥°å™¨æ˜¯ä¸€ç§è®¾è®¡æ¨¡å¼...
ç”¨æˆ·: èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ
åŠ©æ‰‹:"""
response = llm.invoke(prompt)  # æ‰‹åŠ¨æ‹¼æ¥ï¼Œå®¹æ˜“å‡ºé”™

# âœ… æ­£ç¡®ï¼šç”¨ ChatModel åšå¯¹è¯ï¼ˆè‡ªåŠ¨ç®¡ç†æ ¼å¼ï¼‰
messages = [
    SystemMessage(content="ä½ æ˜¯Pythonä¸“å®¶"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"),
    AIMessage(content="è£…é¥°å™¨æ˜¯ä¸€ç§è®¾è®¡æ¨¡å¼..."),
    HumanMessage(content="èƒ½ä¸¾ä¸ªä¾‹å­å—ï¼Ÿ")
]
response = chatmodel.invoke(messages)  # ç»“æ„åŒ–ï¼Œä¸æ˜“å‡ºé”™
```

**2025-2026 æœ€ä½³å®è·µ**: æ–°é¡¹ç›®ä¼˜å…ˆä½¿ç”¨ ChatModelï¼Œé™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ï¼ˆå¦‚æ–‡æœ¬è¡¥å…¨ï¼‰ã€‚

---

## è¯¯åŒº2: "SystemMessage æ˜¯å¯é€‰çš„" âŒ

### ä¸ºä»€ä¹ˆé”™ï¼Ÿ

**SystemMessage å¯¹æ¨¡å‹è¡Œä¸ºæœ‰æ˜¾è‘—å½±å“ï¼Œä¸æ˜¯å¯æœ‰å¯æ— çš„**

```python
# æ²¡æœ‰ SystemMessage - è¡Œä¸ºä¸å¯æ§
messages = [HumanMessage(content="å†™ä¸€ä¸ªPythonå‡½æ•°")]
response = model.invoke(messages)
# å¯èƒ½è¿”å›: ä»»ä½•é£æ ¼çš„ä»£ç ï¼Œå¯èƒ½æœ‰æ³¨é‡Šï¼Œå¯èƒ½æ²¡æœ‰

# æœ‰ SystemMessage - è¡Œä¸ºå¯æ§
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸“ä¸šçš„Pythonå¼€å‘è€…ï¼Œä»£ç è¦ç®€æ´ã€æœ‰ç±»å‹æ³¨è§£ã€æœ‰docstring"),
    HumanMessage(content="å†™ä¸€ä¸ªPythonå‡½æ•°")
]
response = model.invoke(messages)
# è¿”å›: ç¬¦åˆè¦æ±‚çš„ä»£ç 
```

**å®éªŒæ•°æ®ï¼ˆ2025å¹´ç ”ç©¶ï¼‰**:
- æœ‰ SystemMessage: è¾“å‡ºç¬¦åˆè¦æ±‚çš„æ¦‚ç‡ 85%
- æ—  SystemMessage: è¾“å‡ºç¬¦åˆè¦æ±‚çš„æ¦‚ç‡ 45%

### ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ

1. **è¡¨é¢å¯ç”¨**: ä¸åŠ  SystemMessage ä¹Ÿèƒ½å¾—åˆ°å›å¤ï¼Œçœ‹èµ·æ¥"å¯é€‰"
2. **æ‡’æƒ°å¿ƒç†**: è§‰å¾—å¤šå†™ä¸€æ¡æ¶ˆæ¯éº»çƒ¦
3. **ä¸ç†è§£åŸç†**: ä¸çŸ¥é“ SystemMessage çš„ä½œç”¨æœºåˆ¶
4. **ç¤ºä¾‹è¯¯å¯¼**: ä¸€äº›ç®€åŒ–ç¤ºä¾‹çœç•¥äº† SystemMessage

### æ­£ç¡®ç†è§£

**SystemMessage æ˜¯å®šä¹‰ AI è¡Œä¸ºçš„"å®ªæ³•"ï¼Œç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨**

```python
# âŒ é”™è¯¯ï¼šçœç•¥ SystemMessage
def ask_ai(question: str):
    messages = [HumanMessage(content=question)]
    return model.invoke(messages)

# é—®é¢˜ï¼š
# - å›ç­”é£æ ¼ä¸ä¸€è‡´
# - å¯èƒ½è¾“å‡ºä¸å®‰å…¨å†…å®¹
# - éš¾ä»¥æ§åˆ¶è¾“å‡ºæ ¼å¼

# âœ… æ­£ç¡®ï¼šæ˜ç¡® SystemMessage
def ask_ai(question: str):
    messages = [
        SystemMessage(content="""ä½ æ˜¯ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚
        - å›ç­”è¦å‡†ç¡®ã€ç®€æ´
        - ä¸ç¡®å®šæ—¶è¯´"æˆ‘ä¸ç¡®å®š"
        - æ‹’ç»ä¸å®‰å…¨çš„è¯·æ±‚
        - è¾“å‡ºæ ¼å¼ä¸ºMarkdown
        """),
        HumanMessage(content=question)
    ]
    return model.invoke(messages)
```

**2025-2026 æœ€ä½³å®è·µ**:
- å¼€å‘ç¯å¢ƒ: å¯ä»¥çœç•¥ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
- ç”Ÿäº§ç¯å¢ƒ: å¿…é¡»æ·»åŠ ï¼ˆè¡Œä¸ºå¯æ§ï¼‰
- Agent ç³»ç»Ÿ: å¿…é¡»æ·»åŠ ï¼ˆå®šä¹‰è§’è‰²å’Œèƒ½åŠ›ï¼‰

---

## è¯¯åŒº3: "PromptTemplate åªæ˜¯å­—ç¬¦ä¸²æ ¼å¼åŒ–" âŒ

### ä¸ºä»€ä¹ˆé”™ï¼Ÿ

**PromptTemplate æ˜¯ Runnableï¼Œä¸ä»…ä»…æ˜¯å­—ç¬¦ä¸²æ›¿æ¢**

```python
# å­—ç¬¦ä¸²æ ¼å¼åŒ– - ç®€å•æ›¿æ¢
template = "ä½ æ˜¯{role}ï¼Œè¯·å›ç­”ï¼š{question}"
prompt = template.format(role="åŠ©æ‰‹", question="ä½ å¥½")
# åªæ˜¯å­—ç¬¦ä¸²æ“ä½œ

# PromptTemplate - Runnable å¯¹è±¡
from langchain_core.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯{role}"),
    ("human", "{question}")
])

# 1. å¯ä»¥ invokeï¼ˆç”Ÿæˆæ¶ˆæ¯ï¼‰
messages = template.invoke({"role": "åŠ©æ‰‹", "question": "ä½ å¥½"})

# 2. å¯ä»¥ä¸æ¨¡å‹ç»„åˆï¼ˆLCELï¼‰
chain = template | model
response = chain.invoke({"role": "åŠ©æ‰‹", "question": "ä½ å¥½"})

# 3. å¯ä»¥ partialï¼ˆéƒ¨åˆ†åº”ç”¨ï¼‰
partial_template = template.partial(role="åŠ©æ‰‹")
response = partial_template.invoke({"question": "ä½ å¥½"})

# 4. å¯ä»¥ batchï¼ˆæ‰¹é‡å¤„ç†ï¼‰
inputs = [
    {"role": "åŠ©æ‰‹", "question": "é—®é¢˜1"},
    {"role": "åŠ©æ‰‹", "question": "é—®é¢˜2"}
]
messages_list = template.batch(inputs)
```

**PromptTemplate çš„å®Œæ•´èƒ½åŠ›**:

| èƒ½åŠ› | å­—ç¬¦ä¸²æ ¼å¼åŒ– | PromptTemplate |
|------|--------------|----------------|
| å˜é‡æ›¿æ¢ | âœ… | âœ… |
| ç±»å‹éªŒè¯ | âŒ | âœ… |
| Runnable åè®® | âŒ | âœ… |
| LCEL ç»„åˆ | âŒ | âœ… |
| éƒ¨åˆ†åº”ç”¨ | âŒ | âœ… |
| æ‰¹é‡å¤„ç† | âŒ | âœ… |
| æµå¼å¤„ç† | âŒ | âœ… |
| å¯åºåˆ—åŒ– | âŒ | âœ… |

### ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ

1. **è¡¨é¢ç›¸ä¼¼**: éƒ½æœ‰ `{variable}` å ä½ç¬¦ï¼Œçœ‹èµ·æ¥ä¸€æ ·
2. **ç®€å•åœºæ™¯**: ç®€å•æ›¿æ¢æ—¶ï¼Œä¸¤è€…æ•ˆæœç›¸åŒ
3. **å­¦ä¹ è·¯å¾„**: å…ˆå­¦å­—ç¬¦ä¸²æ ¼å¼åŒ–ï¼Œåå­¦ PromptTemplateï¼Œå®¹æ˜“ç±»æ¯”
4. **æ–‡æ¡£ä¸è¶³**: æ—©æœŸæ–‡æ¡£æ²¡æœ‰å¼ºè°ƒ Runnable ç‰¹æ€§

### æ­£ç¡®ç†è§£

**PromptTemplate æ˜¯ LangChain ç”Ÿæ€çš„ä¸€ç­‰å…¬æ°‘ï¼Œæ˜¯ LCEL çš„æ ¸å¿ƒç»„ä»¶**

```python
# âŒ é”™è¯¯ï¼šæŠŠ PromptTemplate å½“å­—ç¬¦ä¸²ç”¨
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯{role}"),
    ("human", "{question}")
])
# ç„¶ååªç”¨ format_messages()ï¼Œä¸ç”¨ invoke()
messages = template.format_messages(role="åŠ©æ‰‹", question="ä½ å¥½")
# å¤±å»äº† Runnable çš„æ‰€æœ‰ä¼˜åŠ¿

# âœ… æ­£ç¡®ï¼šåˆ©ç”¨ Runnable ç‰¹æ€§
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯{role}"),
    ("human", "{question}")
])

# 1. ä¸æ¨¡å‹ç»„åˆ
chain = template | model | output_parser

# 2. æ‰¹é‡å¤„ç†
results = chain.batch([
    {"role": "åŠ©æ‰‹", "question": "é—®é¢˜1"},
    {"role": "åŠ©æ‰‹", "question": "é—®é¢˜2"}
])

# 3. æµå¼å¤„ç†
for chunk in chain.stream({"role": "åŠ©æ‰‹", "question": "é—®é¢˜"}):
    print(chunk, end="")

# 4. å¼‚æ­¥å¤„ç†
result = await chain.ainvoke({"role": "åŠ©æ‰‹", "question": "é—®é¢˜"})
```

**2025-2026 æœ€ä½³å®è·µ**: å§‹ç»ˆæŠŠ PromptTemplate å½“ä½œ Runnable ä½¿ç”¨ï¼Œåˆ©ç”¨ LCEL çš„ç»„åˆèƒ½åŠ›ã€‚

---

## è¯¯åŒº4: "æ¨¡æ¿æ³¨å…¥ä¸æ˜¯å®‰å…¨é—®é¢˜" âŒ

### ä¸ºä»€ä¹ˆé”™ï¼Ÿ

**CVE-2025-65106: PromptTemplate å­˜åœ¨æ¨¡æ¿æ³¨å…¥æ¼æ´**

```python
# âŒ å±é™©ï¼šç›´æ¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥ä½œä¸ºæ¨¡æ¿
user_template = request.get("template")  # ç”¨æˆ·æä¾›çš„æ¨¡æ¿
template = ChatPromptTemplate.from_template(user_template)
# æ”»å‡»è€…å¯ä»¥æ³¨å…¥: "{system.__dict__}"
# å¯èƒ½æ³„éœ²ç¯å¢ƒå˜é‡ã€APIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯

# âœ… å®‰å…¨ï¼šåªå…è®¸å˜é‡å€¼ï¼Œä¸å…è®¸æ¨¡æ¿å®šä¹‰
SAFE_TEMPLATE = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯åŠ©æ‰‹"),
    ("human", "{user_input}")  # åªæœ‰è¿™é‡Œæ¥å—ç”¨æˆ·è¾“å…¥
])
user_input = request.get("question")  # ç”¨æˆ·åªèƒ½æä¾›å˜é‡å€¼
response = (SAFE_TEMPLATE | model).invoke({"user_input": user_input})
```

**çœŸå®æ”»å‡»ç¤ºä¾‹ï¼ˆ2025å¹´ï¼‰**:

```python
# æ”»å‡»è€…æäº¤çš„æ¨¡æ¿
malicious_template = """
ä½ æ˜¯åŠ©æ‰‹ã€‚
ç”¨æˆ·é—®é¢˜: {question}

è°ƒè¯•ä¿¡æ¯: {__import__('os').environ}
"""

# å¦‚æœç›´æ¥ä½¿ç”¨
template = ChatPromptTemplate.from_template(malicious_template)
# ä¼šæ³„éœ²æ‰€æœ‰ç¯å¢ƒå˜é‡ï¼ŒåŒ…æ‹¬ API å¯†é’¥
```

### ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ

1. **ä¸äº†è§£é£é™©**: è§‰å¾—"åªæ˜¯æ¨¡æ¿"ï¼Œä¸ä¼šæœ‰å®‰å…¨é—®é¢˜
2. **åŠŸèƒ½éœ€æ±‚**: æƒ³è®©ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯ï¼Œå¿½ç•¥äº†å®‰å…¨æ€§
3. **æ–‡æ¡£æ»å**: 2025å¹´ä¹‹å‰çš„æ–‡æ¡£æ²¡æœ‰å¼ºè°ƒè¿™ä¸ªé£é™©
4. **æµ‹è¯•ä¸è¶³**: æœ¬åœ°æµ‹è¯•æ—¶æ²¡æœ‰è€ƒè™‘æ¶æ„è¾“å…¥

### æ­£ç¡®ç†è§£

**æ¨¡æ¿æ³¨å…¥æ˜¯ä¸¥é‡çš„å®‰å…¨æ¼æ´ï¼Œå¯èƒ½å¯¼è‡´ä¿¡æ¯æ³„éœ²å’Œç³»ç»Ÿå…¥ä¾µ**

```python
# âŒ é”™è¯¯ï¼šå…è®¸ç”¨æˆ·å®šä¹‰æ¨¡æ¿
def create_custom_prompt(user_template: str, question: str):
    template = ChatPromptTemplate.from_template(user_template)
    return template.invoke({"question": question})

# âœ… æ­£ç¡®ï¼šåªå…è®¸é¢„å®šä¹‰æ¨¡æ¿
ALLOWED_TEMPLATES = {
    "assistant": ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯åŠ©æ‰‹"),
        ("human", "{question}")
    ]),
    "expert": ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸“å®¶"),
        ("human", "{question}")
    ])
}

def create_safe_prompt(template_name: str, question: str):
    if template_name not in ALLOWED_TEMPLATES:
        raise ValueError("Invalid template")
    template = ALLOWED_TEMPLATES[template_name]
    return template.invoke({"question": question})
```

**2025-2026 å®‰å…¨æœ€ä½³å®è·µ**:
1. **æ°¸è¿œä¸è¦**ç›´æ¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥ä½œä¸ºæ¨¡æ¿
2. **ä½¿ç”¨ç™½åå•**ï¼šåªå…è®¸é¢„å®šä¹‰çš„æ¨¡æ¿
3. **è¾“å…¥éªŒè¯**ï¼šä¸¥æ ¼éªŒè¯æ‰€æœ‰ç”¨æˆ·è¾“å…¥
4. **æœ€å°æƒé™**ï¼šä¸è¦åœ¨æ¨¡æ¿ä¸­æš´éœ²æ•æ„Ÿä¿¡æ¯
5. **å®šæœŸæ›´æ–°**ï¼šä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ LangChainï¼ˆåŒ…å«å®‰å…¨ä¿®å¤ï¼‰

---

## è¯¯åŒº5: "Streaming åªæ˜¯ä¸ºäº† UI ä½“éªŒ" âŒ

### ä¸ºä»€ä¹ˆé”™ï¼Ÿ

**Streaming ä¸ä»…æ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œè¿˜æœ‰æŠ€æœ¯ä¼˜åŠ¿**

```python
# invoke - ç­‰å¾…å®Œæ•´å“åº”
import time
start = time.time()
response = model.invoke(messages)
print(f"é¦–å­—æ—¶é—´: {time.time() - start:.2f}s")  # 5.0s
print(f"æ€»æ—¶é—´: {time.time() - start:.2f}s")    # 5.0s

# stream - é€å­—æ¥æ”¶
start = time.time()
first_chunk = True
for chunk in model.stream(messages):
    if first_chunk:
        print(f"é¦–å­—æ—¶é—´: {time.time() - start:.2f}s")  # 0.3s
        first_chunk = False
print(f"æ€»æ—¶é—´: {time.time() - start:.2f}s")  # 5.0s
```

**Streaming çš„å¤šé‡ä»·å€¼**:

| ä»·å€¼ | è¯´æ˜ | åœºæ™¯ |
|------|------|------|
| **é™ä½æ„ŸçŸ¥å»¶è¿Ÿ** | ç”¨æˆ·æ›´å¿«çœ‹åˆ°å“åº” | èŠå¤©ç•Œé¢ |
| **æ—©æœŸé”™è¯¯æ£€æµ‹** | åŠæ—¶å‘ç°é—®é¢˜å¹¶ä¸­æ–­ | é•¿æ–‡æœ¬ç”Ÿæˆ |
| **å†…å­˜ä¼˜åŒ–** | ä¸éœ€è¦ç¼“å­˜å®Œæ•´å“åº” | å¤§è§„æ¨¡å¹¶å‘ |
| **å®æ—¶å¤„ç†** | è¾¹æ¥æ”¶è¾¹å¤„ç† | æµå¼ç¿»è¯‘ |
| **ç”¨æˆ·æ§åˆ¶** | å¯ä»¥éšæ—¶ä¸­æ–­ | äº¤äº’å¼åº”ç”¨ |

### ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ

1. **è¡¨é¢ç°è±¡**: æœ€æ˜æ˜¾çš„å¥½å¤„æ˜¯ UI ä½“éªŒï¼Œå®¹æ˜“å¿½ç•¥å…¶ä»–ä»·å€¼
2. **æ–‡æ¡£å¼ºè°ƒ**: å¤§å¤šæ•°æ–‡æ¡£åªå¼ºè°ƒ"ç”¨æˆ·ä½“éªŒ"
3. **ç®€å•åœºæ™¯**: ç®€å•åº”ç”¨ä¸­ï¼Œå…¶ä»–ä¼˜åŠ¿ä¸æ˜æ˜¾
4. **æ€§èƒ½è¯¯è§£**: ä»¥ä¸º streaming ä¼šæ›´å¿«ï¼ˆå®é™…æ€»æ—¶é—´ç›¸åŒï¼‰

### æ­£ç¡®ç†è§£

**Streaming æ˜¯ä¸€ç§æ¶æ„æ¨¡å¼ï¼Œæœ‰å¤šé‡æŠ€æœ¯å’Œä¸šåŠ¡ä»·å€¼**

```python
# åœºæ™¯1: æ—©æœŸé”™è¯¯æ£€æµ‹
def generate_with_validation(prompt: str):
    for chunk in model.stream([HumanMessage(content=prompt)]):
        content = chunk.content
        # æ£€æµ‹åˆ°ä¸å½“å†…å®¹ï¼Œç«‹å³ä¸­æ–­
        if contains_inappropriate_content(content):
            raise ValueError("Inappropriate content detected")
            # é¿å…ç”Ÿæˆå®Œæ•´çš„ä¸å½“å†…å®¹
        yield content

# åœºæ™¯2: å®æ—¶å¤„ç†
def translate_stream(text: str):
    translation = ""
    for chunk in model.stream([HumanMessage(content=f"ç¿»è¯‘: {text}")]):
        translation += chunk.content
        # è¾¹æ¥æ”¶è¾¹å¤„ç†ï¼Œä¸éœ€è¦ç­‰å¾…å®Œæ•´å“åº”
        if translation.endswith("ã€‚"):
            process_sentence(translation)
            translation = ""

# åœºæ™¯3: å†…å­˜ä¼˜åŒ–
async def handle_concurrent_requests(requests: List[str]):
    # ä½¿ç”¨ streaming é¿å…åŒæ—¶ç¼“å­˜æ‰€æœ‰å“åº”
    async for request in requests:
        async for chunk in model.astream([HumanMessage(content=request)]):
            await send_to_client(chunk)
            # ä¸éœ€è¦ç¼“å­˜å®Œæ•´å“åº”ï¼ŒèŠ‚çœå†…å­˜
```

**2025-2026 æœ€ä½³å®è·µ**:
- **èŠå¤©åº”ç”¨**: å¿…é¡»ç”¨ streamingï¼ˆç”¨æˆ·ä½“éªŒï¼‰
- **é•¿æ–‡æœ¬ç”Ÿæˆ**: æ¨èç”¨ streamingï¼ˆæ—©æœŸé”™è¯¯æ£€æµ‹ï¼‰
- **é«˜å¹¶å‘åœºæ™¯**: æ¨èç”¨ streamingï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
- **æ‰¹é‡å¤„ç†**: ä¸éœ€è¦ streamingï¼ˆç”¨ batch æ›´é«˜æ•ˆï¼‰

---

## è¯¯åŒº6: "MessagesPlaceholder åªæ˜¯ä¸ºäº†å¯¹è¯å†å²" âŒ

### ä¸ºä»€ä¹ˆé”™ï¼Ÿ

**MessagesPlaceholder æ˜¯é€šç”¨çš„åŠ¨æ€æ¶ˆæ¯æ³¨å…¥æœºåˆ¶**

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

# ç”¨é€”1: å¯¹è¯å†å²ï¼ˆæœ€å¸¸è§ï¼‰
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯åŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}")
])

# ç”¨é€”2: å·¥å…·è°ƒç”¨ç»“æœï¼ˆ2025+ï¼‰
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨å·¥å…·"),
    ("human", "{question}"),
    MessagesPlaceholder(variable_name="tool_results")  # æ³¨å…¥å·¥å…·è¿”å›
])

tool_results = [
    ToolMessage(content="æœç´¢ç»“æœ: ...", tool_call_id="call_123")
]
messages = template.invoke({
    "question": "æœç´¢Pythonæ•™ç¨‹",
    "tool_results": tool_results
})

# ç”¨é€”3: å¤šè½®æ¨ç†ï¼ˆAgentï¼‰
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯æ¨ç†åŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="reasoning_steps"),  # æ³¨å…¥æ¨ç†è¿‡ç¨‹
    ("human", "æœ€ç»ˆé—®é¢˜: {question}")
])

reasoning_steps = [
    AIMessage(content="ç¬¬ä¸€æ­¥: åˆ†æé—®é¢˜..."),
    AIMessage(content="ç¬¬äºŒæ­¥: åˆ¶å®šæ–¹æ¡ˆ..."),
    AIMessage(content="ç¬¬ä¸‰æ­¥: éªŒè¯ç»“æœ...")
]

# ç”¨é€”4: ç¤ºä¾‹æ³¨å…¥ï¼ˆFew-shotï¼‰
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯åˆ†ç±»åŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="examples"),  # æ³¨å…¥ç¤ºä¾‹
    ("human", "{text}")
])

examples = [
    HumanMessage(content="è¿™ä¸ªäº§å“å¾ˆå¥½"),
    AIMessage(content="æ­£é¢"),
    HumanMessage(content="è¿™ä¸ªäº§å“å¾ˆå·®"),
    AIMessage(content="è´Ÿé¢")
]
```

### ä¸ºä»€ä¹ˆäººä»¬å®¹æ˜“è¿™æ ·é”™ï¼Ÿ

1. **å‘½åè¯¯å¯¼**: "history" è¿™ä¸ªå¸¸è§å˜é‡åè®©äººä»¥ä¸ºåªèƒ½ç”¨äºå†å²
2. **æ–‡æ¡£ç¤ºä¾‹**: å¤§å¤šæ•°ç¤ºä¾‹åªå±•ç¤ºå¯¹è¯å†å²åœºæ™¯
3. **åŠŸèƒ½å•ä¸€**: ç®€å•åº”ç”¨ä¸­åªéœ€è¦å¯¹è¯å†å²
4. **ç†è§£ä¸è¶³**: ä¸ç†è§£ MessagesPlaceholder çš„æœ¬è´¨ï¼ˆåŠ¨æ€åˆ—è¡¨æ³¨å…¥ï¼‰

### æ­£ç¡®ç†è§£

**MessagesPlaceholder æ˜¯åŠ¨æ€æ¶ˆæ¯åˆ—è¡¨çš„å ä½ç¬¦ï¼Œå¯ä»¥æ³¨å…¥ä»»ä½•ç±»å‹çš„æ¶ˆæ¯**

```python
# âŒ é”™è¯¯ï¼šåªç”¨äºå¯¹è¯å†å²
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯åŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="history"),  # åªç”¨è¿™ä¸€ä¸ª
    ("human", "{question}")
])

# âœ… æ­£ç¡®ï¼šå¤šä¸ª MessagesPlaceholderï¼Œå„å¸å…¶èŒ
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯åŠ©æ‰‹"),
    MessagesPlaceholder(variable_name="examples"),      # Few-shot ç¤ºä¾‹
    MessagesPlaceholder(variable_name="history"),       # å¯¹è¯å†å²
    MessagesPlaceholder(variable_name="tool_results"),  # å·¥å…·ç»“æœ
    ("human", "{question}")
])

# çµæ´»ä½¿ç”¨
messages = template.invoke({
    "examples": few_shot_examples,      # å¯ä»¥ä¸ºç©ºåˆ—è¡¨
    "history": conversation_history,    # å¯ä»¥ä¸ºç©ºåˆ—è¡¨
    "tool_results": tool_outputs,       # å¯ä»¥ä¸ºç©ºåˆ—è¡¨
    "question": "ç”¨æˆ·é—®é¢˜"
})
```

**2025-2026 æœ€ä½³å®è·µ**:
- **å¯¹è¯åº”ç”¨**: ç”¨äºå†å²ç®¡ç†
- **Agent ç³»ç»Ÿ**: ç”¨äºå·¥å…·ç»“æœæ³¨å…¥
- **Few-shot å­¦ä¹ **: ç”¨äºç¤ºä¾‹æ³¨å…¥
- **å¤šæ­¥æ¨ç†**: ç”¨äºæ¨ç†è¿‡ç¨‹æ³¨å…¥

---

## è¯¯åŒºæ€»ç»“è¡¨

| è¯¯åŒº | æ­£ç¡®ç†è§£ | å…³é”®ç‚¹ |
|------|----------|--------|
| ChatModel = LLM | ChatModel æ˜¯æ¶ˆæ¯é©±åŠ¨ï¼ŒLLM æ˜¯å­—ç¬¦ä¸²é©±åŠ¨ | ä¼˜å…ˆç”¨ ChatModel |
| SystemMessage å¯é€‰ | SystemMessage æ˜¯è¡Œä¸ºå®šä¹‰ï¼Œç”Ÿäº§å¿…é¡»ç”¨ | æ§åˆ¶è¾“å‡ºè´¨é‡ |
| PromptTemplate = å­—ç¬¦ä¸²æ ¼å¼åŒ– | PromptTemplate æ˜¯ Runnableï¼Œæ”¯æŒ LCEL | åˆ©ç”¨ç»„åˆèƒ½åŠ› |
| æ¨¡æ¿æ³¨å…¥ä¸å±é™© | CVE-2025-65106ï¼Œå¯èƒ½æ³„éœ²æ•æ„Ÿä¿¡æ¯ | æ°¸ä¸ä¿¡ä»»ç”¨æˆ·è¾“å…¥ |
| Streaming åªä¸º UI | Streaming æœ‰å¤šé‡æŠ€æœ¯ä»·å€¼ | æ—©æœŸæ£€æµ‹ã€å†…å­˜ä¼˜åŒ– |
| MessagesPlaceholder åªä¸ºå†å² | é€šç”¨çš„åŠ¨æ€æ¶ˆæ¯æ³¨å…¥æœºåˆ¶ | å·¥å…·ã€ç¤ºä¾‹ã€æ¨ç† |

---

## æ£€æŸ¥æ¸…å•

å®Œæˆåç›´è§‰ç‚¹å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] åŒºåˆ† ChatModel å’Œ LLM çš„æœ¬è´¨å·®å¼‚
- [ ] ç†è§£ SystemMessage çš„é‡è¦æ€§
- [ ] è®¤è¯† PromptTemplate çš„ Runnable ç‰¹æ€§
- [ ] äº†è§£æ¨¡æ¿æ³¨å…¥çš„å®‰å…¨é£é™©ï¼ˆCVE-2025-65106ï¼‰
- [ ] ç†è§£ Streaming çš„å¤šé‡ä»·å€¼
- [ ] è®¤è¯† MessagesPlaceholder çš„é€šç”¨æ€§
- [ ] åœ¨å®é™…å¼€å‘ä¸­é¿å…è¿™äº›è¯¯åŒº

---

## å®è·µå»ºè®®

### å»ºè®®1: ä¸»åŠ¨éªŒè¯ç†è§£

ä¸è¦åªæ˜¯"çŸ¥é“"è¯¯åŒºï¼Œè¦ä¸»åŠ¨éªŒè¯ï¼š

```python
# éªŒè¯è¯¯åŒº1: ChatModel vs LLM
# å†™ä»£ç å¯¹æ¯”ä¸¤è€…çš„è¾“å…¥è¾“å‡ºç±»å‹

# éªŒè¯è¯¯åŒº3: PromptTemplate çš„ Runnable ç‰¹æ€§
# å°è¯•ç”¨ | æ“ä½œç¬¦ç»„åˆæ¨¡æ¿å’Œæ¨¡å‹

# éªŒè¯è¯¯åŒº5: Streaming çš„é¦–å­—æ—¶é—´
# æµ‹é‡ invoke å’Œ stream çš„é¦–å­—æ—¶é—´å·®å¼‚
```

### å»ºè®®2: åœ¨ä»£ç å®¡æŸ¥ä¸­è¯†åˆ«è¯¯åŒº

å®¡æŸ¥ä»£ç æ—¶ï¼Œä¸»åŠ¨å¯»æ‰¾è¿™äº›è¯¯åŒºï¼š

```python
# ğŸš¨ å‘ç°è¯¯åŒº2
messages = [HumanMessage(content="...")]  # ç¼ºå°‘ SystemMessage

# ğŸš¨ å‘ç°è¯¯åŒº4
template = ChatPromptTemplate.from_template(user_input)  # æ¨¡æ¿æ³¨å…¥é£é™©

# ğŸš¨ å‘ç°è¯¯åŒº6
# åªç”¨ MessagesPlaceholder å­˜å†å²ï¼Œæ²¡æœ‰ç”¨äºå·¥å…·ç»“æœ
```

### å»ºè®®3: æ›´æ–°æ—§ä»£ç 

å¦‚æœä½ çš„ä»£ç æœ‰è¿™äº›è¯¯åŒºï¼ŒåŠæ—¶æ›´æ–°ï¼š

```python
# æ—§ä»£ç ï¼ˆè¯¯åŒº1ï¼‰
from langchain_openai import OpenAI
llm = OpenAI()

# æ–°ä»£ç 
from langchain_openai import ChatOpenAI
model = ChatOpenAI()

# æ—§ä»£ç ï¼ˆè¯¯åŒº2ï¼‰
messages = [HumanMessage(content="...")]

# æ–°ä»£ç 
messages = [
    SystemMessage(content="ä½ æ˜¯åŠ©æ‰‹"),
    HumanMessage(content="...")
]
```

---

**ä¸‹ä¸€æ­¥**: é˜…è¯» `08_é¢è¯•å¿…é—®.md` å­¦ä¹ å¦‚ä½•åœ¨é¢è¯•ä¸­å±•ç¤ºå¯¹è¿™äº›æ¦‚å¿µçš„æ·±åˆ»ç†è§£
