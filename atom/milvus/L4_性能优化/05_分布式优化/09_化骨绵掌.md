# 化骨绵掌

> 10个2分钟知识卡片，系统掌握 Milvus 分布式优化

---

## 卡片1：分布式系统的本质挑战

**一句话：** 分布式系统的性能瓶颈不在单个节点，而在节点间的协调和通信。

**举例：**
```
单机 Milvus：1000 QPS
10节点集群（理论）：10000 QPS
10节点集群（实际）：7500 QPS

损失的 2500 QPS 去哪了？
- 负载不均：500 QPS
- 网络开销：1500 QPS
- 协调开销：500 QPS
```

**应用：** 在 RAG 系统中，不要盲目增加节点，先优化单节点性能，再考虑横向扩展。

---

## 卡片2：一致性哈希的魔法

**一句话：** 一致性哈希让节点增删时，只需要迁移少量数据，而不是全部重新分配。

**举例：**
```python
# 传统哈希
node = hash(segment_id) % 3  # 3个节点
# 增加到4个节点 → 75%的数据需要迁移

# 一致性哈希
# 增加到4个节点 → 只有25%的数据需要迁移
```

**应用：** Milvus 使用一致性哈希分配 Segment，支持动态扩缩容而不影响服务。

---

## 卡片3：压缩的权衡

**一句话：** 网络压缩不是总是好的，取决于网络速度和 CPU 资源。

**举例：**
```
内网（10Gbps）：
- 传输 10MB：8ms
- 压缩 + 传输：5ms（压缩）+ 2.4ms（传输）+ 3ms（解压）= 10.4ms
→ 不启用压缩更快

公网（100Mbps）：
- 传输 10MB：800ms
- 压缩 + 传输：5ms + 24ms + 3ms = 32ms
→ 启用压缩更快（25x）
```

**应用：** 内网部署不启用压缩，跨地域部署启用压缩。

---

## 卡片4：副本数的边际效益

**一句话：** 副本数从1增加到2，性能提升明显；从2增加到3，提升递减。

**举例：**
```
副本数  查询性能  写入性能  存储成本  可用性
1       1x        1x        1x        99%
2       1.7x      0.8x      2x        99.9%
3       2.3x      0.6x      3x        99.99%
5       3.5x      0.4x      5x        99.999%

从2到3：性能提升35%，成本增加50%
从3到5：性能提升52%，成本增加67%
```

**应用：** 大多数场景2个副本足够，不要追求过多副本。

---

## 卡片5：一致性级别的选择

**一句话：** 强一致性保证数据最新，但延迟高；最终一致性延迟低，但可能读到旧数据。

**举例：**
```
一致性级别    延迟      适用场景
Strong        +100ms    金融交易
Bounded       +20ms     文档检索（推荐）
Session       +5ms      用户个人数据
Eventually    +0ms      推荐系统
```

**应用：** RAG 文档检索使用 Bounded，实时推荐使用 Eventually。

---

## 卡片6：WAL 的作用

**一句话：** WAL（Write-Ahead Log）先写日志再写数据，保证数据不丢失。

**举例：**
```
写入流程：
1. 写入 WAL（磁盘）→ 返回成功
2. 异步写入 Object Storage
3. WAL 清理

好处：即使系统崩溃，也能从 WAL 恢复
代价：磁盘 I/O 开销
```

**应用：** 读多写少的场景（如 RAG），可以延长 WAL 刷盘间隔，提升写入性能。

---

## 卡片7：Compaction 的必要性

**一句话：** Compaction 合并小 Segment，清理已删除数据，优化存储空间和查询性能。

**举例：**
```
Compaction 前：
- 100个小 Segment
- 30% 数据已删除但未释放
- 查询需要扫描100个 Segment

Compaction 后：
- 10个大 Segment
- 已删除数据释放
- 查询只需扫描10个 Segment（10x 更快）
```

**应用：** 定期在业务低峰期（凌晨）执行 Compaction。

---

## 卡片8：缓存的80/20法则

**一句话：** 80%的查询访问20%的数据，缓存这20%的热数据可以大幅提升性能。

**举例：**
```
缓存大小    命中率    边际收益
1GB         60%       -
2GB         75%       15%
5GB         85%       10%
10GB        90%       5%
20GB        92%       2%

从10GB到20GB：命中率只提升2%，成本增加100%
```

**应用：** 缓存大小 = 热数据大小 × 1.2，不要追求100%缓存命中率。

---

## 卡片9：数据本地化的价值

**一句话：** 让查询在本地节点完成，减少跨节点数据传输，降低延迟。

**举例：**
```
跨节点查询：
QueryNode 1 → QueryNode 2（传输10MB向量）→ 返回结果
延迟：50ms

本地化查询：
QueryNode 1 → 本地数据 → 返回结果
延迟：15ms（3.3x 更快）
```

**应用：** 使用分区将相关数据放在同一节点，如按用户ID、时间、类别分片。

---

## 卡片10：监控驱动优化

**一句话：** 分布式优化不是一次性配置，而是持续监控和调整的过程。

**举例：**
```
监控循环：
1. 监控指标（QPS、延迟、资源使用）
   ↓
2. 发现瓶颈（某个节点过载）
   ↓
3. 分析原因（热点数据、负载不均）
   ↓
4. 调整配置（增加副本、调整缓存）
   ↓
5. 验证效果（性能提升）
   ↓
（回到步骤1）
```

**应用：** 使用 Prometheus 监控关键指标，定期分析并优化。

---

## 知识卡片总结

| 卡片 | 核心概念 | 关键洞察 |
|------|----------|----------|
| 1 | 分布式挑战 | 瓶颈在协调，不在节点 |
| 2 | 一致性哈希 | 最小化数据迁移 |
| 3 | 压缩权衡 | 取决于网络速度 |
| 4 | 副本数 | 边际效益递减 |
| 5 | 一致性级别 | 性能与新鲜度权衡 |
| 6 | WAL | 保证数据不丢失 |
| 7 | Compaction | 优化存储和性能 |
| 8 | 缓存 | 80/20法则 |
| 9 | 数据本地化 | 减少跨节点通信 |
| 10 | 监控优化 | 持续改进循环 |

---

## 学习检查清单

完成本知识点学习后，你应该能够：

- [ ] 理解分布式系统的性能瓶颈
- [ ] 解释一致性哈希的工作原理
- [ ] 判断何时启用网络压缩
- [ ] 选择合适的副本数
- [ ] 根据场景选择一致性级别
- [ ] 配置 WAL 参数
- [ ] 执行 Compaction 优化存储
- [ ] 合理配置缓存大小
- [ ] 实现数据本地化
- [ ] 监控并持续优化集群性能

---

## 下一步学习

掌握分布式优化后，可以继续学习：

1. **L5_生产实践**：Docker部署、监控与健康检查、备份与恢复
2. **L6_RAG集成实战**：文档问答系统实现、多租户知识库
3. **高级主题**：Kubernetes部署、安全与权限管理、高可用集群

---

## 快速参考卡

### 负载均衡配置

```yaml
queryCoord:
  balanceIntervalSeconds: 60
  overloadedMemoryThresholdPercentage: 90
```

### 网络压缩配置

```yaml
grpc:
  compressionEnabled: true  # 公网启用
  compressionAlgorithm: "gzip"
```

### 存储引擎配置

```yaml
dataNode:
  wal:
    flushInterval: 1  # 秒
queryNode:
  cache:
    memoryLimit: 21474836480  # 20GB
```

### 副本数配置

```python
collection.load(replica_number=2)  # 推荐
```

### 一致性级别配置

```python
collection.search(
    data=query_vector,
    consistency_level="Bounded"  # 推荐
)
```

---

**记住**：分布式优化是在性能、可靠性和成本之间找到平衡点，没有银弹，只有权衡。
