# 实战代码：生产级实践

## 概述

**本文涵盖生产环境必需的监控、容错、性能优化、安全等实践,确保任务调度系统稳定可靠运行。**

---

## 实践1:监控与可观测性

### 指标收集

```python
import time
from dataclasses import dataclass
from typing import Dict
from collections import defaultdict

@dataclass
class QueueMetrics:
    """队列指标"""
    total_tasks: int = 0
    completed_tasks: int = 0
    failed_tasks: int = 0
    retried_tasks: int = 0
    queue_length: int = 0
    avg_wait_time: float = 0.0
    avg_process_time: float = 0.0

class MonitoredQueue:
    """带监控的队列"""

    def __init__(self):
        self.queue = []
        self.metrics = QueueMetrics()
        self.task_start_times: Dict[str, float] = {}
        self.task_enqueue_times: Dict[str, float] = {}

    async def enqueue(self, task_id: str, task):
        """入队"""
        self.queue.append((task_id, task))
        self.task_enqueue_times[task_id] = time.time()
        self.metrics.total_tasks += 1
        self.metrics.queue_length = len(self.queue)

    async def dequeue(self):
        """出队"""
        if not self.queue:
            return None

        task_id, task = self.queue.pop(0)
        self.task_start_times[task_id] = time.time()
        self.metrics.queue_length = len(self.queue)

        # 计算等待时间
        wait_time = self.task_start_times[task_id] - self.task_enqueue_times[task_id]
        self._update_avg_wait_time(wait_time)

        return task_id, task

    async def mark_completed(self, task_id: str):
        """标记完成"""
        self.metrics.completed_tasks += 1

        # 计算处理时间
        process_time = time.time() - self.task_start_times[task_id]
        self._update_avg_process_time(process_time)

        # 清理
        del self.task_start_times[task_id]
        del self.task_enqueue_times[task_id]

    async def mark_failed(self, task_id: str):
        """标记失败"""
        self.metrics.failed_tasks += 1

    async def mark_retried(self, task_id: str):
        """标记重试"""
        self.metrics.retried_tasks += 1

    def _update_avg_wait_time(self, wait_time: float):
        """更新平均等待时间"""
        n = self.metrics.completed_tasks + self.metrics.failed_tasks
        if n == 0:
            self.metrics.avg_wait_time = wait_time
        else:
            self.metrics.avg_wait_time = (
                (self.metrics.avg_wait_time * (n - 1) + wait_time) / n
            )

    def _update_avg_process_time(self, process_time: float):
        """更新平均处理时间"""
        n = self.metrics.completed_tasks
        if n == 1:
            self.metrics.avg_process_time = process_time
        else:
            self.metrics.avg_process_time = (
                (self.metrics.avg_process_time * (n - 1) + process_time) / n
            )

    def get_metrics(self) -> QueueMetrics:
        """获取指标"""
        return self.metrics

# 使用
queue = MonitoredQueue()

# 添加任务
await queue.enqueue("task_1", {"data": "test"})

# 处理任务
task_id, task = await queue.dequeue()
# ... 处理任务
await queue.mark_completed(task_id)

# 获取指标
metrics = queue.get_metrics()
print(f"完成率: {metrics.completed_tasks / metrics.total_tasks * 100:.2f}%")
print(f"平均等待时间: {metrics.avg_wait_time:.2f}秒")
```

### Prometheus集成

```python
from prometheus_client import Counter, Gauge, Histogram, start_http_server

class PrometheusMonitor:
    """Prometheus监控"""

    def __init__(self):
        # 计数器
        self.tasks_total = Counter('queue_tasks_total', 'Total tasks')
        self.tasks_completed = Counter('queue_tasks_completed', 'Completed tasks')
        self.tasks_failed = Counter('queue_tasks_failed', 'Failed tasks')

        # 仪表盘
        self.queue_length = Gauge('queue_length', 'Current queue length')

        # 直方图
        self.task_duration = Histogram('task_duration_seconds', 'Task duration')
        self.wait_time = Histogram('task_wait_seconds', 'Task wait time')

    def record_task_enqueued(self):
        """记录任务入队"""
        self.tasks_total.inc()

    def record_task_completed(self, duration: float):
        """记录任务完成"""
        self.tasks_completed.inc()
        self.task_duration.observe(duration)

    def record_task_failed(self):
        """记录任务失败"""
        self.tasks_failed.inc()

    def update_queue_length(self, length: int):
        """更新队列长度"""
        self.queue_length.set(length)

    def record_wait_time(self, wait_time: float):
        """记录等待时间"""
        self.wait_time.observe(wait_time)

# 启动Prometheus HTTP服务器
start_http_server(8000)

# 使用
monitor = PrometheusMonitor()
monitor.record_task_enqueued()
monitor.update_queue_length(10)
```

---

## 实践2:容错与重试

### 指数退避重试

```python
import asyncio
from typing import Callable, Any

class ExponentialBackoff:
    """指数退避重试"""

    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):
        self.max_retries = max_retries
        self.base_delay = base_delay

    async def execute(self, func: Callable, *args, **kwargs) -> Any:
        """执行带重试的函数"""
        for attempt in range(self.max_retries):
            try:
                return await func(*args, **kwargs)

            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise

                # 计算延迟时间
                delay = self.base_delay * (2 ** attempt)
                print(f"重试 {attempt + 1}/{self.max_retries}, 延迟{delay}秒")

                await asyncio.sleep(delay)

# 使用
backoff = ExponentialBackoff(max_retries=3, base_delay=1.0)

async def unreliable_task():
    """不可靠的任务"""
    import random
    if random.random() < 0.7:
        raise Exception("任务失败")
    return "成功"

result = await backoff.execute(unreliable_task)
```

### 死信队列(DLQ)

```python
class DeadLetterQueue:
    """死信队列"""

    def __init__(self, max_retries: int = 3):
        self.main_queue = []
        self.dlq = []
        self.retry_counts: Dict[str, int] = {}
        self.max_retries = max_retries

    async def enqueue(self, task_id: str, task):
        """入队"""
        self.main_queue.append((task_id, task))
        self.retry_counts[task_id] = 0

    async def dequeue(self):
        """出队"""
        if not self.main_queue:
            return None
        return self.main_queue.pop(0)

    async def handle_failure(self, task_id: str, task, error: Exception):
        """处理失败"""
        self.retry_counts[task_id] += 1

        if self.retry_counts[task_id] < self.max_retries:
            # 重新入队
            print(f"重试任务 {task_id} ({self.retry_counts[task_id]}/{self.max_retries})")
            self.main_queue.append((task_id, task))
        else:
            # 移到死信队列
            print(f"任务 {task_id} 移到死信队列")
            self.dlq.append({
                'task_id': task_id,
                'task': task,
                'error': str(error),
                'retry_count': self.retry_counts[task_id]
            })

    def get_dlq_tasks(self):
        """获取死信队列任务"""
        return self.dlq

# 使用
dlq = DeadLetterQueue(max_retries=3)

await dlq.enqueue("task_1", {"data": "test"})

task_id, task = await dlq.dequeue()
try:
    # 处理任务
    pass
except Exception as e:
    await dlq.handle_failure(task_id, task, e)
```

### 熔断器模式

```python
from enum import Enum
from datetime import datetime, timedelta

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    """熔断器"""

    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    async def call(self, func: Callable, *args, **kwargs):
        """调用函数"""
        if self.state == CircuitState.OPEN:
            # 检查是否可以进入半开状态
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("熔断器打开,拒绝请求")

        try:
            result = await func(*args, **kwargs)

            # 成功,重置计数
            if self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.CLOSED
            self.failure_count = 0

            return result

        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = datetime.now()

            if self.failure_count >= self.failure_threshold:
                self.state = CircuitState.OPEN
                print("熔断器打开")

            raise

    def _should_attempt_reset(self) -> bool:
        """是否应该尝试重置"""
        if self.last_failure_time is None:
            return True

        elapsed = (datetime.now() - self.last_failure_time).total_seconds()
        return elapsed >= self.timeout

# 使用
breaker = CircuitBreaker(failure_threshold=3, timeout=60)

async def unreliable_service():
    """不可靠的服务"""
    import random
    if random.random() < 0.8:
        raise Exception("服务失败")
    return "成功"

try:
    result = await breaker.call(unreliable_service)
except Exception as e:
    print(f"调用失败: {e}")
```

---

## 实践3:性能优化

### 批量处理

```python
class BatchProcessor:
    """批量处理器"""

    def __init__(self, batch_size: int = 100, timeout: float = 1.0):
        self.batch_size = batch_size
        self.timeout = timeout
        self.batch = []

    async def add_task(self, task):
        """添加任务"""
        self.batch.append(task)

        if len(self.batch) >= self.batch_size:
            await self._process_batch()

    async def _process_batch(self):
        """处理批次"""
        if not self.batch:
            return

        print(f"批量处理 {len(self.batch)} 个任务")

        # 批量处理
        tasks = [self._process_single(task) for task in self.batch]
        await asyncio.gather(*tasks)

        self.batch.clear()

    async def _process_single(self, task):
        """处理单个任务"""
        await asyncio.sleep(0.1)

    async def flush(self):
        """刷新剩余任务"""
        await self._process_batch()

# 使用
processor = BatchProcessor(batch_size=10)

for i in range(25):
    await processor.add_task(f"task_{i}")

await processor.flush()
```

### 连接池

```python
import asyncio
from typing import List

class ConnectionPool:
    """连接池"""

    def __init__(self, size: int = 10):
        self.size = size
        self.connections = asyncio.Queue(maxsize=size)
        self._initialize()

    def _initialize(self):
        """初始化连接"""
        for i in range(self.size):
            self.connections.put_nowait(f"connection_{i}")

    async def acquire(self):
        """获取连接"""
        return await self.connections.get()

    async def release(self, connection):
        """释放连接"""
        await self.connections.put(connection)

    async def execute(self, func: Callable, *args, **kwargs):
        """使用连接执行函数"""
        connection = await self.acquire()
        try:
            return await func(connection, *args, **kwargs)
        finally:
            await self.release(connection)

# 使用
pool = ConnectionPool(size=5)

async def query(connection, sql):
    """执行查询"""
    print(f"使用 {connection} 执行: {sql}")
    await asyncio.sleep(0.1)

result = await pool.execute(query, "SELECT * FROM users")
```

### 缓存策略

```python
from functools import wraps
import hashlib
import json

class TaskCache:
    """任务缓存"""

    def __init__(self, ttl: int = 300):
        self.cache: Dict[str, tuple] = {}
        self.ttl = ttl

    def _make_key(self, func_name: str, args, kwargs) -> str:
        """生成缓存键"""
        key_data = {
            'func': func_name,
            'args': args,
            'kwargs': kwargs
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()

    def get(self, key: str):
        """获取缓存"""
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            else:
                del self.cache[key]
        return None

    def set(self, key: str, value):
        """设置缓存"""
        self.cache[key] = (value, time.time())

    def cached(self, func):
        """缓存装饰器"""
        @wraps(func)
        async def wrapper(*args, **kwargs):
            key = self._make_key(func.__name__, args, kwargs)

            # 检查缓存
            cached_value = self.get(key)
            if cached_value is not None:
                print(f"缓存命中: {key}")
                return cached_value

            # 执行函数
            result = await func(*args, **kwargs)

            # 缓存结果
            self.set(key, result)

            return result

        return wrapper

# 使用
cache = TaskCache(ttl=300)

@cache.cached
async def expensive_task(param):
    """耗时任务"""
    await asyncio.sleep(2)
    return f"结果: {param}"

result = await expensive_task("test")  # 执行2秒
result = await expensive_task("test")  # 缓存命中,立即返回
```

---

## 实践4:安全性

### 任务验证

```python
from pydantic import BaseModel, validator

class TaskInput(BaseModel):
    """任务输入验证"""
    task_id: str
    action: str
    data: dict

    @validator('task_id')
    def validate_task_id(cls, v):
        """验证任务ID"""
        if not v or len(v) > 100:
            raise ValueError("任务ID无效")
        return v

    @validator('action')
    def validate_action(cls, v):
        """验证操作"""
        allowed_actions = ['process', 'analyze', 'summarize']
        if v not in allowed_actions:
            raise ValueError(f"操作必须是: {allowed_actions}")
        return v

# 使用
try:
    task = TaskInput(
        task_id="task_1",
        action="process",
        data={"key": "value"}
    )
except Exception as e:
    print(f"验证失败: {e}")
```

### 速率限制

```python
from collections import defaultdict
from datetime import datetime, timedelta

class RateLimiter:
    """速率限制器"""

    def __init__(self, max_requests: int = 100, window: int = 60):
        self.max_requests = max_requests
        self.window = window
        self.requests: Dict[str, List[datetime]] = defaultdict(list)

    def is_allowed(self, client_id: str) -> bool:
        """检查是否允许"""
        now = datetime.now()
        window_start = now - timedelta(seconds=self.window)

        # 清理过期请求
        self.requests[client_id] = [
            req_time for req_time in self.requests[client_id]
            if req_time > window_start
        ]

        # 检查限制
        if len(self.requests[client_id]) >= self.max_requests:
            return False

        # 记录请求
        self.requests[client_id].append(now)
        return True

# 使用
limiter = RateLimiter(max_requests=10, window=60)

if limiter.is_allowed("client_1"):
    # 处理请求
    pass
else:
    print("速率限制")
```

---

## 实践5:告警系统

### 告警规则

```python
from dataclasses import dataclass
from typing import Callable

@dataclass
class AlertRule:
    """告警规则"""
    name: str
    condition: Callable
    message: str
    severity: str

class AlertManager:
    """告警管理器"""

    def __init__(self):
        self.rules: List[AlertRule] = []

    def add_rule(self, rule: AlertRule):
        """添加规则"""
        self.rules.append(rule)

    def check_alerts(self, metrics: QueueMetrics):
        """检查告警"""
        alerts = []

        for rule in self.rules:
            if rule.condition(metrics):
                alerts.append({
                    'name': rule.name,
                    'message': rule.message,
                    'severity': rule.severity,
                    'timestamp': datetime.now()
                })

        return alerts

    def send_alert(self, alert: dict):
        """发送告警"""
        print(f"[{alert['severity']}] {alert['name']}: {alert['message']}")

# 使用
manager = AlertManager()

# 添加规则
manager.add_rule(AlertRule(
    name="队列长度过长",
    condition=lambda m: m.queue_length > 1000,
    message="队列长度超过1000",
    severity="WARNING"
))

manager.add_rule(AlertRule(
    name="失败率过高",
    condition=lambda m: m.failed_tasks / m.total_tasks > 0.1 if m.total_tasks > 0 else False,
    message="任务失败率超过10%",
    severity="CRITICAL"
))

# 检查告警
alerts = manager.check_alerts(metrics)
for alert in alerts:
    manager.send_alert(alert)
```

---

## 实践6:完整生产系统

```python
import asyncio
from typing import Dict, List, Callable
from dataclasses import dataclass
from enum import Enum

class ProductionQueue:
    """生产级队列系统"""

    def __init__(
        self,
        num_workers: int = 5,
        rate_limit: int = 100,
        max_retries: int = 3,
        enable_monitoring: bool = True
    ):
        self.queue = MonitoredQueue()
        self.num_workers = num_workers
        self.limiter = RateLimiter(max_requests=rate_limit, window=60)
        self.dlq = DeadLetterQueue(max_retries=max_retries)
        self.breaker = CircuitBreaker()
        self.cache = TaskCache()
        self.alert_manager = AlertManager()
        self.monitor = PrometheusMonitor() if enable_monitoring else None

        # 配置告警
        self._setup_alerts()

    def _setup_alerts(self):
        """配置告警"""
        self.alert_manager.add_rule(AlertRule(
            name="队列积压",
            condition=lambda m: m.queue_length > 1000,
            message="队列长度超过1000",
            severity="WARNING"
        ))

        self.alert_manager.add_rule(AlertRule(
            name="高失败率",
            condition=lambda m: m.failed_tasks / m.total_tasks > 0.1 if m.total_tasks > 0 else False,
            message="失败率超过10%",
            severity="CRITICAL"
        ))

    async def submit_task(self, client_id: str, task_id: str, task: dict):
        """提交任务"""
        # 速率限制
        if not self.limiter.is_allowed(client_id):
            raise Exception("速率限制")

        # 验证
        validated_task = TaskInput(**task)

        # 入队
        await self.queue.enqueue(task_id, validated_task.dict())

        if self.monitor:
            self.monitor.record_task_enqueued()
            self.monitor.update_queue_length(self.queue.metrics.queue_length)

    async def worker(self, worker_id: int):
        """工作协程"""
        while True:
            result = await self.queue.dequeue()
            if not result:
                await asyncio.sleep(0.1)
                continue

            task_id, task = result

            try:
                # 使用熔断器执行
                await self.breaker.call(self._process_task, task_id, task)

                await self.queue.mark_completed(task_id)

                if self.monitor:
                    duration = time.time() - self.queue.task_start_times.get(task_id, time.time())
                    self.monitor.record_task_completed(duration)

            except Exception as e:
                print(f"Worker {worker_id}: 任务{task_id}失败: {e}")
                await self.queue.mark_failed(task_id)
                await self.dlq.handle_failure(task_id, task, e)

                if self.monitor:
                    self.monitor.record_task_failed()

            # 检查告警
            alerts = self.alert_manager.check_alerts(self.queue.get_metrics())
            for alert in alerts:
                self.alert_manager.send_alert(alert)

    async def _process_task(self, task_id: str, task: dict):
        """处理任务"""
        print(f"处理任务: {task_id}")
        await asyncio.sleep(0.1)

    async def start(self):
        """启动系统"""
        workers = [
            asyncio.create_task(self.worker(i))
            for i in range(self.num_workers)
        ]
        await asyncio.gather(*workers)

    def get_metrics(self):
        """获取指标"""
        return self.queue.get_metrics()

    def get_dlq_tasks(self):
        """获取死信队列"""
        return self.dlq.get_dlq_tasks()

# 使用
async def main():
    system = ProductionQueue(
        num_workers=5,
        rate_limit=100,
        max_retries=3,
        enable_monitoring=True
    )

    # 提交任务
    for i in range(100):
        await system.submit_task(
            client_id="client_1",
            task_id=f"task_{i}",
            task={
                'task_id': f"task_{i}",
                'action': 'process',
                'data': {'index': i}
            }
        )

    # 启动系统
    await asyncio.wait_for(system.start(), timeout=60)

    # 打印指标
    metrics = system.get_metrics()
    print(f"完成: {metrics.completed_tasks}/{metrics.total_tasks}")
    print(f"失败: {metrics.failed_tasks}")
    print(f"平均等待时间: {metrics.avg_wait_time:.2f}秒")

asyncio.run(main())
```

---

## 总结

### 生产级检查清单

#### 监控
- [ ] 指标收集(队列长度、延迟、吞吐量)
- [ ] Prometheus集成
- [ ] 日志记录
- [ ] 分布式追踪

#### 容错
- [ ] 重试机制(指数退避)
- [ ] 死信队列
- [ ] 熔断器
- [ ] 超时处理

#### 性能
- [ ] 批量处理
- [ ] 连接池
- [ ] 缓存策略
- [ ] 异步I/O

#### 安全
- [ ] 输入验证
- [ ] 速率限制
- [ ] 认证授权
- [ ] 数据加密

#### 告警
- [ ] 告警规则
- [ ] 告警通知
- [ ] 告警聚合
- [ ] 告警静默

### 关键指标

| 指标 | 目标 | 告警阈值 |
|------|------|----------|
| 队列长度 | < 100 | > 1000 |
| 平均延迟 | < 1秒 | > 5秒 |
| 失败率 | < 1% | > 10% |
| 吞吐量 | > 100/秒 | < 10/秒 |

### 最佳实践

1. **监控优先**:先监控再优化
2. **容错设计**:假设一切都会失败
3. **渐进式优化**:先正确再快速
4. **文档完善**:记录所有决策
5. **定期演练**:故障演练和恢复

---

**恭喜**:你已完成Queue与任务调度系统的完整学习!现在你可以构建生产级的任务调度系统了。

**下一步建议**:
1. 在实际项目中应用这些知识
2. 阅读开源项目源码(Celery、Hatchet)
3. 参与社区讨论和贡献
4. 持续学习最新技术和最佳实践
