# å®æˆ˜ä»£ç 04: RAGæµå¼é—®ç­”

> å®ç°å®Œæ•´çš„ RAG æµå¼é—®ç­”ç³»ç»Ÿ,å…ˆæ˜¾ç¤ºæ£€ç´¢ç»“æœ,å†æµå¼ç”Ÿæˆç­”æ¡ˆ

---

## æ¦‚è¿°

æœ¬èŠ‚å®ç°å®Œæ•´çš„ RAG æµå¼é—®ç­”ç³»ç»Ÿ,ä½¿ç”¨æ··åˆç²’åº¦ç­–ç•¥:æ£€ç´¢é˜¶æ®µç”¨å®Œæ•´å“åº”æµå¼,ç”Ÿæˆé˜¶æ®µç”¨ Token æµå¼ã€‚

**å­¦ä¹ ç›®æ ‡:**
- å®ç° RAG æµå¼é—®ç­”å®Œæ•´æµç¨‹
- æŒæ¡æ··åˆç²’åº¦ç­–ç•¥
- ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ
- å¤„ç†é”™è¯¯å’Œè¾¹ç¼˜æƒ…å†µ

---

## 1. åŸºç¡€ RAG æµå¼é—®ç­”

```python
"""
åŸºç¡€ RAG æµå¼é—®ç­”
æ–‡ä»¶: examples/streaming/rag_stream_basic.py
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
import json

app = FastAPI()

# åˆå§‹åŒ–ç»„ä»¶
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
llm = ChatOpenAI(model="gpt-3.5-turbo")

@app.post("/rag-qa")
async def rag_qa(question: str):
    """åŸºç¡€ RAG æµå¼é—®ç­”"""
    async def generate():
        try:
            # 1. æ£€ç´¢é˜¶æ®µ
            yield f"event: retrieval_start\ndata: {json.dumps({'status': 'retrieving'})}\n\n"

            docs = await retriever.ainvoke(question)

            # å‘é€æ£€ç´¢ç»“æœ
            docs_data = [{
                "content": doc.page_content,
                "metadata": doc.metadata
            } for doc in docs]
            yield f"event: retrieval_done\ndata: {json.dumps({'docs': docs_data, 'count': len(docs)})}\n\n"

            # 2. ç”Ÿæˆé˜¶æ®µ
            yield f"event: generation_start\ndata: {json.dumps({'status': 'generating'})}\n\n"

            # æ„å»º prompt
            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = ChatPromptTemplate.from_template(
                "æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {question}\n\nç­”æ¡ˆ:"
            )

            # Token æµå¼ç”Ÿæˆ
            chain = prompt | llm
            async for chunk in chain.astream({"context": context, "question": question}):
                if chunk.content:
                    yield f"data: {chunk.content}\n\n"

            # 3. å®Œæˆ
            yield f"event: done\ndata: {json.dumps({'status': 'completed'})}\n\n"

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

# è¿è¡Œ: uvicorn examples.streaming.rag_stream_basic:app --reload
```

---

## 2. å®Œæ•´çš„ RAG æµå¼é—®ç­”

```python
"""
å®Œæ•´çš„ RAG æµå¼é—®ç­”
æ–‡ä»¶: examples/streaming/rag_stream_complete.py
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from pydantic import BaseModel
from typing import Optional
import json
import time

app = FastAPI()

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–ç»„ä»¶
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

class RAGRequest(BaseModel):
    question: str
    top_k: int = 3
    include_sources: bool = True

@app.post("/rag-stream")
async def rag_stream(request: RAGRequest):
    """å®Œæ•´çš„ RAG æµå¼é—®ç­”"""
    async def generate():
        start_time = time.time()

        try:
            # 1. æ£€ç´¢é˜¶æ®µ
            retrieval_start = time.time()
            yield f"event: retrieval_start\ndata: {json.dumps({
                'status': 'retrieving',
                'top_k': request.top_k
            })}\n\n"

            # æ‰§è¡Œæ£€ç´¢
            retriever = vectorstore.as_retriever(
                search_kwargs={"k": request.top_k}
            )
            docs = await retriever.ainvoke(request.question)

            retrieval_duration = time.time() - retrieval_start

            # å‘é€æ£€ç´¢ç»“æœ
            docs_data = []
            for i, doc in enumerate(docs):
                doc_info = {
                    "id": i,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "full_content": doc.page_content,
                    "metadata": doc.metadata,
                    "relevance_score": getattr(doc, 'score', None)
                }
                docs_data.append(doc_info)

            yield f"event: retrieval_done\ndata: {json.dumps({
                'docs': docs_data,
                'count': len(docs),
                'duration': retrieval_duration
            })}\n\n"

            # 2. ç”Ÿæˆé˜¶æ®µ
            generation_start = time.time()
            yield f"event: generation_start\ndata: {json.dumps({'status': 'generating'})}\n\n"

            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"æ–‡æ¡£ {i+1}:\n{doc.page_content}"
                for i, doc in enumerate(docs)
            ])

            # æ„å»º prompt
            prompt = ChatPromptTemplate.from_template(
                """æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,è¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

æ–‡æ¡£:
{context}

é—®é¢˜: {question}

ç­”æ¡ˆ:"""
            )

            # Token æµå¼ç”Ÿæˆ
            chain = prompt | llm
            full_response = ""
            token_count = 0

            async for chunk in chain.astream({
                "context": context,
                "question": request.question
            }):
                if chunk.content:
                    full_response += chunk.content
                    token_count += 1
                    yield f"data: {chunk.content}\n\n"

            generation_duration = time.time() - generation_start

            # 3. å‘é€å¼•ç”¨æ¥æº (å¦‚æœéœ€è¦)
            if request.include_sources:
                sources = [{
                    "id": i,
                    "title": doc.metadata.get('title', f'æ–‡æ¡£ {i+1}'),
                    "source": doc.metadata.get('source', 'Unknown')
                } for i, doc in enumerate(docs)]

                yield f"event: sources\ndata: {json.dumps({'sources': sources})}\n\n"

            # 4. å®Œæˆ
            total_duration = time.time() - start_time
            yield f"event: done\ndata: {json.dumps({
                'status': 'completed',
                'full_response': full_response,
                'token_count': token_count,
                'retrieval_duration': retrieval_duration,
                'generation_duration': generation_duration,
                'total_duration': total_duration,
                'tokens_per_second': token_count / generation_duration if generation_duration > 0 else 0
            })}\n\n"

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({
                'error': str(e),
                'type': type(e).__name__
            })}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## 3. å¸¦é‡æ’åºçš„ RAG æµå¼

```python
"""
å¸¦é‡æ’åºçš„ RAG æµå¼é—®ç­”
æ–‡ä»¶: examples/streaming/rag_stream_rerank.py
"""

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

@app.post("/rag-stream-rerank")
async def rag_stream_rerank(question: str, top_k: int = 5, rerank_top_k: int = 3):
    """å¸¦é‡æ’åºçš„ RAG æµå¼é—®ç­”"""
    async def generate():
        try:
            # 1. åˆå§‹æ£€ç´¢
            yield f"event: retrieval_start\ndata: {json.dumps({'status': 'retrieving', 'top_k': top_k})}\n\n"

            base_retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
            docs = await base_retriever.ainvoke(question)

            yield f"event: retrieval_done\ndata: {json.dumps({'count': len(docs)})}\n\n"

            # 2. é‡æ’åº
            yield f"event: rerank_start\ndata: {json.dumps({'status': 'reranking'})}\n\n"

            # ä½¿ç”¨ LLM è¿›è¡Œé‡æ’åº
            compressor = LLMChainExtractor.from_llm(llm)
            compression_retriever = ContextualCompressionRetriever(
                base_compressor=compressor,
                base_retriever=base_retriever
            )

            compressed_docs = await compression_retriever.ainvoke(question)
            compressed_docs = compressed_docs[:rerank_top_k]

            docs_data = [{
                "content": doc.page_content,
                "metadata": doc.metadata
            } for doc in compressed_docs]

            yield f"event: rerank_done\ndata: {json.dumps({
                'docs': docs_data,
                'count': len(compressed_docs)
            })}\n\n"

            # 3. ç”Ÿæˆ
            yield f"event: generation_start\ndata: {json.dumps({'status': 'generating'})}\n\n"

            context = "\n\n".join([doc.page_content for doc in compressed_docs])
            prompt = ChatPromptTemplate.from_template(
                "æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {question}\n\nç­”æ¡ˆ:"
            )

            chain = prompt | llm
            async for chunk in chain.astream({"context": context, "question": question}):
                if chunk.content:
                    yield f"data: {chunk.content}\n\n"

            yield f"event: done\ndata: Completed\n\n"

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## 4. å‰ç«¯å®ç°

### 4.1 React å®ç°

```javascript
/**
 * React RAG æµå¼é—®ç­”
 * æ–‡ä»¶: examples/streaming/frontend/ReactRAGStream.jsx
 */

import React, { useState } from 'react';

function RAGStreamQA() {
    const [question, setQuestion] = useState('');
    const [docs, setDocs] = useState([]);
    const [answer, setAnswer] = useState('');
    const [sources, setSources] = useState([]);
    const [isStreaming, setIsStreaming] = useState(false);
    const [stage, setStage] = useState(''); // retrieval, generation, done
    const [stats, setStats] = useState(null);

    const askQuestion = () => {
        if (!question.trim() || isStreaming) return;

        // é‡ç½®çŠ¶æ€
        setDocs([]);
        setAnswer('');
        setSources([]);
        setStats(null);
        setStage('');
        setIsStreaming(true);

        // åˆ›å»ºè¯·æ±‚
        fetch('http://localhost:8000/rag-stream', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                question: question,
                top_k: 3,
                include_sources: true
            })
        }).then(response => {
            const reader = response.body.getReader();
            const decoder = new TextDecoder();

            const readStream = () => {
                reader.read().then(({ done, value }) => {
                    if (done) {
                        setIsStreaming(false);
                        return;
                    }

                    const chunk = decoder.decode(value);
                    const lines = chunk.split('\n');

                    for (const line of lines) {
                        if (line.startsWith('event: ')) {
                            const eventType = line.slice(7);
                            handleEvent(eventType);
                        } else if (line.startsWith('data: ')) {
                            const data = line.slice(6);
                            handleData(data);
                        }
                    }

                    readStream();
                });
            };

            readStream();
        });
    };

    const handleEvent = (eventType) => {
        if (eventType === 'retrieval_start') {
            setStage('retrieval');
        } else if (eventType === 'generation_start') {
            setStage('generation');
        } else if (eventType === 'done') {
            setStage('done');
        }
    };

    const handleData = (data) => {
        try {
            const parsed = JSON.parse(data);

            if (parsed.docs) {
                setDocs(parsed.docs);
            } else if (parsed.sources) {
                setSources(parsed.sources);
            } else if (parsed.status === 'completed') {
                setStats(parsed);
            }
        } catch {
            // æ™®é€šæ–‡æœ¬æ•°æ® (ç”Ÿæˆçš„ç­”æ¡ˆ)
            setAnswer(prev => prev + data);
        }
    };

    return (
        <div style={{ padding: '20px', maxWidth: '1200px', margin: '0 auto' }}>
            <h1>RAG æµå¼é—®ç­”</h1>

            {/* è¾“å…¥åŒºåŸŸ */}
            <div style={{ marginBottom: '20px', display: 'flex', gap: '10px' }}>
                <input
                    type="text"
                    value={question}
                    onChange={(e) => setQuestion(e.target.value)}
                    onKeyPress={(e) => e.key === 'Enter' && askQuestion()}
                    placeholder="è¾“å…¥é—®é¢˜..."
                    disabled={isStreaming}
                    style={{ flex: 1, padding: '10px', fontSize: '16px' }}
                />
                <button
                    onClick={askQuestion}
                    disabled={isStreaming}
                    style={{ padding: '10px 20px', fontSize: '16px' }}
                >
                    {isStreaming ? 'å¤„ç†ä¸­...' : 'æé—®'}
                </button>
            </div>

            {/* è¿›åº¦æŒ‡ç¤º */}
            {isStreaming && (
                <div style={{ marginBottom: '20px', padding: '10px', background: '#e3f2fd' }}>
                    {stage === 'retrieval' && 'ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...'}
                    {stage === 'generation' && 'âœï¸ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...'}
                </div>
            )}

            <div style={{ display: 'grid', gridTemplateColumns: '1fr 2fr', gap: '20px' }}>
                {/* æ£€ç´¢ç»“æœ */}
                <div>
                    <h3>æ£€ç´¢åˆ°çš„æ–‡æ¡£ ({docs.length})</h3>
                    <div style={{ maxHeight: '500px', overflowY: 'auto' }}>
                        {docs.map((doc, index) => (
                            <div
                                key={index}
                                style={{
                                    marginBottom: '10px',
                                    padding: '10px',
                                    background: '#f5f5f5',
                                    borderRadius: '4px',
                                    fontSize: '14px'
                                }}
                            >
                                <div style={{ fontWeight: 'bold', marginBottom: '5px' }}>
                                    æ–‡æ¡£ {doc.id + 1}
                                </div>
                                <div>{doc.content}</div>
                                {doc.metadata.source && (
                                    <div style={{ marginTop: '5px', fontSize: '12px', color: '#666' }}>
                                        æ¥æº: {doc.metadata.source}
                                    </div>
                                )}
                            </div>
                        ))}
                    </div>
                </div>

                {/* ç”Ÿæˆçš„ç­”æ¡ˆ */}
                <div>
                    <h3>ç­”æ¡ˆ</h3>
                    <div style={{
                        padding: '15px',
                        background: 'white',
                        border: '1px solid #ccc',
                        borderRadius: '4px',
                        minHeight: '200px',
                        lineHeight: '1.6'
                    }}>
                        {answer}
                        {isStreaming && stage === 'generation' && (
                            <span className="cursor">â–‹</span>
                        )}
                    </div>

                    {/* å¼•ç”¨æ¥æº */}
                    {sources.length > 0 && (
                        <div style={{ marginTop: '20px' }}>
                            <h4>å¼•ç”¨æ¥æº:</h4>
                            <ul>
                                {sources.map((source, index) => (
                                    <li key={index}>
                                        {source.title} - {source.source}
                                    </li>
                                ))}
                            </ul>
                        </div>
                    )}

                    {/* ç»Ÿè®¡ä¿¡æ¯ */}
                    {stats && (
                        <div style={{ marginTop: '20px', fontSize: '14px', color: '#666' }}>
                            <div>æ£€ç´¢æ—¶é—´: {stats.retrieval_duration.toFixed(2)}s</div>
                            <div>ç”Ÿæˆæ—¶é—´: {stats.generation_duration.toFixed(2)}s</div>
                            <div>æ€»æ—¶é—´: {stats.total_duration.toFixed(2)}s</div>
                            <div>ç”Ÿæˆé€Ÿåº¦: {stats.tokens_per_second.toFixed(1)} tokens/s</div>
                        </div>
                    )}
                </div>
            </div>

            <style>{`
                @keyframes blink {
                    0%, 50% { opacity: 1; }
                    51%, 100% { opacity: 0; }
                }
                .cursor {
                    animation: blink 1s infinite;
                }
            `}</style>
        </div>
    );
}

export default RAGStreamQA;
```

---

## 5. æ€§èƒ½ä¼˜åŒ–

### 5.1 å¹¶è¡Œæ£€ç´¢å’Œç”Ÿæˆ

```python
"""
å¹¶è¡Œæ£€ç´¢å’Œç”Ÿæˆä¼˜åŒ–
"""

import asyncio

@app.post("/rag-stream-parallel")
async def rag_stream_parallel(question: str):
    """å¹¶è¡Œæ£€ç´¢å’Œç”Ÿæˆ"""
    async def generate():
        try:
            # åŒæ—¶å¼€å§‹æ£€ç´¢å’Œç”Ÿæˆ prompt
            retrieval_task = retriever.ainvoke(question)

            # ç­‰å¾…æ£€ç´¢å®Œæˆ
            docs = await retrieval_task

            yield f"event: retrieval_done\ndata: {json.dumps({'count': len(docs)})}\n\n"

            # ç«‹å³å¼€å§‹ç”Ÿæˆ
            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = ChatPromptTemplate.from_template(
                "æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {question}\n\nç­”æ¡ˆ:"
            )

            chain = prompt | llm
            async for chunk in chain.astream({"context": context, "question": question}):
                if chunk.content:
                    yield f"data: {chunk.content}\n\n"

            yield f"event: done\ndata: Completed\n\n"

        except Exception as e:
            yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

### 5.2 ç¼“å­˜æ£€ç´¢ç»“æœ

```python
"""
ç¼“å­˜æ£€ç´¢ç»“æœ
"""

from functools import lru_cache
import hashlib

# ç®€å•çš„å†…å­˜ç¼“å­˜
retrieval_cache = {}

def get_cache_key(question: str, top_k: int) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    return hashlib.md5(f"{question}:{top_k}".encode()).hexdigest()

@app.post("/rag-stream-cached")
async def rag_stream_cached(question: str, top_k: int = 3):
    """å¸¦ç¼“å­˜çš„ RAG æµå¼é—®ç­”"""
    async def generate():
        try:
            cache_key = get_cache_key(question, top_k)

            # æ£€æŸ¥ç¼“å­˜
            if cache_key in retrieval_cache:
                docs = retrieval_cache[cache_key]
                yield f"event: retrieval_cached\ndata: {json.dumps({'count': len(docs)})}\n\n"
            else:
                # æ‰§è¡Œæ£€ç´¢
                yield f"event: retrieval_start\ndata: Retrieving...\n\n"
                retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
                docs = await retriever.ainvoke(question)

                # ç¼“å­˜ç»“æœ
                retrieval_cache[cache_key] = docs
                yield f"event: retrieval_done\ndata: {json.dumps({'count': len(docs)})}\n\n"

            # ç”Ÿæˆç­”æ¡ˆ
            yield f"event: generation_start\ndata: Generating...\n\n"

            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = ChatPromptTemplate.from_template(
                "æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {question}\n\nç­”æ¡ˆ:"
            )

            chain = prompt | llm
            async for chunk in chain.astream({"context": context, "question": question}):
                if chunk.content:
                    yield f"data: {chunk.content}\n\n"

            yield f"event: done\ndata: Completed\n\n"

        except Exception as e:
            yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## 6. æµ‹è¯•

### 6.1 å•å…ƒæµ‹è¯•

```python
"""
RAG æµå¼é—®ç­”å•å…ƒæµ‹è¯•
æ–‡ä»¶: tests/test_rag_stream.py
"""

import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_rag_stream():
    """æµ‹è¯• RAG æµå¼é—®ç­”"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        async with client.stream(
            "POST",
            "/rag-stream",
            json={"question": "ä»€ä¹ˆæ˜¯AI?", "top_k": 3}
        ) as response:
            assert response.status_code == 200

            events = []
            async for line in response.aiter_lines():
                if line.startswith("event: "):
                    events.append(line[7:])

            # éªŒè¯äº‹ä»¶é¡ºåº
            assert "retrieval_start" in events
            assert "retrieval_done" in events
            assert "generation_start" in events
            assert "done" in events
```

---

## æ€»ç»“

**æœ¬èŠ‚è¦ç‚¹:**

1. **RAG æµå¼é—®ç­”**: æ£€ç´¢ + ç”Ÿæˆçš„å®Œæ•´æµç¨‹
2. **æ··åˆç²’åº¦**: æ£€ç´¢ç”¨å®Œæ•´å“åº”,ç”Ÿæˆç”¨ Token æµå¼
3. **ç”¨æˆ·ä½“éªŒ**: å®æ—¶æ˜¾ç¤ºæ£€ç´¢ç»“æœå’Œç”Ÿæˆè¿›åº¦
4. **æ€§èƒ½ä¼˜åŒ–**: å¹¶è¡Œå¤„ç†ã€ç¼“å­˜ã€é‡æ’åº
5. **å®Œæ•´å®ç°**: åŒ…å«å‰ç«¯å’Œåç«¯çš„å®Œæ•´ä»£ç 

**å…³é”®ä»£ç :**
```python
# æ£€ç´¢é˜¶æ®µ
docs = await retriever.ainvoke(question)
yield f"event: retrieval_done\ndata: {json.dumps({'docs': docs})}\n\n"

# ç”Ÿæˆé˜¶æ®µ
async for chunk in llm.astream(prompt):
    yield f"data: {chunk.content}\n\n"
```

**ä¸‹ä¸€æ­¥:**

æŒæ¡äº† RAG æµå¼é—®ç­”å,å¯ä»¥å­¦ä¹ :
- Agent æµå¼æ‰§è¡Œ
- å‰ç«¯é›†æˆç¤ºä¾‹
- é”™è¯¯å¤„ç†ä¸é‡è¯•

---

**è®°ä½:** RAG æµå¼é—®ç­”æ˜¯ AI Agent çš„æ ¸å¿ƒåŠŸèƒ½,æŒæ¡å®ƒæ˜¯æ„å»ºç”Ÿäº§çº§ AI åº”ç”¨çš„å…³é”®ã€‚
