# å®æˆ˜ä»£ç _åœºæ™¯3ï¼šLangGraphçŠ¶æ€å›¾Agent

> **ç›®æ ‡**ï¼šä½¿ç”¨LangGraphæ„å»ºä¸€ä¸ªå¤šæ­¥éª¤æ¨ç†çš„RAG Agent

---

## ä¸€ã€åœºæ™¯æè¿°

**éœ€æ±‚**ï¼šæ„å»ºä¸€ä¸ªæ–‡æ¡£é—®ç­”Agentï¼ŒåŒ…å«æ£€ç´¢ã€æ¨ç†ã€ç”Ÿæˆä¸‰ä¸ªæ­¥éª¤

**æµç¨‹**ï¼š
```
ç”¨æˆ·æŸ¥è¯¢ â†’ æ£€ç´¢æ–‡æ¡£ â†’ æ¨ç†åˆ†æ â†’ ç”Ÿæˆç­”æ¡ˆ â†’ éªŒè¯ â†’ è¾“å‡º
                â†“ å¤±è´¥                    â†“ å¤±è´¥
                é‡è¯• â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â† é‡è¯•
```

---

## äºŒã€å®Œæ•´å®ç°

```python
from typing import TypedDict, List, Annotated
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import operator

# 1. å®šä¹‰çŠ¶æ€
class RAGState(TypedDict):
    """RAG AgentçŠ¶æ€"""
    query: str
    documents: Annotated[List[str], operator.add]
    context: str
    answer: str
    retry_count: int
    validation_passed: bool

# 2. å®šä¹‰èŠ‚ç‚¹
def retrieve_node(state: RAGState) -> RAGState:
    """æ£€ç´¢èŠ‚ç‚¹"""
    query = state["query"]
    print(f"ğŸ” æ£€ç´¢: {query}")

    # æ¨¡æ‹Ÿå‘é‡æ£€ç´¢
    documents = [
        f"æ–‡æ¡£1: LangGraphæ˜¯ç”¨äºæ„å»ºæœ‰çŠ¶æ€AI Agentçš„æ¡†æ¶",
        f"æ–‡æ¡£2: LangGraphåŸºäºçŠ¶æ€æœºæ¨¡å‹ï¼Œæä¾›ç¡®å®šæ€§æ§åˆ¶",
        f"æ–‡æ¡£3: LangGraphæ”¯æŒcheckpointingå’Œäººæœºåä½œ"
    ]

    return {"documents": documents}

def reason_node(state: RAGState) -> RAGState:
    """æ¨ç†èŠ‚ç‚¹"""
    docs = state["documents"]
    print(f"ğŸ§  æ¨ç†: åˆ†æ{len(docs)}ä¸ªæ–‡æ¡£")

    # ä½¿ç”¨LLMæ¨ç†
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    context = "\n".join(docs)

    prompt = f"""åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

{context}

å…³é”®ä¿¡æ¯ï¼š"""

    response = llm.invoke([HumanMessage(content=prompt)])
    analyzed_context = response.content

    return {"context": analyzed_context}

def generate_node(state: RAGState) -> RAGState:
    """ç”ŸæˆèŠ‚ç‚¹"""
    query = state["query"]
    context = state["context"]
    print(f"âœï¸ ç”Ÿæˆç­”æ¡ˆ")

    # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
    llm = ChatOpenAI(model="gpt-4", temperature=0.7)
    prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š"""

    response = llm.invoke([HumanMessage(content=prompt)])
    answer = response.content

    return {"answer": answer}

def validate_node(state: RAGState) -> RAGState:
    """éªŒè¯èŠ‚ç‚¹"""
    answer = state["answer"]
    print(f"âœ… éªŒè¯ç­”æ¡ˆ")

    # ç®€å•éªŒè¯
    validation_passed = len(answer) > 10

    return {"validation_passed": validation_passed}

# 3. æ¡ä»¶è·¯ç”±
def should_retry(state: RAGState) -> str:
    """å†³å®šæ˜¯å¦é‡è¯•"""
    if state["validation_passed"]:
        return "end"
    elif state.get("retry_count", 0) < 3:
        return "retrieve"
    else:
        return "failed"

# 4. æ„å»ºå›¾
def create_rag_agent():
    """åˆ›å»ºRAG Agent"""
    graph = StateGraph(RAGState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("reason", reason_node)
    graph.add_node("generate", generate_node)
    graph.add_node("validate", validate_node)

    # æ·»åŠ è¾¹
    graph.add_edge(START, "retrieve")
    graph.add_edge("retrieve", "reason")
    graph.add_edge("reason", "generate")
    graph.add_edge("generate", "validate")

    # æ¡ä»¶è¾¹
    graph.add_conditional_edges(
        "validate",
        should_retry,
        {
            "end": END,
            "retrieve": "retrieve",
            "failed": END
        }
    )

    return graph.compile()

# 5. ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=== LangGraph RAG Agent ===\n")

    app = create_rag_agent()

    # è¿è¡Œ
    result = app.invoke({
        "query": "ä»€ä¹ˆæ˜¯LangGraphï¼Ÿ",
        "documents": [],
        "context": "",
        "answer": "",
        "retry_count": 0,
        "validation_passed": False
    })

    print("\n=== ç»“æœ ===")
    print(f"é—®é¢˜: {result['query']}")
    print(f"ç­”æ¡ˆ: {result['answer']}")
    print(f"éªŒè¯: {result['validation_passed']}")

    # å¯è§†åŒ–
    print("\n=== Mermaidå›¾ ===")
    print(app.get_graph().draw_mermaid())
```

---

## ä¸‰ã€å¸¦Checkpointingçš„ç‰ˆæœ¬

```python
from langgraph.checkpoint.memory import MemorySaver

def create_rag_agent_with_checkpoint():
    """åˆ›å»ºå¸¦Checkpointingçš„RAG Agent"""
    graph = StateGraph(RAGState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("reason", reason_node)
    graph.add_node("generate", generate_node)
    graph.add_node("validate", validate_node)

    # æ·»åŠ è¾¹
    graph.add_edge(START, "retrieve")
    graph.add_edge("retrieve", "reason")
    graph.add_edge("reason", "generate")
    graph.add_edge("generate", "validate")
    graph.add_conditional_edges("validate", should_retry, {...})

    # ç¼–è¯‘æ—¶æ·»åŠ checkpointer
    checkpointer = MemorySaver()
    return graph.compile(checkpointer=checkpointer)

# ä½¿ç”¨
app = create_rag_agent_with_checkpoint()
config = {"configurable": {"thread_id": "demo"}}

result = app.invoke(initial_state, config=config)

# è·å–çŠ¶æ€
state = app.get_state(config)
print(f"å½“å‰çŠ¶æ€: {state.values}")
```

---

## å››ã€æµå¼è¾“å‡ºç‰ˆæœ¬

```python
def run_with_streaming():
    """æµå¼æ‰§è¡ŒAgent"""
    app = create_rag_agent()

    print("=== æµå¼è¾“å‡º ===\n")

    for chunk in app.stream({
        "query": "ä»€ä¹ˆæ˜¯LangGraphï¼Ÿ",
        "documents": [],
        "context": "",
        "answer": "",
        "retry_count": 0,
        "validation_passed": False
    }):
        print(f"èŠ‚ç‚¹: {list(chunk.keys())[0]}")
        print(f"çŠ¶æ€: {chunk}")
        print()

if __name__ == "__main__":
    run_with_streaming()
```

---

## äº”ã€æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **StateGraph**ï¼šå®šä¹‰çŠ¶æ€å’ŒèŠ‚ç‚¹
2. **æ¡ä»¶è·¯ç”±**ï¼šåŠ¨æ€å†³ç­–ä¸‹ä¸€æ­¥
3. **Checkpointing**ï¼šçŠ¶æ€æŒä¹…åŒ–
4. **æµå¼è¾“å‡º**ï¼šå®æ—¶æ˜¾ç¤ºè¿›åº¦

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-14
**ä»£ç è¡Œæ•°**: ~200è¡Œ
**å¯è¿è¡Œ**: âœ… Python 3.13+ (éœ€è¦OpenAI API Key)
