# 实战代码3: 多任务处理

> **完整可运行的多任务并行处理示例**

---

## 环境准备

```bash
# 安装依赖
uv add langchain langchain-openai langchain-anthropic python-dotenv

# 配置 API Keys
echo "OPENAI_API_KEY=your_openai_key" > .env
echo "ANTHROPIC_API_KEY=your_anthropic_key" >> .env
```

---

## 示例1: 多模型并行调用

```python
"""
同时调用多个 LLM 模型进行对比
"""
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

# 定义多个模型
gpt4_chain = ChatPromptTemplate.from_template("{prompt}") | ChatOpenAI(model="gpt-4")
claude_chain = ChatPromptTemplate.from_template("{prompt}") | ChatAnthropic(model="claude-3-opus-20240229")

# 并行调用
comparison_chain = {
    "gpt4": gpt4_chain,
    "claude": claude_chain,
}

prompt = "用一句话解释量子计算"
results = comparison_chain.invoke({"prompt": prompt})

print("=== GPT-4 ===")
print(results["gpt4"].content)

print("\n=== Claude ===")
print(results["claude"].content)
```

---

## 示例2: 批处理优化

```python
"""
批量处理多个输入，提高效率
"""
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(model="gpt-4")

# 定义任务
summary_chain = ChatPromptTemplate.from_template("总结: {text}") | model
keywords_chain = ChatPromptTemplate.from_template("关键词: {text}") | model

# 批量输入
texts = [
    "LangChain 是一个 AI 框架",
    "Python 是一种编程语言",
    "机器学习改变世界",
]

# 批处理
results = []
for text in texts:
    result = {
        "summary": summary_chain,
        "keywords": keywords_chain,
    }.invoke({"text": text})
    results.append(result)

# 输出结果
for i, result in enumerate(results):
    print(f"\n=== 文本 {i+1} ===")
    print("摘要:", result["summary"].content)
    print("关键词:", result["keywords"].content)
```

---

## 示例3: 错误处理和降级

```python
"""
生产级错误处理和降级策略
"""
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

model = ChatOpenAI(model="gpt-4")

def safe_chain(chain, name: str, fallback_value=None):
    """带错误处理和降级的链"""
    def wrapper(x):
        try:
            logger.info(f"[{name}] 执行中...")
            result = chain.invoke(x)
            logger.info(f"[{name}] 成功")
            return {
                "success": True,
                "result": result,
                "source": "primary",
            }
        except Exception as e:
            logger.error(f"[{name}] 失败: {e}")
            if fallback_value:
                logger.info(f"[{name}] 使用降级值")
                return {
                    "success": True,
                    "result": fallback_value,
                    "source": "fallback",
                }
            return {
                "success": False,
                "error": str(e),
                "source": None,
            }
    return RunnableLambda(wrapper)

# 使用安全包装
analysis_chain = {
    "summary": safe_chain(
        ChatPromptTemplate.from_template("总结: {text}") | model,
        name="摘要",
        fallback_value="无法生成摘要"
    ),
    "keywords": safe_chain(
        ChatPromptTemplate.from_template("关键词: {text}") | model,
        name="关键词",
        fallback_value="无关键词"
    ),
}

result = analysis_chain.invoke({"text": "LangChain 很强大"})

for key, value in result.items():
    if value["success"]:
        print(f"✅ {key}: {value['result']} (来源: {value['source']})")
    else:
        print(f"❌ {key}: {value['error']}")
```

---

## 示例4: 性能监控

```python
"""
监控并行任务的性能
"""
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv
import time

load_dotenv()

model = ChatOpenAI(model="gpt-4")

def monitored_chain(chain, name: str):
    """带性能监控的链"""
    def wrapper(x):
        start = time.time()
        try:
            result = chain.invoke(x)
            elapsed = time.time() - start
            return {
                "result": result,
                "elapsed": elapsed,
                "status": "success",
            }
        except Exception as e:
            elapsed = time.time() - start
            return {
                "result": None,
                "elapsed": elapsed,
                "status": "failed",
                "error": str(e),
            }
    return RunnableLambda(wrapper)

# 使用监控包装
analysis_chain = {
    "summary": monitored_chain(
        ChatPromptTemplate.from_template("总结: {text}") | model,
        name="摘要"
    ),
    "keywords": monitored_chain(
        ChatPromptTemplate.from_template("关键词: {text}") | model,
        name="关键词"
    ),
}

result = analysis_chain.invoke({"text": "LangChain 让开发更简单"})

print("=== 性能报告 ===")
for key, value in result.items():
    print(f"{key}:")
    print(f"  状态: {value['status']}")
    print(f"  耗时: {value['elapsed']:.2f}秒")
    if value['status'] == 'success':
        print(f"  结果: {value['result'].content}")
```

---

## 示例5: 动态任务分配

```python
"""
根据输入动态分配任务
"""
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(model="gpt-4")

def create_analysis_tasks(task_types: list[str]):
    """动态创建分析任务"""
    tasks = {}
    for task_type in task_types:
        if task_type == "summary":
            tasks["summary"] = ChatPromptTemplate.from_template("总结: {text}") | model
        elif task_type == "keywords":
            tasks["keywords"] = ChatPromptTemplate.from_template("关键词: {text}") | model
        elif task_type == "sentiment":
            tasks["sentiment"] = ChatPromptTemplate.from_template("情感: {text}") | model
    return tasks

# 用户指定需要的分析类型
requested_tasks = ["summary", "sentiment"]
chain = create_analysis_tasks(requested_tasks)

result = chain.invoke({"text": "LangChain 非常棒"})

for key, value in result.items():
    print(f"{key}: {value.content}")
```

---

## 示例6: 优先级任务处理

```python
"""
按优先级处理任务
"""
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(model="gpt-4")

# 定义任务优先级
high_priority = {
    "critical_summary": ChatPromptTemplate.from_template("紧急总结: {text}") | model,
}

low_priority = {
    "detailed_analysis": ChatPromptTemplate.from_template("详细分析: {text}") | model,
    "keywords": ChatPromptTemplate.from_template("关键词: {text}") | model,
}

# 先执行高优先级，再执行低优先级
def process_with_priority(text):
    # 高优先级任务
    high_results = high_priority.copy()
    high_output = {k: v.invoke({"text": text}) for k, v in high_results.items()}

    # 低优先级任务（并行）
    low_output = low_priority.copy()
    low_output = {k: v for k, v in low_output.items()}
    low_results = low_output.invoke({"text": text}) if low_output else {}

    return {**high_output, **low_results}

result = process_with_priority("LangChain 是一个强大的框架")
print("高优先级:", result.get("critical_summary", {}).content if hasattr(result.get("critical_summary", {}), 'content') else result.get("critical_summary"))
```

---

## 示例7: 条件任务执行

```python
"""
根据条件决定执行哪些任务
"""
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(model="gpt-4")

def conditional_tasks(text: str, text_length: int):
    """根据文本长度决定任务"""
    tasks = {
        "summary": ChatPromptTemplate.from_template("总结: {text}") | model,
    }

    # 长文本添加额外任务
    if text_length > 100:
        tasks["detailed_keywords"] = ChatPromptTemplate.from_template(
            "提取10个关键词: {text}"
        ) | model
    else:
        tasks["simple_keywords"] = ChatPromptTemplate.from_template(
            "提取3个关键词: {text}"
        ) | model

    return tasks

# 测试
short_text = "LangChain 很好"
long_text = "LangChain 是一个非常强大的 AI 应用开发框架，它提供了丰富的组件和工具，让开发者能够轻松构建复杂的 AI 应用。"

print("=== 短文本 ===")
short_chain = conditional_tasks(short_text, len(short_text))
short_result = short_chain.invoke({"text": short_text})
for k, v in short_result.items():
    print(f"{k}: {v.content}")

print("\n=== 长文本 ===")
long_chain = conditional_tasks(long_text, len(long_text))
long_result = long_chain.invoke({"text": long_text})
for k, v in long_result.items():
    print(f"{k}: {v.content}")
```

---

## 示例8: 完整的生产级多任务系统

```python
"""
企业级多任务处理系统
包含：错误处理、性能监控、降级策略、日志记录
"""
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv
import logging
import time
from typing import Dict, Any, Callable

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

model = ChatOpenAI(model="gpt-4")

class TaskExecutor:
    """生产级任务执行器"""

    def __init__(self, name: str, chain, fallback=None, max_retries=2):
        self.name = name
        self.chain = chain
        self.fallback = fallback
        self.max_retries = max_retries

    def __call__(self, x: Any) -> Dict[str, Any]:
        """执行任务"""
        start_time = time.time()
        last_error = None

        # 重试逻辑
        for attempt in range(self.max_retries):
            try:
                logger.info(f"[{self.name}] 尝试 {attempt + 1}/{self.max_retries}")
                result = self.chain.invoke(x)
                elapsed = time.time() - start_time

                logger.info(f"[{self.name}] 成功 (耗时: {elapsed:.2f}s)")
                return {
                    "success": True,
                    "result": result,
                    "elapsed": elapsed,
                    "attempts": attempt + 1,
                    "source": "primary",
                }
            except Exception as e:
                last_error = e
                logger.warning(f"[{self.name}] 尝试 {attempt + 1} 失败: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(0.5)

        # 尝试降级
        if self.fallback:
            try:
                logger.info(f"[{self.name}] 使用降级方案")
                result = self.fallback(x)
                elapsed = time.time() - start_time

                return {
                    "success": True,
                    "result": result,
                    "elapsed": elapsed,
                    "attempts": self.max_retries,
                    "source": "fallback",
                }
            except Exception as e:
                logger.error(f"[{self.name}] 降级方案失败: {e}")

        # 完全失败
        elapsed = time.time() - start_time
        logger.error(f"[{self.name}] 完全失败")
        return {
            "success": False,
            "result": None,
            "elapsed": elapsed,
            "attempts": self.max_retries,
            "error": str(last_error),
            "source": None,
        }

# 定义任务
summary_task = ChatPromptTemplate.from_template("总结: {text}") | model
keywords_task = ChatPromptTemplate.from_template("关键词: {text}") | model
sentiment_task = ChatPromptTemplate.from_template("情感: {text}") | model

# 降级方案
def fallback_summary(x):
    return f"无法生成摘要: {x['text'][:50]}..."

def fallback_keywords(x):
    return "无关键词"

# 构建生产级任务链
production_chain = {
    "summary": RunnableLambda(TaskExecutor(
        name="摘要生成",
        chain=summary_task,
        fallback=fallback_summary,
        max_retries=2
    )),
    "keywords": RunnableLambda(TaskExecutor(
        name="关键词提取",
        chain=keywords_task,
        fallback=fallback_keywords,
        max_retries=2
    )),
    "sentiment": RunnableLambda(TaskExecutor(
        name="情感分析",
        chain=sentiment_task,
        max_retries=2
    )),
}

# 执行
result = production_chain.invoke({"text": "LangChain 让 AI 开发变得简单高效"})

# 生成报告
print("\n=== 执行报告 ===")
total_time = sum(v["elapsed"] for v in result.values())
success_count = sum(1 for v in result.values() if v["success"])

print(f"总耗时: {total_time:.2f}秒")
print(f"成功任务: {success_count}/{len(result)}")

print("\n=== 任务详情 ===")
for key, value in result.items():
    status = "✅" if value["success"] else "❌"
    print(f"{status} {key}:")
    print(f"  成功: {value['success']}")
    print(f"  尝试次数: {value['attempts']}")
    print(f"  耗时: {value['elapsed']:.2f}秒")
    if value.get('source'):
        print(f"  数据来源: {value['source']}")
    if value['success']:
        result_content = value['result'].content if hasattr(value['result'], 'content') else value['result']
        print(f"  结果: {result_content}")
    else:
        print(f"  错误: {value.get('error')}")
    print()
```

---

## 常见模式总结

### 模式1: 多模型对比
```python
{
    "model1": chain1,
    "model2": chain2,
}
```

### 模式2: 批处理
```python
for item in items:
    result = chain.invoke(item)
```

### 模式3: 错误处理
```python
{
    "task1": safe_chain(task1),
    "task2": safe_chain(task2),
}
```

### 模式4: 性能监控
```python
{
    "task1": monitored_chain(task1),
    "task2": monitored_chain(task2),
}
```

### 模式5: 动态任务
```python
tasks = create_tasks(config)
chain = tasks
```

---

## 性能优化建议

1. **控制并发数量**
   ```python
   # 避免过多并发
   # 建议: 5-10 个任务一批
   ```

2. **使用批处理**
   ```python
   # 批量调用 API
   results = model.batch(inputs)
   ```

3. **添加缓存**
   ```python
   from functools import lru_cache

   @lru_cache(maxsize=100)
   def cached_task(input):
       return expensive_operation(input)
   ```

4. **实现降级策略**
   ```python
   def with_fallback(primary, fallback):
       try:
           return primary()
       except:
           return fallback()
   ```

---

## 监控和调试

### 添加日志
```python
import logging

logger = logging.getLogger(__name__)
logger.info(f"任务开始: {task_name}")
```

### 性能分析
```python
import time

start = time.time()
result = chain.invoke(input)
print(f"耗时: {time.time() - start:.2f}s")
```

### 错误追踪
```python
try:
    result = chain.invoke(input)
except Exception as e:
    logger.error(f"错误: {e}", exc_info=True)
```

---

## 下一步学习

- [实战代码4: 上下文保持](./07_实战代码_04_上下文保持.md) - RunnablePassthrough.assign()
- [核心概念3: 结果合并策略](./03_核心概念_03_结果合并策略.md) - 错误处理深入
- [面试必问](./08_面试必问.md) - 面试准备

---

**版本**: v1.0
**最后更新**: 2026-02-19
**适用**: LangChain 0.3+, Python 3.13+
