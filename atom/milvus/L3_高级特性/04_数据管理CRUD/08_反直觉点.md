# 反直觉点 - Milvus 数据管理 CRUD

> 揭示 Milvus CRUD 操作中的常见误区和反直觉现象

---

## 为什么需要关注反直觉点？

**反直觉点**是指与我们的直觉或经验相悖的现象。在 Milvus CRUD 中，有很多操作与传统数据库不同，容易产生误解。

**学习反直觉点的价值**：
1. **避免踩坑**：提前了解常见误区
2. **深入理解**：理解设计背后的原因
3. **优化性能**：掌握正确的使用方式

---

## 反直觉点 1: Milvus 没有原生的 Update 操作

### 直觉认知

**直觉**：数据库应该支持 Update 操作，就像 SQL 的 `UPDATE` 语句一样。

```sql
-- 传统数据库
UPDATE users SET age = 26 WHERE id = 1;
```

### 反直觉现实

**现实**：Milvus 没有原生的 Update 操作，只能通过 **Delete + Insert** 或 **Upsert** 实现。

```python
# Milvus 2.2 及之前：手动 Delete + Insert
collection.delete(expr="id == 1")
collection.insert([[1], ["Alice"], [[0.1, 0.2, 0.3]]])
collection.flush()

# Milvus 2.3+：使用 Upsert
collection.upsert([[1], ["Alice"], [[0.1, 0.2, 0.3]]])
collection.flush()
```

### 为什么会这样？

**原因 1: 向量索引的不可变性**

向量索引（如 HNSW）是基于向量的相似度关系构建的图结构。如果修改向量，需要重建整个索引。

```
原始向量: [0.1, 0.2, 0.3]
         ↓ 构建索引
      HNSW 图结构
         ↓ 修改向量为 [0.9, 0.8, 0.7]
      索引失效！需要重建
```

**原因 2: 性能考虑**

重建索引的代价非常高：
- 小规模数据（10万条）：重建需要几秒
- 大规模数据（1亿条）：重建需要几小时

**原因 3: 列式存储的特性**

Milvus 使用列式存储，数据是不可变的 Segment。修改数据需要创建新的 Segment。

### 正确的做法

**场景 1: 少量数据更新**

```python
# 使用 Upsert（推荐）
collection.upsert(data)
collection.flush()
```

**场景 2: 大量数据更新**

```python
# 批量删除 + 批量插入
collection.delete(expr="id in [1, 2, 3, ..., 10000]")
collection.insert(new_data)
collection.flush()
```

**场景 3: 只更新标量字段**

```python
# 错误：Milvus 不支持只更新部分字段
# collection.update({"id": 1, "text": "new text"})  # 不存在

# 正确：必须提供所有字段
collection.upsert([[1], ["new text"], [original_embedding]])
```

### 关键洞察

- **Milvus 的 Update 是重量级操作**：不像传统数据库那样轻量
- **Upsert 是原子操作**：比手动 Delete + Insert 更安全
- **频繁更新不适合 Milvus**：如果需要频繁更新，考虑其他数据库

---

## 反直觉点 2: Insert 后必须 flush() 和 load() 才能检索

### 直觉认知

**直觉**：插入数据后应该立即可以检索，就像传统数据库一样。

```sql
-- 传统数据库
INSERT INTO users VALUES (1, 'Alice');
SELECT * FROM users WHERE id = 1;  -- 立即可以查到
```

### 反直觉现实

**现实**：Milvus 插入数据后，必须调用 `flush()` 和 `load()` 才能检索。

```python
# 错误：插入后立即检索
collection.insert(data)
results = collection.search(...)  # 报错：collection not loaded

# 正确：插入 → flush → load → 检索
collection.insert(data)
collection.flush()  # 持久化
collection.load()   # 加载到内存
results = collection.search(...)  # 成功
```

### 为什么会这样？

**原因 1: 延迟持久化**

Milvus 使用延迟持久化策略，数据先写入内存缓冲区（MemTable），调用 `flush()` 才写入磁盘。

```
insert() → MemTable（内存缓冲区）
           ↓
flush()  → Segment（磁盘）
           ↓
load()   → Memory（内存索引）
           ↓
search() → 检索成功
```

**原因 2: 显式加载**

Milvus 要求用户显式加载 Collection 到内存，以便控制内存使用。

**原因 3: 索引构建**

`flush()` 后，Milvus 会构建索引。`load()` 会加载索引到内存。

### 正确的做法

**场景 1: 单次插入**

```python
collection.insert(data)
collection.flush()  # 持久化
collection.load()   # 加载到内存
```

**场景 2: 批量插入**

```python
for batch in batches:
    collection.insert(batch)

collection.flush()  # 批量持久化
collection.load()   # 加载到内存
```

**场景 3: 已加载的 Collection**

```python
# 如果 Collection 已经加载，只需要 flush
collection.insert(data)
collection.flush()  # 新数据会自动加载
```

### 常见错误

**错误 1: 忘记 flush()**

```python
collection.insert(data)
# 忘记 flush()
collection.load()
results = collection.search(...)  # 查不到新数据
```

**错误 2: 忘记 load()**

```python
collection.insert(data)
collection.flush()
# 忘记 load()
results = collection.search(...)  # 报错：collection not loaded
```

**错误 3: 每次插入都 flush()**

```python
# 错误：性能低下
for data in batches:
    collection.insert(data)
    collection.flush()  # 每次都 flush，性能差

# 正确：批量 flush
for data in batches:
    collection.insert(data)
collection.flush()  # 批量 flush，性能好
```

### 关键洞察

- **flush() 是必须的**：不 flush 数据可能丢失
- **load() 是必须的**：不 load 无法检索
- **批量 flush 更高效**：减少磁盘 I/O 次数

---

## 反直觉点 3: Delete 是软删除，需要 Compaction 才能释放空间

### 直觉认知

**直觉**：删除数据后，空间应该立即释放。

```sql
-- 传统数据库
DELETE FROM users WHERE id = 1;
-- 空间立即释放
```

### 反直觉现实

**现实**：Milvus 的 Delete 是软删除，只标记删除，不立即释放空间。需要 Compaction 才能真正释放。

```python
# 删除数据
collection.delete(expr="id in [1, 2, 3]")
collection.flush()

# 此时空间并未释放！
# 需要 Compaction
collection.compact()

# 等待 Compaction 完成
collection.wait_for_compaction_completed()

# 现在空间才真正释放
```

### 为什么会这样？

**原因 1: Segment 的不可变性**

Milvus 的数据存储在不可变的 Segment 中。删除数据不能直接修改 Segment，只能标记删除。

```
Segment 1: [数据1, 数据2, 数据3]
           ↓ 删除数据2
Segment 1: [数据1, 数据2(已删除), 数据3]
           ↓ Compaction
Segment 1': [数据1, 数据3]  # 创建新 Segment
```

**原因 2: 性能考虑**

立即释放空间需要重写 Segment，代价很高。Compaction 是后台任务，不影响前台操作。

**原因 3: 一致性保证**

软删除保证了删除操作的原子性和一致性。

### 正确的做法

**场景 1: 手动触发 Compaction**

```python
# 删除数据
collection.delete(expr="id in [1, 2, 3]")
collection.flush()

# 手动触发 Compaction
collection.compact()

# 等待完成
collection.wait_for_compaction_completed()
```

**场景 2: 自动 Compaction**

```python
# Milvus 会自动触发 Compaction
# 默认策略：
# - 删除比例 > 10%
# - Segment 大小 > 512MB

# 只需要删除和 flush
collection.delete(expr="id in [1, 2, 3]")
collection.flush()

# Milvus 会在后台自动 Compaction
```

**场景 3: 定期 Compaction**

```python
import schedule

def compact_collection():
    collection.compact()
    collection.wait_for_compaction_completed()

# 每天凌晨 2 点执行 Compaction
schedule.every().day.at("02:00").do(compact_collection)
```

### 常见错误

**错误 1: 删除后立即检查空间**

```python
# 错误：删除后立即检查空间
collection.delete(expr="id in [1, 2, 3]")
collection.flush()
print(collection.num_entities)  # 数量减少了
# 但磁盘空间并未释放！
```

**错误 2: 频繁触发 Compaction**

```python
# 错误：每次删除都 Compaction
for id in ids:
    collection.delete(expr=f"id == {id}")
    collection.flush()
    collection.compact()  # 频繁 Compaction，性能差

# 正确：批量删除后 Compaction
collection.delete(expr=f"id in {ids}")
collection.flush()
collection.compact()  # 一次 Compaction
```

**错误 3: 不等待 Compaction 完成**

```python
# 错误：不等待 Compaction 完成
collection.compact()
# 立即检查空间，还未释放

# 正确：等待 Compaction 完成
collection.compact()
collection.wait_for_compaction_completed()
# 现在空间已释放
```

### 关键洞察

- **Delete 是软删除**：只标记删除，不立即释放空间
- **Compaction 是必须的**：要释放空间必须 Compaction
- **自动 Compaction 有延迟**：可能需要等待几分钟到几小时

---

## 反直觉点 4: Query 和 Search 是两个完全不同的操作

### 直觉认知

**直觉**：Query 和 Search 应该是类似的操作，只是参数不同。

### 反直觉现实

**现实**：Query 和 Search 是两个完全不同的操作，底层实现、性能特征、使用场景都不同。

| 特性 | Query | Search |
|------|-------|--------|
| **查询方式** | 标量查询（精确匹配） | 向量检索（相似度匹配） |
| **查询条件** | 表达式（expr） | 向量（data） |
| **返回结果** | 所有匹配的数据 | Top-K 最相似的数据 |
| **是否使用索引** | 不使用向量索引 | 使用向量索引 |
| **性能特征** | 全表扫描，O(n) | 索引加速，O(log n) |
| **适用场景** | 按 ID 查询、条件过滤 | 相似度检索、推荐系统 |

### 示例对比

**Query: 精确查询**

```python
# 查询 ID 为 1, 2, 3 的数据
results = collection.query(
    expr="id in [1, 2, 3]",
    output_fields=["id", "text"]
)

# 返回所有匹配的数据
for result in results:
    print(result)  # {"id": 1, "text": "..."}
```

**Search: 向量检索**

```python
# 查询与给定向量最相似的 5 条数据
results = collection.search(
    data=[[0.1, 0.2, 0.3, ...]],  # 查询向量
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=5  # Top-5
)

# 返回最相似的 5 条数据
for hits in results:
    for hit in hits:
        print(hit.id, hit.distance)  # ID 和距离
```

### 为什么会这样？

**原因 1: 不同的查询需求**

- **Query**：用于精确查询，如按 ID 查询、条件过滤
- **Search**：用于相似度检索，如推荐系统、图像搜索

**原因 2: 不同的底层实现**

- **Query**：扫描标量字段，不涉及向量计算
- **Search**：使用向量索引，涉及向量距离计算

**原因 3: 不同的性能特征**

- **Query**：全表扫描，性能取决于数据量
- **Search**：索引加速，性能取决于索引类型

### 正确的做法

**场景 1: 按 ID 查询**

```python
# 使用 Query
results = collection.query(
    expr="id in [1, 2, 3]",
    output_fields=["id", "text", "embedding"]
)
```

**场景 2: 相似度检索**

```python
# 使用 Search
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    param={"metric_type": "L2"},
    limit=5
)
```

**场景 3: 混合检索（向量 + 标量过滤）**

```python
# 使用 Search + expr
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    param={"metric_type": "L2"},
    limit=5,
    expr="category == 'tech'"  # 标量过滤
)
```

### 常见错误

**错误 1: 用 Query 做相似度检索**

```python
# 错误：Query 不支持向量检索
results = collection.query(
    expr="embedding == [0.1, 0.2, 0.3]"  # 不支持！
)
```

**错误 2: 用 Search 做精确查询**

```python
# 错误：Search 返回的是 Top-K，不是所有匹配的数据
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    limit=100,  # 即使 limit 很大，也只返回最相似的 100 条
    expr="id in [1, 2, 3]"  # 不如直接用 Query
)
```

### 关键洞察

- **Query 和 Search 是互补的**：不是替代关系
- **选择合适的操作**：根据查询需求选择
- **混合检索**：可以结合使用（Search + expr）

---

## 反直觉点 5: 列式存储的插入格式

### 直觉认知

**直觉**：插入数据应该是行式的，每行是一个对象。

```python
# 直觉：行式存储
data = [
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]}
]
```

### 反直觉现实

**现实**：Milvus 使用列式存储，插入数据必须是列式的。

```python
# 现实：列式存储
data = [
    [1, 2],  # id 列
    ["A", "B"],  # text 列
    [[0.1, 0.2], [0.3, 0.4]]  # embedding 列
]

collection.insert(data)
```

### 为什么会这样？

**原因 1: 压缩效率**

列式存储的同类型数据连续存储，压缩率更高。

```
行式存储: [1, "A", [0.1, 0.2]], [2, "B", [0.3, 0.4]]
         ↓ 压缩率低（类型混杂）

列式存储: [1, 2], ["A", "B"], [[0.1, 0.2], [0.3, 0.4]]
         ↓ 压缩率高（同类型连续）
```

**原因 2: 向量计算效率**

向量数据连续存储，SIMD 加速效果更好。

**原因 3: 内存占用**

只加载需要的列，减少内存占用。

### 正确的做法

**场景 1: 手动构建列式数据**

```python
# 行式数据
rows = [
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]}
]

# 转换为列式数据
ids = [row["id"] for row in rows]
texts = [row["text"] for row in rows]
embeddings = [row["embedding"] for row in rows]

# 插入
collection.insert([ids, texts, embeddings])
```

**场景 2: 使用 Pandas**

```python
import pandas as pd

# 行式数据（DataFrame）
df = pd.DataFrame([
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]}
])

# 转换为列式数据
data = [
    df["id"].tolist(),
    df["text"].tolist(),
    df["embedding"].tolist()
]

# 插入
collection.insert(data)
```

### 常见错误

**错误 1: 使用行式数据**

```python
# 错误：行式数据
data = [
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]}
]
collection.insert(data)  # 报错！
```

**错误 2: 列的顺序错误**

```python
# 错误：列的顺序必须与 Schema 一致
data = [
    ["A", "B"],  # text 列
    [1, 2],  # id 列
    [[0.1, 0.2], [0.3, 0.4]]  # embedding 列
]
collection.insert(data)  # 报错！类型不匹配

# 正确：按 Schema 顺序
data = [
    [1, 2],  # id 列（第一个字段）
    ["A", "B"],  # text 列（第二个字段）
    [[0.1, 0.2], [0.3, 0.4]]  # embedding 列（第三个字段）
]
collection.insert(data)
```

### 关键洞察

- **列式存储是 Milvus 的核心特性**：不是可选的
- **需要转换数据格式**：从行式转换为列式
- **列的顺序很重要**：必须与 Schema 一致

---

## 反直觉点总结

| 反直觉点 | 直觉认知 | 反直觉现实 | 关键洞察 |
|---------|---------|-----------|---------|
| **Update 操作** | 应该支持 Update | 没有原生 Update，只能 Delete + Insert 或 Upsert | 向量索引不可变 |
| **flush() 和 load()** | 插入后立即可检索 | 必须 flush() + load() | 延迟持久化 + 显式加载 |
| **Delete 释放空间** | 删除后立即释放空间 | 软删除，需要 Compaction | Segment 不可变 |
| **Query vs Search** | 类似的操作 | 完全不同的操作 | 精确查询 vs 相似度检索 |
| **列式存储** | 行式插入 | 列式插入 | 压缩效率 + 向量计算 |

---

## 从反直觉到正确实践

理解了反直觉点后，你可以：

1. **避免常见错误**：
   - 不要期望原生 Update
   - 记得 flush() 和 load()
   - 理解软删除和 Compaction
   - 区分 Query 和 Search
   - 使用列式数据格式

2. **优化性能**：
   - 使用 Upsert 简化更新
   - 批量 flush 提升性能
   - 定期 Compaction 释放空间
   - 选择合适的查询方式
   - 利用列式存储的优势

3. **设计更好的系统**：
   - 避免频繁更新
   - 合理规划 Compaction 策略
   - 混合使用 Query 和 Search
   - 优化数据格式转换

---

**下一步**: 学习 [03_核心概念_1_数据插入策略.md](./03_核心概念_1_数据插入策略.md) 深入理解插入操作
