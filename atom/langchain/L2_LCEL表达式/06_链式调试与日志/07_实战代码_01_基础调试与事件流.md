# å®æˆ˜ä»£ç 1ï¼šåŸºç¡€è°ƒè¯•ä¸äº‹ä»¶æµ

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›å®Œæ•´çš„å®æˆ˜ä»£ç ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ `astream_events()` v2 è°ƒè¯•LCELé“¾ã€‚

**æ¶µç›–åœºæ™¯**ï¼š
1. åŸºç¡€é“¾è°ƒè¯• - æŸ¥çœ‹æ‰€æœ‰äº‹ä»¶
2. äº‹ä»¶è¿‡æ»¤å’Œæ£€æŸ¥ - æŒ‰ç±»å‹ã€åç§°è¿‡æ»¤
3. è°ƒè¯•å¤æ‚å¤šæ­¥éª¤é“¾ - RAGé“¾
4. ä¸­é—´ç»“æœæ£€æŸ¥ - æµå¼è¾“å‡º

**å‰ç½®è¦æ±‚**ï¼š
- Python 3.13+
- LangChain v0.3+
- OpenAI APIå¯†é’¥

---

## åœºæ™¯1ï¼šåŸºç¡€é“¾è°ƒè¯•

### ç›®æ ‡

æŸ¥çœ‹ä¸€ä¸ªç®€å•é“¾çš„æ‰€æœ‰æ‰§è¡Œäº‹ä»¶ï¼Œç†è§£é“¾çš„æ‰§è¡Œæµç¨‹ã€‚

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯1ï¼šåŸºç¡€é“¾è°ƒè¯•
æŸ¥çœ‹æ‰€æœ‰äº‹ä»¶ï¼Œç†è§£é“¾çš„æ‰§è¡Œæµç¨‹
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import asyncio

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ„å»ºé“¾
prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()
chain = prompt | model | parser


async def debug_basic_chain():
    """è°ƒè¯•åŸºç¡€é“¾ï¼ŒæŸ¥çœ‹æ‰€æœ‰äº‹ä»¶"""
    print("=== åŸºç¡€é“¾è°ƒè¯• ===\n")

    async for event in chain.astream_events(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        version="v2"  # å¿…é¡»æŒ‡å®šv2
    ):
        kind = event["event"]
        name = event["name"]
        run_id = event.get("run_id", "")[:8]  # åªæ˜¾ç¤ºå‰8ä½

        # æ‰“å°äº‹ä»¶ä¿¡æ¯
        print(f"[{kind:25}] {name:20} (run_id: {run_id})")

        # æ˜¾ç¤ºå…³é”®æ•°æ®
        if kind == "on_prompt_end":
            messages = event["data"]["output"].to_messages()
            print(f"  â†’ Prompt: {messages[0].content[:50]}...")

        elif kind == "on_chat_model_start":
            messages = event["data"]["input"]["messages"]
            print(f"  â†’ Input: {len(messages)} messages")

        elif kind == "on_chat_model_end":
            output = event["data"]["output"]
            print(f"  â†’ Output: {output.content[:50]}...")

        elif kind == "on_parser_end":
            output = event["data"]["output"]
            print(f"  â†’ Parsed: {output[:50]}...")


if __name__ == "__main__":
    asyncio.run(debug_basic_chain())
```

### è¾“å‡ºç¤ºä¾‹

```
=== åŸºç¡€é“¾è°ƒè¯• ===

[on_chain_start           ] RunnableSequence     (run_id: a1b2c3d4)
[on_prompt_start          ] ChatPromptTemplate   (run_id: e5f6g7h8)
[on_prompt_end            ] ChatPromptTemplate   (run_id: e5f6g7h8)
  â†’ Prompt: å›ç­”é—®é¢˜: ä»€ä¹ˆæ˜¯LCEL?...
[on_chat_model_start      ] ChatOpenAI           (run_id: i9j0k1l2)
  â†’ Input: 1 messages
[on_chat_model_stream     ] ChatOpenAI           (run_id: i9j0k1l2)
[on_chat_model_stream     ] ChatOpenAI           (run_id: i9j0k1l2)
...
[on_chat_model_end        ] ChatOpenAI           (run_id: i9j0k1l2)
  â†’ Output: LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§...
[on_parser_start          ] StrOutputParser      (run_id: m3n4o5p6)
[on_parser_stream         ] StrOutputParser      (run_id: m3n4o5p6)
[on_parser_end            ] StrOutputParser      (run_id: m3n4o5p6)
  â†’ Parsed: LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§...
[on_chain_stream          ] RunnableSequence     (run_id: a1b2c3d4)
[on_chain_end             ] RunnableSequence     (run_id: a1b2c3d4)
```

### å…³é”®ç‚¹

1. **äº‹ä»¶é¡ºåº**ï¼šé“¾æŒ‰é¡ºåºæ‰§è¡Œï¼ˆprompt â†’ model â†’ parserï¼‰
2. **run_id**ï¼šæ¯ä¸ªç»„ä»¶æœ‰å”¯ä¸€çš„run_id
3. **æµå¼äº‹ä»¶**ï¼š`on_chat_model_stream` ä¼šè§¦å‘å¤šæ¬¡
4. **æ•°æ®è®¿é—®**ï¼šé€šè¿‡ `event["data"]` è®¿é—®è¯¦ç»†æ•°æ®

---

## åœºæ™¯2ï¼šäº‹ä»¶è¿‡æ»¤å’Œæ£€æŸ¥

### ç›®æ ‡

åªæŸ¥çœ‹ç‰¹å®šç±»å‹çš„äº‹ä»¶ï¼Œå‡å°‘å™ªéŸ³ï¼Œèšç„¦å…³é”®ä¿¡æ¯ã€‚

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯2ï¼šäº‹ä»¶è¿‡æ»¤å’Œæ£€æŸ¥
æŒ‰ç±»å‹ã€åç§°è¿‡æ»¤äº‹ä»¶
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import asyncio

load_dotenv()

prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model | StrOutputParser()


async def filter_by_type():
    """æŒ‰ç±»å‹è¿‡æ»¤ï¼šåªçœ‹èŠå¤©æ¨¡å‹äº‹ä»¶"""
    print("=== æŒ‰ç±»å‹è¿‡æ»¤ï¼šåªçœ‹èŠå¤©æ¨¡å‹äº‹ä»¶ ===\n")

    async for event in chain.astream_events(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        version="v2",
        include_types=["chat_model"]  # åªçœ‹chat_modeläº‹ä»¶
    ):
        kind = event["event"]
        print(f"[{kind}]")

        if kind == "on_chat_model_start":
            messages = event["data"]["input"]["messages"]
            print(f"  å‘é€ç»™LLM: {messages[0].content}")

        elif kind == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            print(chunk.content, end="", flush=True)

        elif kind == "on_chat_model_end":
            output = event["data"]["output"]
            tokens = output.response_metadata.get("token_usage", {})
            print(f"\n  Tokenä½¿ç”¨: {tokens}")


async def filter_by_name():
    """æŒ‰åç§°è¿‡æ»¤ï¼šåªçœ‹ç‰¹å®šç»„ä»¶"""
    print("\n=== æŒ‰åç§°è¿‡æ»¤ï¼šåªçœ‹ChatOpenAI ===\n")

    async for event in chain.astream_events(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        version="v2",
        include_names=["ChatOpenAI"]
    ):
        kind = event["event"]
        name = event["name"]
        print(f"[{kind}] {name}")


async def exclude_types():
    """æ’é™¤ç±»å‹ï¼šä¸çœ‹è§£æå™¨äº‹ä»¶"""
    print("\n=== æ’é™¤è§£æå™¨äº‹ä»¶ ===\n")

    async for event in chain.astream_events(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        version="v2",
        exclude_types=["parser"]  # æ’é™¤parseräº‹ä»¶
    ):
        kind = event["event"]
        name = event["name"]
        print(f"[{kind}] {name}")


async def stream_llm_output():
    """å®æ—¶æŸ¥çœ‹LLMè¾“å‡º"""
    print("\n=== å®æ—¶æŸ¥çœ‹LLMè¾“å‡º ===\n")

    async for event in chain.astream_events(
        {"question": "ç”¨ä¸€å¥è¯è§£é‡ŠLCEL"},
        version="v2"
    ):
        kind = event["event"]

        # åªå¤„ç†LLMæµå¼è¾“å‡º
        if kind == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            print(chunk.content, end="", flush=True)

    print("\n")


if __name__ == "__main__":
    asyncio.run(filter_by_type())
    asyncio.run(filter_by_name())
    asyncio.run(exclude_types())
    asyncio.run(stream_llm_output())
```

### è¾“å‡ºç¤ºä¾‹

```
=== æŒ‰ç±»å‹è¿‡æ»¤ï¼šåªçœ‹èŠå¤©æ¨¡å‹äº‹ä»¶ ===

[on_chat_model_start]
  å‘é€ç»™LLM: å›ç­”é—®é¢˜: ä»€ä¹ˆæ˜¯LCEL?
[on_chat_model_stream]
LCEL[on_chat_model_stream]
ï¼ˆ[on_chat_model_stream]
LangChain...
[on_chat_model_end]
  Tokenä½¿ç”¨: {'prompt_tokens': 15, 'completion_tokens': 120, 'total_tokens': 135}

=== æŒ‰åç§°è¿‡æ»¤ï¼šåªçœ‹ChatOpenAI ===

[on_chat_model_start] ChatOpenAI
[on_chat_model_stream] ChatOpenAI
...
[on_chat_model_end] ChatOpenAI

=== æ’é™¤è§£æå™¨äº‹ä»¶ ===

[on_chain_start] RunnableSequence
[on_prompt_start] ChatPromptTemplate
[on_prompt_end] ChatPromptTemplate
[on_chat_model_start] ChatOpenAI
...

=== å®æ—¶æŸ¥çœ‹LLMè¾“å‡º ===

LCELæ˜¯LangChainçš„å£°æ˜å¼é“¾æ„å»ºè¯­è¨€ï¼Œç”¨ç®¡é“ç¬¦è¿æ¥ç»„ä»¶ã€‚
```

### å…³é”®ç‚¹

1. **include_types**ï¼šåªçœ‹ç‰¹å®šç±»å‹çš„äº‹ä»¶
2. **include_names**ï¼šåªçœ‹ç‰¹å®šåç§°çš„ç»„ä»¶
3. **exclude_types**ï¼šæ’é™¤ä¸å…³å¿ƒçš„äº‹ä»¶
4. **æµå¼è¾“å‡º**ï¼šç›‘å¬ `on_chat_model_stream` å®æ—¶æŸ¥çœ‹

---

## åœºæ™¯3ï¼šè°ƒè¯•å¤æ‚å¤šæ­¥éª¤é“¾ï¼ˆRAGé“¾ï¼‰

### ç›®æ ‡

è°ƒè¯•ä¸€ä¸ªåŒ…å«æ£€ç´¢å™¨çš„RAGé“¾ï¼ŒæŸ¥çœ‹æ£€ç´¢ç»“æœå’Œç”Ÿæˆè¿‡ç¨‹ã€‚

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯3ï¼šè°ƒè¯•RAGé“¾
æŸ¥çœ‹æ£€ç´¢ç»“æœå’Œç”Ÿæˆè¿‡ç¨‹
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from dotenv import load_dotenv
import asyncio

load_dotenv()

# æ„å»ºRAGé“¾
def build_rag_chain():
    """æ„å»ºRAGé“¾"""
    # åˆ›å»ºå‘é‡å­˜å‚¨
    texts = [
        "LCELæ˜¯LangChain Expression Languageçš„ç¼©å†™",
        "LCELä½¿ç”¨ç®¡é“ç¬¦|è¿æ¥ç»„ä»¶",
        "LCELæ”¯æŒæµå¼å¤„ç†å’Œå¼‚æ­¥æ‰§è¡Œ",
        "LCELå¯ä»¥è½»æ¾æ„å»ºå¤æ‚çš„AIåº”ç”¨"
    ]
    vectorstore = Chroma.from_texts(
        texts,
        embedding=OpenAIEmbeddings()
    )
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    # æ„å»ºprompt
    template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

å›ç­”:"""
    prompt = ChatPromptTemplate.from_template(template)

    # æ„å»ºé“¾
    def format_docs(docs):
        return "\n".join(doc.page_content for doc in docs)

    chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | ChatOpenAI(model="gpt-4o-mini")
        | StrOutputParser()
    )

    return chain


async def debug_rag_chain():
    """è°ƒè¯•RAGé“¾"""
    print("=== è°ƒè¯•RAGé“¾ ===\n")

    chain = build_rag_chain()

    async for event in chain.astream_events(
        "ä»€ä¹ˆæ˜¯LCEL?",
        version="v2"
    ):
        kind = event["event"]
        name = event["name"]

        # æŸ¥çœ‹æ£€ç´¢å¼€å§‹
        if kind == "on_retriever_start":
            query = event["data"]["input"]
            print(f"ğŸ” æ£€ç´¢æŸ¥è¯¢: {query}\n")

        # æŸ¥çœ‹æ£€ç´¢ç»“æœ
        elif kind == "on_retriever_end":
            docs = event["data"]["output"]
            print(f"ğŸ“„ æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£:")
            for i, doc in enumerate(docs, 1):
                print(f"  {i}. {doc.page_content}")
            print()

        # æŸ¥çœ‹å‘é€ç»™LLMçš„prompt
        elif kind == "on_chat_model_start":
            messages = event["data"]["input"]["messages"]
            print(f"ğŸ’¬ å‘é€ç»™LLMçš„Prompt:")
            print(f"{messages[0].content[:200]}...")
            print()

        # æŸ¥çœ‹LLMè¾“å‡º
        elif kind == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            print(chunk.content, end="", flush=True)

        # æŸ¥çœ‹Tokenä½¿ç”¨
        elif kind == "on_chat_model_end":
            output = event["data"]["output"]
            tokens = output.response_metadata.get("token_usage", {})
            print(f"\n\nğŸ“Š Tokenä½¿ç”¨: {tokens}")


if __name__ == "__main__":
    asyncio.run(debug_rag_chain())
```

### è¾“å‡ºç¤ºä¾‹

```
=== è°ƒè¯•RAGé“¾ ===

ğŸ” æ£€ç´¢æŸ¥è¯¢: ä»€ä¹ˆæ˜¯LCEL?

ğŸ“„ æ£€ç´¢åˆ° 2 ä¸ªæ–‡æ¡£:
  1. LCELæ˜¯LangChain Expression Languageçš„ç¼©å†™
  2. LCELä½¿ç”¨ç®¡é“ç¬¦|è¿æ¥ç»„ä»¶

ğŸ’¬ å‘é€ç»™LLMçš„Prompt:
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:

ä¸Šä¸‹æ–‡:
LCELæ˜¯LangChain Expression Languageçš„ç¼©å†™
LCELä½¿ç”¨ç®¡é“ç¬¦|è¿æ¥ç»„ä»¶

é—®é¢˜: ä»€ä¹ˆæ˜¯LCEL?

å›ç­”:...

LCELæ˜¯LangChain Expression Languageçš„ç¼©å†™ï¼Œå®ƒæ˜¯ä¸€ç§ç”¨äºæ„å»ºAIåº”ç”¨çš„å£°æ˜å¼è¯­è¨€ã€‚LCELä½¿ç”¨ç®¡é“ç¬¦ï¼ˆ|ï¼‰æ¥è¿æ¥ä¸åŒçš„ç»„ä»¶ï¼Œä½¿å¾—æ„å»ºå¤æ‚çš„AIå·¥ä½œæµå˜å¾—ç®€å•ç›´è§‚ã€‚

ğŸ“Š Tokenä½¿ç”¨: {'prompt_tokens': 45, 'completion_tokens': 68, 'total_tokens': 113}
```

### å…³é”®ç‚¹

1. **æ£€ç´¢è¿½è¸ª**ï¼š`on_retriever_start/end` æŸ¥çœ‹æ£€ç´¢è¿‡ç¨‹
2. **æ–‡æ¡£æŸ¥çœ‹**ï¼šæ£€æŸ¥æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
3. **Promptæ£€æŸ¥**ï¼šç¡®è®¤å‘é€ç»™LLMçš„å®Œæ•´prompt
4. **Tokenç»Ÿè®¡**ï¼šäº†è§£èµ„æºæ¶ˆè€—

---

## åœºæ™¯4ï¼šä¸­é—´ç»“æœæ£€æŸ¥

### ç›®æ ‡

æ£€æŸ¥é“¾ä¸­æ¯ä¸ªæ­¥éª¤çš„ä¸­é—´ç»“æœï¼ŒéªŒè¯æ•°æ®è½¬æ¢æ˜¯å¦æ­£ç¡®ã€‚

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯4ï¼šä¸­é—´ç»“æœæ£€æŸ¥
æ£€æŸ¥æ¯ä¸ªæ­¥éª¤çš„è¾“å…¥è¾“å‡º
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv
import asyncio
import json

load_dotenv()


def preprocess(inputs):
    """é¢„å¤„ç†ï¼šè½¬æ¢è¾“å…¥æ ¼å¼"""
    question = inputs["question"]
    return {"question": f"è¯·è¯¦ç»†å›ç­”: {question}"}


def postprocess(output):
    """åå¤„ç†ï¼šæ ¼å¼åŒ–è¾“å‡º"""
    return f"å›ç­”: {output}"


# æ„å»ºåŒ…å«å¤šä¸ªè½¬æ¢æ­¥éª¤çš„é“¾
prompt = ChatPromptTemplate.from_template("é—®é¢˜: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = (
    RunnableLambda(preprocess)
    | prompt
    | model
    | parser
    | RunnableLambda(postprocess)
)


async def check_intermediate_results():
    """æ£€æŸ¥ä¸­é—´ç»“æœ"""
    print("=== æ£€æŸ¥ä¸­é—´ç»“æœ ===\n")

    step_outputs = {}

    async for event in chain.astream_events(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        version="v2"
    ):
        kind = event["event"]
        name = event["name"]

        # è®°å½•æ¯ä¸ªæ­¥éª¤çš„è¾“å‡º
        if kind.endswith("_end"):
            output = event["data"].get("output")
            if output is not None:
                step_outputs[name] = output

                # æ‰“å°æ­¥éª¤è¾“å‡º
                print(f"ğŸ“ æ­¥éª¤: {name}")
                if isinstance(output, str):
                    print(f"   è¾“å‡º: {output[:100]}...")
                elif hasattr(output, "to_messages"):
                    messages = output.to_messages()
                    print(f"   è¾“å‡º: {messages[0].content[:100]}...")
                elif hasattr(output, "content"):
                    print(f"   è¾“å‡º: {output.content[:100]}...")
                else:
                    print(f"   è¾“å‡º: {str(output)[:100]}...")
                print()

    # æ‰“å°å®Œæ•´çš„æ•°æ®æµ
    print("\n=== å®Œæ•´æ•°æ®æµ ===")
    for name, output in step_outputs.items():
        print(f"{name}:")
        if isinstance(output, str):
            print(f"  {output[:80]}...")
        else:
            print(f"  {str(output)[:80]}...")


async def trace_data_transformation():
    """è¿½è¸ªæ•°æ®è½¬æ¢"""
    print("\n=== è¿½è¸ªæ•°æ®è½¬æ¢ ===\n")

    transformations = []

    async for event in chain.astream_events(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        version="v2"
    ):
        kind = event["event"]
        name = event["name"]

        # è®°å½•è¾“å…¥è¾“å‡º
        if kind == "on_chain_start" or kind.endswith("_start"):
            input_data = event["data"].get("input")
            if input_data:
                transformations.append({
                    "step": name,
                    "type": "input",
                    "data": str(input_data)[:100]
                })

        elif kind.endswith("_end"):
            output_data = event["data"].get("output")
            if output_data:
                transformations.append({
                    "step": name,
                    "type": "output",
                    "data": str(output_data)[:100]
                })

    # æ‰“å°è½¬æ¢è¿‡ç¨‹
    current_step = None
    for t in transformations:
        if t["step"] != current_step:
            current_step = t["step"]
            print(f"\nğŸ”„ {current_step}")

        print(f"  {t['type']:6}: {t['data']}...")


if __name__ == "__main__":
    asyncio.run(check_intermediate_results())
    asyncio.run(trace_data_transformation())
```

### è¾“å‡ºç¤ºä¾‹

```
=== æ£€æŸ¥ä¸­é—´ç»“æœ ===

ğŸ“ æ­¥éª¤: preprocess
   è¾“å‡º: {'question': 'è¯·è¯¦ç»†å›ç­”: ä»€ä¹ˆæ˜¯LCEL?'}

ğŸ“ æ­¥éª¤: ChatPromptTemplate
   è¾“å‡º: é—®é¢˜: è¯·è¯¦ç»†å›ç­”: ä»€ä¹ˆæ˜¯LCEL?

ğŸ“ æ­¥éª¤: ChatOpenAI
   è¾“å‡º: LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾æ„å»ºè¯­è¨€...

ğŸ“ æ­¥éª¤: StrOutputParser
   è¾“å‡º: LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾æ„å»ºè¯­è¨€...

ğŸ“ æ­¥éª¤: postprocess
   è¾“å‡º: å›ç­”: LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾æ„å»ºè¯­è¨€...

=== å®Œæ•´æ•°æ®æµ ===
preprocess:
  {'question': 'è¯·è¯¦ç»†å›ç­”: ä»€ä¹ˆæ˜¯LCEL?'}
ChatPromptTemplate:
  messages=[HumanMessage(content='é—®é¢˜: è¯·è¯¦ç»†å›ç­”: ä»€ä¹ˆæ˜¯LCEL?')]
ChatOpenAI:
  content='LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾æ„å»ºè¯­è¨€...'
StrOutputParser:
  LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾æ„å»ºè¯­è¨€...
postprocess:
  å›ç­”: LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾æ„å»ºè¯­è¨€...

=== è¿½è¸ªæ•°æ®è½¬æ¢ ===

ğŸ”„ RunnableSequence
  input : {'question': 'ä»€ä¹ˆæ˜¯LCEL?'}...
  output: å›ç­”: LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾æ„å»ºè¯­è¨€...

ğŸ”„ preprocess
  input : {'question': 'ä»€ä¹ˆæ˜¯LCEL?'}...
  output: {'question': 'è¯·è¯¦ç»†å›ç­”: ä»€ä¹ˆæ˜¯LCEL?'}...

ğŸ”„ ChatPromptTemplate
  input : {'question': 'è¯·è¯¦ç»†å›ç­”: ä»€ä¹ˆæ˜¯LCEL?'}...
  output: messages=[HumanMessage(content='é—®é¢˜: è¯·è¯¦ç»†å›ç­”: ä»€ä¹ˆæ˜¯LCEL?')]...
```

### å…³é”®ç‚¹

1. **æ­¥éª¤è¿½è¸ª**ï¼šè®°å½•æ¯ä¸ªæ­¥éª¤çš„è¾“å‡º
2. **æ•°æ®éªŒè¯**ï¼šç¡®è®¤æ•°æ®è½¬æ¢æ˜¯å¦æ­£ç¡®
3. **è°ƒè¯•è½¬æ¢**ï¼šå¿«é€Ÿå®šä½æ•°æ®è½¬æ¢é—®é¢˜
4. **å®Œæ•´è§†å›¾**ï¼šæŸ¥çœ‹æ•´ä¸ªæ•°æ®æµ

---

## æ€§èƒ½åˆ†æç¤ºä¾‹

### å®Œæ•´ä»£ç 

```python
"""
æ€§èƒ½åˆ†æï¼šç»Ÿè®¡æ¯ä¸ªæ­¥éª¤çš„è€—æ—¶
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import asyncio
import time

load_dotenv()

prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model | StrOutputParser()


async def analyze_performance():
    """æ€§èƒ½åˆ†æ"""
    print("=== æ€§èƒ½åˆ†æ ===\n")

    start_times = {}
    durations = {}

    async for event in chain.astream_events(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        version="v2"
    ):
        kind = event["event"]
        name = event["name"]

        if kind.endswith("_start"):
            start_times[name] = time.time()

        elif kind.endswith("_end"):
            if name in start_times:
                duration = time.time() - start_times[name]
                durations[name] = duration

    # æ‰“å°æ€§èƒ½æŠ¥å‘Š
    print("ğŸ“Š æ€§èƒ½æŠ¥å‘Š:\n")
    total_duration = sum(durations.values())

    for name, duration in sorted(durations.items(), key=lambda x: x[1], reverse=True):
        percentage = (duration / total_duration) * 100
        print(f"{name:25} {duration:6.3f}s ({percentage:5.1f}%)")

    print(f"\n{'æ€»è®¡':25} {total_duration:6.3f}s (100.0%)")


if __name__ == "__main__":
    asyncio.run(analyze_performance())
```

### è¾“å‡ºç¤ºä¾‹

```
=== æ€§èƒ½åˆ†æ ===

ğŸ“Š æ€§èƒ½æŠ¥å‘Š:

ChatOpenAI                1.234s ( 99.5%)
ChatPromptTemplate        0.003s (  0.2%)
StrOutputParser           0.002s (  0.2%)
RunnableSequence          0.001s (  0.1%)

æ€»è®¡                      1.240s (100.0%)
```

---

## æ€»ç»“

**æ ¸å¿ƒæŠ€å·§**ï¼š

1. **æŸ¥çœ‹æ‰€æœ‰äº‹ä»¶**ï¼šç†è§£é“¾çš„æ‰§è¡Œæµç¨‹
2. **è¿‡æ»¤äº‹ä»¶**ï¼šèšç„¦å…³é”®ä¿¡æ¯
3. **æ£€æŸ¥ä¸­é—´ç»“æœ**ï¼šéªŒè¯æ•°æ®è½¬æ¢
4. **æ€§èƒ½åˆ†æ**ï¼šæ‰¾åˆ°ç“¶é¢ˆ

**æœ€ä½³å®è·µ**ï¼š

- å¼€å‘æ—¶ä½¿ç”¨è¯¦ç»†è°ƒè¯•
- ä½¿ç”¨è¿‡æ»¤å‡å°‘å™ªéŸ³
- æ£€æŸ¥å…³é”®æ­¥éª¤çš„è¾“å…¥è¾“å‡º
- ç»Ÿè®¡æ€§èƒ½æ‰¾åˆ°ä¼˜åŒ–ç‚¹

---

**ç‰ˆæœ¬ä¿¡æ¯**
- LangChain: v0.3+ (2025-2026)
- Python: 3.13+
- æœ€åæ›´æ–°: 2026-02-20
