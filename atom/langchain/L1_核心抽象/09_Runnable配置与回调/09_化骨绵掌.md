# åŒ–éª¨ç»µæŒ

> **10 å¼ é€ŸæŸ¥å¡ç‰‡ï¼Œéšæ—¶æŸ¥é˜…å…³é”®çŸ¥è¯†**

---

## å¡ç‰‡ 1ï¼šRunnableConfig æ ¸å¿ƒå­—æ®µ

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    # ç›‘æ§å’Œæ—¥å¿—
    callbacks=[CostTracker(), Logger()],

    # è¿‡æ»¤å’Œåˆ†ç±»
    tags=["production", "critical"],

    # è¿½è¸ªå’Œå½’å› 
    metadata={"user_id": "123", "session": "abc"},

    # å¯è¯»æ ‡è¯†
    run_name="user_query_123",

    # å¹¶å‘æ§åˆ¶
    max_concurrency=10,

    # é€’å½’é™åˆ¶ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
    recursion_limit=25,

    # è¿è¡Œæ—¶å¯é…ç½®å­—æ®µ
    configurable={"temp": 0.7, "model": "gpt-4"}
)
```

**è®°å¿†å£è¯€**ï¼š
- callbacks = ç›‘æ§
- tags = è¿‡æ»¤
- metadata = è¿½è¸ª
- configurable = åŠ¨æ€è°ƒæ•´

---

## å¡ç‰‡ 2ï¼šBaseCallbackHandler æ ¸å¿ƒæ–¹æ³•

```python
from langchain_core.callbacks import BaseCallbackHandler

class MyCallback(BaseCallbackHandler):
    # LLM å›è°ƒ
    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLM å¼€å§‹è°ƒç”¨"""
        pass

    def on_llm_end(self, response, **kwargs):
        """LLM è°ƒç”¨ç»“æŸï¼ˆæœ€å¸¸ç”¨ï¼‰"""
        pass

    def on_llm_error(self, error, **kwargs):
        """LLM è°ƒç”¨å¤±è´¥"""
        pass

    # Chain å›è°ƒ
    def on_chain_start(self, serialized, inputs, **kwargs):
        """Chain å¼€å§‹æ‰§è¡Œ"""
        pass

    def on_chain_end(self, outputs, **kwargs):
        """Chain æ‰§è¡Œç»“æŸ"""
        pass

    def on_chain_error(self, error, **kwargs):
        """Chain æ‰§è¡Œå¤±è´¥"""
        pass

    # æµå¼è¾“å‡º
    def on_llm_new_token(self, token: str, **kwargs):
        """æ¯ä¸ª token ç”Ÿæˆæ—¶è§¦å‘"""
        pass

    # Retriever å›è°ƒ
    def on_retriever_end(self, documents, **kwargs):
        """æ£€ç´¢å®Œæˆ"""
        pass

    # Tool å›è°ƒ
    def on_tool_start(self, tool, input_str, **kwargs):
        """å·¥å…·å¼€å§‹è°ƒç”¨"""
        pass

    # Agent å›è°ƒ
    def on_agent_action(self, action, **kwargs):
        """Agent æ‰§è¡ŒåŠ¨ä½œ"""
        pass
```

**80% åœºæ™¯åªéœ€è¦**ï¼š
- `on_llm_start`
- `on_llm_end`
- `on_llm_error`

---

## å¡ç‰‡ 3ï¼šé…ç½®ä¼ é€’çš„ 3 ç§æ–¹å¼

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

# æ–¹å¼ 1ï¼šæ„é€ æ—¶é…ç½®ï¼ˆé™æ€ï¼‰
llm_static = ChatOpenAI(
    callbacks=[logger],
    tags=["default"]
)

# æ–¹å¼ 2ï¼šwith_configï¼ˆé“¾çº§é…ç½®ï¼‰
llm_with_config = llm.with_config(
    tags=["production"],
    metadata={"env": "prod"}
)

# æ–¹å¼ 3ï¼šinvoke æ—¶é…ç½®ï¼ˆåŠ¨æ€ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
llm.invoke(
    "ä½ å¥½",
    config={
        "callbacks": [tracker],
        "tags": ["critical"],
        "metadata": {"user_id": "123"}
    }
)
```

**ä¼˜å…ˆçº§**ï¼šinvoke æ—¶ > with_config > æ„é€ æ—¶

---

## å¡ç‰‡ 4ï¼šæˆæœ¬è¿½è¸ªæ¨¡æ¿

```python
from langchain_core.callbacks import BaseCallbackHandler

class CostTracker(BaseCallbackHandler):
    def __init__(self):
        self.total_cost = 0
        self.total_tokens = 0

    def on_llm_end(self, response, **kwargs):
        # è·å– token ä½¿ç”¨é‡
        usage = response.llm_output.get("token_usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        # è®¡ç®—æˆæœ¬ï¼ˆGPT-4 ä»·æ ¼ï¼‰
        cost = (
            prompt_tokens * 0.00003 +
            completion_tokens * 0.00006
        )

        self.total_cost += cost
        self.total_tokens += usage.get("total_tokens", 0)

        # è®°å½•åˆ°æ—¥å¿—/æ•°æ®åº“
        logger.info(f"æˆæœ¬: ${cost:.4f}, Tokens: {usage.get('total_tokens', 0)}")

# ä½¿ç”¨
tracker = CostTracker()
llm.invoke("ä½ å¥½", config={"callbacks": [tracker]})
print(f"æ€»æˆæœ¬: ${tracker.total_cost:.4f}")
```

**å…³é”®ç‚¹**ï¼š
- ä» `response.llm_output["token_usage"]` è·å– token æ•°
- ä¸åŒæ¨¡å‹ä»·æ ¼ä¸åŒï¼Œéœ€è¦åŠ¨æ€è®¡ç®—
- å¯ä»¥æŒ‰ç”¨æˆ·å½’å› ï¼ˆä» metadata è·å– user_idï¼‰

---

## å¡ç‰‡ 5ï¼šç»“æ„åŒ–æ—¥å¿—æ¨¡æ¿

```python
import json
import time
from langchain_core.callbacks import BaseCallbackHandler

class StructuredLogger(BaseCallbackHandler):
    def __init__(self):
        self.start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()
        metadata = kwargs.get("metadata", {})

        log = {
            "event": "llm_start",
            "timestamp": time.time(),
            "prompts": prompts,
            "metadata": metadata
        }
        print(json.dumps(log, ensure_ascii=False))

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time

        log = {
            "event": "llm_end",
            "timestamp": time.time(),
            "duration_ms": int(duration * 1000),
            "output": response.generations[0][0].text[:100],
            "tokens": response.llm_output.get("token_usage", {})
        }
        print(json.dumps(log, ensure_ascii=False))
```

**è¾“å‡ºæ ¼å¼**ï¼šJSON Linesï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰

---

## å¡ç‰‡ 6ï¼šLangSmith å¿«é€Ÿå¯ç”¨

```bash
# 1. è®¾ç½®ç¯å¢ƒå˜é‡
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=your_api_key
export LANGCHAIN_PROJECT=my_project

# å¯é€‰ï¼šè‡ªå®šä¹‰ç«¯ç‚¹
export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
```

```python
# 2. ä»£ç æ— éœ€æ”¹åŠ¨ï¼Œè‡ªåŠ¨è¿½è¸ª
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
llm.invoke("ä½ å¥½")  # â† è‡ªåŠ¨å‘é€åˆ° LangSmith

# 3. è®¿é—® https://smith.langchain.com æŸ¥çœ‹è¿½è¸ª
```

**è‡ªåŠ¨è®°å½•**ï¼š
- è¾“å…¥è¾“å‡º
- Token æ¶ˆè€—
- å»¶è¿Ÿ
- æˆæœ¬
- é”™è¯¯å †æ ˆ

**é›¶ä»£ç ä¾µå…¥**ï¼

---

## å¡ç‰‡ 7ï¼šå¯é…ç½®å­—æ®µæ¨¡æ¿

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField

# 1. å£°æ˜å¯é…ç½®å­—æ®µ
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
).configurable_fields(
    model=ConfigurableField(
        id="model_name",
        name="Model Name",
        description="è¦ä½¿ç”¨çš„æ¨¡å‹"
    ),
    temperature=ConfigurableField(
        id="temp",
        name="Temperature",
        description="æ§åˆ¶éšæœºæ€§"
    )
)

# 2. è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´
# ä½¿ç”¨ GPT-3.5ï¼ˆä¾¿å®œï¼‰
llm.invoke("ä½ å¥½", config={
    "configurable": {
        "model_name": "gpt-3.5-turbo",
        "temp": 0.7
    }
})

# ä½¿ç”¨ GPT-4ï¼ˆè´¨é‡é«˜ï¼‰
llm.invoke("ä½ å¥½", config={
    "configurable": {
        "model_name": "gpt-4",
        "temp": 0.3
    }
})
```

**åº”ç”¨åœºæ™¯**ï¼š
- A/B æµ‹è¯•
- æˆæœ¬ä¼˜åŒ–
- é™çº§ç­–ç•¥

---

## å¡ç‰‡ 8ï¼šé”™è¯¯å‘Šè­¦æ¨¡æ¿

```python
from langchain_core.callbacks import BaseCallbackHandler
import requests

class ErrorAlerter(BaseCallbackHandler):
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url

    def on_llm_error(self, error, **kwargs):
        metadata = kwargs.get("metadata", {})

        # æ„é€ å‘Šè­¦æ¶ˆæ¯
        alert = {
            "text": "ğŸš¨ LLM è°ƒç”¨å¤±è´¥",
            "error": str(error),
            "user_id": metadata.get("user_id"),
            "env": metadata.get("env"),
            "timestamp": time.time()
        }

        # å‘é€åˆ° Slack/é’‰é’‰/ä¼ä¸šå¾®ä¿¡
        try:
            requests.post(
                self.webhook_url,
                json=alert,
                timeout=5
            )
        except Exception as e:
            logger.error(f"å‘Šè­¦å‘é€å¤±è´¥: {e}")

    def on_chain_error(self, error, **kwargs):
        # åŒæ ·çš„é€»è¾‘
        self.on_llm_error(error, **kwargs)

# ä½¿ç”¨
alerter = ErrorAlerter(webhook_url="https://hooks.slack.com/...")
llm.invoke("ä½ å¥½", config={"callbacks": [alerter]})
```

**æ”¯æŒçš„å‘Šè­¦æ¸ é“**ï¼š
- Slack
- é’‰é’‰
- ä¼ä¸šå¾®ä¿¡
- PagerDuty
- Email

---

## å¡ç‰‡ 9ï¼šæŒ‰ç”¨æˆ·å½’å› æˆæœ¬

```python
from collections import defaultdict
from langchain_core.callbacks import BaseCallbackHandler

class UserCostTracker(BaseCallbackHandler):
    def __init__(self):
        self.user_costs = defaultdict(float)
        self.user_tokens = defaultdict(int)

    def on_llm_end(self, response, **kwargs):
        # ä» metadata è·å– user_id
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        # è®¡ç®—æˆæœ¬
        usage = response.llm_output.get("token_usage", {})
        tokens = usage.get("total_tokens", 0)
        cost = tokens * 0.00002  # GPT-3.5 ä»·æ ¼

        # å½’å› åˆ°ç”¨æˆ·
        self.user_costs[user_id] += cost
        self.user_tokens[user_id] += tokens

    def get_report(self):
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        for user_id in sorted(self.user_costs.keys()):
            print(f"ç”¨æˆ· {user_id}:")
            print(f"  æˆæœ¬: ${self.user_costs[user_id]:.4f}")
            print(f"  Tokens: {self.user_tokens[user_id]}")

# ä½¿ç”¨
tracker = UserCostTracker()

# å¤šä¸ªç”¨æˆ·çš„è°ƒç”¨
for user_id in ["user_001", "user_002", "user_001"]:
    llm.invoke(
        "ä½ å¥½",
        config={
            "callbacks": [tracker],
            "metadata": {"user_id": user_id}
        }
    )

tracker.get_report()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ç”¨æˆ· user_001:
  æˆæœ¬: $0.0008
  Tokens: 400
ç”¨æˆ· user_002:
  æˆæœ¬: $0.0004
  Tokens: 200
```

---

## å¡ç‰‡ 10ï¼šRAG æ£€ç´¢è´¨é‡ç›‘æ§

```python
from langchain_core.callbacks import BaseCallbackHandler

class RetrievalMonitor(BaseCallbackHandler):
    def on_retriever_start(self, serialized, query, **kwargs):
        """æ£€ç´¢å¼€å§‹"""
        self.start_time = time.time()
        logger.info(f"æ£€ç´¢æŸ¥è¯¢: {query}")

    def on_retriever_end(self, documents, **kwargs):
        """æ£€ç´¢ç»“æŸ"""
        duration = time.time() - self.start_time

        # ç»Ÿè®¡æ£€ç´¢è´¨é‡
        num_docs = len(documents)
        scores = [d.metadata.get("score", 0) for d in documents]
        avg_score = sum(scores) / num_docs if num_docs > 0 else 0

        # è®°å½•æŒ‡æ ‡
        metrics.record("retrieval.duration", duration)
        metrics.record("retrieval.num_docs", num_docs)
        metrics.record("retrieval.avg_score", avg_score)

        # å‘Šè­¦ï¼šæ£€ç´¢è´¨é‡ä½
        if avg_score < 0.5:
            alert("æ£€ç´¢è´¨é‡ä½", avg_score=avg_score)

        logger.info(
            f"æ£€ç´¢å®Œæˆ: {num_docs} æ–‡æ¡£, "
            f"å¹³å‡åˆ†æ•°: {avg_score:.2f}, "
            f"è€—æ—¶: {duration:.2f}s"
        )

    def on_retriever_error(self, error, **kwargs):
        """æ£€ç´¢å¤±è´¥"""
        logger.error(f"æ£€ç´¢å¤±è´¥: {error}")
        alert("æ£€ç´¢å¤±è´¥", error=str(error))

# ä½¿ç”¨
from langchain.chains import RetrievalQA

monitor = RetrievalMonitor()
rag_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=vectorstore.as_retriever()
)

rag_chain.invoke(
    {"query": "ä»€ä¹ˆæ˜¯é‡å­çº ç¼ ï¼Ÿ"},
    config={"callbacks": [monitor]}
)
```

**ç›‘æ§æŒ‡æ ‡**ï¼š
- æ£€ç´¢å»¶è¿Ÿ
- æ–‡æ¡£æ•°é‡
- ç›¸ä¼¼åº¦åˆ†æ•°
- æ£€ç´¢å¤±è´¥ç‡

---

## å¿«é€Ÿå‚è€ƒè¡¨

### RunnableConfig å­—æ®µé€ŸæŸ¥

| å­—æ®µ | ç±»å‹ | ç”¨é€” |
|------|------|------|
| `callbacks` | `List[BaseCallbackHandler]` | ç›‘æ§ã€æ—¥å¿— |
| `tags` | `List[str]` | è¿‡æ»¤ã€åˆ†ç±» |
| `metadata` | `Dict[str, Any]` | è¿½è¸ªã€å½’å›  |
| `run_name` | `str` | å¯è¯»æ ‡è¯† |
| `max_concurrency` | `int` | å¹¶å‘æ§åˆ¶ |
| `recursion_limit` | `int` | é€’å½’é™åˆ¶ |
| `configurable` | `Dict[str, Any]` | åŠ¨æ€é…ç½® |

### BaseCallbackHandler æ–¹æ³•é€ŸæŸ¥

| æ–¹æ³• | è§¦å‘æ—¶æœº | å¸¸ç”¨åœºæ™¯ |
|------|----------|----------|
| `on_llm_start` | LLM å¼€å§‹ | è®°å½•è¾“å…¥ |
| `on_llm_end` | LLM ç»“æŸ | æˆæœ¬è¿½è¸ª |
| `on_llm_error` | LLM å¤±è´¥ | é”™è¯¯å‘Šè­¦ |
| `on_llm_new_token` | Token ç”Ÿæˆ | æµå¼ç›‘æ§ |
| `on_chain_start` | Chain å¼€å§‹ | é“¾è·¯è¿½è¸ª |
| `on_chain_end` | Chain ç»“æŸ | æ€§èƒ½ç›‘æ§ |
| `on_retriever_end` | æ£€ç´¢å®Œæˆ | æ£€ç´¢è´¨é‡ |
| `on_tool_start` | å·¥å…·è°ƒç”¨ | å·¥å…·ç›‘æ§ |

### å¸¸è§æ¨¡å‹ä»·æ ¼ï¼ˆ2025ï¼‰

| æ¨¡å‹ | Input ($/1K tokens) | Output ($/1K tokens) |
|------|---------------------|----------------------|
| GPT-4 | $0.03 | $0.06 |
| GPT-3.5 Turbo | $0.0005 | $0.0015 |
| Claude 3 Opus | $0.015 | $0.075 |
| Claude 3 Sonnet | $0.003 | $0.015 |

### é…ç½®ä¼˜å…ˆçº§

```
invoke æ—¶é…ç½® > with_config > æ„é€ æ—¶é…ç½®
```

### Metadata å¯åºåˆ—åŒ–ç±»å‹

âœ… å…è®¸ï¼š`str`, `int`, `float`, `bool`, `None`, `list`, `dict`
âŒ ç¦æ­¢ï¼šå¯¹è±¡å®ä¾‹ã€å‡½æ•°ã€datetimeã€è‡ªå®šä¹‰ç±»

### å¸¸è§é”™è¯¯é€ŸæŸ¥

| é”™è¯¯ | åŸå›  | è§£å†³ |
|------|------|------|
| `KeyError: 'metadata'` | ç›´æ¥è®¿é—® kwargs | ä½¿ç”¨ `kwargs.get("metadata", {})` |
| `TypeError: not JSON serializable` | metadata åŒ…å«å¯¹è±¡ | åªä¼ å…¥åŸºæœ¬ç±»å‹ |
| å›è°ƒä¸æ‰§è¡Œ | tags è¿‡æ»¤ | æ£€æŸ¥ tags åŒ¹é… |
| æˆæœ¬ç»Ÿè®¡ä¸å‡† | æœªå¤„ç†æµå¼è¾“å‡º | ä½¿ç”¨ `on_llm_new_token` |

---

## ä¸€å¥è¯æ€»ç»“

**è¿™ 10 å¼ å¡ç‰‡æ¶µç›–äº† RunnableConfig å’Œå›è°ƒç³»ç»Ÿ 90% çš„ä½¿ç”¨åœºæ™¯ï¼Œéšæ—¶æŸ¥é˜…å³å¯å¿«é€Ÿè§£å†³é—®é¢˜ã€‚**

---

## å‚è€ƒèµ„æ–™

- [RunnableConfig API Reference](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.config.RunnableConfig.html)
- [BaseCallbackHandler API Reference](https://reference.langchain.com/v0.3/python/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html)
- [LangChain Custom Callbacks Guide](https://python.langchain.com/docs/how_to/custom_callbacks/)
- [LangSmith Observability Platform](https://www.langchain.com/langsmith/observability)
