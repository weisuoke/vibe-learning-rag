# 核心概念6：实体关系提取

> 从文本中自动提取实体和关系 - 构建知识图谱的关键技术

---

## 一句话定义

**实体关系提取是从非结构化文本中识别实体并提取它们之间关系的技术，是自动构建知识图谱和GraphRAG系统的核心步骤。**

---

## 核心原理

### 两个核心任务

```python
# 输入：非结构化文本
text = "张三在阿里巴巴工作，阿里巴巴位于杭州。马云创立了阿里巴巴。"

# 任务1：实体识别（NER）
entities = ["张三", "阿里巴巴", "杭州", "马云"]

# 任务2：关系提取（RE）
relations = [
    ("张三", "工作于", "阿里巴巴"),
    ("阿里巴巴", "位于", "杭州"),
    ("马云", "创立", "阿里巴巴")
]
```

**核心流程：**
1. 文本预处理
2. 实体识别（NER）
3. 关系提取（RE）
4. 三元组生成

---

## 方法1：基于LLM的提取（推荐）⭐⭐⭐⭐⭐

### 使用OpenAI API

```python
from openai import OpenAI
import os
import json
from typing import List, Tuple

class LLMEntityRelationExtractor:
    """基于LLM的实体关系提取器"""

    def __init__(self, api_key=None):
        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))

    def extract_triples(self, text: str) -> List[Tuple[str, str, str]]:
        """提取三元组"""

        prompt = f"""
从以下文本中提取知识三元组。

文本：
{text}

要求：
1. 识别所有实体（人名、组织、地点等）
2. 提取实体之间的关系
3. 返回JSON格式的三元组列表

输出格式：
{{
  "triples": [
    {{"subject": "实体1", "predicate": "关系", "object": "实体2"}},
    ...
  ]
}}

只返回JSON，不要其他解释。
"""

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "你是一个知识图谱构建专家。"},
                {"role": "user", "content": prompt}
            ],
            temperature=0
        )

        # 解析响应
        content = response.choices[0].message.content
        try:
            data = json.loads(content)
            triples = [
                (t["subject"], t["predicate"], t["object"])
                for t in data["triples"]
            ]
            return triples
        except json.JSONDecodeError:
            print(f"解析失败: {content}")
            return []

# 使用示例
extractor = LLMEntityRelationExtractor()
text = "张三在阿里巴巴工作，阿里巴巴位于杭州。马云创立了阿里巴巴。"
triples = extractor.extract_triples(text)

for s, p, o in triples:
    print(f"({s}, {p}, {o})")

# 输出：
# (张三, 工作于, 阿里巴巴)
# (阿里巴巴, 位于, 杭州)
# (马云, 创立, 阿里巴巴)
```

### Few-shot提示优化

```python
def extract_with_examples(self, text: str) -> List[Tuple[str, str, str]]:
    """使用Few-shot提示提取"""

    prompt = f"""
从文本中提取知识三元组。

示例1：
文本：李四是北京大学的教授，北京大学位于北京。
三元组：
- (李四, 职业, 教授)
- (李四, 工作于, 北京大学)
- (北京大学, 位于, 北京)

示例2：
文本：苹果公司由史蒂夫·乔布斯创立，总部在加利福尼亚。
三元组：
- (苹果公司, 创始人, 史蒂夫·乔布斯)
- (苹果公司, 总部位于, 加利福尼亚)

现在提取以下文本的三元组：
文本：{text}
三元组：
"""

    response = self.client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    # 解析响应
    content = response.choices[0].message.content
    return self._parse_triples(content)

def _parse_triples(self, text: str) -> List[Tuple[str, str, str]]:
    """解析三元组文本"""
    import re
    triples = []
    pattern = r'\(([^,]+),\s*([^,]+),\s*([^)]+)\)'
    matches = re.findall(pattern, text)
    for match in matches:
        s, p, o = match
        triples.append((s.strip(), p.strip(), o.strip()))
    return triples
```

---

## 方法2：基于规则的提取

### 使用spaCy

```python
import spacy
from typing import List, Tuple

class RuleBasedExtractor:
    """基于规则的实体关系提取"""

    def __init__(self):
        # 加载中文模型
        self.nlp = spacy.load("zh_core_web_sm")

    def extract_entities(self, text: str) -> List[str]:
        """提取实体（NER）"""
        doc = self.nlp(text)
        entities = []

        for ent in doc.ents:
            entities.append({
                "text": ent.text,
                "label": ent.label_,
                "start": ent.start_char,
                "end": ent.end_char
            })

        return entities

    def extract_relations(self, text: str) -> List[Tuple[str, str, str]]:
        """提取关系（基于依存句法）"""
        doc = self.nlp(text)
        triples = []

        for sent in doc.sents:
            # 查找动词
            for token in sent:
                if token.pos_ == "VERB":
                    # 查找主语
                    subject = None
                    for child in token.children:
                        if child.dep_ == "nsubj":
                            subject = child.text
                            break

                    # 查找宾语
                    object = None
                    for child in token.children:
                        if child.dep_ in ["dobj", "pobj"]:
                            object = child.text
                            break

                    if subject and object:
                        triples.append((subject, token.text, object))

        return triples

# 使用
extractor = RuleBasedExtractor()
text = "张三在阿里巴巴工作。"
entities = extractor.extract_entities(text)
relations = extractor.extract_relations(text)

print("实体:", entities)
print("关系:", relations)
```

---

## 方法3：混合方法（推荐生产环境）⭐⭐⭐⭐⭐

```python
class HybridExtractor:
    """混合提取器：规则 + LLM"""

    def __init__(self):
        self.rule_extractor = RuleBasedExtractor()
        self.llm_extractor = LLMEntityRelationExtractor()

    def extract(self, text: str) -> List[Tuple[str, str, str]]:
        """混合提取策略"""

        # 1. 先用规则提取（快速、便宜）
        rule_triples = self.rule_extractor.extract_relations(text)

        # 2. 如果规则提取结果不足，使用LLM
        if len(rule_triples) < 2:
            llm_triples = self.llm_extractor.extract_triples(text)
            return llm_triples

        # 3. 使用LLM验证和补充规则提取的结果
        combined = self._verify_and_enhance(text, rule_triples)

        return combined

    def _verify_and_enhance(self, text, rule_triples):
        """LLM验证和增强规则提取结果"""
        prompt = f"""
文本：{text}

规则提取的三元组：
{rule_triples}

请：
1. 验证这些三元组是否正确
2. 补充遗漏的三元组
3. 返回完整的三元组列表
"""
        # 调用LLM
        return self.llm_extractor.extract_triples(prompt)
```

---

## 在AI Agent中的应用

### 应用1：自动构建知识图谱⭐⭐⭐⭐⭐

```python
class AutoKGBuilder:
    """自动知识图谱构建器"""

    def __init__(self, extractor):
        self.extractor = extractor
        self.kg = TripleStore()

    def build_from_documents(self, documents: List[str]):
        """从文档批量构建知识图谱"""

        for doc in documents:
            print(f"处理文档: {doc[:50]}...")

            # 提取三元组
            triples = self.extractor.extract_triples(doc)

            # 添加到知识图谱
            for s, p, o in triples:
                self.kg.add(s, p, o)

            print(f"  提取 {len(triples)} 个三元组")

        print(f"\n总计: {self.kg.count()} 个三元组")
        return self.kg

# 使用
documents = [
    "张三在阿里巴巴工作，阿里巴巴位于杭州。",
    "马云创立了阿里巴巴，他出生于杭州。",
    "阿里巴巴是一家电商公司。"
]

builder = AutoKGBuilder(LLMEntityRelationExtractor())
kg = builder.build_from_documents(documents)

# 查询
print(kg.query_spo("张三"))
print(kg.query_spo("阿里巴巴"))
```

### 应用2：增量更新知识图谱

```python
class IncrementalKGUpdater:
    """增量更新知识图谱"""

    def __init__(self, kg, extractor):
        self.kg = kg
        self.extractor = extractor

    def update_from_text(self, text: str):
        """从新文本更新知识图谱"""

        # 提取新三元组
        new_triples = self.extractor.extract_triples(text)

        # 检查冲突
        for s, p, o in new_triples:
            existing = self.kg.query_spo(s, p)

            if existing and o not in existing:
                # 发现冲突，需要解决
                self._resolve_conflict(s, p, existing, o)
            else:
                # 无冲突，直接添加
                self.kg.add(s, p, o)

    def _resolve_conflict(self, s, p, old_values, new_value):
        """解决冲突"""
        print(f"冲突: ({s}, {p}, {old_values}) vs {new_value}")
        # 策略1：保留最新
        # 策略2：保留所有（多值）
        # 策略3：询问用户
        self.kg.add(s, p, new_value)  # 多值存储
```

### 应用3：实体消歧

```python
class EntityDisambiguator:
    """实体消歧器"""

    def __init__(self, llm):
        self.llm = llm
        self.entity_aliases = {}  # 实体别名映射

    def disambiguate(self, entity: str, context: str) -> str:
        """消歧实体"""

        # 检查是否已知别名
        if entity in self.entity_aliases:
            return self.entity_aliases[entity]

        # 使用LLM消歧
        prompt = f"""
实体：{entity}
上下文：{context}

这个实体指的是哪个标准实体？
如果是新实体，返回原实体名。
如果是已知实体的别名，返回标准名称。

标准实体名：
"""

        response = self.llm.generate(prompt)
        canonical = response.strip()

        # 缓存结果
        self.entity_aliases[entity] = canonical

        return canonical

# 使用
disambiguator = EntityDisambiguator(llm)

# "阿里" 和 "阿里巴巴" 是同一个实体
entity1 = disambiguator.disambiguate("阿里", "张三在阿里工作")
entity2 = disambiguator.disambiguate("阿里巴巴", "阿里巴巴位于杭州")

print(entity1 == entity2)  # True
```

---

## 实战：完整的提取管道

```python
class ExtractionPipeline:
    """完整的实体关系提取管道"""

    def __init__(self):
        self.extractor = LLMEntityRelationExtractor()
        self.disambiguator = EntityDisambiguator(self.extractor.client)

    def process(self, text: str) -> List[Tuple[str, str, str]]:
        """处理文本，返回标准化的三元组"""

        # 1. 文本预处理
        text = self._preprocess(text)

        # 2. 提取三元组
        raw_triples = self.extractor.extract_triples(text)

        # 3. 实体消歧
        disambiguated_triples = []
        for s, p, o in raw_triples:
            s_canonical = self.disambiguator.disambiguate(s, text)
            o_canonical = self.disambiguator.disambiguate(o, text)
            disambiguated_triples.append((s_canonical, p, o_canonical))

        # 4. 去重
        unique_triples = list(set(disambiguated_triples))

        # 5. 验证
        validated_triples = self._validate(unique_triples)

        return validated_triples

    def _preprocess(self, text: str) -> str:
        """文本预处理"""
        # 去除多余空格
        text = " ".join(text.split())
        # 统一标点
        text = text.replace("，", ",").replace("。", ".")
        return text

    def _validate(self, triples):
        """验证三元组"""
        valid = []
        for s, p, o in triples:
            # 检查实体和关系是否为空
            if s and p and o:
                # 检查实体长度
                if len(s) > 1 and len(o) > 1:
                    valid.append((s, p, o))
        return valid

# 使用
pipeline = ExtractionPipeline()
text = "张三在阿里工作。阿里巴巴位于杭州。"
triples = pipeline.process(text)

for s, p, o in triples:
    print(f"({s}, {p}, {o})")
```

---

## 性能优化

### 批量处理

```python
def batch_extract(texts: List[str], batch_size=10):
    """批量提取（减少API调用）"""
    extractor = LLMEntityRelationExtractor()
    all_triples = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        combined_text = "\n\n".join(batch)

        # 一次API调用处理多个文本
        triples = extractor.extract_triples(combined_text)
        all_triples.extend(triples)

    return all_triples
```

### 缓存结果

```python
import hashlib
import json

class CachedExtractor:
    """带缓存的提取器"""

    def __init__(self, extractor):
        self.extractor = extractor
        self.cache = {}

    def extract(self, text: str):
        """提取（带缓存）"""
        # 计算文本哈希
        text_hash = hashlib.md5(text.encode()).hexdigest()

        # 检查缓存
        if text_hash in self.cache:
            return self.cache[text_hash]

        # 提取
        triples = self.extractor.extract_triples(text)

        # 缓存结果
        self.cache[text_hash] = triples

        return triples
```

---

## 评估指标

```python
def evaluate_extraction(predicted, ground_truth):
    """评估提取质量"""

    pred_set = set(predicted)
    gt_set = set(ground_truth)

    # 精确率
    precision = len(pred_set & gt_set) / len(pred_set) if pred_set else 0

    # 召回率
    recall = len(pred_set & gt_set) / len(gt_set) if gt_set else 0

    # F1分数
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

# 使用
predicted = [("张三", "工作于", "阿里巴巴"), ("阿里巴巴", "位于", "杭州")]
ground_truth = [("张三", "工作于", "阿里巴巴"), ("马云", "创立", "阿里巴巴")]

metrics = evaluate_extraction(predicted, ground_truth)
print(f"Precision: {metrics['precision']:.2%}")
print(f"Recall: {metrics['recall']:.2%}")
print(f"F1: {metrics['f1']:.2%}")
```

---

## 总结

**实体关系提取核心方法：**

| 方法 | 优势 | 劣势 | 适用场景 |
|------|------|------|----------|
| **LLM提取** | 准确率高、灵活 | 成本高、速度慢 | 高质量要求 |
| **规则提取** | 快速、便宜 | 准确率低、不灵活 | 简单场景 |
| **混合方法** | 平衡质量和成本 | 实现复杂 | 生产环境 ⭐ |

**在AI Agent中的应用：**
- 自动构建知识图谱
- 增量更新知识库
- 实体消歧和标准化
- 文档到图的转换

**关键洞察：**
- LLM是当前最佳方案（2026年）
- Few-shot提示显著提升质量
- 实体消歧是关键步骤
- 缓存和批处理优化性能

---

**下一步：** 学习 `03_核心概念_07_GraphRAG架构.md`
