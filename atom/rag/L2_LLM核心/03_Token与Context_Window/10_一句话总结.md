# Token与Context Window - 一句话总结

**Token 是 LLM 处理文本的最小单位，Context Window 是一次能处理的 Token 上限；理解这两个概念是 RAG 开发的基础，因为它们直接决定了能注入多少检索内容、如何分配 Token 预算、以及 API 调用成本。**

---

## 核心要点回顾

| 概念 | 一句话 |
|------|--------|
| Token | LLM 的"文字积木"，介于字符和单词之间 |
| Context Window | LLM 的"工作台大小"，输入+输出的上限 |
| Token 预算 | 在有限空间中合理分配各部分 Token |

---

## 在 RAG 开发中的应用

```
检索内容 → Token 计算 → 预算分配 → 截断/选择 → 构建 Prompt
                ↑
        这就是 Token 管理的核心流程
```

---

## 学习检查清单

- [ ] 能用 tiktoken 计算任意文本的 Token 数量
- [ ] 知道主流模型的 Context Window 大小
- [ ] 理解 Token 预算分配的原则
- [ ] 能实现检索内容的截断逻辑
- [ ] 了解 Lost in the Middle 问题及应对策略

---

## 下一步学习

- → **Prompt Engineering 基础**：学会设计高质量的系统提示词
- → **文本分块 Chunking**：学会如何切分长文档以适应 Token 限制

---

**上一节：** [09_化骨绵掌.md](./09_化骨绵掌.md)
**返回目录：** [../k.md](../k.md)
