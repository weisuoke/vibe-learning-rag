# 核心概念11：分布式架构

## 一句话定义

**分布式架构通过将向量数据和计算负载分散到多个节点，实现水平扩展、高可用和容错能力，是支撑亿级向量检索的关键技术，2026年主流方案包括分片、副本、负载均衡等机制。**

---

## 详细原理讲解

### 1. 为什么需要分布式？

单机向量数据库面临三大瓶颈：

**瓶颈1：容量限制**
```
单机内存：64GB-256GB
1亿向量（768维）：~30GB
→ 单机最多支持3-8亿向量
```

**瓶颈2：性能限制**
```
单机QPS：5000-10000
大规模应用需求：50000+ QPS
→ 单机无法满足高并发
```

**瓶颈3：可靠性限制**
```
单点故障：服务不可用
数据丢失：无法恢复
→ 生产环境不可接受
```

**类比理解**：
```
单机 vs 分布式：
- 单机：个人电脑，性能有限
- 分布式：数据中心，无限扩展

图书馆 vs 图书馆系统：
- 单馆：容量有限，一旦关闭无法服务
- 连锁：多个分馆，任意扩展，高可用
```

---

### 2. 分布式架构核心概念

#### 2.1 分片（Sharding）

**定义**：将数据水平切分到多个节点

**分片策略**：

**策略1：哈希分片**
```python
# 根据ID哈希分配到不同分片
shard_id = hash(document_id) % num_shards

# 示例：10个分片
doc_1 → hash(1) % 10 = 1 → Shard 1
doc_2 → hash(2) % 10 = 2 → Shard 2
doc_10 → hash(10) % 10 = 0 → Shard 0
```

**优势**：
- 数据均匀分布
- 简单高效

**劣势**：
- 无法按业务逻辑分片
- 扩容需要重新分片

**策略2：范围分片**
```python
# 按ID范围分片
if 0 <= doc_id < 1000000:
    shard_id = 0
elif 1000000 <= doc_id < 2000000:
    shard_id = 1
# ...
```

**优势**：
- 支持范围查询
- 扩容简单（添加新范围）

**劣势**：
- 数据可能不均匀
- 热点问题

**策略3：一致性哈希**
```python
# 一致性哈希环
# 节点和数据都映射到环上
# 数据分配到顺时针最近的节点

class ConsistentHash:
    def __init__(self, nodes, virtual_nodes=150):
        self.ring = {}
        self.sorted_keys = []
        self.virtual_nodes = virtual_nodes

        for node in nodes:
            self.add_node(node)

    def add_node(self, node):
        """添加节点（包含虚拟节点）"""
        for i in range(self.virtual_nodes):
            key = self._hash(f"{node}:{i}")
            self.ring[key] = node
            self.sorted_keys.append(key)
        self.sorted_keys.sort()

    def get_node(self, data_key):
        """获取数据应该存储的节点"""
        if not self.ring:
            return None

        hash_key = self._hash(data_key)

        # 找到顺时针最近的节点
        for key in self.sorted_keys:
            if hash_key <= key:
                return self.ring[key]

        # 环形，返回第一个节点
        return self.ring[self.sorted_keys[0]]

    def _hash(self, key):
        """哈希函数"""
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
```

**优势**：
- 扩容时只影响相邻节点
- 数据迁移量小

**劣势**：
- 实现复杂
- 需要虚拟节点保证均匀

---

#### 2.2 副本（Replication）

**定义**：数据的多个拷贝，提高可用性和读性能

**副本策略**：

**策略1：主从复制（Master-Slave）**
```
架构：
┌─────────┐
│ Master  │ ← 写入
└────┬────┘
     │ 复制
     ├──────┬──────┐
     ↓      ↓      ↓
┌────────┐┌────────┐┌────────┐
│ Slave1 ││ Slave2 ││ Slave3 │ ← 读取
└────────┘└────────┘└────────┘
```

**特点**：
- 写入：只写Master
- 读取：从Slave读取
- 复制：异步或同步

**优势**：
- 读性能线性扩展
- 实现简单

**劣势**：
- 写入无法扩展
- Master单点故障

**策略2：多主复制（Multi-Master）**
```
架构：
┌─────────┐     ┌─────────┐
│ Master1 │ ←→  │ Master2 │
└─────────┘     └─────────┘
     ↕               ↕
┌─────────┐     ┌─────────┐
│ Master3 │ ←→  │ Master4 │
└─────────┘     └─────────┘
```

**特点**：
- 所有节点都可写入
- 节点间相互复制
- 冲突解决机制

**优势**：
- 写入性能扩展
- 无单点故障

**劣势**：
- 冲突处理复杂
- 一致性难保证

**策略3：Raft/Paxos共识**
```
架构：
┌─────────┐
│ Leader  │ ← 写入
└────┬────┘
     │ 复制
     ├──────┬──────┐
     ↓      ↓      ↓
┌────────┐┌────────┐┌────────┐
│Follower││Follower││Follower│
└────────┘└────────┘└────────┘

选举：
- Leader故障 → 自动选举新Leader
- 多数派确认 → 数据持久化
```

**优势**：
- 强一致性
- 自动故障转移

**劣势**：
- 性能开销大
- 实现复杂

---

#### 2.3 负载均衡

**定义**：将请求均匀分配到多个节点

**负载均衡策略**：

**策略1：轮询（Round Robin）**
```python
class RoundRobinBalancer:
    def __init__(self, nodes):
        self.nodes = nodes
        self.current = 0

    def get_node(self):
        node = self.nodes[self.current]
        self.current = (self.current + 1) % len(self.nodes)
        return node
```

**策略2：加权轮询（Weighted Round Robin）**
```python
class WeightedRoundRobinBalancer:
    def __init__(self, nodes_with_weights):
        # nodes_with_weights = [("node1", 3), ("node2", 2), ("node3", 1)]
        self.nodes = []
        for node, weight in nodes_with_weights:
            self.nodes.extend([node] * weight)
        self.current = 0

    def get_node(self):
        node = self.nodes[self.current]
        self.current = (self.current + 1) % len(self.nodes)
        return node
```

**策略3：最少连接（Least Connections）**
```python
class LeastConnectionsBalancer:
    def __init__(self, nodes):
        self.connections = {node: 0 for node in nodes}

    def get_node(self):
        # 选择连接数最少的节点
        return min(self.connections, key=self.connections.get)

    def increment(self, node):
        self.connections[node] += 1

    def decrement(self, node):
        self.connections[node] -= 1
```

**策略4：一致性哈希**
```python
# 根据请求ID哈希到固定节点
# 保证相同请求总是路由到同一节点
node = consistent_hash.get_node(request_id)
```

---

### 3. Milvus分布式架构

#### 3.1 架构设计

```
Milvus 2.x 分布式架构：

┌─────────────────────────────────────┐
│         Access Layer                │
│  ┌──────┐  ┌──────┐  ┌──────┐      │
│  │Proxy1│  │Proxy2│  │Proxy3│      │
│  └──────┘  └──────┘  └──────┘      │
│     ↓          ↓          ↓         │
├─────────────────────────────────────┤
│       Coordinator Service           │
│  ┌────────┐ ┌────────┐ ┌────────┐  │
│  │Root    │ │Query   │ │Data    │  │
│  │Coord   │ │Coord   │ │Coord   │  │
│  └────────┘ └────────┘ └────────┘  │
│     ↓          ↓          ↓         │
├─────────────────────────────────────┤
│         Worker Nodes                │
│  ┌────────┐ ┌────────┐ ┌────────┐  │
│  │Query   │ │Data    │ │Index   │  │
│  │Node    │ │Node    │ │Node    │  │
│  └────────┘ └────────┘ └────────┘  │
│     ↓          ↓          ↓         │
├─────────────────────────────────────┤
│         Storage Layer               │
│  ┌────────┐ ┌────────┐ ┌────────┐  │
│  │etcd    │ │MinIO/S3│ │Pulsar  │  │
│  │(Meta)  │ │(Vector)│ │(Log)   │  │
│  └────────┘ └────────┘ └────────┘  │
└─────────────────────────────────────┘
```

**组件职责**：

**Proxy（代理层）**：
- 接收客户端请求
- 负载均衡
- 请求路由

**Coordinator（协调层）**：
- Root Coord：元数据管理
- Query Coord：查询任务调度
- Data Coord：数据写入协调
- Index Coord：索引构建调度

**Worker（工作层）**：
- Query Node：执行查询
- Data Node：数据写入
- Index Node：索引构建

**Storage（存储层）**：
- etcd：元数据存储
- MinIO/S3：向量数据存储
- Pulsar/Kafka：WAL日志

---

#### 3.2 数据分片

```python
from pymilvus import connections, Collection, utility

# 连接Milvus集群
connections.connect(host="milvus-proxy", port="19530")

# 创建collection with分片
from pymilvus import FieldSchema, CollectionSchema, DataType

fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535)
]
schema = CollectionSchema(fields, description="分片collection")

# 创建8个分片
collection = Collection(
    name="documents",
    schema=schema,
    shards_num=8  # 8个分片，分布到多个Data Node
)

# 插入数据（自动分片）
entities = [embeddings, texts]
collection.insert(entities)
# Milvus自动根据哈希将数据分配到8个分片

# 查询（自动路由到所有分片）
results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param={"metric_type": "COSINE", "params": {"ef": 100}},
    limit=10
)
# Milvus自动：
# 1. 路由查询到所有分片
# 2. 并行执行查询
# 3. 合并结果
# 4. 返回Top 10
```

---

#### 3.3 副本配置

```python
# 创建collection with副本
collection = Collection("documents", schema=schema)

# 加载collection with副本
collection.load(replica_number=2)  # 2个副本

# 查询自动负载均衡到副本
results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param={"metric_type": "COSINE", "params": {"ef": 100}},
    limit=10
)
# Milvus自动：
# 1. 选择负载最低的副本
# 2. 执行查询
# 3. 返回结果
```

---

### 4. 在RAG中的应用

#### 4.1 大规模RAG系统

**场景**：1亿文档，10万QPS

**架构设计**：
```
┌─────────────────────────────────────┐
│         Load Balancer               │
│  (Nginx/HAProxy)                    │
└────────────┬────────────────────────┘
             │
     ┌───────┴───────┐
     ↓               ↓
┌─────────┐     ┌─────────┐
│ Proxy 1 │     │ Proxy 2 │
└────┬────┘     └────┬────┘
     │               │
     └───────┬───────┘
             ↓
┌─────────────────────────────────────┐
│         Milvus Cluster              │
│  - 8 Shards                         │
│  - 2 Replicas per Shard             │
│  - 16 Query Nodes                   │
│  - 8 Data Nodes                     │
└─────────────────────────────────────┘
```

**实现**：
```python
from pymilvus import connections, Collection
from sentence_transformers import SentenceTransformer
import random

class DistributedRAG:
    """分布式RAG系统"""

    def __init__(self, milvus_hosts):
        self.milvus_hosts = milvus_hosts
        self.model = SentenceTransformer('all-mpnet-base-v2')

        # 连接到Milvus集群（通过负载均衡器）
        connections.connect(
            alias="default",
            host=random.choice(milvus_hosts),  # 客户端负载均衡
            port="19530"
        )

        self.collection = Collection("documents")
        self.collection.load(replica_number=2)  # 2个副本

    def add_documents(self, documents, batch_size=10000):
        """批量添加文档"""
        embeddings = self.model.encode(documents, batch_size=128)

        # 分批插入
        for i in range(0, len(documents), batch_size):
            batch_embeddings = embeddings[i:i+batch_size]
            batch_texts = documents[i:i+batch_size]

            entities = [batch_embeddings.tolist(), batch_texts]
            self.collection.insert(entities)

        # 刷新
        self.collection.flush()

    def search(self, query, top_k=10):
        """分布式检索"""
        query_embedding = self.model.encode(query)

        # 查询自动分布到所有分片和副本
        results = self.collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 100}},
            limit=top_k,
            output_fields=["text"]
        )

        documents = []
        for hits in results:
            for hit in hits:
                documents.append({
                    "text": hit.entity.get('text'),
                    "distance": hit.distance
                })

        return documents

# 使用
rag = DistributedRAG(milvus_hosts=["proxy1.example.com", "proxy2.example.com"])

# 添加1亿文档（自动分片）
rag.add_documents(documents)

# 查询（自动负载均衡）
results = rag.search("什么是RAG？", top_k=10)
```

---

#### 4.2 高可用配置

```python
class HighAvailabilityRAG:
    """高可用RAG系统"""

    def __init__(self, milvus_clusters):
        # 多个Milvus集群（跨区域）
        self.clusters = milvus_clusters
        self.model = SentenceTransformer('all-mpnet-base-v2')

    def search_with_failover(self, query, top_k=10):
        """带故障转移的检索"""
        query_embedding = self.model.encode(query)

        # 尝试每个集群
        for cluster in self.clusters:
            try:
                connections.connect(
                    alias="default",
                    host=cluster["host"],
                    port=cluster["port"],
                    timeout=5  # 5秒超时
                )

                collection = Collection("documents")
                results = collection.search(
                    data=[query_embedding.tolist()],
                    anns_field="embedding",
                    param={"metric_type": "COSINE", "params": {"ef": 100}},
                    limit=top_k
                )

                return self._format_results(results)

            except Exception as e:
                print(f"Cluster {cluster['host']} failed: {e}")
                continue

        raise Exception("All clusters failed")

# 使用
rag = HighAvailabilityRAG(milvus_clusters=[
    {"host": "cluster1.us-west.example.com", "port": "19530"},
    {"host": "cluster2.us-east.example.com", "port": "19530"},
    {"host": "cluster3.eu-west.example.com", "port": "19530"}
])

# 查询（自动故障转移）
results = rag.search_with_failover("什么是RAG？")
```

---

### 5. 性能优化

#### 5.1 查询优化

**优化1：批量查询**
```python
# ❌ 慢：逐个查询
for query in queries:
    results = collection.search([query_embedding], limit=10)

# ✅ 快：批量查询
query_embeddings = [model.encode(q) for q in queries]
results = collection.search(query_embeddings, limit=10)
# 性能提升：10-100倍
```

**优化2：分区裁剪**
```python
# 只搜索相关分区
results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param={"metric_type": "COSINE", "params": {"ef": 100}},
    limit=10,
    partition_names=["2025_Q4", "2026_Q1"]  # 只搜索最近两个季度
)
# 性能提升：2-10倍
```

**优化3：结果缓存**
```python
import redis

class CachedRAG:
    def __init__(self):
        self.redis = redis.Redis(host='localhost', port=6379)
        self.collection = Collection("documents")

    def search(self, query, top_k=10):
        # 检查缓存
        cache_key = f"search:{query}:{top_k}"
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        # 执行查询
        query_embedding = model.encode(query)
        results = self.collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 100}},
            limit=top_k
        )

        # 缓存结果（1小时）
        self.redis.setex(cache_key, 3600, json.dumps(results))

        return results
```

---

#### 5.2 写入优化

**优化1：批量写入**
```python
# ❌ 慢：逐个插入
for doc in documents:
    collection.insert([[embedding], [doc]])

# ✅ 快：批量插入
batch_size = 10000
for i in range(0, len(documents), batch_size):
    batch = documents[i:i+batch_size]
    batch_embeddings = model.encode(batch)
    collection.insert([batch_embeddings.tolist(), batch])
# 性能提升：100-1000倍
```

**优化2：异步写入**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncRAG:
    def __init__(self):
        self.collection = Collection("documents")
        self.executor = ThreadPoolExecutor(max_workers=10)

    async def add_documents_async(self, documents):
        """异步批量添加"""
        embeddings = await asyncio.get_event_loop().run_in_executor(
            self.executor,
            model.encode,
            documents
        )

        await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self.collection.insert,
            [embeddings.tolist(), documents]
        )

# 使用
rag = AsyncRAG()
await rag.add_documents_async(documents)
```

---

### 6. 监控与运维

#### 6.1 监控指标

```python
from prometheus_client import Counter, Histogram, Gauge

# 查询指标
query_counter = Counter('milvus_queries_total', 'Total queries')
query_latency = Histogram('milvus_query_latency_seconds', 'Query latency')
query_errors = Counter('milvus_query_errors_total', 'Query errors')

# 写入指标
insert_counter = Counter('milvus_inserts_total', 'Total inserts')
insert_latency = Histogram('milvus_insert_latency_seconds', 'Insert latency')

# 集群指标
cluster_nodes = Gauge('milvus_cluster_nodes', 'Number of nodes')
cluster_shards = Gauge('milvus_cluster_shards', 'Number of shards')

def monitored_search(collection, query_embedding, top_k):
    """带监控的查询"""
    query_counter.inc()

    start_time = time.time()
    try:
        results = collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 100}},
            limit=top_k
        )

        query_latency.observe(time.time() - start_time)
        return results

    except Exception as e:
        query_errors.inc()
        raise
```

---

#### 6.2 故障处理

```python
class FaultTolerantRAG:
    """容错RAG系统"""

    def __init__(self, milvus_hosts):
        self.milvus_hosts = milvus_hosts
        self.current_host_index = 0
        self.max_retries = 3

    def search_with_retry(self, query, top_k=10):
        """带重试的检索"""
        for attempt in range(self.max_retries):
            try:
                # 选择节点
                host = self.milvus_hosts[self.current_host_index]

                # 连接
                connections.connect(
                    alias="default",
                    host=host,
                    port="19530",
                    timeout=5
                )

                # 查询
                collection = Collection("documents")
                results = collection.search(
                    data=[query_embedding],
                    anns_field="embedding",
                    param={"metric_type": "COSINE", "params": {"ef": 100}},
                    limit=top_k
                )

                return results

            except Exception as e:
                print(f"Attempt {attempt + 1} failed: {e}")

                # 切换到下一个节点
                self.current_host_index = (self.current_host_index + 1) % len(self.milvus_hosts)

                # 最后一次尝试失败，抛出异常
                if attempt == self.max_retries - 1:
                    raise

                # 等待后重试
                time.sleep(2 ** attempt)  # 指数退避
```

---

## 总结

**分布式架构的核心要点**：
1. **分片**：水平扩展容量和性能
2. **副本**：提高可用性和读性能
3. **负载均衡**：均匀分配请求
4. **故障转移**：自动处理节点故障

**Milvus分布式特性**：
- 存算分离架构
- 自动分片和副本
- 云原生设计
- 支持数十亿向量

**2026年最佳实践**：
- 8-16个分片
- 2-3个副本
- Kubernetes部署
- Prometheus监控
- 自动故障转移

---

## 引用来源

1. **Milvus架构**：https://milvus.io/docs/architecture_overview.md
2. **分布式系统设计**：https://redis.io/blog/rag-at-scale
3. **一致性哈希**：https://en.wikipedia.org/wiki/Consistent_hashing
4. **Raft共识**：https://raft.github.io/
5. **负载均衡策略**：https://www.nginx.com/blog/choosing-nginx-plus-load-balancing-techniques/
6. **Kubernetes部署**：https://milvus.io/docs/install_cluster-helm.md

---

**记住**：分布式架构是支撑大规模向量检索的基础，合理的分片、副本和负载均衡策略是关键。
