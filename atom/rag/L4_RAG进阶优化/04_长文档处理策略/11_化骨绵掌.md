# 化骨绵掌

> 10张知识卡片，覆盖长文档处理策略的所有核心知识点

---

## 卡片1：三大策略全景图

```
长文档处理策略 = 三大核心技术

┌─────────────────────────────────────────────────┐
│                 长文档处理策略                    │
├─────────────────────────────────────────────────┤
│                                                 │
│  ┌──────────────┐  ┌──────────────┐  ┌────────┐│
│  │ 分层索引      │  │ 摘要链        │  │MapReduce││
│  │Hierarchical  │  │Summary Chain │  │Strategy││
│  │Indexing      │  │              │  │        ││
│  ├──────────────┤  ├──────────────┤  ├────────┤│
│  │空间维度分解  │  │信息维度压缩  │  │时间维度││
│  │              │  │              │  │并行化  ││
│  │章节→段落→句子│  │粗→中→细摘要 │  │Map→    ││
│  │              │  │              │  │Reduce  ││
│  │              │  │              │  │        ││
│  │适用：结构化  │  │适用：全局理解│  │适用：  ││
│  │文档          │  │              │  │多文档  ││
│  │              │  │              │  │对比    ││
│  │准确率：0.87  │  │准确率：0.82  │  │准确率：││
│  │时间：0.8s    │  │时间：2.5s    │  │0.85    ││
│  │成本：1x      │  │成本：2.5x    │  │时间：  ││
│  │              │  │              │  │1.2s    ││
│  │              │  │              │  │成本：  ││
│  │              │  │              │  │1.8x    ││
│  └──────────────┘  └──────────────┘  └────────┘│
│                                                 │
│              混合策略（智能路由）                │
│              准确率：0.91                       │
│              时间：1.5s                         │
│              成本：2x                           │
└─────────────────────────────────────────────────┘
```

**关键记忆**：
- 分层索引 = 空间分解（章节→段落）
- 摘要链 = 信息压缩（粗→中→细）
- MapReduce = 时间并行（Map→Reduce）
- 混合策略 = 智能路由（最优解）

---

## 卡片2：Context Rot（上下文衰减）

```
Context Rot = LLM的"失忆症"

┌─────────────────────────────────────────┐
│         长上下文中的注意力分布           │
├─────────────────────────────────────────┤
│                                         │
│  开头部分：█████████ 0.92 准确率        │
│                                         │
│  中间部分：████ 0.58 准确率 ⚠️          │
│            （下降37%！）                │
│                                         │
│  结尾部分：████████ 0.89 准确率         │
│                                         │
└─────────────────────────────────────────┘

Lost in the Middle 现象：
关键信息在中间 → 准确率大幅下降

解决方案：
1. 分层索引：只检索相关章节
2. 摘要链：压缩文档长度
3. 关键信息前置：放在开头或结尾
4. 分段处理：避免长上下文
```

**关键记忆**：
- 中间信息准确率下降30-50%
- 即使有长上下文窗口，仍需智能策略
- 2026年仍是核心问题

---

## 卡片3：2026年长上下文LLM现状

```
2026年长上下文LLM对比

┌──────────────┬──────────┬──────────┬──────────┐
│ 模型          │ 上下文   │ 成本     │ Context  │
│              │ 窗口     │ (相对)   │ Rot      │
├──────────────┼──────────┼──────────┼──────────┤
│ Gemini 3 Pro │ 1M       │ 5x       │ 存在     │
│              │ tokens   │          │          │
├──────────────┼──────────┼──────────┼──────────┤
│ Llama 4      │ 10M      │ 8x       │ 存在     │
│ Scout        │ tokens   │          │          │
├──────────────┼──────────┼──────────┼──────────┤
│ Grok 4.1     │ 2M       │ 6x       │ 存在     │
│ Fast         │ tokens   │          │          │
├──────────────┼──────────┼──────────┼──────────┤
│ InfiniteHiP  │ 3M       │ 4x       │ 缓解     │
│              │ tokens   │          │          │
│              │ (单GPU)  │          │          │
└──────────────┴──────────┴──────────┴──────────┘

结论：
✅ 长上下文窗口越来越大
❌ Context Rot仍然存在
❌ 成本仍然高昂（3-10倍）
❌ 延迟仍然线性增长

智能策略仍然必需！
```

**关键记忆**：
- 10M tokens是2026年最大窗口
- 成本是标准上下文的3-10倍
- Context Rot仍未解决

---

## 卡片4：最佳Chunk Size（2025-2026）

```
Chunk Size vs 性能

┌─────────────────────────────────────────┐
│  Chunk Size  │ 召回率 │ 准确率 │ 推荐   │
├──────────────┼────────┼────────┼────────┤
│  100 tokens  │ 0.72   │ 0.68   │ ❌     │
│  200 tokens  │ 0.78   │ 0.75   │ ⚠️     │
│  400-512     │ 0.91   │ 0.87   │ ✅     │
│  tokens      │        │        │        │
│  1000 tokens │ 0.85   │ 0.82   │ ⚠️     │
│  2000 tokens │ 0.79   │ 0.76   │ ⚠️     │
└──────────────┴────────┴────────┴────────┘

为什么400-512 tokens最优？
1. 上下文完整性：不破坏语义
2. 检索精度：平衡精度和召回率
3. Embedding质量：模型最佳表现区间

2025-2026生产级推荐：
- 标准场景：400-512 tokens
- 分层索引：章节级2000，段落级500
- 摘要链：4000-8000 tokens
- Chunk Overlap：10-20%
```

**关键记忆**：
- 400-512 tokens是黄金区间
- Chunk越小不一定越精准
- 需要平衡精度和上下文完整性

---

## 卡片5：分层索引实现要点

```
分层索引 = 两层索引 + 两阶段检索

┌─────────────────────────────────────────┐
│           分层索引架构                   │
├─────────────────────────────────────────┤
│                                         │
│  第一层：章节级索引（粗粒度）            │
│  ┌─────────────────────────────────┐   │
│  │ Chunk Size: 2000 tokens         │   │
│  │ Top-K: 2-3                      │   │
│  │ 作用：快速定位相关章节           │   │
│  └─────────────────────────────────┘   │
│           ↓                             │
│  第二层：段落级索引（细粒度）            │
│  ┌─────────────────────────────────┐   │
│  │ Chunk Size: 500 tokens          │   │
│  │ Top-K: 5-10                     │   │
│  │ 作用：精确检索相关段落           │   │
│  └─────────────────────────────────┘   │
│                                         │
└─────────────────────────────────────────┘

核心代码（Python）：
```python
# 1. 分层分块
chapter_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000, chunk_overlap=200
)
paragraph_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500, chunk_overlap=50
)

# 2. 构建两层索引
chapter_store = Chroma.from_texts(chapters, embeddings)
paragraph_store = Chroma.from_texts(paragraphs, embeddings)

# 3. 两阶段检索
relevant_chapters = chapter_store.similarity_search(query, k=2)
relevant_paragraphs = paragraph_store.similarity_search(query, k=5)
```

**关键记忆**：
- 两层索引：章节级（2000）+ 段落级（500）
- 两阶段检索：先粗后细
- 2026技术：LATTICE、BookRAG、HiRAG

---

## 卡片6：摘要链实现要点

```
摘要链 = 迭代式摘要 + 多层次摘要

┌─────────────────────────────────────────┐
│           摘要链架构                     │
├─────────────────────────────────────────┤
│                                         │
│  第一层：整体摘要（最粗）                │
│  ┌─────────────────────────────────┐   │
│  │ 压缩比：10x                     │   │
│  │ 作用：快速了解全文               │   │
│  └─────────────────────────────────┘   │
│           ↓                             │
│  第二层：分段摘要（中等）                │
│  ┌─────────────────────────────────┐   │
│  │ 压缩比：5x                      │   │
│  │ 作用：了解关键章节               │   │
│  └─────────────────────────────────┘   │
│           ↓                             │
│  第三层：详细摘要（最细）                │
│  ┌─────────────────────────────────┐   │
│  │ 压缩比：3x                      │   │
│  │ 作用：深入理解细节               │   │
│  └─────────────────────────────────┘   │
│                                         │
└─────────────────────────────────────────┘

核心代码（Python）：
```python
# 迭代式摘要链（refine策略）
chain = load_summarize_chain(
    llm,
    chain_type="refine",  # 关键：迭代式精炼
    verbose=True
)
summary = chain.run(docs)

# 多层次摘要
level1 = summary_chain(text, chunk_size=8000)  # 粗
level2 = summary_chain(text, chunk_size=4000)  # 中
level3 = summary_chain(text, chunk_size=2000)  # 细
```

**关键记忆**：
- 迭代式摘要：refine策略
- 多层次摘要：粗→中→细
- 2026技术：CoS、CoD、CoTHSSum

---

## 卡片7：MapReduce实现要点

```
MapReduce = Map并行处理 + Reduce聚合

┌─────────────────────────────────────────┐
│           MapReduce架构                  │
├─────────────────────────────────────────┤
│                                         │
│  Map阶段：并行处理                       │
│  ┌─────────────────────────────────┐   │
│  │ 文档1 → Worker1 → 结果1         │   │
│  │ 文档2 → Worker2 → 结果2         │   │
│  │ 文档3 → Worker3 → 结果3         │   │
│  │ 文档4 → Worker4 → 结果4         │   │
│  │ 文档5 → Worker5 → 结果5         │   │
│  └─────────────────────────────────┘   │
│           ↓                             │
│  Reduce阶段：聚合结果                    │
│  ┌─────────────────────────────────┐   │
│  │ 结果1 + 结果2 + ... → 最终结果   │   │
│  └─────────────────────────────────┘   │
│                                         │
└─────────────────────────────────────────┘

核心代码（Python）：
```python
# Map阶段：并行处理
with ThreadPoolExecutor(max_workers=5) as executor:
    map_results = list(executor.map(process_doc, documents))

# Reduce阶段：聚合结果
combined = "\n\n".join(map_results)
final_result = reduce_chain.run(combined)
```

**关键记忆**：
- 并发数：5-10最优
- Map：并行处理每个文档
- Reduce：聚合所有结果
- 2026技术：LLMxMapReduce V3

---

## 卡片8：混合策略路由决策树

```
混合策略 = 智能路由 + 动态选择

┌─────────────────────────────────────────┐
│           路由决策树                     │
├─────────────────────────────────────────┤
│                                         │
│              用户查询                    │
│                 ↓                       │
│         文档类型识别                     │
│                 ↓                       │
│    ┌────────────┼────────────┐         │
│    ↓            ↓            ↓         │
│  结构化      全局理解      多文档       │
│  文档        需求          对比         │
│    ↓            ↓            ↓         │
│  分层索引    摘要链      MapReduce      │
│  (LATTICE)   (CoS)      (LLMxMR)       │
│    ↓            ↓            ↓         │
│  0.87准确率  0.82准确率  0.85准确率     │
│  0.8s时间    2.5s时间    1.2s时间       │
│  1x成本      2.5x成本    1.8x成本       │
│                                         │
│         混合策略（必要时）                │
│         0.91准确率                      │
│         1.5s时间                        │
│         2x成本                          │
│                                         │
└─────────────────────────────────────────┘

决策代码（Python）：
```python
def route_strategy(doc_type, query_type, doc_length):
    if doc_type == "structured" and doc_length > 50000:
        return "hierarchical_indexing"
    elif query_type == "global_understanding":
        return "summary_chain"
    elif query_type == "multi_doc_comparison":
        return "mapreduce"
    else:
        return "hybrid"
```

**关键记忆**：
- 智能路由 > 简单叠加
- 根据文档类型和查询类型动态选择
- 混合策略准确率最高（0.91）

---

## 卡片9：2025-2026生产级最佳实践

```
生产级最佳实践（2025-2026）

┌─────────────────────────────────────────┐
│           参数配置                       │
├─────────────────────────────────────────┤
│                                         │
│  Chunk Size:                            │
│  ├─ 标准场景：400-512 tokens            │
│  ├─ 章节级：2000 tokens                 │
│  ├─ 段落级：500 tokens                  │
│  └─ 摘要链：4000-8000 tokens            │
│                                         │
│  Chunk Overlap:                         │
│  └─ 10-20%                              │
│                                         │
│  并发数:                                 │
│  └─ 5-10（最优）                        │
│                                         │
│  Top-K:                                 │
│  ├─ 章节级：2-3                         │
│  └─ 段落级：5-10                        │
│                                         │
│  索引类型:                               │
│  ├─ HNSW（速度提升10倍）                │
│  ├─ IVF_SQ8（内存减少75%）              │
│  └─ GPU索引（速度提升5倍）              │
│                                         │
└─────────────────────────────────────────┘

性能基准：
┌──────────────┬──────┬──────┬──────┐
│ 策略          │ 时间 │ 准确率│ 成本 │
├──────────────┼──────┼──────┼──────┤
│ 分层索引      │ 0.8s │ 0.87 │ 1x   │
│ 摘要链        │ 2.5s │ 0.82 │ 2.5x │
│ MapReduce     │ 1.2s │ 0.85 │ 1.8x │
│ 混合策略      │ 1.5s │ 0.91 │ 2x   │
└──────────────┴──────┴──────┴──────┘
```

**关键记忆**：
- 400-512 tokens是标准配置
- 10-20% overlap
- 5-10并发最优
- 混合策略是生产级标准

---

## 卡片10：2026年新技术全景

```
2026年长文档处理新技术

┌─────────────────────────────────────────┐
│           分层索引技术                   │
├─────────────────────────────────────────┤
│  ✅ LATTICE: 层次化检索框架              │
│     - 动态层级调整                       │
│     - 准确率提升23%                      │
│                                         │
│  ✅ BookRAG: 结构感知索引                │
│     - 专为书籍设计                       │
│     - 准确率提升35%                      │
│                                         │
│  ✅ HiRAG: 混合层次检索                  │
│     - 章节+段落级索引                    │
│     - 检索时间减少60%                    │
│                                         │
│  ✅ T-Retriever: 树结构检索器            │
│     - 基于树结构的检索                   │
│                                         │
└─────────────────────────────────────────┘

┌─────────────────────────────────────────┐
│           摘要链技术                     │
├─────────────────────────────────────────┤
│  ✅ Chain of Summaries (CoS):           │
│     - 基于黑格尔辩证法                   │
│     - 信息密度提升3倍                    │
│                                         │
│  ✅ Chain of Density (CoD):             │
│     - 密度递增摘要                       │
│     - 准确率提升18%                      │
│                                         │
│  ✅ CoTHSSum:                           │
│     - 思维链驱动的层次摘要               │
│     - 长文档理解准确率提升27%            │
│                                         │
└─────────────────────────────────────────┘

┌─────────────────────────────────────────┐
│           MapReduce技术                  │
├─────────────────────────────────────────┤
│  ✅ LLMxMapReduce V3:                   │
│     - 支持MCP驱动的智能代理              │
│     - 并行效率提升5倍                    │
│                                         │
│  ✅ DocETL:                             │
│     - 文档ETL管道                        │
│     - 支持复杂Map-Reduce操作             │
│                                         │
│  ✅ Chain-of-Agents (Google):           │
│     - 多代理协作框架                     │
│     - 多文档对比准确率提升32%            │
│                                         │
└─────────────────────────────────────────┘

┌─────────────────────────────────────────┐
│           长上下文LLM                    │
├─────────────────────────────────────────┤
│  ✅ InfiniteHiP:                        │
│     - 3M tokens上下文                    │
│     - 单GPU运行                          │
│                                         │
│  ✅ LongPO:                             │
│     - 长上下文偏好优化                   │
│                                         │
│  ✅ Context Rot Analysis:               │
│     - 上下文衰减分析                     │
│                                         │
└─────────────────────────────────────────┘

┌─────────────────────────────────────────┐
│           评估与优化                     │
├─────────────────────────────────────────┤
│  ✅ RAGAS: RAG评估框架                  │
│     - 50+评估器                          │
│                                         │
│  ✅ Context Utilization Rate:           │
│     - 新评估指标                         │
│     - 衡量上下文利用率                   │
│                                         │
│  ✅ LlamaIndex Evaluation:              │
│     - Faithfulness评估                   │
│     - Relevancy评估                      │
│                                         │
└─────────────────────────────────────────┘
```

**关键记忆**：
- 分层索引：LATTICE、BookRAG、HiRAG
- 摘要链：CoS、CoD、CoTHSSum
- MapReduce：LLMxMapReduce V3、DocETL
- 长上下文：InfiniteHiP（3M tokens）
- 评估：RAGAS、Context Utilization Rate

---

## 总结：10张卡片速记

1. **三大策略全景图**：分层索引、摘要链、MapReduce
2. **Context Rot**：中间信息准确率下降30-50%
3. **2026年长上下文LLM**：10M tokens窗口，但仍需智能策略
4. **最佳Chunk Size**：400-512 tokens黄金区间
5. **分层索引实现**：两层索引（2000+500）+ 两阶段检索
6. **摘要链实现**：迭代式摘要 + 多层次摘要
7. **MapReduce实现**：Map并行（5-10并发）+ Reduce聚合
8. **混合策略路由**：智能路由 > 简单叠加
9. **生产级最佳实践**：400-512 tokens、10-20% overlap、5-10并发
10. **2026年新技术**：LATTICE、CoS、LLMxMapReduce V3、InfiniteHiP

---

**版本**: v1.0 (2025-2026 Research Edition)
**最后更新**: 2026-02-17
