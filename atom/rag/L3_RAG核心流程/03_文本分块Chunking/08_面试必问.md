# 面试必问

文本分块是 RAG 系统的核心环节，面试中经常被问到。本文包含 2025-2026 年最新技术的面试题。

---

## 问题1："什么是文本分块？为什么 RAG 系统需要分块？"

### 普通回答（❌ 不出彩）

"文本分块就是把长文档切成小块，因为 LLM 有长度限制。"

### 出彩回答（✅ 推荐）

> **文本分块有三层含义：**
>
> 1. **技术层面**：将长文档切分成适合 embedding 模型和 LLM 处理的小片段，突破输入长度限制。
>
> 2. **检索层面**：通过控制信息粒度来平衡检索精度和上下文完整性。块太小会丢失上下文，块太大会引入噪音。
>
> 3. **成本层面**：只检索和传输相关片段，而不是整个文档，显著降低 API 调用成本。
>
> **为什么需要分块？三个核心原因：**
> - Embedding 模型有输入长度限制（512-8191 tokens）
> - LLM Context Window 有限且成本高
> - 长文档的向量无法精准匹配查询（"平均语义"问题）
>
> **2025-2026 年演进**：从固定规则分块演进到智能分块，包括代理式分块（IBM，提升 15-20% 准确率）和上下文感知分块（Anthropic，减少 49-67% 检索失败）。

**为什么这个回答出彩？**
1. ✅ 多层次解释（技术/检索/成本）
2. ✅ 具体数据支撑（512-8191 tokens）
3. ✅ 包含最新研究（2025-2026）
4. ✅ 展示对技术演进的理解

---

## 问题2："NVIDIA 2025 年关于分块的研究有什么重要发现？"⭐ NEW

### 普通回答（❌ 不出彩）

"NVIDIA 研究了不同的分块策略。"

### 出彩回答（✅ 推荐）

> **NVIDIA 2025 年的三大核心发现：**
>
> 1. **页面级分块表现最佳**
>    - 准确率：0.648（最高）
>    - 方差：0.107（最稳定）
>    - 推翻了"块越小越精准"的传统观念
>
> 2. **查询类型决定最优块大小**
>    - 事实查询（Factual）：256-512 tokens
>    - 分析查询（Analytical）：1024+ tokens
>    - 混合查询：512-768 tokens
>
> 3. **15% 重叠率最优**
>    - 过低（<10%）：边界信息丢失
>    - 过高（>20%）：成本增加但收益递减
>    - 最优：15%（平衡成本和效果）
>
> **实际应用**：在生产环境中，我会根据查询类型动态调整 chunk_size，并固定使用 15% 重叠率。

**研究来源**: [NVIDIA 2025 Chunking Benchmark](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)

**为什么这个回答出彩？**
1. ✅ 具体数据（0.648 准确率，15% 重叠率）
2. ✅ 推翻传统观念（块越小越精准）
3. ✅ 实际应用建议
4. ✅ 引用权威研究来源

---

## 问题3："什么是代理式分块（Agentic Chunking）？"⭐ NEW 2025-2026

### 普通回答（❌ 不出彩）

"代理式分块是用 LLM 来分块。"

### 出彩回答（✅ 推荐）

> **代理式分块是 IBM 2025-2026 年提出的智能分块方法，让 LLM 像人类编辑一样理解文档结构并智能确定分块边界。**
>
> **4步工作流程：**
> 1. **文本准备**：清洗和格式化文档
> 2. **智能分块**：LLM 理解文档结构，动态确定边界
> 3. **元数据标注**：自动生成主题、关键词、章节信息
> 4. **向量化**：将 chunk + 元数据一起向量化
>
> **效果提升：**
> - 检索准确率提升 15-20%
> - 自动生成丰富的元数据
> - 适应不同文档类型
>
> **成本分析**：
> - 10万字文档成本约 $0.82
> - 适合高价值场景（医疗、法律、金融）
> - ROI 极高（误诊成本 $10,000+ vs 分块成本 $0.82）
>
> **与传统方法对比**：
> - 传统分块：基于固定规则，无法理解语义
> - 代理式分块：LLM 理解结构，智能确定边界
> - 类比：机械切分 vs AI 编辑整理

**研究来源**: [IBM Agentic Chunking](https://www.ibm.com/think/topics/agentic-chunking)

**为什么这个回答出彩？**
1. ✅ 完整流程（4步）
2. ✅ 具体数据（15-20% 提升，$0.82 成本）
3. ✅ ROI 分析（展示商业思维）
4. ✅ 对比传统方法
5. ✅ 引用权威来源

---

## 问题4："Anthropic 的上下文感知分块是如何工作的？"⭐ NEW 2024-2025

### 普通回答（❌ 不出彩）

"给每个 chunk 添加一些上下文信息。"

### 出彩回答（✅ 推荐）

> **上下文感知分块是 Anthropic 2024-2025 年提出的方法，为每个 chunk 添加 50-100 token 的文档级上下文说明。**
>
> **核心问题**：传统分块后，chunk 脱离文档上下文，语义模糊。
> - 例如："这个方法返回 True"
> - 问题：哪个方法？什么情况下返回 True？
>
> **解决方案**：使用 LLM 为每个 chunk 生成上下文前缀。
> - 上下文感知后："在 Python 字符串处理章节中，str.isdigit() 方法返回 True"
> - 效果：语义清晰，检索准确
>
> **效果数据**：
> - 单独使用：减少 49% 检索失败
> - 结合 reranking：减少 67% 检索失败
> - Top-20 召回率提升 5.7-10.2%
>
> **成本优化**：
> - 使用 Claude 3 Haiku：$0.02/10万字
> - Prompt Caching：成本降低 90%
> - 性价比极高，推荐生产环境使用
>
> **实现要点**：
> - 上下文长度：50-100 tokens
> - 模型选择：Claude 3 Haiku（性价比最高）
> - 缓存策略：缓存文档内容，降低成本

**研究来源**: [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)

**为什么这个回答出彩？**
1. ✅ 问题-解决方案结构清晰
2. ✅ 具体数据（49-67% 减少失败）
3. ✅ 成本分析（$0.02/10万字）
4. ✅ 实现要点（可操作）
5. ✅ 引用权威来源

---

## 问题5："如何选择合适的分块策略？"

### 普通回答（❌ 不出彩）

"根据文档类型选择不同的分块方法。"

### 出彩回答（✅ 推荐）

> **选择分块策略需要考虑三个维度：文档价值、查询类型、成本预算。**
>
> **决策框架（2025-2026）：**
>
> **1. 按文档价值选择：**
> - **高价值**（医疗、法律、金融）→ 代理式分块
>   - 成本：$0.82/10万字
>   - 准确率：+15-20%
>   - ROI：极高
>
> - **中等价值**（企业知识库）→ 递归分块 + 上下文感知
>   - 成本：$0.10/10万字
>   - 准确率：+12%
>   - 推荐：生产环境标配
>
> - **普通文档**（一般内容）→ 递归分块
>   - 成本：$0
>   - 准确率：基准
>   - 推荐：快速原型
>
> **2. 按查询类型调整块大小（NVIDIA 2025）：**
> - 事实查询：512 tokens
> - 分析查询：1024 tokens
> - 混合查询：768 tokens
> - 重叠率：15%（固定）
>
> **3. 按文档类型选择分隔符：**
> - Markdown：`["\n## ", "\n### ", "\n\n", "\n"]`
> - 代码：`["\n\n\n", "\n\n", "\n"]`
> - 中文：`["\n\n", "\n", "。", "！", "？"]`
>
> **实际案例**：
> - 医疗诊断知识库：代理式分块（误诊成本高）
> - 企业技术文档：递归 + 上下文感知（平衡成本和效果）
> - 个人博客：递归分块（成本敏感）

**为什么这个回答出彩？**
1. ✅ 多维度决策框架
2. ✅ 具体数据和成本分析
3. ✅ 实际案例说明
4. ✅ 展示系统性思考
5. ✅ 包含最新研究（2025-2026）

---

## 问题6："分块大小和重叠率如何设置？"

### 普通回答（❌ 不出彩）

"一般设置 chunk_size=500，overlap=50。"

### 出彩回答（✅ 推荐）

> **2025-2026 年的最佳实践基于 NVIDIA 研究：**
>
> **块大小设置（查询类型自适应）：**
> - **事实查询**：512 tokens
>   - 原因：需要精准定位具体信息
>   - 场景：问答系统、知识检索
>
> - **分析查询**：1024 tokens
>   - 原因：需要更多上下文进行推理
>   - 场景：文档摘要、深度分析
>
> - **混合查询**：768 tokens
>   - 原因：平衡精度和上下文
>   - 场景：通用 RAG 系统
>
> **重叠率设置（NVIDIA 验证）：**
> - **推荐：15%**
> - 计算公式：`chunk_overlap = chunk_size * 0.15`
> - 数据支撑：
>   - 0% 重叠：准确率 0.612
>   - 10% 重叠：准确率 0.635
>   - **15% 重叠：准确率 0.648（最优）**
>   - 20% 重叠：准确率 0.651（收益递减）
>
> **常见误区**：
> - ❌ "块越小越精准"：NVIDIA 证明页面级分块最优
> - ❌ "重叠越大越好"：15% 是成本和效果的最佳平衡
> - ❌ "一个配置通吃"：需要根据查询类型调整
>
> **实现代码**：
> ```python
> # NVIDIA 2025 推荐配置
> from langchain.text_splitter import RecursiveCharacterTextSplitter
>
> def create_optimal_splitter(query_type: str = "factual"):
>     if query_type == "factual":
>         chunk_size = 512
>     elif query_type == "analytical":
>         chunk_size = 1024
>     else:
>         chunk_size = 768
>
>     return RecursiveCharacterTextSplitter(
>         chunk_size=chunk_size,
>         chunk_overlap=int(chunk_size * 0.15)  # 15% overlap
>     )
> ```

**研究来源**: [NVIDIA 2025 Chunking Benchmark](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)

**为什么这个回答出彩？**
1. ✅ 基于权威研究（NVIDIA 2025）
2. ✅ 具体数据支撑（0.648 准确率）
3. ✅ 查询类型自适应
4. ✅ 常见误区分析
5. ✅ 可运行代码示例

---

## 问题7："语义分块和递归分块有什么区别？什么时候用哪个？"

### 普通回答（❌ 不出彩）

"语义分块基于相似度，递归分块基于分隔符。"

### 出彩回答（✅ 推荐）

> **核心区别：**
>
> **递归分块（规则驱动）：**
> - 原理：按分隔符优先级递归切分（段落→句子→词→字符）
> - 成本：$0（无 API 调用）
> - 速度：快
> - 准确率：0.640（基准）
> - 适用：90% 的通用场景
>
> **语义分块（embedding 驱动）：**
> - 原理：计算句子 embedding 相似度，在语义变化处切分
> - 成本：高（需要 embedding API）
> - 速度：慢
> - 准确率：0.655（仅提升 2-3%）
> - 适用：主题频繁变化的文档
>
> **2025-2026 成本效益分析：**
> - 语义分块只提升 2-3% 准确率
> - 但成本增加 100 倍
> - **更好的选择**：递归分块 + 上下文感知
>   - 成本：仅为语义分块的 1/10
>   - 准确率：提升 12%（0.640 → 0.720）
>
> **决策建议：**
> - **推荐**：递归分块 + 上下文感知（性价比最高）
> - **不推荐**：单独使用语义分块（成本效益比低）
> - **特殊场景**：主题多变文档可考虑语义分块
>
> **实际案例**：
> - 技术文档：递归分块（有明显段落结构）
> - 新闻聚合：语义分块（主题频繁变化）
> - 企业知识库：递归 + 上下文感知（平衡成本和效果）

**为什么这个回答出彩？**
1. ✅ 多维度对比（原理/成本/速度/准确率）
2. ✅ 成本效益分析
3. ✅ 推荐更优方案（递归 + 上下文感知）
4. ✅ 实际案例说明
5. ✅ 展示商业思维

---

## 核心研究来源

1. **NVIDIA 2025**: [Finding the Best Chunking Strategy](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)
2. **Anthropic 2024-2025**: [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)
3. **IBM 2025-2026**: [Agentic Chunking](https://www.ibm.com/think/topics/agentic-chunking)

---

**下一步：** [09_化骨绵掌](./09_化骨绵掌.md) - 10个2分钟知识卡片
