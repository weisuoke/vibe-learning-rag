# 最小可用

掌握以下 20% 的核心知识，就能完成 80% 的 RAG 生成任务。

---

## 4.1 基础 Prompt 模板

**一个能用的 RAG Prompt 模板只需要三部分：**

```python
RAG_PROMPT = """基于以下参考资料回答问题。如果资料中没有相关信息，请说"无法回答"。

【参考资料】
{context}

【问题】
{question}
"""
```

**这就够了！** 不需要复杂的角色设定、输出格式要求，先跑起来再优化。

---

## 4.2 LangChain 最简 RAG 链

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# 1. 准备组件
llm = ChatOpenAI(model="gpt-3.5-turbo")
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(documents, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 2. 定义 Prompt 模板
prompt = ChatPromptTemplate.from_template("""
基于以下参考资料回答问题。

【参考资料】
{context}

【问题】
{question}
""")

# 3. 构建 RAG 链
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 4. 使用
answer = rag_chain.invoke("公司的年假政策是什么？")
print(answer)
```

**核心要点：**
- `retriever | format_docs`：检索 + 格式化
- `RunnablePassthrough()`：透传用户问题
- `prompt | llm | StrOutputParser()`：组装 → 生成 → 解析

---

## 4.3 Context 格式化（最简版）

```python
def format_docs(docs):
    """最简单的格式化：用换行分隔"""
    return "\n\n".join(doc.page_content for doc in docs)

# 稍微好一点的版本：加编号
def format_docs_with_index(docs):
    """带编号的格式化"""
    formatted = []
    for i, doc in enumerate(docs, 1):
        formatted.append(f"[{i}] {doc.page_content}")
    return "\n\n".join(formatted)
```

---

## 4.4 生成参数（只需记住两个）

```python
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0,      # 0 = 确定性输出，适合问答
    max_tokens=1000     # 限制输出长度
)
```

**temperature 速查：**
- `0`：确定性输出，适合事实性问答（推荐）
- `0.3-0.7`：平衡创意和准确性
- `1.0`：高创意，适合写作

---

## 4.5 处理"找不到答案"的情况

```python
RAG_PROMPT = """基于以下参考资料回答问题。

【重要】如果参考资料中没有相关信息，请直接回答："抱歉，根据现有资料无法回答这个问题。"
不要编造或猜测。

【参考资料】
{context}

【问题】
{question}
"""
```

**关键：** 明确告诉 LLM "不知道就说不知道"，减少幻觉。

---

## 最小可用清单

| 组件 | 最简实现 | 说明 |
|------|----------|------|
| Prompt 模板 | 三段式（资料+问题+指令） | 先跑起来 |
| Context 格式化 | `"\n\n".join()` | 换行分隔即可 |
| 生成参数 | `temperature=0` | 确定性输出 |
| 幻觉控制 | "不知道就说不知道" | 加一句指令 |
| RAG 链 | LangChain LCEL | 5 行代码搞定 |

---

## 这些知识足以：

- ✅ 构建一个能用的 RAG 问答系统
- ✅ 基于检索结果生成准确答案
- ✅ 处理"找不到答案"的边界情况
- ✅ 为后续优化打下基础

---

**下一步：** [05_双重类比](./05_双重类比.md) - 用熟悉的概念理解上下文注入
