# 实战代码_02_并发控制与限流

> 实现高级并发控制和限流策略的完整代码示例

---

## 场景描述

**任务**：批量处理 1000 个查询，需要：
- 遵守 API 速率限制（500 RPM）
- 动态调整并发数
- 实现令牌桶限流
- 监控系统资源

---

## 完整代码示例

```python
"""
并发控制与限流示例
展示如何实现高级并发控制和限流策略
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
import time
import threading
from collections import deque
from dataclasses import dataclass
from typing import List, Optional
import psutil
from dotenv import load_dotenv

load_dotenv()

# ============================================
# 1. 令牌桶限流器
# ============================================

class TokenBucket:
    """令牌桶限流器"""

    def __init__(self, rate: float, capacity: int):
        """
        rate: 每秒生成的令牌数
        capacity: 桶的容量
        """
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()
        self.lock = threading.Lock()

    def consume(self, tokens: int = 1) -> bool:
        """尝试消费令牌"""
        with self.lock:
            now = time.time()
            # 补充令牌
            elapsed = now - self.last_update
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now

            # 检查是否有足够的令牌
            if self.tokens >= tokens:
                self.tokens -= tokens
                return True
            return False

    def wait_for_token(self, tokens: int = 1):
        """等待直到有足够的令牌"""
        while not self.consume(tokens):
            time.sleep(0.1)

# ============================================
# 2. 自适应并发控制器
# ============================================

class AdaptiveConcurrencyController:
    """自适应并发控制器"""

    def __init__(self, initial_concurrency: int = 10):
        self.concurrency = initial_concurrency
        self.min_concurrency = 5
        self.max_concurrency = 50

        # 记录最近的成功率和延迟
        self.success_rate = deque(maxlen=100)
        self.latencies = deque(maxlen=100)

    def record_result(self, success: bool, latency: float):
        """记录执行结果"""
        self.success_rate.append(1 if success else 0)
        self.latencies.append(latency)
        self._adjust()

    def _adjust(self):
        """根据成功率和延迟动态调整并发数"""
        if len(self.success_rate) < 10:
            return

        recent_success = sum(self.success_rate) / len(self.success_rate)
        avg_latency = sum(self.latencies) / len(self.latencies)

        # 成功率高且延迟低：增加并发
        if recent_success > 0.95 and avg_latency < 3.0:
            self.concurrency = min(
                self.concurrency + 2,
                self.max_concurrency
            )
            print(f"[调整] 增加并发数到 {self.concurrency}")

        # 成功率低或延迟高：降低并发
        elif recent_success < 0.80 or avg_latency > 5.0:
            self.concurrency = max(
                self.concurrency - 2,
                self.min_concurrency
            )
            print(f"[调整] 降低并发数到 {self.concurrency}")

    def get_concurrency(self) -> int:
        """获取当前推荐的并发数"""
        return self.concurrency

# ============================================
# 3. 资源监控器
# ============================================

class ResourceMonitor:
    """系统资源监控器"""

    def check_resources(self) -> dict:
        """检查系统资源"""
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()

        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_available_mb": memory.available / 1024 / 1024
        }

    def should_throttle(self) -> bool:
        """判断是否应该限流"""
        resources = self.check_resources()

        # CPU 或内存使用率过高
        if resources["cpu_percent"] > 80 or resources["memory_percent"] > 80:
            print(f"[警告] 资源使用率过高: CPU {resources['cpu_percent']:.1f}%, "
                  f"内存 {resources['memory_percent']:.1f}%")
            return True

        return False

# ============================================
# 4. 高级批处理处理器
# ============================================

@dataclass
class BatchResult:
    """批处理结果"""
    success_count: int
    failure_count: int
    total_time: float
    avg_latency: float
    throughput: float

class AdvancedBatchProcessor:
    """高级批处理处理器"""

    def __init__(
        self,
        chain,
        rate_limit_rpm: int = 500,
        initial_concurrency: int = 10
    ):
        self.chain = chain

        # 令牌桶限流器（500 RPM = 8.33 RPS）
        self.rate_limiter = TokenBucket(
            rate=rate_limit_rpm / 60,
            capacity=rate_limit_rpm
        )

        # 自适应并发控制器
        self.concurrency_controller = AdaptiveConcurrencyController(
            initial_concurrency=initial_concurrency
        )

        # 资源监控器
        self.resource_monitor = ResourceMonitor()

    def batch(self, inputs: List[dict]) -> BatchResult:
        """带限流和并发控制的批处理"""
        print(f"\n开始批处理 {len(inputs)} 个任务")
        print(f"速率限制: {self.rate_limiter.rate:.2f} RPS")
        print(f"初始并发数: {self.concurrency_controller.get_concurrency()}\n")

        start_time = time.time()
        results = []
        success_count = 0
        failure_count = 0

        # 分批处理
        batch_size = self.concurrency_controller.get_concurrency()
        for i in range(0, len(inputs), batch_size):
            batch = inputs[i:i + batch_size]

            # 检查资源
            if self.resource_monitor.should_throttle():
                print("[限流] 等待资源释放...")
                time.sleep(2)

            # 等待令牌
            for _ in batch:
                self.rate_limiter.wait_for_token()

            # 批处理执行
            batch_start = time.time()
            try:
                config = RunnableConfig(
                    max_concurrency=self.concurrency_controller.get_concurrency()
                )
                batch_results = self.chain.batch(batch, config=config)

                results.extend(batch_results)
                success_count += len(batch)

                # 记录成功
                latency = time.time() - batch_start
                self.concurrency_controller.record_result(
                    success=True,
                    latency=latency / len(batch)
                )

            except Exception as e:
                print(f"[错误] 批处理失败: {e}")
                failure_count += len(batch)
                results.extend([None] * len(batch))

                # 记录失败
                latency = time.time() - batch_start
                self.concurrency_controller.record_result(
                    success=False,
                    latency=latency / len(batch)
                )

            # 打印进度
            progress = (i + len(batch)) / len(inputs) * 100
            print(f"[进度] {progress:.1f}% ({i + len(batch)}/{len(inputs)}), "
                  f"并发数: {self.concurrency_controller.get_concurrency()}")

        # 计算统计信息
        total_time = time.time() - start_time
        avg_latency = total_time / len(inputs)
        throughput = len(inputs) / total_time

        return BatchResult(
            success_count=success_count,
            failure_count=failure_count,
            total_time=total_time,
            avg_latency=avg_latency,
            throughput=throughput
        )

# ============================================
# 5. 主程序
# ============================================

def main():
    # 创建链
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("计算 {a} + {b} = ?")
    chain = prompt | llm

    # 准备测试数据（1000 个任务）
    inputs = [{"a": i, "b": i + 1} for i in range(1000)]

    # 创建高级批处理处理器
    processor = AdvancedBatchProcessor(
        chain=chain,
        rate_limit_rpm=500,  # 500 RPM 限制
        initial_concurrency=10
    )

    # 执行批处理
    result = processor.batch(inputs)

    # 打印结果
    print("\n" + "=" * 50)
    print("批处理完成")
    print("=" * 50)
    print(f"成功: {result.success_count}")
    print(f"失败: {result.failure_count}")
    print(f"总时间: {result.total_time:.2f}秒")
    print(f"平均延迟: {result.avg_latency:.3f}秒/个")
    print(f"吞吐量: {result.throughput:.2f} 个/秒")

if __name__ == "__main__":
    main()
```

---

## 代码解释

### 1. 令牌桶限流器

```python
class TokenBucket:
    def consume(self, tokens=1):
        # 补充令牌
        elapsed = now - self.last_update
        self.tokens = min(capacity, self.tokens + elapsed * rate)

        # 消费令牌
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        return False
```

**工作原理**：
- 令牌以固定速率生成
- 每次请求消费一个令牌
- 令牌不足时等待

**参考来源**：[LangChain 最佳实践](https://www.swarnendu.de/blog/langchain-best-practices/)

---

### 2. 自适应并发控制

```python
def _adjust(self):
    if recent_success > 0.95 and avg_latency < 3.0:
        self.concurrency += 2  # 增加并发
    elif recent_success < 0.80 or avg_latency > 5.0:
        self.concurrency -= 2  # 降低并发
```

**调整策略**：
- 成功率高 + 延迟低 → 增加并发
- 成功率低 或 延迟高 → 降低并发

---

### 3. 资源监控

```python
def should_throttle(self):
    if cpu_percent > 80 or memory_percent > 80:
        return True  # 需要限流
    return False
```

**监控指标**：
- CPU 使用率
- 内存使用率
- 可用内存

---

## 运行结果

```
开始批处理 1000 个任务
速率限制: 8.33 RPS
初始并发数: 10

[进度] 1.0% (10/1000), 并发数: 10
[进度] 2.0% (20/1000), 并发数: 10
[调整] 增加并发数到 12
[进度] 3.2% (32/1000), 并发数: 12
[进度] 4.4% (44/1000), 并发数: 12
[调整] 增加并发数到 14
...
[警告] 资源使用率过高: CPU 82.3%, 内存 75.2%
[限流] 等待资源释放...
...
[进度] 98.0% (980/1000), 并发数: 16
[进度] 100.0% (1000/1000), 并发数: 16

==================================================
批处理完成
==================================================
成功: 1000
失败: 0
总时间: 125.34秒
平均延迟: 0.125秒/个
吞吐量: 7.98 个/秒
```

---

## 关键观察

### 1. 自适应调整

- 初始并发数：10
- 最终并发数：16
- 自动根据性能调整

### 2. 限流效果

- 速率限制：8.33 RPS
- 实际吞吐量：7.98 RPS
- 成功遵守限制

### 3. 资源保护

- 检测到资源使用率过高
- 自动限流保护系统

---

## 最佳实践

### 1. 根据 API Tier 设置限流

```python
# OpenAI Tier 1: 500 RPM
processor = AdvancedBatchProcessor(
    chain=chain,
    rate_limit_rpm=500
)

# OpenAI Tier 2: 5000 RPM
processor = AdvancedBatchProcessor(
    chain=chain,
    rate_limit_rpm=5000
)
```

### 2. 监控并发数变化

```python
# 记录并发数历史
concurrency_history = []

def record_concurrency():
    concurrency_history.append(
        controller.get_concurrency()
    )

# 绘制并发数变化曲线
import matplotlib.pyplot as plt
plt.plot(concurrency_history)
plt.xlabel("批次")
plt.ylabel("并发数")
plt.show()
```

### 3. 实现断路器模式

```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.state = "closed"  # closed, open, half_open

    def call(self, func, *args):
        if self.state == "open":
            if time.time() - self.last_failure > self.timeout:
                self.state = "half_open"
            else:
                raise Exception("断路器打开")

        try:
            result = func(*args)
            if self.state == "half_open":
                self.state = "closed"
                self.failures = 0
            return result
        except Exception as e:
            self.failures += 1
            if self.failures >= self.failure_threshold:
                self.state = "open"
            raise
```

---

## 常见问题

### Q1: 令牌桶和漏桶有什么区别？

**A**:
- 令牌桶：允许突发流量（桶满时）
- 漏桶：严格限制流量（固定速率）

LangChain 批处理更适合令牌桶。

### Q2: 如何选择初始并发数？

**A**: 根据 API 限制的 20-30%：
- 500 RPM → 初始并发数 = 10
- 5000 RPM → 初始并发数 = 50

### Q3: 自适应调整的频率？

**A**: 建议每 10-20 个批次调整一次，避免频繁调整。

### Q4: 资源监控的阈值？

**A**:
- CPU: 80%
- 内存: 80%
- 可根据实际情况调整

---

## 扩展示例

### 示例1: 多级限流

```python
class MultiLevelRateLimiter:
    def __init__(self):
        self.minute_limiter = TokenBucket(rate=500/60, capacity=500)
        self.hour_limiter = TokenBucket(rate=10000/3600, capacity=10000)

    def wait_for_token(self):
        self.minute_limiter.wait_for_token()
        self.hour_limiter.wait_for_token()
```

### 示例2: 优先级队列

```python
import heapq

class PriorityBatchProcessor:
    def __init__(self, chain):
        self.chain = chain
        self.queue = []

    def add_task(self, inp, priority):
        heapq.heappush(self.queue, (-priority, inp))

    def process(self):
        batch = []
        for _ in range(10):
            if self.queue:
                _, inp = heapq.heappop(self.queue)
                batch.append(inp)

        return self.chain.batch(batch)
```

---

## 参考来源

1. [LangChain 最佳实践](https://www.swarnendu.de/blog/langchain-best-practices/) - 限流策略
2. [LangGraph 并行节点最佳实践](https://forum.langchain.com/t/best-practices-for-parallel-nodes-fanouts/1900) - 并发控制
3. [LangChain 生产实践](https://medium.com/@kasimoluwasegun/langchain-in-production-beyond-the-tutorials-e7b7f2506572) - 资源监控

---

## 总结

并发控制与限流的核心要点：

1. **令牌桶限流**：
   - 遵守 API 速率限制
   - 允许突发流量
   - 平滑流量控制

2. **自适应并发**：
   - 根据成功率调整
   - 根据延迟调整
   - 动态优化性能

3. **资源监控**：
   - 监控 CPU 和内存
   - 自动限流保护
   - 避免系统崩溃

4. **最佳实践**：
   - 根据 API Tier 设置限流
   - 实现断路器保护
   - 监控并发数变化

---

**下一步**：阅读 `07_实战代码_03_异步批处理与性能优化.md` 学习异步批处理的高级技巧
