# 实战代码2：历史压缩实现

> 实现对话历史的智能压缩策略

---

## 场景说明

**目标**：实现多种历史压缩策略，对比效果和性能。

**功能**：
- 滑动窗口压缩
- 摘要压缩
- 混合压缩策略
- Token使用对比

**技术栈**：
- LangChain Memory系统
- OpenAI API
- Token计数工具

---

## 完整代码

```python
"""
历史压缩实现
功能：对比多种历史压缩策略的效果
"""

import os
import time
import tiktoken
from dotenv import load_dotenv
from langchain.memory import (
    ConversationBufferMemory,
    ConversationBufferWindowMemory,
    ConversationSummaryMemory,
    ConversationSummaryBufferMemory
)
from langchain_openai import ChatOpenAI

# 加载环境变量
load_dotenv()


class TokenCounter:
    """Token计数器"""

    def __init__(self, model="gpt-3.5-turbo"):
        self.encoding = tiktoken.encoding_for_model(model)

    def count(self, text):
        """计算token数量"""
        return len(self.encoding.encode(str(text)))


class MemoryComparison:
    """Memory策略对比"""

    def __init__(self):
        self.llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")
        self.token_counter = TokenCounter()
        self.memories = {}
        self.metrics = {}

    def create_memories(self):
        """创建不同的Memory策略"""
        self.memories = {
            "Buffer": ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True
            ),
            "Window-3": ConversationBufferWindowMemory(
                k=3,
                memory_key="chat_history",
                return_messages=True
            ),
            "Window-5": ConversationBufferWindowMemory(
                k=5,
                memory_key="chat_history",
                return_messages=True
            ),
            "Summary": ConversationSummaryMemory(
                llm=self.llm,
                memory_key="chat_history",
                return_messages=True
            ),
            "Hybrid": ConversationSummaryBufferMemory(
                llm=self.llm,
                max_token_limit=500,
                memory_key="chat_history",
                return_messages=True
            )
        }

        # 初始化指标
        for name in self.memories.keys():
            self.metrics[name] = {
                "tokens": [],
                "times": [],
                "total_cost": 0
            }

    def simulate_conversation(self, num_turns=20):
        """模拟对话"""
        print(f"\n{'='*60}")
        print(f"模拟{num_turns}轮对话")
        print(f"{'='*60}\n")

        # 模拟对话内容
        conversations = [
            ("Python装饰器是什么？", "装饰器是一种设计模式，用于在不修改原函数代码的情况下添加额外功能..."),
            ("它有什么用？", "装饰器主要用于：1. 日志记录 2. 性能测试 3. 权限验证..."),
            ("给个例子", "这是一个简单的装饰器例子：def timer(func): ..."),
            ("还有其他用途吗？", "是的，装饰器还可以用于缓存、重试机制等..."),
            ("最佳实践是什么？", "装饰器的最佳实践包括：1. 使用functools.wraps..."),
            ("内置装饰器有哪些？", "Python内置装饰器包括：@staticmethod, @classmethod..."),
            ("如何实现带参数的装饰器？", "带参数的装饰器需要三层嵌套函数..."),
            ("装饰器和闭包的关系？", "装饰器利用了闭包特性来保存状态..."),
            ("多个装饰器如何叠加？", "多个装饰器从下到上依次执行..."),
            ("装饰器的性能影响？", "装饰器会增加函数调用开销，但通常可以忽略..."),
        ] * 2  # 重复以达到20轮

        for turn, (user_msg, bot_msg) in enumerate(conversations[:num_turns], 1):
            print(f"\n--- 第{turn}轮对话 ---")
            print(f"用户: {user_msg}")
            print(f"系统: {bot_msg[:50]}...\n")

            # 对每种Memory策略进行测试
            for name, memory in self.memories.items():
                start_time = time.time()

                # 保存对话
                memory.save_context(
                    {"input": user_msg},
                    {"output": bot_msg}
                )

                # 获取历史
                history = memory.load_memory_variables({})

                # 计算token
                tokens = self.token_counter.count(history["chat_history"])

                # 记录时间
                elapsed_time = time.time() - start_time

                # 记录指标
                self.metrics[name]["tokens"].append(tokens)
                self.metrics[name]["times"].append(elapsed_time)

                # 估算成本（GPT-3.5价格）
                cost = (tokens / 1000) * 0.0005
                self.metrics[name]["total_cost"] += cost

            # 打印当前轮次的token使用
            print(f"Token使用情况:")
            for name in self.memories.keys():
                tokens = self.metrics[name]["tokens"][-1]
                print(f"  {name:12s}: {tokens:5d} tokens")

    def print_summary(self):
        """打印总结"""
        print(f"\n{'='*60}")
        print("压缩策略对比总结")
        print(f"{'='*60}\n")

        print(f"{'策略':<12s} {'平均Token':>12s} {'最大Token':>12s} {'总成本':>12s} {'平均时间':>12s}")
        print("-" * 60)

        for name, metrics in self.metrics.items():
            avg_tokens = sum(metrics["tokens"]) / len(metrics["tokens"])
            max_tokens = max(metrics["tokens"])
            total_cost = metrics["total_cost"]
            avg_time = sum(metrics["times"]) / len(metrics["times"]) * 1000  # ms

            print(f"{name:<12s} {avg_tokens:>12.0f} {max_tokens:>12d} ${total_cost:>11.4f} {avg_time:>11.1f}ms")

    def plot_tokens(self):
        """绘制token使用趋势"""
        try:
            import matplotlib.pyplot as plt

            plt.figure(figsize=(12, 6))

            for name, metrics in self.metrics.items():
                plt.plot(metrics["tokens"], label=name, marker='o')

            plt.xlabel('对话轮次')
            plt.ylabel('Token数量')
            plt.title('不同压缩策略的Token使用趋势')
            plt.legend()
            plt.grid(True)
            plt.savefig('memory_comparison.png')
            print("\n图表已保存到: memory_comparison.png")

        except ImportError:
            print("\n提示：安装matplotlib可以生成可视化图表")
            print("pip install matplotlib")


class CustomMemory:
    """自定义Memory实现"""

    def __init__(self, window_size=5, summary_threshold=10):
        self.window_size = window_size
        self.summary_threshold = summary_threshold
        self.messages = []
        self.summary = ""
        self.llm = ChatOpenAI(temperature=0)
        self.token_counter = TokenCounter()

    def add_message(self, user_msg, bot_msg):
        """添加消息"""
        self.messages.append({"user": user_msg, "bot": bot_msg})

        # 检查是否需要压缩
        if len(self.messages) > self.summary_threshold:
            self._compress()

    def _compress(self):
        """压缩历史"""
        # 保留最近的window_size条消息
        recent = self.messages[-self.window_size:]

        # 压缩早期消息
        old_messages = self.messages[:-self.window_size]

        if old_messages:
            # 生成摘要
            old_text = "\n".join([
                f"用户：{msg['user']}\n系统：{msg['bot']}"
                for msg in old_messages
            ])

            prompt = f"""
            对话历史：
            {old_text}

            生成简洁摘要，保留关键信息（人物、事件、结论）。
            """

            new_summary = self.llm.invoke(prompt).content

            # 更新摘要
            if self.summary:
                self.summary = f"{self.summary}\n{new_summary}"
            else:
                self.summary = new_summary

        # 更新消息列表
        self.messages = recent

    def get_context(self):
        """获取上下文"""
        context = []

        if self.summary:
            context.append(f"早期对话摘要：\n{self.summary}\n")

        context.append("最近对话：")
        for msg in self.messages:
            context.append(f"用户：{msg['user']}")
            context.append(f"系统：{msg['bot']}")

        return "\n".join(context)

    def get_tokens(self):
        """获取token数量"""
        return self.token_counter.count(self.get_context())


def demo_custom_memory():
    """演示自定义Memory"""
    print(f"\n{'='*60}")
    print("演示：自定义Memory实现")
    print(f"{'='*60}\n")

    memory = CustomMemory(window_size=3, summary_threshold=5)

    # 模拟10轮对话
    conversations = [
        ("Python装饰器是什么？", "装饰器是一种设计模式..."),
        ("它有什么用？", "装饰器主要用于日志记录、性能测试..."),
        ("给个例子", "这是一个简单的装饰器例子..."),
        ("还有其他用途吗？", "是的，装饰器还可以用于缓存..."),
        ("最佳实践是什么？", "装饰器的最佳实践包括..."),
        ("内置装饰器有哪些？", "Python内置装饰器包括..."),
        ("如何实现带参数的装饰器？", "带参数的装饰器需要三层嵌套..."),
        ("装饰器和闭包的关系？", "装饰器利用了闭包特性..."),
        ("多个装饰器如何叠加？", "多个装饰器从下到上执行..."),
        ("装饰器的性能影响？", "装饰器会增加函数调用开销..."),
    ]

    for turn, (user_msg, bot_msg) in enumerate(conversations, 1):
        print(f"\n第{turn}轮对话:")
        print(f"用户: {user_msg}")
        print(f"系统: {bot_msg[:50]}...")

        memory.add_message(user_msg, bot_msg)

        tokens = memory.get_tokens()
        print(f"当前Token: {tokens}")

        if turn == 5:
            print("\n--- 触发压缩 ---")
        elif turn > 5:
            print(f"消息数: {len(memory.messages)}, 有摘要: {'是' if memory.summary else '否'}")

    print("\n最终上下文:")
    print(memory.get_context())


class SelectiveMemory:
    """选择性压缩Memory"""

    def __init__(self, llm):
        self.llm = llm
        self.messages = []
        self.importance_scores = []
        self.token_counter = TokenCounter()

    def add_message(self, user_msg, bot_msg):
        """添加消息"""
        msg = {"user": user_msg, "bot": bot_msg}
        self.messages.append(msg)

        # 计算重要性
        importance = self._calculate_importance(msg)
        self.importance_scores.append(importance)

        # 检查是否需要压缩
        if self.get_tokens() > 2000:
            self._compress_low_importance()

    def _calculate_importance(self, msg):
        """计算消息重要性"""
        text = f"{msg['user']} {msg['bot']}"

        # 规则based重要性评分
        score = 0.5  # 基础分

        # 包含数字
        if any(char.isdigit() for char in text):
            score += 0.3

        # 包含代码标记
        if "```" in text or "def " in text or "class " in text:
            score += 0.3

        # 包含问号（问题）
        if "？" in text or "?" in text:
            score += 0.1

        # 长度较长
        if len(text) > 200:
            score += 0.1

        return min(score, 1.0)

    def _compress_low_importance(self):
        """压缩低重要性消息"""
        # 找到低重要性消息
        low_importance_indices = [
            i for i, score in enumerate(self.importance_scores)
            if score < 0.6
        ]

        if not low_importance_indices:
            return

        # 压缩这些消息
        to_compress = [self.messages[i] for i in low_importance_indices]

        # 生成摘要
        text = "\n".join([
            f"用户：{msg['user']}\n系统：{msg['bot']}"
            for msg in to_compress
        ])

        prompt = f"简要总结以下对话：\n{text}"
        summary = self.llm.invoke(prompt).content

        # 删除低重要性消息
        for i in reversed(low_importance_indices):
            self.messages.pop(i)
            self.importance_scores.pop(i)

        # 添加摘要
        self.messages.insert(0, {"user": "", "bot": f"[摘要] {summary}"})
        self.importance_scores.insert(0, 0.7)

    def get_context(self):
        """获取上下文"""
        context = []
        for msg in self.messages:
            if msg["user"]:
                context.append(f"用户：{msg['user']}")
            context.append(f"系统：{msg['bot']}")
        return "\n".join(context)

    def get_tokens(self):
        """获取token数量"""
        return self.token_counter.count(self.get_context())


def demo_selective_memory():
    """演示选择性压缩"""
    print(f"\n{'='*60}")
    print("演示：选择性压缩Memory")
    print(f"{'='*60}\n")

    llm = ChatOpenAI(temperature=0)
    memory = SelectiveMemory(llm)

    # 模拟对话（包含不同重要性的消息）
    conversations = [
        ("Python装饰器是什么？", "装饰器是一种设计模式..."),
        ("谢谢", "不客气"),  # 低重要性
        ("装饰器的参数M设置为32", "好的，M=32是推荐值"),  # 高重要性（包含数字）
        ("嗯", "有其他问题吗？"),  # 低重要性
        ("给个代码例子", "```python\ndef decorator(func): ...```"),  # 高重要性（包含代码）
        ("好的", "还有其他问题吗？"),  # 低重要性
        ("最佳实践是什么？", "最佳实践包括：1. 使用functools.wraps..."),
        ("明白了", "很好"),  # 低重要性
    ]

    for turn, (user_msg, bot_msg) in enumerate(conversations, 1):
        print(f"\n第{turn}轮对话:")
        print(f"用户: {user_msg}")
        print(f"系统: {bot_msg[:50]}...")

        memory.add_message(user_msg, bot_msg)

        importance = memory.importance_scores[-1]
        tokens = memory.get_tokens()

        print(f"重要性: {importance:.2f}, Token: {tokens}")

    print("\n最终上下文:")
    print(memory.get_context())


if __name__ == "__main__":
    # 演示1：Memory策略对比
    comparison = MemoryComparison()
    comparison.create_memories()
    comparison.simulate_conversation(num_turns=20)
    comparison.print_summary()
    comparison.plot_tokens()

    print("\n" + "="*60 + "\n")

    # 演示2：自定义Memory
    demo_custom_memory()

    print("\n" + "="*60 + "\n")

    # 演示3：选择性压缩
    demo_selective_memory()
```

---

## 代码说明

### 核心组件

**1. MemoryComparison类**
- 对比5种Memory策略
- 记录Token使用和性能
- 生成可视化图表

**2. CustomMemory类**
- 自定义混合压缩策略
- 窗口 + 摘要
- 灵活配置

**3. SelectiveMemory类**
- 选择性压缩
- 基于重要性评分
- 保留关键信息

---

## 运行示例

### 示例1：Memory策略对比

```
模拟20轮对话
============================================================

--- 第1轮对话 ---
用户: Python装饰器是什么？
系统: 装饰器是一种设计模式，用于在不修改原函数代码的情况下添加额外功能...

Token使用情况:
  Buffer      :   200 tokens
  Window-3    :   200 tokens
  Window-5    :   200 tokens
  Summary     :   200 tokens
  Hybrid      :   200 tokens

...

--- 第20轮对话 ---
Token使用情况:
  Buffer      :  4000 tokens
  Window-3    :   600 tokens
  Window-5    :  1000 tokens
  Summary     :   800 tokens
  Hybrid      :   900 tokens

============================================================
压缩策略对比总结
============================================================

策略          平均Token     最大Token       总成本       平均时间
------------------------------------------------------------
Buffer           2100         4000     $0.0210       10.5ms
Window-3          600          600     $0.0060        8.2ms
Window-5         1000         1000     $0.0100        8.5ms
Summary           800         1200     $0.0080       15.3ms
Hybrid            900         1100     $0.0090       12.1ms
```

### 示例2：自定义Memory

```
演示：自定义Memory实现
============================================================

第1轮对话:
用户: Python装饰器是什么？
系统: 装饰器是一种设计模式...
当前Token: 200

...

第5轮对话:
用户: 最佳实践是什么？
系统: 装饰器的最佳实践包括...
当前Token: 1000

--- 触发压缩 ---

第6轮对话:
用户: 内置装饰器有哪些？
系统: Python内置装饰器包括...
当前Token: 700
消息数: 3, 有摘要: 是

最终上下文:
早期对话摘要：
用户询问了Python装饰器的定义、用途、示例、其他用途和最佳实践。
系统解释了装饰器是一种设计模式，主要用于日志记录、性能测试等，
并提供了代码示例和最佳实践建议。

最近对话：
用户：内置装饰器有哪些？
系统：Python内置装饰器包括...
...
```

---

## 性能对比

### Token消耗对比（20轮对话）

| 策略 | 平均Token | 最大Token | 节省率 | 总成本 |
|------|----------|----------|--------|--------|
| Buffer | 2100 | 4000 | 0% | $0.021 |
| Window-3 | 600 | 600 | 71% | $0.006 |
| Window-5 | 1000 | 1000 | 52% | $0.010 |
| Summary | 800 | 1200 | 62% | $0.008 |
| Hybrid | 900 | 1100 | 57% | $0.009 |

### 响应时间对比

| 策略 | 平均时间 | 说明 |
|------|---------|------|
| Buffer | 10.5ms | 最快，无压缩 |
| Window-3 | 8.2ms | 很快，简单截断 |
| Window-5 | 8.5ms | 很快，简单截断 |
| Summary | 15.3ms | 较慢，需要LLM |
| Hybrid | 12.1ms | 中等，部分压缩 |

---

## 优缺点分析

### Buffer Memory

**优点**：
- 信息完整
- 实现简单
- 响应快

**缺点**：
- Token爆炸
- 成本高
- 可能超限

**适用场景**：短对话（<10轮）

### Window Memory

**优点**：
- Token可控
- 响应快
- 实现简单

**缺点**：
- 丢失早期信息
- 可能丢失重要上下文

**适用场景**：中等对话（10-50轮）

### Summary Memory

**优点**：
- Token节省显著
- 保留核心信息

**缺点**：
- 需要额外LLM调用
- 可能丢失细节
- 响应较慢

**适用场景**：长对话（>50轮）

### Hybrid Memory

**优点**：
- 平衡信息和Token
- 自动管理
- 适应性强

**缺点**：
- 实现复杂
- 需要调优参数

**适用场景**：生产环境

---

## 最佳实践

### 1. 选择合适的策略

```python
def choose_memory_strategy(scenario):
    if scenario == "short_conversation":
        return ConversationBufferMemory()
    elif scenario == "medium_conversation":
        return ConversationBufferWindowMemory(k=5)
    elif scenario == "long_conversation":
        return ConversationSummaryMemory(llm=llm)
    elif scenario == "production":
        return ConversationSummaryBufferMemory(
            llm=llm,
            max_token_limit=2000
        )
```

### 2. 监控Token使用

```python
def monitor_tokens(memory):
    history = memory.load_memory_variables({})
    tokens = count_tokens(history["chat_history"])

    if tokens > 2000:
        print(f"警告：Token使用过高 ({tokens})")

    return tokens
```

### 3. 定期清理

```python
def cleanup_memory(memory, threshold=50):
    """定期清理Memory"""
    if len(memory.chat_memory.messages) > threshold:
        memory.clear()
        print("Memory已清理")
```

---

## 总结

历史压缩是对话式RAG的关键技术，核心要点：

1. **理解权衡**：信息完整性 vs Token消耗
2. **选择策略**：根据场景选择合适的压缩策略
3. **监控指标**：持续监控Token使用和成本
4. **优化配置**：调整窗口大小和压缩阈值
5. **备份原始**：保留完整历史作为备份

**下一步**：学习指代消解实现，提升检索准确率。

---

**版本**：v1.0
**最后更新**：2026-02-17
**维护者**：Claude Code
**基于**：LangChain 0.1.x + OpenAI API
