# Token与Context Window - 第一性原理

## 什么是第一性原理？

**第一性原理**：回到事物最基本的真理，从源头思考问题。

不是问"Token 怎么计算"，而是问"为什么 LLM 需要 Token？"

---

## Token 的第一性原理

### 1. 最基础的定义

**Token = LLM 处理文本的最小单位**

```
计算机不认识"文字"，只认识"数字"
    ↓
必须把文字切成小块，每块对应一个数字
    ↓
这个"小块"就是 Token
```

### 2. 为什么不直接用字符？

**核心问题：字符粒度太细，效率太低**

| 方式 | "Hello World" | 问题 |
|------|---------------|------|
| 按字符 | 11 个单位 | 太碎，计算量大 |
| 按单词 | 2 个单位 | 无法处理新词 |
| 按 Token | 2-3 个单位 | 平衡效率和灵活性 |

**推导链：**

```
1. LLM 需要把文字转成数字才能计算
   ↓
2. 按字符切太碎（效率低）
   ↓
3. 按单词切无法处理新词/中文
   ↓
4. 发明 Token：介于字符和单词之间的单位
   ↓
5. 用 BPE 等算法自动学习最优切分
```

### 3. Token 的三层价值

#### 价值1：统一不同语言

```python
# 英文：1 词 ≈ 1-2 Token
"Hello" → ["Hello"]  # 1 Token

# 中文：1 字 ≈ 1-2 Token
"你好" → ["你", "好"]  # 2 Token

# 代码：符号也是 Token
"print()" → ["print", "(", ")"]  # 3 Token
```

#### 价值2：控制计算成本

```
Token 数量 → 直接决定 → 计算量 → 直接决定 → 费用
```

API 按 Token 计费，不是按字符或请求次数。

#### 价值3：定义模型能力边界

```
Context Window = 最大 Token 数
    ↓
决定了模型能"看到"多少内容
    ↓
这是 RAG 设计的硬约束
```

---

## Context Window 的第一性原理

### 1. 最基础的定义

**Context Window = LLM 一次能处理的最大 Token 数量**

```
输入 Token + 输出 Token ≤ Context Window
```

### 2. 为什么有这个限制？

**核心问题：注意力机制的计算复杂度是 O(n²)**

```
Token 数量翻倍 → 计算量翻 4 倍 → 显存需求翻 4 倍
```

| Context Window | 相对计算量 | 相对显存 |
|----------------|-----------|----------|
| 4K | 1x | 1x |
| 8K | 4x | 4x |
| 128K | 1024x | 1024x |

**推导链：**

```
1. Transformer 的注意力机制需要每个 Token 关注所有其他 Token
   ↓
2. 计算复杂度是 O(n²)，n 是 Token 数量
   ↓
3. Token 越多，计算量和显存需求指数增长
   ↓
4. 硬件有限，必须设置上限
   ↓
5. 这个上限就是 Context Window
```

### 3. 从第一性原理推导 RAG 应用

**推理链：**

```
1. LLM 有 Context Window 限制
   ↓
2. RAG 需要把检索内容塞进 Context
   ↓
3. 检索内容 + 问题 + 系统提示 + 生成空间 ≤ Context Window
   ↓
4. 必须精打细算每一个 Token
   ↓
5. Token 预算分配是 RAG 设计的核心问题
```

**RAG 的 Token 预算分配：**

```
Context Window (如 128K)
├── 系统提示词：~500 Token
├── 用户问题：~100 Token
├── 检索内容：~120K Token（主要空间）
└── 生成空间：~7K Token（留给输出）
```

### 4. 一句话总结第一性原理

**Token 是 LLM 的"文字积木"，Context Window 是"工作台大小"，因为注意力机制的 O(n²) 复杂度，必须限制 Token 数量，这直接决定了 RAG 能注入多少检索内容。**

---

**上一节：** [01_30字核心.md](./01_30字核心.md)
**下一节：** [03_核心概念.md](./03_核心概念.md)
