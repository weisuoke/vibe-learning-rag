# 实战代码4：LLM错误处理

**场景：** 处理 OpenAI API 的各种错误（Rate Limit、Timeout、API Error）

---

## 完整代码

```python
"""
LLM 错误处理
处理：Rate Limit、Timeout、API Error
"""

import asyncio
from openai import AsyncOpenAI, Timeout, RateLimitError, APIError
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import structlog

logger = structlog.get_logger()
client = AsyncOpenAI()

# ===== 自定义异常 =====
class LLMError(Exception):
    def __init__(self, provider: str, error: str):
        self.provider = provider
        self.error = error
        super().__init__(f"LLM 服务错误: {provider} - {error}")

class LLMTimeoutError(LLMError):
    pass

class LLMRateLimitError(LLMError):
    pass

# ===== LLM 调用（带错误处理）=====
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((Timeout, RateLimitError, APIError))
)
async def call_llm(prompt: str, timeout: float = 30) -> str:
    """调用 LLM（自动重试）"""
    try:
        async with asyncio.timeout(timeout):
            response = await client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                timeout=timeout - 5
            )
            return response.choices[0].message.content
    except Timeout:
        logger.error("llm_timeout", provider="OpenAI", timeout=timeout)
        raise LLMTimeoutError("OpenAI", f"请求超时（{timeout}s）")
    except RateLimitError as e:
        retry_after = int(e.response.headers.get("Retry-After", 60))
        logger.warning("llm_rate_limit", provider="OpenAI", retry_after=retry_after)
        raise LLMRateLimitError("OpenAI", f"请求频率过高，请等待 {retry_after}s")
    except APIError as e:
        logger.error("llm_api_error", provider="OpenAI", error=str(e))
        raise LLMError("OpenAI", str(e))

# ===== FastAPI 集成 =====
from fastapi import FastAPI, HTTPException

app = FastAPI()

@app.post("/chat")
async def chat(message: str):
    try:
        response = await call_llm(message)
        return {"response": response}
    except LLMTimeoutError:
        raise HTTPException(status_code=504, detail="AI 服务响应超时")
    except LLMRateLimitError as e:
        raise HTTPException(status_code=429, detail=e.error)
    except LLMError:
        raise HTTPException(status_code=503, detail="AI 服务暂时不可用")

# ===== 测试 =====
async def main():
    print("=== 测试1：正常调用 ===")
    response = await call_llm("你好")
    print(f"响应: {response}")

    print("\n=== 测试2：超时处理 ===")
    try:
        response = await call_llm("很长的提示词...", timeout=1)
    except LLMTimeoutError as e:
        print(f"超时: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 运行输出

```bash
python llm_error_handling.py

# 输出：
=== 测试1：正常调用 ===
响应: 你好！有什么我可以帮助你的吗？

=== 测试2：超时处理 ===
# 重试 1/3
# 重试 2/3
# 重试 3/3
超时: LLM 服务错误: OpenAI - 请求超时（1s）
```
