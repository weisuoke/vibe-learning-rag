# 核心概念2：递归字符分块（Recursive Character Splitting）

**按层级分隔符递归切分，优先在自然边界（段落、句子）处切分，是 LangChain 的默认策略，也是 2025-2026 年最推荐的通用方案。**

---

## 一句话定义

**递归字符分块是按照分隔符优先级（段落→句子→词→字符）递归切分文本的方法，通过在自然语义边界处切分来保持内容完整性，是平衡实现复杂度和效果的最佳通用策略。**

---

## 核心原理

### 分隔符优先级

```
优先级从高到低：

1. "\n\n"  → 段落边界（最自然的切分点）
2. "\n"    → 换行边界
3. "。"    → 句子边界（中文）
4. "."     → 句子边界（英文）
5. " "     → 单词边界
6. ""      → 字符边界（最后手段）

示例：
┌────────────────────────────────────────┐
│ 第一段内容...                           │
│                                        │
│ 第二段内容...                           │  ← 优先在 \n\n 处切分
│                                        │
│ 第三段是一个很长很长的段落，超过了       │
│ chunk_size，所以需要在句子边界切分。     │  ← 段落太长，在句号处切分
│ 这是第二句。这是第三句。                 │
└────────────────────────────────────────┘
```

### 递归切分流程

```
1. 尝试用最高优先级分隔符（\n\n）切分
   ↓
2. 检查每个部分是否 <= chunk_size
   ↓
3. 如果某部分 > chunk_size：
   - 用下一级分隔符（\n）继续切分
   ↓
4. 重复步骤 2-3，直到所有部分 <= chunk_size
   ↓
5. 或者用完所有分隔符，最后按字符切分
```

---

## Python 实现

### LangChain 实现（推荐）

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# NVIDIA 2025 推荐配置
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,           # NVIDIA: 适合事实查询
    chunk_overlap=77,         # 15% overlap（NVIDIA 最优）
    separators=["\n\n", "\n", "。", ".", "！", "？", " ", ""],
    length_function=len,
)

# 分块
text = """
第一段内容，介绍了Python的基本概念。

第二段内容，详细讲解了变量和数据类型。这是一个比较长的段落，
包含了很多重要的知识点。我们需要理解变量的本质是什么。

第三段内容，讲解了控制流程。
"""

chunks = splitter.split_text(text)
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1} ({len(chunk)} chars): {chunk[:50]}...")
```

### 手动实现（理解原理）

```python
def recursive_split(
    text: str,
    chunk_size: int = 512,
    separators: list[str] = None
) -> list[str]:
    """
    递归字符分块

    按分隔符优先级递归切分：
    1. 先尝试按段落切分（\n\n）
    2. 段落太长则按换行切分（\n）
    3. 还是太长则按句子切分（。！？）
    4. 最后按字符切分
    """
    if separators is None:
        separators = ["\n\n", "\n", "。", "！", "？", ".", "!", "?", " ", ""]

    def split_recursive(text: str, seps: list[str]) -> list[str]:
        if not seps or len(text) <= chunk_size:
            # 没有分隔符了或文本已经足够小
            if len(text) <= chunk_size:
                return [text]
            # 强制按字符切分
            return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

        sep = seps[0]
        if sep == "":
            # 空分隔符，按字符切分
            return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

        parts = text.split(sep)
        result = []
        current = ""

        for part in parts:
            if len(current) + len(part) + len(sep) <= chunk_size:
                current += part + sep
            else:
                if current:
                    result.append(current.strip())
                if len(part) > chunk_size:
                    # 这部分还是太长，用下一级分隔符继续切
                    result.extend(split_recursive(part, seps[1:]))
                else:
                    current = part + sep

        if current:
            result.append(current.strip())

        return result

    return split_recursive(text, separators)

# 使用示例
text = "你的长文本..."
chunks = recursive_split(text, chunk_size=512)
```

---

## 2025-2026 最佳配置

### 通用场景（NVIDIA 推荐）

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 事实查询（问答系统）
factual_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,  # 15%
    separators=["\n\n", "\n", "。", ".", "！", "？", " ", ""]
)

# 分析查询（文档摘要）
analytical_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1024,
    chunk_overlap=154,  # 15%
    separators=["\n\n", "\n", "。", ".", "！", "？", " ", ""]
)
```

**研究来源**: [NVIDIA 2025 Chunking Benchmark](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)

### 中文文档优化

```python
# 中文文档专用配置
chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,
    separators=[
        "\n\n",      # 段落（最优先）
        "\n",        # 换行
        "。",        # 句号
        "！",        # 感叹号
        "？",        # 问号
        "；",        # 分号
        "，",        # 逗号
        " ",         # 空格
        "",          # 字符（最后手段）
    ]
)
```

### Markdown 文档优化

```python
# Markdown 文档专用配置
markdown_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,
    separators=[
        "\n## ",     # 二级标题（最优先）
        "\n### ",    # 三级标题
        "\n\n",      # 段落
        "\n",        # 换行
        "。", ".",   # 句子
        " ",         # 单词
        "",          # 字符
    ]
)
```

### 代码文档优化

```python
# 代码文档专用配置
code_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,      # 代码需要更大的块
    chunk_overlap=160,   # 20% overlap（保持函数完整）
    separators=[
        "\n\n\n",    # 函数间空行（最优先）
        "\n\n",      # 段落
        "\n",        # 换行
        " ",         # 空格
        "",          # 字符
    ]
)
```

---

## 优缺点分析

### 优点

| 优点 | 说明 | 适用场景 |
|------|------|---------|
| ✅ **保持语义完整性** | 在自然边界切分，不破坏句子和段落 | 通用文档 |
| ✅ **适用范围广** | 适合多种文档类型 | 90% 的场景 |
| ✅ **实现成熟** | LangChain 默认策略，经过大量验证 | 生产环境 |
| ✅ **可定制性强** | 可调整分隔符列表适应不同文档 | 特殊格式 |
| ✅ **无额外成本** | 不需要 API 调用 | 成本敏感 |

### 缺点

| 缺点 | 说明 | 影响 |
|------|------|------|
| ❌ **块大小不完全均匀** | 优先保持语义，块大小会有波动 | 可接受 |
| ❌ **需要调整分隔符** | 不同语言和格式需要不同配置 | 需要经验 |
| ❌ **对特殊格式支持有限** | 表格、代码块可能被切断 | 需要专门处理 |

---

## 在 RAG 开发中的应用

### 适用场景（✅ 推荐）

1. **通用文档**
   - 文章、报告、书籍
   - 技术文档、API 文档
   - 示例：博客文章、技术白皮书

2. **Markdown 文档**
   - README、文档站点
   - 技术笔记、知识库
   - 示例：GitHub README、Notion 导出

3. **结构化文本**
   - 有明显段落分隔的内容
   - 有标题层级的文档
   - 示例：新闻报道、学术论文

4. **多语言文档**
   - 通过调整分隔符支持不同语言
   - 中英文混合文档
   - 示例：国际化产品文档

### 最佳实践

**实践1：根据文档类型选择分隔符**

```python
def create_splitter_by_type(doc_type: str) -> RecursiveCharacterTextSplitter:
    """根据文档类型创建最优分块器"""
    base_config = {
        "chunk_size": 512,
        "chunk_overlap": 77,
    }

    if doc_type == "markdown":
        return RecursiveCharacterTextSplitter(
            **base_config,
            separators=["\n## ", "\n### ", "\n\n", "\n", ".", " ", ""]
        )
    elif doc_type == "code":
        return RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=160,
            separators=["\n\n\n", "\n\n", "\n", " ", ""]
        )
    elif doc_type == "chinese":
        return RecursiveCharacterTextSplitter(
            **base_config,
            separators=["\n\n", "\n", "。", "！", "？", "；", "，", " ", ""]
        )
    else:
        return RecursiveCharacterTextSplitter(
            **base_config,
            separators=["\n\n", "\n", ".", " ", ""]
        )
```

**实践2：保留文档元数据**

```python
from langchain.document_loaders import PyPDFLoader

# 加载文档
loader = PyPDFLoader("document.pdf")
documents = loader.load()

# 分块（保留元数据）
splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=77)
chunks = splitter.split_documents(documents)

# 元数据被保留
for chunk in chunks:
    print(f"内容: {chunk.page_content[:100]}...")
    print(f"来源: {chunk.metadata['source']}")
    print(f"页码: {chunk.metadata['page']}")
```

**实践3：结合上下文感知（Anthropic 方法）**

```python
from openai import OpenAI

client = OpenAI()

def recursive_chunk_with_context(text: str) -> list[dict]:
    """递归分块 + 上下文感知（减少 49% 失败）"""
    # 1. 递归分块
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=77
    )
    chunks = splitter.split_text(text)

    # 2. 为每个 chunk 添加上下文
    contextual_chunks = []
    for i, chunk in enumerate(chunks):
        context = generate_context(chunk, text, i, len(chunks))
        contextual_chunks.append({
            'chunk': chunk,
            'context': context,
            'final_text': f"{context}\n\n{chunk}"
        })

    return contextual_chunks

def generate_context(chunk: str, full_text: str, index: int, total: int) -> str:
    """为 chunk 生成上下文（Anthropic 方法）"""
    prompt = f"""
    为文档片段生成 50-100 token 的上下文说明。

    文档开头：{full_text[:1000]}
    片段位置：第 {index+1}/{total} 块
    片段内容：{chunk[:200]}

    要求：说明片段在文档中的位置和主题。
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150
    )
    return response.choices[0].message.content.strip()
```

**研究来源**: [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)

---

## 与其他策略对比

### 递归字符 vs 固定大小

| 特性 | 递归字符 | 固定大小 |
|------|----------|----------|
| **语义完整性** | ✅✅ 好 | ❌ 差 |
| **实现复杂度** | ⭐⭐ 中等 | ⭐ 简单 |
| **块大小可控** | ✅ 大致可控 | ✅✅ 精确 |
| **处理速度** | 🚀🚀 快 | 🚀🚀🚀 最快 |
| **推荐场景** | **通用场景（首选）** | 快速原型 |

### 递归字符 vs 语义分块

| 特性 | 递归字符 | 语义分块 |
|------|----------|----------|
| **语义完整性** | ✅ 好 | ✅✅ 最好 |
| **API 成本** | 无 | 高（需要 embedding） |
| **处理速度** | 🚀🚀 快 | 🚀 慢 |
| **适用范围** | 广 | 特定场景 |
| **推荐场景** | **通用场景（首选）** | 高质量要求 |

---

## 实战示例

### 示例1：处理技术文档

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 加载技术文档
loader = TextLoader("technical_doc.md")
document = loader.load()

# 创建 Markdown 专用分块器
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,
    separators=["\n## ", "\n### ", "\n\n", "\n", ".", " ", ""]
)

# 分块
chunks = splitter.split_documents(document)

print(f"文档分块数: {len(chunks)}")
for i, chunk in enumerate(chunks[:3]):
    print(f"\nChunk {i+1}:")
    print(f"内容: {chunk.page_content[:100]}...")
    print(f"大小: {len(chunk.page_content)} 字符")
```

### 示例2：批量处理多种格式

```python
from pathlib import Path

def batch_process_documents(directory: str) -> dict:
    """批量处理目录下的所有文档"""
    results = {}

    for file_path in Path(directory).glob('*'):
        if file_path.suffix == '.md':
            splitter = create_splitter_by_type("markdown")
        elif file_path.suffix == '.py':
            splitter = create_splitter_by_type("code")
        else:
            splitter = create_splitter_by_type("general")

        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        chunks = splitter.split_text(text)
        results[file_path.name] = {
            'chunks': chunks,
            'count': len(chunks),
            'avg_size': sum(len(c) for c in chunks) // len(chunks)
        }

    return results
```

### 示例3：验证分块质量

```python
def validate_recursive_chunks(chunks: list[str], target_size: int = 512):
    """验证递归分块的质量"""
    sizes = [len(c) for c in chunks]

    print(f"总块数: {len(chunks)}")
    print(f"目标大小: {target_size}")
    print(f"实际大小: 最小={min(sizes)}, 最大={max(sizes)}, 平均={sum(sizes)//len(sizes)}")

    # 检查语义完整性（是否在句子边界切分）
    complete_sentences = sum(
        1 for c in chunks
        if c.strip().endswith(('.', '。', '!', '！', '?', '？'))
    )
    print(f"完整句子结尾: {complete_sentences}/{len(chunks)} ({complete_sentences/len(chunks):.1%})")

    # 检查段落保持
    paragraph_breaks = sum(1 for c in chunks if '\n\n' in c)
    print(f"包含段落分隔: {paragraph_breaks}/{len(chunks)} ({paragraph_breaks/len(chunks):.1%})")
```

---

## 常见问题

### Q1: 递归字符分块适合生产环境吗？

**A**: ✅ **非常适合**，这是 2025-2026 年最推荐的通用方案。

- LangChain 默认策略
- 经过大量生产验证
- 平衡了效果和成本
- NVIDIA 2025 研究验证有效

### Q2: 如何选择分隔符列表？

**A**: 根据文档类型选择：

- **通用文档**: `["\n\n", "\n", ".", " ", ""]`
- **中文文档**: `["\n\n", "\n", "。", "！", "？", "，", " ", ""]`
- **Markdown**: `["\n## ", "\n### ", "\n\n", "\n", ".", " ", ""]`
- **代码**: `["\n\n\n", "\n\n", "\n", " ", ""]`

### Q3: 块大小不均匀会影响效果吗？

**A**: **影响很小**。

- 语义完整性 > 块大小均匀
- NVIDIA 研究显示递归分块效果好
- 实际应用中块大小波动在 ±20% 是可接受的

### Q4: 如何处理特殊格式（表格、代码块）？

**A**: 使用专门的分隔符：

```python
# 保持代码块完整
code_aware_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    separators=[
        "\n```\n",   # 代码块边界
        "\n\n\n",    # 函数间空行
        "\n\n",      # 段落
        "\n",        # 换行
    ]
)
```

---

## 核心研究来源

1. **NVIDIA 2025**: [Finding the Best Chunking Strategy](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)
   - 验证递归分块有效性
   - 15% 重叠率最优
   - 查询类型决定块大小

2. **Anthropic 2024-2025**: [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)
   - 可与递归分块结合
   - 减少 49-67% 检索失败

3. **LangChain Documentation**: 递归字符分块是默认推荐策略

---

## 下一步学习

**同级概念：**
- ← [03_核心概念_01_固定大小分块](./03_核心概念_01_固定大小分块.md) - 最简单的方法
- → [03_核心概念_03_语义分块](./03_核心概念_03_语义分块.md) - 基于语义相似度

**进阶技术：**
- → [03_核心概念_04_代理式分块Agentic](./03_核心概念_04_代理式分块Agentic.md) - IBM 2025-2026 最新技术
- → [03_核心概念_05_上下文感知分块Contextual](./03_核心概念_05_上下文感知分块Contextual.md) - Anthropic 方法

**实战代码：**
- → [07_实战代码_02_递归分块实现](./07_实战代码_02_递归分块实现.md) - 完整实现代码
