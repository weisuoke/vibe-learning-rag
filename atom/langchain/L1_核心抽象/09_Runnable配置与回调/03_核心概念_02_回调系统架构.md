# 核心概念 02：回调系统架构

> **深入理解 LangChain 回调系统的生命周期、执行流程和事件传播机制**

---

## 概述

LangChain 的回调系统是一个**事件驱动架构**，在链执行的不同阶段触发回调函数，实现监控、日志、成本追踪等功能。

**核心特点**：
- **非侵入式**：业务逻辑与监控逻辑解耦
- **可组合**：多个回调可以同时使用
- **异步支持**：支持同步和异步回调
- **事件丰富**：覆盖 LLM、Chain、Tool、Retriever、Agent 等所有组件

**引用**：
> "Callbacks provide hooks into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks."
> — [LangChain Custom Callbacks Guide](https://python.langchain.com/docs/how_to/custom_callbacks/)

---

## 回调生命周期

### 完整生命周期图

```
用户调用
  ↓
on_chain_start (Chain 开始)
  ↓
on_llm_start (LLM 开始)
  ↓
on_llm_new_token (流式输出，可选)
  ↓
on_llm_end (LLM 结束) 或 on_llm_error (LLM 失败)
  ↓
on_chain_end (Chain 结束) 或 on_chain_error (Chain 失败)
  ↓
返回结果
```

### 代码示例

```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI

class LifecycleLogger(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        print("1. Chain 开始执行")

    def on_llm_start(self, serialized, prompts, **kwargs):
        print("2. LLM 开始调用")

    def on_llm_new_token(self, token: str, **kwargs):
        print(f"3. 生成 token: {token}")

    def on_llm_end(self, response, **kwargs):
        print("4. LLM 调用结束")

    def on_chain_end(self, outputs, **kwargs):
        print("5. Chain 执行结束")

# 使用
llm = ChatOpenAI(streaming=True)
llm.invoke("你好", config={"callbacks": [LifecycleLogger()]})
```

**输出**：
```
1. Chain 开始执行
2. LLM 开始调用
3. 生成 token: 你
3. 生成 token: 好
3. 生成 token: ！
4. LLM 调用结束
5. Chain 执行结束
```

---

## 事件类型详解

### 1. LLM 事件

#### on_llm_start

**触发时机**：LLM 开始调用前

**参数**：
```python
def on_llm_start(
    self,
    serialized: Dict[str, Any],  # LLM 配置信息
    prompts: List[str],          # 输入的 prompts
    **kwargs                     # tags, metadata, run_id 等
) -> None:
```

**使用场景**：
- 记录输入 prompt
- 开始计时
- 记录请求元数据

**示例**：
```python
class InputLogger(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        metadata = kwargs.get("metadata", {})
        logger.info(f"LLM 调用开始", extra={
            "prompts": prompts,
            "user_id": metadata.get("user_id"),
            "model": serialized.get("name")
        })
```

---

#### on_llm_end

**触发时机**：LLM 调用成功结束后

**参数**：
```python
def on_llm_end(
    self,
    response: LLMResult,  # LLM 响应对象
    **kwargs
) -> None:
```

**LLMResult 结构**：
```python
response.generations[0][0].text  # 生成的文本
response.llm_output["token_usage"]  # Token 使用量
  - prompt_tokens: int
  - completion_tokens: int
  - total_tokens: int
response.llm_output["model_name"]  # 使用的模型
```

**使用场景**：
- 成本追踪
- 性能监控
- 输出记录

**示例**：
```python
class CostTracker(BaseCallbackHandler):
    def __init__(self):
        self.start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time

        # 获取 token 使用量
        usage = response.llm_output.get("token_usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        # 计算成本（GPT-4 价格）
        cost = prompt_tokens * 0.00003 + completion_tokens * 0.00006

        logger.info(f"LLM 调用完成", extra={
            "duration": duration,
            "tokens": usage.get("total_tokens", 0),
            "cost": cost
        })
```

---

#### on_llm_error

**触发时机**：LLM 调用失败时

**参数**：
```python
def on_llm_error(
    self,
    error: Union[Exception, KeyboardInterrupt],
    **kwargs
) -> None:
```

**使用场景**：
- 错误告警
- 失败重试
- 降级策略

**示例**：
```python
class ErrorHandler(BaseCallbackHandler):
    def on_llm_error(self, error, **kwargs):
        metadata = kwargs.get("metadata", {})

        # 记录错误
        logger.error(f"LLM 调用失败: {error}", extra={
            "user_id": metadata.get("user_id"),
            "error_type": type(error).__name__
        })

        # 发送告警
        if "rate_limit" in str(error).lower():
            alert("触发 API 限流", error=str(error))
        else:
            alert("LLM 调用失败", error=str(error))
```

---

#### on_llm_new_token

**触发时机**：流式输出时，每生成一个 token

**参数**：
```python
def on_llm_new_token(
    self,
    token: str,  # 新生成的 token
    **kwargs
) -> None:
```

**使用场景**：
- 实时显示输出
- 流式监控
- 速度统计

**示例**：
```python
class StreamMonitor(BaseCallbackHandler):
    def __init__(self):
        self.tokens = []
        self.start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs):
        self.tokens.append(token)

        # 实时速度统计
        elapsed = time.time() - self.start_time
        tokens_per_second = len(self.tokens) / elapsed

        print(f"Token: {token} | 速度: {tokens_per_second:.1f} tokens/s")
```

---

### 2. Chain 事件

#### on_chain_start

**触发时机**：Chain 开始执行前

**参数**：
```python
def on_chain_start(
    self,
    serialized: Dict[str, Any],  # Chain 配置信息
    inputs: Dict[str, Any],      # 输入数据
    **kwargs
) -> None:
```

**使用场景**：
- 链路追踪
- 输入记录
- 开始计时

---

#### on_chain_end

**触发时机**：Chain 执行成功结束后

**参数**：
```python
def on_chain_end(
    self,
    outputs: Dict[str, Any],  # 输出数据
    **kwargs
) -> None:
```

**使用场景**：
- 输出记录
- 性能统计
- 链路完成标记

---

#### on_chain_error

**触发时机**：Chain 执行失败时

**参数**：
```python
def on_chain_error(
    self,
    error: Union[Exception, KeyboardInterrupt],
    **kwargs
) -> None:
```

---

### 3. Tool 事件

#### on_tool_start

**触发时机**：工具开始调用前

**参数**：
```python
def on_tool_start(
    self,
    serialized: Dict[str, Any],  # 工具配置
    input_str: str,              # 工具输入
    **kwargs
) -> None:
```

---

#### on_tool_end

**触发时机**：工具调用成功结束后

**参数**：
```python
def on_tool_end(
    self,
    output: str,  # 工具输出
    **kwargs
) -> None:
```

---

#### on_tool_error

**触发时机**：工具调用失败时

---

### 4. Retriever 事件

#### on_retriever_start

**触发时机**：检索开始前

**参数**：
```python
def on_retriever_start(
    self,
    serialized: Dict[str, Any],
    query: str,  # 检索查询
    **kwargs
) -> None:
```

---

#### on_retriever_end

**触发时机**：检索完成后

**参数**：
```python
def on_retriever_end(
    self,
    documents: List[Document],  # 检索到的文档
    **kwargs
) -> None:
```

**使用场景**：
- 检索质量监控
- 文档数量统计
- 相似度分析

**示例**：
```python
class RetrievalMonitor(BaseCallbackHandler):
    def on_retriever_end(self, documents, **kwargs):
        num_docs = len(documents)
        scores = [d.metadata.get("score", 0) for d in documents]
        avg_score = sum(scores) / num_docs if num_docs > 0 else 0

        logger.info(f"检索完成", extra={
            "num_docs": num_docs,
            "avg_score": avg_score
        })

        # 告警：检索质量低
        if avg_score < 0.5:
            alert("检索质量低", avg_score=avg_score)
```

---

### 5. Agent 事件

#### on_agent_action

**触发时机**：Agent 执行动作时

**参数**：
```python
def on_agent_action(
    self,
    action: AgentAction,  # Agent 动作
    **kwargs
) -> None:
```

---

#### on_agent_finish

**触发时机**：Agent 完成任务时

**参数**：
```python
def on_agent_finish(
    self,
    finish: AgentFinish,  # Agent 完成信息
    **kwargs
) -> None:
```

---

## 回调执行顺序

### 单链执行顺序

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 创建链
chain = ChatOpenAI() | StrOutputParser()

# 执行顺序：
# 1. on_chain_start (整个链)
# 2. on_llm_start (ChatOpenAI)
# 3. on_llm_end (ChatOpenAI)
# 4. on_chain_end (整个链)
```

### 多回调执行顺序

```python
class CallbackA(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print("A: llm_start")

    def on_llm_end(self, response, **kwargs):
        print("A: llm_end")

class CallbackB(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print("B: llm_start")

    def on_llm_end(self, response, **kwargs):
        print("B: llm_end")

# 使用
llm.invoke("你好", config={"callbacks": [CallbackA(), CallbackB()]})

# 输出：
# A: llm_start
# B: llm_start
# A: llm_end
# B: llm_end
```

**规则**：
1. **同一事件**：按 callbacks 列表顺序执行
2. **不同事件**：按生命周期顺序执行
3. **父子链**：父链回调先于子链

---

### 嵌套链执行顺序

```python
from langchain_core.runnables import RunnablePassthrough

# 嵌套链
chain = (
    RunnablePassthrough()
    | ChatOpenAI()
    | StrOutputParser()
)

# 执行顺序：
# 1. on_chain_start (外层链)
# 2. on_chain_start (RunnablePassthrough)
# 3. on_chain_end (RunnablePassthrough)
# 4. on_llm_start (ChatOpenAI)
# 5. on_llm_end (ChatOpenAI)
# 6. on_chain_start (StrOutputParser)
# 7. on_chain_end (StrOutputParser)
# 8. on_chain_end (外层链)
```

---

## 事件传播机制

### 父子链传播

```python
from langchain_core.callbacks import BaseCallbackHandler

class ParentCallback(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        print(f"父链开始: {kwargs.get('run_id')}")

# 父链配置
parent_config = RunnableConfig(
    callbacks=[ParentCallback()],
    tags=["parent"]
)

# 子链自动继承父链的配置
chain = ChatOpenAI() | StrOutputParser()
chain.invoke("你好", config=parent_config)

# 父链的回调会传播到所有子链
```

### 配置合并

```python
# 父链配置
parent_config = RunnableConfig(
    callbacks=[callback1],
    tags=["parent"]
)

# 子链配置
child_config = RunnableConfig(
    callbacks=[callback2],
    tags=["child"]
)

# 合并规则：子链配置覆盖父链配置
# 最终配置：callbacks=[callback2], tags=["child"]
```

---

## 回调过滤机制

### 使用 tags 过滤

```python
class ProductionOnlyCallback(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        tags = kwargs.get("tags", [])

        # 只在生产环境执行
        if "production" not in tags:
            return

        # 生产环境的监控逻辑
        send_to_monitoring(prompts)

# 使用
config_prod = RunnableConfig(tags=["production"])
config_dev = RunnableConfig(tags=["development"])

llm.invoke("你好", config=config_prod)  # 回调执行
llm.invoke("你好", config=config_dev)   # 回调不执行
```

### 使用 ignore_* 参数

```python
class ChainOnlyCallback(BaseCallbackHandler):
    # 忽略 LLM 事件
    ignore_llm = True

    def on_chain_start(self, serialized, inputs, **kwargs):
        print("Chain 开始")  # 会执行

    def on_llm_start(self, serialized, prompts, **kwargs):
        print("LLM 开始")  # 不会执行（被忽略）
```

**可用的 ignore 参数**：
- `ignore_llm = True`：忽略 LLM 事件
- `ignore_chain = True`：忽略 Chain 事件
- `ignore_agent = True`：忽略 Agent 事件
- `ignore_retriever = True`：忽略 Retriever 事件

---

## 异步回调

### AsyncCallbackHandler

```python
from langchain_core.callbacks import AsyncCallbackHandler
import asyncio

class AsyncMonitor(AsyncCallbackHandler):
    async def on_llm_start(self, serialized, prompts, **kwargs):
        # 异步操作
        await asyncio.sleep(0.1)
        print("异步回调：LLM 开始")

    async def on_llm_end(self, response, **kwargs):
        # 异步发送到监控系统
        await send_to_monitoring_async(response)

# 使用
llm = ChatOpenAI()
await llm.ainvoke("你好", config={"callbacks": [AsyncMonitor()]})
```

### 同步 vs 异步

| 特性 | 同步回调 | 异步回调 |
|------|---------|---------|
| **基类** | `BaseCallbackHandler` | `AsyncCallbackHandler` |
| **执行方式** | 阻塞执行 | 非阻塞执行 |
| **性能** | 回调耗时会影响主流程 | 回调并行执行 |
| **使用场景** | 简单日志、计数 | 网络请求、数据库写入 |

---

## 回调异常处理

### 异常不会中断主流程

```python
class BuggyCallback(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        raise Exception("回调出错了！")

# 回调异常不会影响主流程
result = llm.invoke("你好", config={"callbacks": [BuggyCallback()]})
print(result)  # 正常输出

# 异常会被记录到日志
# WARNING: Error in callback: 回调出错了！
```

### 最佳实践：主动捕获异常

```python
class SafeCallback(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        try:
            # 可能出错的逻辑
            self.send_to_monitoring(response)
        except Exception as e:
            logger.error(f"监控发送失败: {e}", exc_info=True)
            # 不抛出异常，避免影响其他回调
```

---

## 完整示例：生产监控系统

```python
import time
import logging
from collections import defaultdict
from langchain_core.callbacks import BaseCallbackHandler

logger = logging.getLogger(__name__)

class ProductionMonitor(BaseCallbackHandler):
    """生产环境监控回调"""

    def __init__(self):
        self.start_time = None
        self.user_costs = defaultdict(float)
        self.user_tokens = defaultdict(int)

    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLM 开始调用"""
        self.start_time = time.time()
        metadata = kwargs.get("metadata", {})

        logger.info("LLM 调用开始", extra={
            "user_id": metadata.get("user_id"),
            "prompts": prompts,
            "model": serialized.get("name")
        })

    def on_llm_end(self, response, **kwargs):
        """LLM 调用结束"""
        duration = time.time() - self.start_time
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        # 获取 token 使用量
        usage = response.llm_output.get("token_usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        total_tokens = usage.get("total_tokens", 0)

        # 计算成本
        cost = prompt_tokens * 0.00003 + completion_tokens * 0.00006

        # 按用户归因
        self.user_costs[user_id] += cost
        self.user_tokens[user_id] += total_tokens

        # 记录指标
        logger.info("LLM 调用完成", extra={
            "user_id": user_id,
            "duration": duration,
            "tokens": total_tokens,
            "cost": cost
        })

        # 告警：延迟过高
        if duration > 5.0:
            self.alert("LLM 调用延迟过高", duration=duration)

        # 告警：成本过高
        if cost > 0.1:
            self.alert("单次调用成本过高", cost=cost)

    def on_llm_error(self, error, **kwargs):
        """LLM 调用失败"""
        metadata = kwargs.get("metadata", {})

        logger.error(f"LLM 调用失败: {error}", extra={
            "user_id": metadata.get("user_id"),
            "error_type": type(error).__name__
        })

        # 发送告警
        self.alert("LLM 调用失败", error=str(error))

    def alert(self, message: str, **kwargs):
        """发送告警"""
        try:
            # 发送到告警系统（Slack、钉钉、PagerDuty 等）
            requests.post(
                "https://hooks.slack.com/...",
                json={"text": message, **kwargs},
                timeout=5
            )
        except Exception as e:
            logger.error(f"告警发送失败: {e}")

    def get_report(self):
        """生成成本报告"""
        report = []
        for user_id in sorted(self.user_costs.keys()):
            report.append({
                "user_id": user_id,
                "total_cost": f"${self.user_costs[user_id]:.4f}",
                "total_tokens": self.user_tokens[user_id]
            })
        return report

# 使用
monitor = ProductionMonitor()

config = RunnableConfig(
    callbacks=[monitor],
    tags=["production"],
    metadata={"user_id": "user_123", "env": "prod"}
)

chain.invoke({"input": "你好"}, config=config)

# 生成报告
print(monitor.get_report())
```

---

## 总结

### 回调系统核心特点

1. **事件驱动**：在链执行的不同阶段触发回调
2. **非侵入式**：业务逻辑与监控逻辑解耦
3. **可组合**：多个回调可以同时使用
4. **异常安全**：回调异常不会中断主流程
5. **性能友好**：开销 < 0.5%

### 事件类型速查

| 组件 | 开始事件 | 结束事件 | 错误事件 | 流式事件 |
|------|---------|---------|---------|---------|
| **LLM** | `on_llm_start` | `on_llm_end` | `on_llm_error` | `on_llm_new_token` |
| **Chain** | `on_chain_start` | `on_chain_end` | `on_chain_error` | - |
| **Tool** | `on_tool_start` | `on_tool_end` | `on_tool_error` | - |
| **Retriever** | `on_retriever_start` | `on_retriever_end` | `on_retriever_error` | - |
| **Agent** | - | `on_agent_finish` | - | `on_agent_action` |

### 执行顺序规则

1. **同一事件**：按 callbacks 列表顺序执行
2. **不同事件**：按生命周期顺序执行
3. **父子链**：父链回调先于子链
4. **异常处理**：回调异常不会中断主流程

---

## 参考资料

- [BaseCallbackHandler API Reference](https://reference.langchain.com/v0.3/python/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html)
- [LangChain Custom Callbacks Guide](https://python.langchain.com/docs/how_to/custom_callbacks/)
- [AsyncCallbackHandler API Reference](https://reference.langchain.com/v0.3/python/core/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html)
