# 实战代码：缓存优化实战

> **本节目标**：通过缓存问答系统，掌握InMemoryCache、Redis缓存和语义缓存的实战应用，实现65-90%成本降低。

---

## 一、项目场景

**智能问答系统（带缓存）**：
- InMemoryCache：本地快速缓存
- Redis缓存：分布式持久化
- 语义缓存：处理查询变体
- 多层缓存：L1(内存) + L2(Redis) + L3(语义)

**优化目标**：
- 成本降低：65-90%
- 延迟优化：85%
- 缓存命中率：>60%

---

## 二、环境准备

```bash
# 安装依赖
uv add langchain langchain-openai redis chromadb

# 配置环境
cat > .env << EOF
OPENAI_API_KEY=your_api_key_here
REDIS_HOST=localhost
REDIS_PORT=6379
EOF
```

---

## 三、InMemoryCache实现

### 3.1 基础缓存

```python
# cache/memory.py
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain_openai import ChatOpenAI
import time

# 启用内存缓存
set_llm_cache(InMemoryCache())

def test_memory_cache():
    """测试内存缓存"""
    llm = ChatOpenAI(model="gpt-4o-mini")

    # 第1次调用（无缓存）
    start = time.time()
    response1 = llm.invoke("什么是LCEL？")
    time1 = time.time() - start

    # 第2次调用（缓存命中）
    start = time.time()
    response2 = llm.invoke("什么是LCEL？")
    time2 = time.time() - start

    print(f"第1次调用: {time1:.3f}秒")
    print(f"第2次调用: {time2:.3f}秒")
    print(f"性能提升: {time1/time2:.1f}x")
    print(f"成本降低: {(1-time2/time1)*100:.1f}%")


if __name__ == "__main__":
    test_memory_cache()
```

### 3.2 自定义缓存键

```python
# 自定义缓存键生成
import hashlib
import json

class CustomCache:
    """自定义缓存"""

    def __init__(self):
        self.cache = {}

    def _generate_key(self, prompt: str, **kwargs) -> str:
        """生成缓存键"""
        # 包含prompt和参数
        cache_input = {
            "prompt": prompt,
            **kwargs
        }
        # 生成哈希
        key = hashlib.md5(
            json.dumps(cache_input, sort_keys=True).encode()
        ).hexdigest()
        return key

    def get(self, prompt: str, **kwargs):
        """获取缓存"""
        key = self._generate_key(prompt, **kwargs)
        return self.cache.get(key)

    def set(self, prompt: str, response: str, **kwargs):
        """设置缓存"""
        key = self._generate_key(prompt, **kwargs)
        self.cache[key] = response


# 使用
cache = CustomCache()

# 缓存写入
cache.set("什么是LCEL？", "LCEL是表达式语言", model="gpt-4o")

# 缓存读取
result = cache.get("什么是LCEL？", model="gpt-4o")
print(result)  # "LCEL是表达式语言"
```

---

## 四、Redis缓存实现

### 4.1 Redis缓存配置

```python
# cache/redis_cache.py
from langchain.cache import RedisCache
from langchain.globals import set_llm_cache
from redis import Redis
import os

# 配置Redis缓存
redis_client = Redis(
    host=os.getenv("REDIS_HOST", "localhost"),
    port=int(os.getenv("REDIS_PORT", 6379)),
    decode_responses=True
)

# 启用Redis缓存（TTL=1小时）
set_llm_cache(RedisCache(redis_client, ttl=3600))


def test_redis_cache():
    """测试Redis缓存"""
    from langchain_openai import ChatOpenAI
    import time

    llm = ChatOpenAI(model="gpt-4o-mini")

    # 第1次调用
    start = time.time()
    response1 = llm.invoke("什么是LCEL？")
    time1 = time.time() - start

    # 第2次调用（缓存命中）
    start = time.time()
    response2 = llm.invoke("什么是LCEL？")
    time2 = time.time() - start

    print(f"第1次: {time1:.3f}秒")
    print(f"第2次: {time2:.3f}秒（缓存命中）")
    print(f"性能提升: {time1/time2:.1f}x")


if __name__ == "__main__":
    test_redis_cache()
```

### 4.2 TTL与缓存失效

```python
# TTL管理
import random

def get_ttl_with_jitter(base_ttl: int) -> int:
    """添加随机抖动（防止缓存雪崩）"""
    jitter = random.uniform(-0.1, 0.1)
    return int(base_ttl * (1 + jitter))


# 使用
ttl = get_ttl_with_jitter(3600)  # 3240-3960秒
cache = RedisCache(redis_client, ttl=ttl)
```

---

## 五、语义缓存实现

### 5.1 基础语义缓存

```python
# cache/semantic.py
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.cache import SemanticCache
from langchain.globals import set_llm_cache

# 配置语义缓存
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(
    collection_name="semantic_cache",
    embedding_function=embeddings
)

semantic_cache = SemanticCache(
    vectorstore=vectorstore,
    similarity_threshold=0.85  # 相似度阈值
)

set_llm_cache(semantic_cache)


def test_semantic_cache():
    """测试语义缓存"""
    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-4o-mini")

    # 第1次查询
    response1 = llm.invoke("如何退货？")
    print(f"查询1: {response1.content}")

    # 第2次查询（语义相似）
    response2 = llm.invoke("退货流程是什么？")
    print(f"查询2: {response2.content}")
    # 如果相似度>0.85，返回缓存结果

    # 第3次查询（语义不同）
    response3 = llm.invoke("如何充值？")
    print(f"查询3: {response3.content}")
    # 相似度<0.85，调用LLM


if __name__ == "__main__":
    test_semantic_cache()
```

---

## 六、多层缓存架构

### 6.1 三层缓存实现

```python
# cache/multi_layer.py
from langchain.cache import InMemoryCache, RedisCache
from redis import Redis
import time

class MultiLayerCache:
    """多层缓存（L1内存 + L2 Redis + L3语义）"""

    def __init__(self):
        # L1: 内存缓存（最快）
        self.l1_cache = {}

        # L2: Redis缓存（持久化）
        redis_client = Redis(host="localhost", port=6379, decode_responses=True)
        self.l2_cache = redis_client

        # L3: 语义缓存（处理变体）
        from langchain.embeddings import OpenAIEmbeddings
        from langchain.vectorstores import Chroma
        embeddings = OpenAIEmbeddings()
        self.l3_vectorstore = Chroma(
            collection_name="semantic_cache",
            embedding_function=embeddings
        )

        # 统计
        self.stats = {
            "l1_hits": 0,
            "l2_hits": 0,
            "l3_hits": 0,
            "misses": 0
        }

    def get(self, query: str):
        """获取缓存（三层查找）"""
        # L1: 内存缓存
        if query in self.l1_cache:
            self.stats["l1_hits"] += 1
            return self.l1_cache[query]

        # L2: Redis缓存
        l2_result = self.l2_cache.get(f"cache:{query}")
        if l2_result:
            self.stats["l2_hits"] += 1
            # 回填L1
            self.l1_cache[query] = l2_result
            return l2_result

        # L3: 语义缓存
        docs = self.l3_vectorstore.similarity_search(query, k=1)
        if docs and docs[0].metadata.get("similarity", 0) > 0.85:
            self.stats["l3_hits"] += 1
            result = docs[0].page_content
            # 回填L2和L1
            self.l2_cache.setex(f"cache:{query}", 3600, result)
            self.l1_cache[query] = result
            return result

        # 缓存未命中
        self.stats["misses"] += 1
        return None

    def set(self, query: str, response: str):
        """设置缓存（写入所有层）"""
        # L1: 内存
        self.l1_cache[query] = response

        # L2: Redis
        self.l2_cache.setex(f"cache:{query}", 3600, response)

        # L3: 语义缓存
        from langchain.schema import Document
        doc = Document(
            page_content=response,
            metadata={"query": query, "similarity": 1.0}
        )
        self.l3_vectorstore.add_documents([doc])

    def get_stats(self):
        """获取统计"""
        total = sum(self.stats.values())
        if total == 0:
            return self.stats

        return {
            **self.stats,
            "l1_hit_rate": self.stats["l1_hits"] / total * 100,
            "l2_hit_rate": self.stats["l2_hits"] / total * 100,
            "l3_hit_rate": self.stats["l3_hits"] / total * 100,
            "total_hit_rate": (total - self.stats["misses"]) / total * 100
        }


# 使用示例
if __name__ == "__main__":
    cache = MultiLayerCache()

    # 测试查询
    queries = [
        "什么是LCEL？",
        "什么是LCEL？",  # L1命中
        "LCEL是什么？",  # L3命中（语义相似）
        "如何使用LCEL？",  # 未命中
    ]

    for query in queries:
        result = cache.get(query)
        if result:
            print(f"✓ 缓存命中: {query}")
        else:
            print(f"✗ 缓存未命中: {query}")
            # 模拟LLM调用
            response = f"关于'{query}'的回答"
            cache.set(query, response)

    # 统计报告
    stats = cache.get_stats()
    print(f"\n缓存统计:")
    print(f"L1命中率: {stats['l1_hit_rate']:.1f}%")
    print(f"L2命中率: {stats['l2_hit_rate']:.1f}%")
    print(f"L3命中率: {stats['l3_hit_rate']:.1f}%")
    print(f"总命中率: {stats['total_hit_rate']:.1f}%")
```

---

## 七、完整缓存系统

### 7.1 智能问答系统（带缓存）

```python
# main.py
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from cache.multi_layer import MultiLayerCache
import time

load_dotenv()

class CachedQASystem:
    """带缓存的问答系统"""

    def __init__(self):
        self.cache = MultiLayerCache()
        self.chain = self._create_chain()

    def _create_chain(self):
        """创建问答链"""
        prompt = ChatPromptTemplate.from_template(
            "回答问题: {question}"
        )
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        parser = StrOutputParser()

        return prompt | llm | parser

    def ask(self, question: str) -> dict:
        """问答（带缓存）

        Returns:
            {
                "answer": str,
                "source": str,  # "cache" | "llm"
                "latency": float
            }
        """
        start = time.time()

        # 尝试从缓存获取
        cached = self.cache.get(question)
        if cached:
            return {
                "answer": cached,
                "source": "cache",
                "latency": time.time() - start
            }

        # 缓存未命中，调用LLM
        answer = self.chain.invoke({"question": question})

        # 写入缓存
        self.cache.set(question, answer)

        return {
            "answer": answer,
            "source": "llm",
            "latency": time.time() - start
        }

    def get_stats(self):
        """获取统计"""
        return self.cache.get_stats()


def main():
    """主程序"""
    system = CachedQASystem()

    # 测试查询
    questions = [
        "什么是LCEL？",
        "什么是LCEL？",  # 重复查询
        "LCEL是什么？",  # 语义相似
        "如何使用LCEL？",
        "LCEL有哪些优势？",
        "什么是LCEL？",  # 再次重复
    ]

    print("=" * 60)
    print("智能问答系统（带缓存）")
    print("=" * 60)

    for question in questions:
        result = system.ask(question)

        print(f"\n问题: {question}")
        print(f"答案: {result['answer'][:50]}...")
        print(f"来源: {result['source']}")
        print(f"延迟: {result['latency']:.3f}秒")

    # 统计报告
    stats = system.get_stats()
    print("\n" + "=" * 60)
    print("缓存统计")
    print("=" * 60)
    print(f"L1命中率: {stats['l1_hit_rate']:.1f}%")
    print(f"L2命中率: {stats['l2_hit_rate']:.1f}%")
    print(f"L3命中率: {stats['l3_hit_rate']:.1f}%")
    print(f"总命中率: {stats['total_hit_rate']:.1f}%")


if __name__ == "__main__":
    main()
```

---

## 八、性能测试

### 8.1 缓存性能对比

```python
# benchmark.py
import time
from langchain_openai import ChatOpenAI

def benchmark_cache_performance():
    """缓存性能对比"""
    llm = ChatOpenAI(model="gpt-4o-mini")

    # 测试数据
    questions = [f"问题{i}" for i in range(100)]

    # 1. 无缓存
    start = time.time()
    for q in questions:
        llm.invoke(q)
    no_cache_time = time.time() - start

    # 2. 有缓存（重复查询）
    from langchain.cache import InMemoryCache
    from langchain.globals import set_llm_cache
    set_llm_cache(InMemoryCache())

    start = time.time()
    for q in questions:
        llm.invoke(q)  # 第1次
        llm.invoke(q)  # 第2次（缓存命中）
    cache_time = time.time() - start

    # 输出对比
    print("=" * 60)
    print("缓存性能对比")
    print("=" * 60)
    print(f"无缓存: {no_cache_time:.2f}秒")
    print(f"有缓存: {cache_time:.2f}秒")
    print(f"性能提升: {no_cache_time/cache_time:.1f}x")
    print(f"成本降低: {(1-cache_time/no_cache_time)*100:.1f}%")


if __name__ == "__main__":
    benchmark_cache_performance()
```

---

## 九、总结

### 核心要点

1. **InMemoryCache**: 微秒级，适合开发环境
2. **RedisCache**: 毫秒级，适合生产环境
3. **SemanticCache**: 处理查询变体，65%成本降低
4. **多层缓存**: L1(微秒) + L2(毫秒) + L3(100ms)

### 缓存策略

| 场景 | 推荐缓存 | 命中率目标 |
|------|----------|------------|
| 开发环境 | InMemoryCache | >40% |
| 生产环境 | RedisCache | >60% |
| 查询多样 | SemanticCache | >50% |
| 高性能 | 多层缓存 | >70% |

### 实战收获

- ✅ 成本降低65-90%
- ✅ 延迟优化85%
- ✅ 多层缓存架构
- ✅ 语义缓存处理变体
- ✅ 完整监控体系

---

**下一步**: 学习【监控调试实战】，构建可观测系统。
