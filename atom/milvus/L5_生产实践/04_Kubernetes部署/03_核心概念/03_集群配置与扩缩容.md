# 核心概念3：集群配置与扩缩容

## 什么是集群配置与扩缩容？

**集群配置 = 定义Milvus集群的资源、拓扑和行为**
**扩缩容 = 根据负载动态调整集群规模**

一句话定义：**通过调整副本数、资源配置和拓扑结构，使Milvus集群适应不同的负载和性能需求。**

---

## 为什么需要集群配置与扩缩容？

### 问题：固定配置的局限性

```yaml
# 固定配置的问题
queryNode:
  replicas: 5  # 固定5个副本
  resources:
    cpu: 4
    memory: 16Gi

# 问题：
# 1. 高峰期（1000 QPS）：5个副本不够，响应慢
# 2. 低峰期（100 QPS）：5个副本浪费，成本高
# 3. 突发流量：无法快速响应
# 4. 资源不足：无法充分利用硬件
```

### 解决方案：动态配置与自动扩缩容

```yaml
# 动态配置
queryNode:
  replicas: 5  # 基准副本数
  resources:
    requests:
      cpu: 4
      memory: 16Gi
    limits:
      cpu: 8
      memory: 32Gi

# 自动扩缩容
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: milvus-querynode-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: milvus-querynode
  minReplicas: 2   # 最少2个
  maxReplicas: 20  # 最多20个
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # CPU超过70%自动扩容
```

---

## 核心概念1：资源配置

### 1.1 CPU和内存配置

**requests vs limits**

```yaml
resources:
  requests:
    cpu: 4        # 保证分配4核CPU
    memory: 16Gi  # 保证分配16GB内存
  limits:
    cpu: 8        # 最多使用8核CPU
    memory: 32Gi  # 最多使用32GB内存
```

**类比：**
- requests = 预订的酒店房间（保证有）
- limits = 房间最大容纳人数（不能超过）

**配置原则：**

```yaml
# QueryNode：内存密集
queryNode:
  resources:
    requests:
      cpu: 4
      memory: 16Gi  # 内存是CPU的4倍
    limits:
      cpu: 8
      memory: 32Gi

# Proxy：CPU密集
proxy:
  resources:
    requests:
      cpu: 4
      memory: 4Gi   # 内存是CPU的1倍
    limits:
      cpu: 8
      memory: 8Gi

# DataNode：磁盘密集
dataNode:
  resources:
    requests:
      cpu: 2
      memory: 8Gi
    limits:
      cpu: 4
      memory: 16Gi
  volumeMounts:
  - name: data
    mountPath: /var/lib/milvus
```

### 1.2 存储配置

**PersistentVolume配置**

```yaml
# StatefulSet的存储配置
volumeClaimTemplates:
- metadata:
    name: data
  spec:
    accessModes: ["ReadWriteOnce"]
    storageClassName: "fast-ssd"  # 使用SSD存储类
    resources:
      requests:
        storage: 100Gi
```

**存储类型选择：**

| 组件 | 存储类型 | 容量 | IOPS要求 |
|------|---------|------|---------|
| **etcd** | SSD | 20-50Gi | 高（>3000） |
| **DataNode** | SSD | 100-500Gi | 高（>5000） |
| **QueryNode** | 内存 | - | - |
| **MinIO/S3** | HDD/对象存储 | 1-10Ti | 中（>1000） |

### 1.3 网络配置

**Service类型选择**

```yaml
# ClusterIP：集群内部访问
apiVersion: v1
kind: Service
metadata:
  name: milvus-internal
spec:
  type: ClusterIP
  ports:
  - port: 19530

---

# LoadBalancer：外部访问（云环境）
apiVersion: v1
kind: Service
metadata:
  name: milvus-external
spec:
  type: LoadBalancer
  ports:
  - port: 19530
    targetPort: 19530

---

# NodePort：外部访问（本地环境）
apiVersion: v1
kind: Service
metadata:
  name: milvus-nodeport
spec:
  type: NodePort
  ports:
  - port: 19530
    nodePort: 30530
```

---

## 核心概念2：副本配置

### 2.1 副本数量策略

**不同组件的副本配置：**

```yaml
# 生产环境推荐配置
components:
  # Proxy：无状态，可以多副本
  proxy:
    replicas: 3  # 至少3个，支持故障转移

  # QueryNode：无状态，根据负载调整
  queryNode:
    replicas: 5  # 基准5个，可扩展到20+

  # DataNode：有状态，适度副本
  dataNode:
    replicas: 3  # 3个副本，平衡性能和成本

  # IndexNode：无状态，根据索引任务调整
  indexNode:
    replicas: 2  # 2个副本

  # Coordinator：有状态，通常1个
  rootCoord:
    replicas: 1  # 单副本（通过etcd保证高可用）
  dataCoord:
    replicas: 1
  queryCoord:
    replicas: 1
  indexCoord:
    replicas: 1
```

### 2.2 Pod反亲和性

**确保Pod分布在不同节点：**

```yaml
affinity:
  podAntiAffinity:
    # 硬性要求：必须在不同节点
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - milvus-querynode
      topologyKey: kubernetes.io/hostname

    # 软性要求：尽量在不同可用区
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - milvus-querynode
        topologyKey: topology.kubernetes.io/zone
```

**效果：**
```
节点A：querynode-0
节点B：querynode-1
节点C：querynode-2
# 单节点故障不影响服务
```

### 2.3 节点选择器

**将Pod调度到特定节点：**

```yaml
# 给节点打标签
kubectl label nodes node-1 workload=milvus-query
kubectl label nodes node-2 workload=milvus-query
kubectl label nodes node-3 workload=milvus-data

# 配置节点选择器
queryNode:
  nodeSelector:
    workload: milvus-query  # 只调度到query节点

dataNode:
  nodeSelector:
    workload: milvus-data   # 只调度到data节点
```

---

## 核心概念3：自动扩缩容

### 3.1 水平扩缩容（HPA）

**基于CPU的自动扩缩容：**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: milvus-querynode-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: milvus-querynode
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # 扩容前等待60秒
      policies:
      - type: Percent
        value: 50  # 每次扩容50%
        periodSeconds: 60
      - type: Pods
        value: 2   # 或每次扩容2个Pod
        periodSeconds: 60
      selectPolicy: Max  # 选择扩容更多的策略
    scaleDown:
      stabilizationWindowSeconds: 300  # 缩容前等待5分钟
      policies:
      - type: Percent
        value: 10  # 每次缩容10%
        periodSeconds: 60
```

**基于内存的自动扩缩容：**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: milvus-querynode-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: milvus-querynode
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # 内存超过80%扩容
```

**基于自定义指标的扩缩容：**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: milvus-querynode-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: milvus-querynode
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Pods
    pods:
      metric:
        name: milvus_search_latency_p99
      target:
        type: AverageValue
        averageValue: "1000"  # P99延迟超过1秒扩容
  - type: Pods
    pods:
      metric:
        name: milvus_search_qps
      target:
        type: AverageValue
        averageValue: "100"  # 每个Pod QPS超过100扩容
```

### 3.2 垂直扩缩容（VPA）

**自动调整资源限制：**

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: milvus-querynode-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: milvus-querynode
  updatePolicy:
    updateMode: "Auto"  # 自动更新
  resourcePolicy:
    containerPolicies:
    - containerName: querynode
      minAllowed:
        cpu: 2
        memory: 8Gi
      maxAllowed:
        cpu: 16
        memory: 64Gi
      controlledResources: ["cpu", "memory"]
```

### 3.3 手动扩缩容

**使用kubectl扩缩容：**

```bash
# 扩容QueryNode到10个副本
kubectl scale deployment milvus-querynode --replicas=10

# 缩容到3个副本
kubectl scale deployment milvus-querynode --replicas=3

# 查看扩缩容状态
kubectl get deployment milvus-querynode
```

**使用Helm扩缩容：**

```bash
# 扩容
helm upgrade my-milvus milvus/milvus \
  --set queryNode.replicas=10 \
  --reuse-values

# 缩容
helm upgrade my-milvus milvus/milvus \
  --set queryNode.replicas=3 \
  --reuse-values
```

**使用Operator扩缩容：**

```bash
# 修改Milvus CR
kubectl patch milvus my-milvus --type='json' -p='[
  {"op": "replace", "path": "/spec/components/queryNode/replicas", "value": 10}
]'
```

---

## 在RAG系统中的应用

### 场景1：文档问答系统的动态扩缩容

**需求：**
- 工作日白天：1000 QPS
- 工作日夜间：100 QPS
- 周末：50 QPS

**配置：**

```yaml
# 基础配置
queryNode:
  replicas: 5  # 基准5个副本
  resources:
    requests:
      cpu: 4
      memory: 16Gi
    limits:
      cpu: 8
      memory: 32Gi

# HPA配置
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: milvus-querynode-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: milvus-querynode
  minReplicas: 2   # 最少2个（夜间/周末）
  maxReplicas: 20  # 最多20个（高峰期）
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        value: 3  # 每次扩容3个Pod（快速响应）
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 600  # 缩容前等待10分钟（避免频繁波动）
      policies:
      - type: Pods
        value: 1  # 每次缩容1个Pod（平滑缩容）
        periodSeconds: 60
```

**效果：**
```
时间段          QPS    CPU使用率   副本数   成本
工作日 9-18点   1000   75%        15      高
工作日 18-9点   100    40%        3       低
周末            50     30%        2       最低

月度成本节省：60-70%
```

### 场景2：突发流量应对

**场景：**产品发布会，流量突增10倍

**配置：**

```yaml
# 快速扩容策略
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: milvus-querynode-hpa
spec:
  minReplicas: 5
  maxReplicas: 50  # 提高上限
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60  # 降低阈值，提前扩容
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30  # 缩短等待时间
      policies:
      - type: Percent
        value: 100  # 每次扩容100%（翻倍）
        periodSeconds: 30
```

**效果：**
```
时间    QPS     副本数   响应时间
T+0     1000    5       200ms
T+1min  5000    10      250ms（开始扩容）
T+2min  10000   20      300ms（继续扩容）
T+3min  10000   40      200ms（扩容完成）
```

### 场景3：多租户资源隔离

**需求：**不同租户使用不同的QueryNode池

**配置：**

```yaml
# 租户A：高优先级
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-querynode-tenant-a
spec:
  replicas: 10
  template:
    spec:
      nodeSelector:
        tenant: a
      containers:
      - name: querynode
        resources:
          requests:
            cpu: 8
            memory: 32Gi

---

# 租户B：普通优先级
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-querynode-tenant-b
spec:
  replicas: 5
  template:
    spec:
      nodeSelector:
        tenant: b
      containers:
      - name: querynode
        resources:
          requests:
            cpu: 4
            memory: 16Gi
```

---

## 最佳实践

### 1. 资源配置最佳实践

```yaml
# ✅ 推荐：设置requests和limits
resources:
  requests:
    cpu: 4
    memory: 16Gi
  limits:
    cpu: 8
    memory: 32Gi

# ❌ 不推荐：只设置limits
resources:
  limits:
    cpu: 8
    memory: 32Gi
  # 没有requests，调度器无法合理分配资源

# ❌ 不推荐：requests = limits（QoS Guaranteed）
resources:
  requests:
    cpu: 8
    memory: 32Gi
  limits:
    cpu: 8
    memory: 32Gi
  # 资源利用率低，无法超卖
```

### 2. 扩缩容最佳实践

```yaml
# ✅ 推荐：扩容快，缩容慢
behavior:
  scaleUp:
    stabilizationWindowSeconds: 60   # 扩容等待1分钟
    policies:
    - type: Percent
      value: 50  # 每次扩容50%
  scaleDown:
    stabilizationWindowSeconds: 600  # 缩容等待10分钟
    policies:
    - type: Percent
      value: 10  # 每次缩容10%

# ❌ 不推荐：扩缩容都很快
behavior:
  scaleUp:
    stabilizationWindowSeconds: 10
  scaleDown:
    stabilizationWindowSeconds: 10
  # 容易频繁波动，影响稳定性
```

### 3. 副本配置最佳实践

```yaml
# ✅ 推荐：无状态组件多副本
proxy:
  replicas: 3  # 至少3个
queryNode:
  replicas: 5  # 根据负载调整

# ✅ 推荐：有状态组件适度副本
dataNode:
  replicas: 3  # 3个副本平衡性能和成本

# ❌ 不推荐：所有组件都是1个副本
proxy:
  replicas: 1  # 单点故障
queryNode:
  replicas: 1  # 无法扩展
```

### 4. 节点亲和性最佳实践

```yaml
# ✅ 推荐：Pod反亲和性
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - milvus-querynode
      topologyKey: kubernetes.io/hostname

# ✅ 推荐：节点选择器
nodeSelector:
  workload: milvus-query

# ❌ 不推荐：没有任何亲和性配置
# 所有Pod可能调度到同一节点，单点故障
```

---

## 监控和观察

### 关键指标

```yaml
# 需要监控的指标
- milvus_querynode_cpu_usage          # CPU使用率
- milvus_querynode_memory_usage       # 内存使用率
- milvus_search_latency_p99           # P99延迟
- milvus_search_qps                   # QPS
- milvus_querynode_replica_count      # 副本数量
- milvus_querynode_pending_requests   # 待处理请求数
```

### 告警规则

```yaml
# Prometheus告警规则
groups:
- name: milvus-scaling
  rules:
  # CPU持续高于80%
  - alert: HighCPUUsage
    expr: milvus_querynode_cpu_usage > 80
    for: 5m
    annotations:
      summary: "QueryNode CPU usage is high"

  # 内存持续高于85%
  - alert: HighMemoryUsage
    expr: milvus_querynode_memory_usage > 85
    for: 5m
    annotations:
      summary: "QueryNode memory usage is high"

  # P99延迟超过1秒
  - alert: HighLatency
    expr: milvus_search_latency_p99 > 1000
    for: 2m
    annotations:
      summary: "Search latency is high"

  # 副本数低于最小值
  - alert: LowReplicaCount
    expr: milvus_querynode_replica_count < 2
    for: 1m
    annotations:
      summary: "QueryNode replica count is too low"
```

---

## 常见问题

### Q1: 如何确定合适的副本数？

**方法：压力测试**

```bash
# 1. 部署基准配置（5个副本）
helm install milvus milvus/milvus --set queryNode.replicas=5

# 2. 使用压测工具
# 逐步增加QPS，观察响应时间和CPU使用率

# 3. 找到临界点
# 当响应时间开始显著增加时，记录QPS和CPU使用率

# 4. 计算副本数
# 副本数 = 目标QPS / 单副本QPS × 安全系数（1.5-2）
```

### Q2: HPA不生效怎么办？

**排查步骤：**

```bash
# 1. 检查metrics-server是否安装
kubectl get deployment metrics-server -n kube-system

# 2. 检查HPA状态
kubectl describe hpa milvus-querynode-hpa

# 3. 检查Pod资源配置
kubectl get pod <pod-name> -o yaml | grep -A 10 resources

# 4. 检查当前指标
kubectl get hpa milvus-querynode-hpa
# 如果TARGETS显示<unknown>，说明指标获取失败
```

### Q3: 如何避免频繁扩缩容？

**配置稳定窗口：**

```yaml
behavior:
  scaleUp:
    stabilizationWindowSeconds: 60   # 扩容前等待1分钟
  scaleDown:
    stabilizationWindowSeconds: 600  # 缩容前等待10分钟
```

---

## 总结

### 集群配置与扩缩容的核心价值

| 维度 | 价值 |
|------|------|
| **资源优化** | 根据负载动态调整，节省成本 |
| **性能保障** | 高峰期自动扩容，保证服务质量 |
| **高可用** | 多副本 + 反亲和性，避免单点故障 |
| **灵活性** | 支持手动和自动扩缩容 |

### 配置策略总结

| 场景 | 策略 |
|------|------|
| **开发环境** | 最小配置，单副本 |
| **测试环境** | 中等配置，2-3副本 |
| **生产环境** | 高配置，3-5副本 + HPA |
| **大规模生产** | 高配置，5-10副本 + HPA + VPA |

### 下一步

学习完集群配置与扩缩容后，继续学习：
- **实战代码**：完整的部署和扩缩容脚本
- **监控和告警**：保障服务稳定性
- **故障排查**：快速定位和解决问题
