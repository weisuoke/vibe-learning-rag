# å®æˆ˜ä»£ç 8ï¼šå®Œæ•´AI Agenté›†æˆ

## å®Œæ•´å¯è¿è¡Œç¤ºä¾‹

```python
"""
å®Œæ•´AI Agenté›†æˆRedisç¼“å­˜
æ¼”ç¤ºï¼šåœ¨AI Agenté¡¹ç›®ä¸­é›†æˆç²¾ç¡®ç¼“å­˜ã€è¯­ä¹‰ç¼“å­˜ã€Embeddingç¼“å­˜
"""

import redis
import asyncio
import hashlib
import json
import numpy as np
from typing import List, Optional
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# ===== 1. å®Œæ•´ç¼“å­˜ç³»ç»Ÿ =====

class AIAgentCacheSystem:
    """AI Agentå®Œæ•´ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, redis_client: redis.Redis, openai_client: OpenAI):
        self.redis = redis_client
        self.openai = openai_client
        self.stats = {
            "exact_hits": 0,
            "semantic_hits": 0,
            "misses": 0,
            "api_calls": 0
        }

    # ===== Embeddingç¼“å­˜ =====

    def _get_embedding(self, text: str) -> List[float]:
        """è·å–Embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"
        cached = self.redis.get(cache_key)

        if cached:
            return json.loads(cached)

        response = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        embedding = response.data[0].embedding
        self.redis.setex(cache_key, 86400, json.dumps(embedding))
        return embedding

    # ===== ç²¾ç¡®ç¼“å­˜ =====

    def _exact_cache_key(self, prompt: str, model: str) -> str:
        """ç”Ÿæˆç²¾ç¡®ç¼“å­˜key"""
        content = f"{model}:{prompt}"
        return f"llm_exact:{hashlib.md5(content.encode()).hexdigest()}"

    def _get_exact_cache(self, prompt: str, model: str) -> Optional[str]:
        """è·å–ç²¾ç¡®ç¼“å­˜"""
        cache_key = self._exact_cache_key(prompt, model)
        cached = self.redis.get(cache_key)

        if cached:
            self.stats["exact_hits"] += 1
            print("âœ… ç²¾ç¡®ç¼“å­˜å‘½ä¸­")
            return cached

        return None

    def _set_exact_cache(self, prompt: str, model: str, response: str):
        """è®¾ç½®ç²¾ç¡®ç¼“å­˜"""
        cache_key = self._exact_cache_key(prompt, model)
        self.redis.setex(cache_key, 3600, response)

    # ===== è¯­ä¹‰ç¼“å­˜ =====

    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

    def _get_semantic_cache(
        self,
        prompt: str,
        threshold: float = 0.9
    ) -> Optional[str]:
        """è·å–è¯­ä¹‰ç¼“å­˜"""
        prompt_embedding = self._get_embedding(prompt)
        cached_items = self.redis.hgetall("semantic_cache")

        if not cached_items:
            return None

        best_match = None
        best_score = 0.0

        for cache_id, cache_data_json in cached_items.items():
            cache_data = json.loads(cache_data_json)
            cached_embedding = cache_data["embedding"]

            similarity = self._cosine_similarity(
                prompt_embedding,
                cached_embedding
            )

            if similarity > best_score:
                best_score = similarity
                best_match = cache_data["response"]

        if best_score >= threshold:
            self.stats["semantic_hits"] += 1
            print(f"âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦={best_score:.3f}")
            return best_match

        return None

    def _set_semantic_cache(self, prompt: str, response: str):
        """è®¾ç½®è¯­ä¹‰ç¼“å­˜"""
        prompt_embedding = self._get_embedding(prompt)

        cache_data = {
            "prompt": prompt,
            "response": response,
            "embedding": prompt_embedding
        }

        cache_id = hashlib.md5(prompt.encode()).hexdigest()
        self.redis.hset(
            "semantic_cache",
            cache_id,
            json.dumps(cache_data)
        )
        self.redis.expire("semantic_cache", 3600)

    # ===== ç»Ÿä¸€æ¥å£ =====

    async def get_llm_response(
        self,
        prompt: str,
        model: str = "gpt-4o-mini",
        use_semantic: bool = True,
        semantic_threshold: float = 0.9
    ) -> str:
        """è·å–LLMå“åº”ï¼ˆä¸‰å±‚ç¼“å­˜ï¼‰"""

        # 1. ç²¾ç¡®ç¼“å­˜
        exact_cached = self._get_exact_cache(prompt, model)
        if exact_cached:
            return exact_cached

        # 2. è¯­ä¹‰ç¼“å­˜
        if use_semantic:
            semantic_cached = self._get_semantic_cache(prompt, semantic_threshold)
            if semantic_cached:
                # æå‡åˆ°ç²¾ç¡®ç¼“å­˜
                self._set_exact_cache(prompt, model, semantic_cached)
                return semantic_cached

        # 3. è°ƒç”¨LLM API
        self.stats["misses"] += 1
        self.stats["api_calls"] += 1
        print(f"ğŸ¤– è°ƒç”¨LLM API: {model}")

        response = self.openai.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        answer = response.choices[0].message.content

        # 4. åŒæ—¶ç¼“å­˜åˆ°ç²¾ç¡®å’Œè¯­ä¹‰ç¼“å­˜
        self._set_exact_cache(prompt, model, answer)
        if use_semantic:
            self._set_semantic_cache(prompt, answer)

        return answer

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = (
            self.stats["exact_hits"] +
            self.stats["semantic_hits"] +
            self.stats["misses"]
        )

        return {
            **self.stats,
            "total": total,
            "cache_hit_rate": (
                (self.stats["exact_hits"] + self.stats["semantic_hits"]) / total
                if total > 0 else 0.0
            )
        }

# ===== 2. æµ‹è¯•å®Œæ•´ç³»ç»Ÿ =====

async def test_ai_agent_cache():
    """æµ‹è¯•AI Agentç¼“å­˜ç³»ç»Ÿ"""

    redis_client = redis.Redis(
        host='localhost',
        port=6379,
        decode_responses=True
    )
    openai_client = OpenAI()

    cache_system = AIAgentCacheSystem(redis_client, openai_client)

    print("=== æµ‹è¯•AI Agentç¼“å­˜ç³»ç»Ÿ ===\n")

    # æµ‹è¯•ç”¨ä¾‹
    test_queries = [
        "What is Python?",
        "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",  # è¯­ä¹‰ç›¸ä¼¼
        "What is Python?",  # ç²¾ç¡®åŒ¹é…
        "è¯·ä»‹ç»Python",     # è¯­ä¹‰ç›¸ä¼¼
        "What is JavaScript?",
        "JavaScriptæ˜¯ä»€ä¹ˆï¼Ÿ",  # è¯­ä¹‰ç›¸ä¼¼
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"\n[{i}/{len(test_queries)}] æŸ¥è¯¢: {query}")
        response = await cache_system.get_llm_response(query)
        print(f"å“åº”: {response[:80]}...")

    # ç»Ÿè®¡
    print("\n=== ç»Ÿè®¡ä¿¡æ¯ ===")
    stats = cache_system.get_stats()
    print(f"ç²¾ç¡®ç¼“å­˜å‘½ä¸­: {stats['exact_hits']}")
    print(f"è¯­ä¹‰ç¼“å­˜å‘½ä¸­: {stats['semantic_hits']}")
    print(f"ç¼“å­˜æœªå‘½ä¸­: {stats['misses']}")
    print(f"APIè°ƒç”¨æ¬¡æ•°: {stats['api_calls']}")
    print(f"æ€»ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")

    # æˆæœ¬åˆ†æ
    cost_per_call = 0.01
    total_cost_no_cache = stats['total'] * cost_per_call
    total_cost_with_cache = stats['api_calls'] * cost_per_call
    savings = total_cost_no_cache - total_cost_with_cache

    print(f"\n=== æˆæœ¬åˆ†æ ===")
    print(f"æ— ç¼“å­˜æˆæœ¬: ${total_cost_no_cache:.2f}")
    print(f"æœ‰ç¼“å­˜æˆæœ¬: ${total_cost_with_cache:.2f}")
    print(f"èŠ‚çœæˆæœ¬: ${savings:.2f} ({savings/total_cost_no_cache:.0%})")

    # æ¸…ç†
    redis_client.delete("semantic_cache")
    keys = redis_client.keys("llm_exact:*") + redis_client.keys("emb:*")
    if keys:
        redis_client.delete(*keys)
    print("\nâœ… æµ‹è¯•æ•°æ®å·²æ¸…ç†")

# ===== 3. FastAPIé›†æˆ =====

"""
from fastapi import FastAPI, Depends
from pydantic import BaseModel

app = FastAPI()

# å…¨å±€ç¼“å­˜ç³»ç»Ÿ
cache_system = None

@app.on_event("startup")
async def startup():
    global cache_system
    redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
    openai_client = OpenAI()
    cache_system = AIAgentCacheSystem(redis_client, openai_client)

class QueryRequest(BaseModel):
    query: str
    use_semantic: bool = True

class QueryResponse(BaseModel):
    answer: str
    cache_type: str  # "exact", "semantic", "none"

@app.post("/query", response_model=QueryResponse)
async def query_agent(request: QueryRequest):
    answer = await cache_system.get_llm_response(
        request.query,
        use_semantic=request.use_semantic
    )

    # åˆ¤æ–­ç¼“å­˜ç±»å‹
    stats_before = cache_system.get_stats()
    cache_type = "none"
    if stats_before["exact_hits"] > 0:
        cache_type = "exact"
    elif stats_before["semantic_hits"] > 0:
        cache_type = "semantic"

    return QueryResponse(answer=answer, cache_type=cache_type)

@app.get("/stats")
async def get_stats():
    return cache_system.get_stats()
"""

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    asyncio.run(test_ai_agent_cache())
```

## è¿è¡Œè¾“å‡ºç¤ºä¾‹

```
=== æµ‹è¯•AI Agentç¼“å­˜ç³»ç»Ÿ ===

[1/6] æŸ¥è¯¢: What is Python?
ğŸ¤– è°ƒç”¨LLM API: gpt-4o-mini
å“åº”: Python is a high-level, interpreted programming language...

[2/6] æŸ¥è¯¢: Pythonæ˜¯ä»€ä¹ˆï¼Ÿ
âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦=0.952
å“åº”: Python is a high-level, interpreted programming language...

[3/6] æŸ¥è¯¢: What is Python?
âœ… ç²¾ç¡®ç¼“å­˜å‘½ä¸­
å“åº”: Python is a high-level, interpreted programming language...

[4/6] æŸ¥è¯¢: è¯·ä»‹ç»Python
âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦=0.918
å“åº”: Python is a high-level, interpreted programming language...

[5/6] æŸ¥è¯¢: What is JavaScript?
ğŸ¤– è°ƒç”¨LLM API: gpt-4o-mini
å“åº”: JavaScript is a versatile programming language...

[6/6] æŸ¥è¯¢: JavaScriptæ˜¯ä»€ä¹ˆï¼Ÿ
âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦=0.945
å“åº”: JavaScript is a versatile programming language...

=== ç»Ÿè®¡ä¿¡æ¯ ===
ç²¾ç¡®ç¼“å­˜å‘½ä¸­: 1
è¯­ä¹‰ç¼“å­˜å‘½ä¸­: 3
ç¼“å­˜æœªå‘½ä¸­: 2
APIè°ƒç”¨æ¬¡æ•°: 2
æ€»ç¼“å­˜å‘½ä¸­ç‡: 66.7%

=== æˆæœ¬åˆ†æ ===
æ— ç¼“å­˜æˆæœ¬: $0.06
æœ‰ç¼“å­˜æˆæœ¬: $0.02
èŠ‚çœæˆæœ¬: $0.04 (67%)

âœ… æµ‹è¯•æ•°æ®å·²æ¸…ç†
```

## å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ä¸‰å±‚ç¼“å­˜æ¶æ„ï¼ˆç²¾ç¡® + è¯­ä¹‰ + Embeddingï¼‰
- [ ] å®ç°å®Œæ•´çš„AI Agentç¼“å­˜ç³»ç»Ÿ
- [ ] ç»Ÿè®¡ç¼“å­˜å‘½ä¸­ç‡å’Œæˆæœ¬èŠ‚çœ
- [ ] åœ¨FastAPIä¸­é›†æˆç¼“å­˜ç³»ç»Ÿ
- [ ] ç†è§£ç¼“å­˜åœ¨AI Agentä¸­çš„ä»·å€¼

## æ€»ç»“

é€šè¿‡å®Œæ•´çš„Redisç¼“å­˜ç³»ç»Ÿï¼ŒAI Agentå¯ä»¥ï¼š
1. é™ä½70%ä»¥ä¸Šçš„LLM APIæˆæœ¬
2. æå‡å“åº”é€Ÿåº¦20å€ä»¥ä¸Š
3. æå‡ç”¨æˆ·ä½“éªŒï¼ˆå³æ—¶å“åº”ï¼‰
4. æ”¯æŒæ›´é«˜çš„å¹¶å‘è¯·æ±‚

**è®°ä½ï¼š** ç¼“å­˜æ˜¯AI Agentç”Ÿäº§ç¯å¢ƒçš„å¿…å¤‡ç»„ä»¶ï¼
