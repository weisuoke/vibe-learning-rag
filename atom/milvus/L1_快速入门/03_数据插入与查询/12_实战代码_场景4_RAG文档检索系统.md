# 实战代码 - 场景 4: RAG 文档检索系统

本文档展示如何构建一个完整的 RAG 文档问答系统，整合 Milvus、Embedding 模型和 LLM。

---

## 场景描述

**目标**: 构建一个端到端的 RAG 文档问答系统

**核心组件**:
- **Milvus**: 向量存储与检索
- **Sentence Transformers**: 文本向量化
- **OpenAI API**: 答案生成

**完整流程**:
```
文档加载 → 文本分块 → 向量化 → 存储到 Milvus → 用户提问 → 检索相关文档 → LLM 生成答案
```

**适用场景**:
- 企业知识库问答
- 技术文档检索
- 客服智能问答
- 学术论文检索

---

## 完整代码实现

### 环境准备

```python
# 安装依赖
# pip install pymilvus sentence-transformers openai python-dotenv

from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import numpy as np
from typing import List, Dict
import os
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
```

---

## 第一部分: 初始化组件

### 1. 初始化 Milvus

```python
def initialize_milvus(collection_name: str = "rag_knowledge_base"):
    """
    初始化 Milvus 连接和 Collection
    """
    print("=" * 60)
    print("步骤 1: 初始化 Milvus")
    print("=" * 60)

    # 连接 Milvus
    connections.connect(
        alias="default",
        host="localhost",
        port="19530"
    )
    print("✓ 成功连接到 Milvus")

    # 删除旧 Collection（如果存在）
    if utility.has_collection(collection_name):
        utility.drop_collection(collection_name)
        print(f"✓ 删除旧 Collection: {collection_name}")

    # 定义 Schema
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2000),
        FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),
        FieldSchema(name="chunk_id", dtype=DataType.INT64),
    ]

    schema = CollectionSchema(fields, description="RAG knowledge base")

    # 创建 Collection
    collection = Collection(collection_name, schema)
    print(f"✓ 成功创建 Collection: {collection_name}")
    print()

    return collection
```

---

### 2. 初始化 Embedding 模型

```python
def initialize_embedding_model(model_name: str = "all-MiniLM-L6-v2"):
    """
    初始化 Sentence Transformers 模型
    """
    print("=" * 60)
    print("步骤 2: 初始化 Embedding 模型")
    print("=" * 60)

    model = SentenceTransformer(model_name)

    print(f"✓ 成功加载模型: {model_name}")
    print(f"  - 输出维度: {model.get_sentence_embedding_dimension()}")
    print()

    return model
```

---

### 3. 初始化 LLM 客户端

```python
def initialize_llm_client():
    """
    初始化 OpenAI 客户端
    """
    print("=" * 60)
    print("步骤 3: 初始化 LLM 客户端")
    print("=" * 60)

    # 从环境变量获取 API Key
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")

    if not api_key:
        raise ValueError("请设置 OPENAI_API_KEY 环境变量")

    client = OpenAI(api_key=api_key, base_url=base_url)

    print("✓ 成功初始化 OpenAI 客户端")
    print()

    return client
```

---

## 第二部分: 文档加载与向量化

### 1. 准备示例文档

```python
def prepare_sample_documents() -> List[str]:
    """
    准备示例文档
    """
    documents = [
        # Milvus 相关
        "Milvus 是一个开源的向量数据库，专门用于存储和检索高维向量数据。它支持大规模向量数据的高效检索，广泛应用于 RAG、推荐系统、图像检索等场景。",
        "Milvus 支持多种索引类型，包括 FLAT、IVF_FLAT、HNSW 等。HNSW 索引是基于图结构的索引，具有高召回率和快速检索速度，适合大规模数据场景。",
        "在 Milvus 中，数据插入后需要调用 flush() 方法将数据刷新到磁盘，然后创建索引并调用 load() 方法加载到内存，才能执行检索操作。",

        # RAG 相关
        "RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术，通过检索相关文档来增强大语言模型的生成能力，有效缓解幻觉问题。",
        "RAG 系统的核心流程包括：文档加载、文本分块、向量化、存储到向量数据库、用户提问、检索相关文档、LLM 生成答案。",
        "在 RAG 系统中，文本分块（Chunking）是关键步骤，需要平衡块大小和语义完整性。通常推荐 chunk_size=500-1000，chunk_overlap=50-200。",

        # Embedding 相关
        "Embedding 是将文本转换为高维向量的过程，向量能够捕捉文本的语义信息。常用的 Embedding 模型包括 Sentence Transformers、OpenAI Embeddings 等。",
        "Sentence Transformers 是一个开源的 Embedding 模型库，提供多种预训练模型。all-MiniLM-L6-v2 是一个轻量级模型，输出 384 维向量，适合快速原型开发。",
        "在选择 Embedding 模型时，需要考虑维度、性能、语言支持等因素。维度越高，表达能力越强，但计算和存储成本也越高。",

        # 向量检索相关
        "向量相似度检索通过计算查询向量与数据库中向量的相似度，找到最相关的结果。常用的相似度度量方式包括欧氏距离（L2）、内积（IP）、余弦相似度（COSINE）。",
        "COSINE 相似度适合文本向量检索，因为它只关注向量的方向，不关注长度。COSINE 值范围是 [-1, 1]，值越大表示越相似。",
        "在 RAG 系统中，通常先检索 Top-20 到 Top-50 个候选文档，然后使用 ReRank 模型重新排序，最终选择 Top-5 个最相关的文档用于生成答案。",

        # LLM 相关
        "大语言模型（LLM）如 GPT-3.5、GPT-4、Claude 等，具有强大的文本生成能力。在 RAG 系统中，LLM 基于检索到的文档生成答案，而不是依赖自身的参数化知识。",
        "Prompt Engineering 是 RAG 系统的关键技术，需要设计合适的 Prompt 模板，将检索到的文档和用户问题组合起来，引导 LLM 生成准确的答案。",
        "在 RAG 系统中，可以通过设置 temperature=0 来获得确定性的输出，或者设置 temperature=0.7 来获得更有创造性的输出。",
    ]

    return documents
```

---

### 2. 文档向量化与插入

```python
def load_documents_to_milvus(
    collection: Collection,
    model: SentenceTransformer,
    documents: List[str]
):
    """
    将文档向量化并插入 Milvus
    """
    print("=" * 60)
    print("步骤 4: 文档向量化与插入")
    print("=" * 60)

    print(f"开始处理 {len(documents)} 个文档...")

    # 向量化文档
    print("正在向量化文档...")
    embeddings = model.encode(documents, show_progress_bar=True).tolist()
    print(f"✓ 完成向量化，生成 {len(embeddings)} 个向量")

    # 准备插入数据
    doc_ids = [f"doc_{i//3}" for i in range(len(documents))]  # 每 3 个文档一组
    chunk_ids = [i % 3 for i in range(len(documents))]

    data = [
        embeddings,
        documents,
        doc_ids,
        chunk_ids
    ]

    # 插入数据
    print("正在插入数据到 Milvus...")
    result = collection.insert(data)
    print(f"✓ 成功插入 {len(result.primary_keys)} 条数据")

    # 刷新数据
    print("正在刷新数据...")
    collection.flush()
    print(f"✓ 数据已刷新，共 {collection.num_entities} 条")
    print()

    return result
```

---

## 第三部分: 创建索引并加载

```python
def create_index_and_load(collection: Collection):
    """
    创建 HNSW 索引并加载到内存
    """
    print("=" * 60)
    print("步骤 5: 创建索引并加载")
    print("=" * 60)

    # 创建 HNSW 索引
    index_params = {
        "index_type": "HNSW",
        "metric_type": "COSINE",
        "params": {"M": 16, "efConstruction": 256}
    }

    print("正在创建 HNSW 索引...")
    collection.create_index("embedding", index_params)
    print("✓ 索引创建完成")

    # 加载到内存
    print("正在加载 Collection 到内存...")
    collection.load()
    print("✓ Collection 已加载，可以开始检索")
    print()
```

---

## 第四部分: RAG 检索与生成

### 1. RAG 检索函数

```python
def rag_retrieve(
    collection: Collection,
    model: SentenceTransformer,
    question: str,
    top_k: int = 5
) -> List[Dict]:
    """
    RAG 检索函数：根据问题检索相关文档
    """
    # 向量化问题
    query_vector = model.encode(question).tolist()

    # 定义检索参数
    search_params = {"metric_type": "COSINE", "params": {"ef": 64}}

    # 执行检索
    results = collection.search(
        data=[query_vector],
        anns_field="embedding",
        param=search_params,
        limit=top_k,
        output_fields=["text", "doc_id", "chunk_id"]
    )

    # 提取文档
    documents = []
    for hit in results[0]:
        documents.append({
            "id": hit.id,
            "score": hit.score,
            "text": hit.entity.get("text"),
            "doc_id": hit.entity.get("doc_id"),
            "chunk_id": hit.entity.get("chunk_id")
        })

    return documents
```

---

### 2. RAG 生成函数

```python
def rag_generate(
    client: OpenAI,
    question: str,
    documents: List[Dict],
    model: str = "gpt-3.5-turbo",
    temperature: float = 0.7
) -> str:
    """
    RAG 生成函数：基于检索到的文档生成答案
    """
    # 构建上下文
    context = "\n\n".join([f"文档 {i+1}:\n{doc['text']}" for i, doc in enumerate(documents)])

    # 构建 Prompt
    prompt = f"""你是一个专业的技术问答助手。请基于以下上下文回答用户的问题。

要求：
1. 只基于提供的上下文回答，不要使用上下文之外的知识
2. 如果上下文中没有相关信息，请明确说明"根据提供的信息无法回答这个问题"
3. 回答要准确、简洁、易懂
4. 如果可能，引用具体的文档内容

上下文：
{context}

问题：{question}

答案："""

    # 调用 LLM
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature
    )

    answer = response.choices[0].message.content

    return answer
```

---

### 3. 完整的 RAG 流程

```python
def rag_pipeline(
    collection: Collection,
    embedding_model: SentenceTransformer,
    llm_client: OpenAI,
    question: str,
    top_k: int = 5,
    verbose: bool = True
) -> Dict:
    """
    完整的 RAG 流程
    """
    if verbose:
        print("=" * 60)
        print("RAG 问答流程")
        print("=" * 60)
        print(f"问题: {question}\n")

    # 1. 检索相关文档
    if verbose:
        print("步骤 1: 检索相关文档...")

    documents = rag_retrieve(collection, embedding_model, question, top_k)

    if verbose:
        print(f"✓ 检索到 {len(documents)} 个相关文档\n")
        print("检索结果:")
        for i, doc in enumerate(documents):
            print(f"\n文档 {i+1} (相似度: {doc['score']:.4f}):")
            print(f"  {doc['text'][:100]}...")

    # 2. 生成答案
    if verbose:
        print("\n步骤 2: 生成答案...")

    answer = rag_generate(llm_client, question, documents)

    if verbose:
        print(f"✓ 答案生成完成\n")
        print("=" * 60)
        print("答案:")
        print("=" * 60)
        print(answer)
        print("=" * 60)

    return {
        "question": question,
        "documents": documents,
        "answer": answer
    }
```

---

## 第五部分: 测试 RAG 系统

```python
def test_rag_system():
    """
    测试 RAG 系统
    """
    print("\n" + "=" * 80)
    print("RAG 文档检索系统 - 完整测试")
    print("=" * 80)
    print()

    # 1. 初始化组件
    collection = initialize_milvus()
    embedding_model = initialize_embedding_model()
    llm_client = initialize_llm_client()

    # 2. 加载文档
    documents = prepare_sample_documents()
    print(f"准备了 {len(documents)} 个示例文档\n")

    # 3. 向量化并插入
    load_documents_to_milvus(collection, embedding_model, documents)

    # 4. 创建索引并加载
    create_index_and_load(collection)

    # 5. 测试问答
    test_questions = [
        "什么是 Milvus？",
        "RAG 系统的核心流程是什么？",
        "如何选择合适的 Embedding 模型？",
        "COSINE 相似度适合什么场景？",
        "在 RAG 系统中如何使用 LLM？"
    ]

    for i, question in enumerate(test_questions):
        print(f"\n\n{'=' * 80}")
        print(f"测试问题 {i+1}/{len(test_questions)}")
        print(f"{'=' * 80}")

        result = rag_pipeline(
            collection=collection,
            embedding_model=embedding_model,
            llm_client=llm_client,
            question=question,
            top_k=3,
            verbose=True
        )

    # 6. 清理资源
    print("\n\n" + "=" * 80)
    print("清理资源")
    print("=" * 80)
    collection.release()
    print("✓ Collection 已从内存释放")

    print("\n" + "=" * 80)
    print("测试完成！")
    print("=" * 80)
```

---

## 完整代码（一体化版本）

```python
"""
RAG 文档检索系统 - 完整实现
"""

if __name__ == "__main__":
    # 运行测试
    test_rag_system()
```

---

## 输出示例

```
================================================================================
RAG 文档检索系统 - 完整测试
================================================================================

============================================================
步骤 1: 初始化 Milvus
============================================================
✓ 成功连接到 Milvus
✓ 成功创建 Collection: rag_knowledge_base

============================================================
步骤 2: 初始化 Embedding 模型
============================================================
✓ 成功加载模型: all-MiniLM-L6-v2
  - 输出维度: 384

============================================================
步骤 3: 初始化 LLM 客户端
============================================================
✓ 成功初始化 OpenAI 客户端

准备了 15 个示例文档

============================================================
步骤 4: 文档向量化与插入
============================================================
开始处理 15 个文档...
正在向量化文档...
✓ 完成向量化，生成 15 个向量
正在插入数据到 Milvus...
✓ 成功插入 15 条数据
正在刷新数据...
✓ 数据已刷新，共 15 条

============================================================
步骤 5: 创建索引并加载
============================================================
正在创建 HNSW 索引...
✓ 索引创建完成
正在加载 Collection 到内存...
✓ Collection 已加载，可以开始检索

================================================================================
测试问题 1/5
================================================================================
============================================================
RAG 问答流程
============================================================
问题: 什么是 Milvus？

步骤 1: 检索相关文档...
✓ 检索到 3 个相关文档

检索结果:

文档 1 (相似度: 0.8923):
  Milvus 是一个开源的向量数据库，专门用于存储和检索高维向量数据。它支持大规模向量数据的高效检索，广泛应用于 RAG、推荐系统、图像检索等场景。

文档 2 (相似度: 0.7654):
  Milvus 支持多种索引类型，包括 FLAT、IVF_FLAT、HNSW 等。HNSW 索引是基于图结构的索引，具有高召回率和快速检索速度，适合大规模数据场景。

文档 3 (相似度: 0.7123):
  在 Milvus 中，数据插入后需要调用 flush() 方法将数据刷新到磁盘，然后创建索引并调用 load() 方法加载到内存，才能执行检索操作。

步骤 2: 生成答案...
✓ 答案生成完成

============================================================
答案:
============================================================
Milvus 是一个开源的向量数据库，专门用于存储和检索高维向量数据。它的主要特点包括：

1. **核心功能**：支持大规模向量数据的高效检索
2. **应用场景**：广泛应用于 RAG（检索增强生成）、推荐系统、图像检索等场景
3. **索引支持**：支持多种索引类型，如 FLAT、IVF_FLAT、HNSW 等，其中 HNSW 索引基于图结构，具有高召回率和快速检索速度
4. **使用流程**：数据插入后需要调用 flush() 刷新到磁盘，然后创建索引并调用 load() 加载到内存才能执行检索

Milvus 特别适合需要处理大规模向量数据的场景，是构建 RAG 系统的理想选择。
============================================================
```

---

## 关键要点总结

### 1. RAG 系统架构

```
用户问题 → Embedding → 向量检索 → 文档召回 → Prompt 构建 → LLM 生成 → 答案
```

### 2. 核心组件

| 组件 | 作用 | 推荐选择 |
|------|------|---------|
| 向量数据库 | 存储与检索 | Milvus |
| Embedding 模型 | 文本向量化 | Sentence Transformers |
| LLM | 答案生成 | GPT-3.5/GPT-4 |

### 3. 性能优化

- 使用 HNSW 索引（快速检索）
- 批量插入文档（提高效率）
- 合理设置 Top-K（平衡精度和速度）
- 使用 ReRank（提高相关性）

### 4. 生产环境建议

- 添加错误处理和重试机制
- 实现文档更新和删除功能
- 添加日志和监控
- 实现缓存机制
- 支持多用户并发

---

## 下一步

学习 **面试必问**，掌握 Milvus 数据插入与查询的常见面试问题。
