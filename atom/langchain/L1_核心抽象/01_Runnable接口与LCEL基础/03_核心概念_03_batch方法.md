# æ ¸å¿ƒæ¦‚å¿µ 03: batch æ–¹æ³•

> ç†è§£ Runnable çš„æ‰¹é‡å¹¶å‘æ‰§è¡Œæ–¹æ³•

---

## ä»€ä¹ˆæ˜¯ batch æ–¹æ³•ï¼Ÿ

**batch æ˜¯ Runnable åè®®çš„æ‰¹é‡æ‰§è¡Œæ–¹æ³•ï¼Œç”¨äºå¹¶å‘å¤„ç†å¤šä¸ªè¾“å…¥å¹¶è¿”å›å¯¹åº”çš„è¾“å‡ºåˆ—è¡¨ã€‚**

### ä¸€å¥è¯å®šä¹‰

batch æ–¹æ³•æ¥æ”¶ä¸€ä¸ªè¾“å…¥åˆ—è¡¨å’Œå¯é€‰çš„é…ç½®å‚æ•°ï¼Œå¹¶å‘æ‰§è¡Œå¤„ç†é€»è¾‘ï¼Œè¿”å›ä¸€ä¸ªè¾“å‡ºåˆ—è¡¨ã€‚

---

## æ–¹æ³•ç­¾å

```python
from typing import TypeVar, Optional, List
from langchain_core.runnables.config import RunnableConfig

Input = TypeVar("Input")
Output = TypeVar("Output")

def batch(
    self,
    inputs: List[Input],
    config: Optional[RunnableConfig] = None
) -> List[Output]:
    """
    æ‰¹é‡å¹¶å‘æ‰§è¡Œ

    Args:
        inputs: è¾“å…¥æ•°æ®åˆ—è¡¨
        config: è¿è¡Œæ—¶é…ç½®ï¼ˆå¯é€‰ï¼‰

    Returns:
        è¾“å‡ºæ•°æ®åˆ—è¡¨ï¼Œé¡ºåºä¸è¾“å…¥å¯¹åº”

    æ³¨æ„:
        - é»˜è®¤ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        - ä¿è¯è¾“å‡ºé¡ºåºä¸è¾“å…¥é¡ºåºä¸€è‡´
        - å•ä¸ªä»»åŠ¡å¤±è´¥ä¼šå¯¼è‡´æ•´ä¸ª batch å¤±è´¥
    """
    ...
```

---

## å¹¶å‘æ‰§è¡Œæœºåˆ¶

### é»˜è®¤å®ç°ï¼šçº¿ç¨‹æ± 

**LangChain é»˜è®¤ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ batch**[^1]ï¼š

```python
from langchain_core.runnables import RunnableLambda
import time

def slow_process(x: str) -> str:
    time.sleep(1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return x.upper()

runnable = RunnableLambda(slow_process)

# é¡ºåºæ‰§è¡Œï¼ˆä½¿ç”¨ invokeï¼‰
start = time.time()
results = [runnable.invoke(x) for x in ["a", "b", "c"]]
print(f"é¡ºåºæ‰§è¡Œè€—æ—¶: {time.time() - start:.2f}ç§’")  # çº¦ 3 ç§’

# å¹¶å‘æ‰§è¡Œï¼ˆä½¿ç”¨ batchï¼‰
start = time.time()
results = runnable.batch(["a", "b", "c"])
print(f"å¹¶å‘æ‰§è¡Œè€—æ—¶: {time.time() - start:.2f}ç§’")  # çº¦ 1 ç§’
```

### å¹¶å‘æ§åˆ¶

```python
from langchain_core.runnables import RunnableConfig

# é™åˆ¶å¹¶å‘æ•°é‡
config = RunnableConfig(max_concurrency=2)

# æœ€å¤šåŒæ—¶æ‰§è¡Œ 2 ä¸ªä»»åŠ¡
results = runnable.batch(inputs, config=config)
```

---

## æˆæœ¬ä¼˜åŒ–ï¼šlangasync é›†æˆ

### 2025-2026 å¹´é‡å¤§çªç ´ï¼š50% æˆæœ¬é™ä½

**langasync é€šè¿‡æ‰¹å¤„ç† API å®ç° 50% æˆæœ¬èŠ‚çœ**[^2][^3]ï¼š

```python
from langasync import wrap_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# åŸå§‹ LCEL é“¾
prompt = ChatPromptTemplate.from_template("åˆ†ç±»: {text}")
llm = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | llm

# åŒ…è£…ä¸ºæ‰¹å¤„ç†æ¨¡å¼
async_chain = wrap_chain(chain, batch_size=10)

# æ‰¹é‡æ‰§è¡Œï¼ˆæˆæœ¬é™ä½ 50%ï¼‰
inputs = [{"text": f"æ–‡æœ¬{i}"} for i in range(100)]
results = await async_chain.abatch(inputs)
```

### æˆæœ¬å¯¹æ¯”

| æ–¹æ³• | æˆæœ¬ | å»¶è¿Ÿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **invoke å¾ªç¯** | 100% | å®æ—¶ | å®æ—¶æŸ¥è¯¢ |
| **batch** | 100% | å®æ—¶ | å®æ—¶æ‰¹é‡ |
| **langasync + Batch API** | 50% | å»¶è¿Ÿï¼ˆåˆ†é’Ÿçº§ï¼‰ | ç¦»çº¿æ‰¹é‡ |

### é€‚ç”¨åœºæ™¯

**âœ… æ¨èä½¿ç”¨ langasync**ï¼š
- æ‰¹é‡è¯„ä¼°å’Œæµ‹è¯•
- æ•°æ®æ ‡æ³¨ä»»åŠ¡
- ç¦»çº¿åˆ†æå’ŒæŠ¥å‘Š
- éå®æ—¶æ‰¹é‡å¤„ç†

**âŒ ä¸æ¨èä½¿ç”¨ langasync**ï¼š
- å®æ—¶å¯¹è¯åº”ç”¨
- éœ€è¦å³æ—¶å“åº”çš„åœºæ™¯
- å•æ¬¡æŸ¥è¯¢

---

## æ€§èƒ½ä¼˜åŒ–

### æ‰¹é‡å¤§å°é€‰æ‹©

```python
# å°æ‰¹é‡ï¼šä½å»¶è¿Ÿï¼Œä½åå
results = runnable.batch(inputs[:10])

# å¤§æ‰¹é‡ï¼šé«˜å»¶è¿Ÿï¼Œé«˜åå
results = runnable.batch(inputs[:1000])

# æ¨èï¼šåˆ†æ‰¹å¤„ç†
batch_size = 50
for i in range(0, len(inputs), batch_size):
    batch = inputs[i:i+batch_size]
    results = runnable.batch(batch)
    process_results(results)
```

### å¼‚æ­¥æ‰¹å¤„ç†

```python
import asyncio
from langchain_core.runnables import RunnableLambda

async def async_process(x: str) -> str:
    await asyncio.sleep(1)
    return x.upper()

runnable = RunnableLambda(async_process)

# å¼‚æ­¥æ‰¹å¤„ç†ï¼ˆæ›´é«˜æ•ˆï¼‰
results = await runnable.abatch(["a", "b", "c"])
```

---

## å®æˆ˜ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹ 1: æ‰¹é‡æ–‡æ¡£åˆ†ç±»

```python
"""
æ‰¹é‡æ–‡æ¡£åˆ†ç±»
æ¼”ç¤º batch æ–¹æ³•çš„å®é™…åº”ç”¨
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
import time

# å®šä¹‰åˆ†ç±»é“¾
prompt = ChatPromptTemplate.from_template(
    "å°†ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»ä¸ºï¼šæŠ€æœ¯ã€å•†ä¸šã€å¨±ä¹ã€ä½“è‚²\n\næ–‡æœ¬: {text}\n\nåˆ†ç±»:"
)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
parser = StrOutputParser()

chain = prompt | llm | parser

# æµ‹è¯•æ•°æ®
documents = [
    "Python 3.13 å‘å¸ƒäº†æ–°ç‰¹æ€§",
    "è‹¹æœå…¬å¸å‘å¸ƒè´¢æŠ¥",
    "ç”µå½±ã€Šæµæµªåœ°çƒ3ã€‹ä¸Šæ˜ ",
    "NBA æ€»å†³èµ›å¼€å§‹",
    "æœºå™¨å­¦ä¹ ç®—æ³•ä¼˜åŒ–",
]

# ===== æ–¹å¼ 1: é¡ºåºæ‰§è¡Œ =====
print("=== é¡ºåºæ‰§è¡Œ ===")
start = time.time()
results_sequential = []
for doc in documents:
    result = chain.invoke({"text": doc})
    results_sequential.append(result)
elapsed_sequential = time.time() - start
print(f"è€—æ—¶: {elapsed_sequential:.2f}ç§’")
print(f"ç»“æœ: {results_sequential}\n")

# ===== æ–¹å¼ 2: æ‰¹é‡æ‰§è¡Œ =====
print("=== æ‰¹é‡æ‰§è¡Œ ===")
start = time.time()
inputs = [{"text": doc} for doc in documents]
results_batch = chain.batch(inputs)
elapsed_batch = time.time() - start
print(f"è€—æ—¶: {elapsed_batch:.2f}ç§’")
print(f"ç»“æœ: {results_batch}")
print(f"åŠ é€Ÿæ¯”: {elapsed_sequential / elapsed_batch:.2f}x\n")

# ===== æ–¹å¼ 3: å¸¦å¹¶å‘æ§åˆ¶ =====
print("=== å¸¦å¹¶å‘æ§åˆ¶ ===")
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(max_concurrency=2)
start = time.time()
results_controlled = chain.batch(inputs, config=config)
elapsed_controlled = time.time() - start
print(f"è€—æ—¶: {elapsed_controlled:.2f}ç§’")
print(f"ç»“æœ: {results_controlled}")
```

### ç¤ºä¾‹ 2: æˆæœ¬è¿½è¸ª

```python
"""
æ‰¹é‡å¤„ç†æˆæœ¬è¿½è¸ª
æ¼”ç¤ºå¦‚ä½•ç›‘æ§ token ä½¿ç”¨å’Œæˆæœ¬
"""

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.runnables import RunnableConfig

class CostTracker(BaseCallbackHandler):
    """æˆæœ¬è¿½è¸ªå›è°ƒ"""

    def __init__(self):
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    def on_llm_end(self, response, **kwargs):
        """LLM è°ƒç”¨ç»“æŸæ—¶ç»Ÿè®¡ token"""
        if hasattr(response, 'llm_output') and response.llm_output:
            token_usage = response.llm_output.get('token_usage', {})
            self.prompt_tokens += token_usage.get('prompt_tokens', 0)
            self.completion_tokens += token_usage.get('completion_tokens', 0)
            self.total_tokens += token_usage.get('total_tokens', 0)

    def get_cost(self, model="gpt-4o-mini"):
        """è®¡ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰"""
        # gpt-4o-mini ä»·æ ¼ï¼ˆ2026å¹´ï¼‰
        prompt_cost = self.prompt_tokens * 0.00015 / 1000
        completion_cost = self.completion_tokens * 0.0006 / 1000
        return prompt_cost + completion_cost

    def report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        print(f"ğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡:")
        print(f"  - Prompt tokens: {self.prompt_tokens}")
        print(f"  - Completion tokens: {self.completion_tokens}")
        print(f"  - Total tokens: {self.total_tokens}")
        print(f"ğŸ’° é¢„ä¼°æˆæœ¬: ${self.get_cost():.4f}")


# ä½¿ç”¨æˆæœ¬è¿½è¸ª
tracker = CostTracker()
config = RunnableConfig(callbacks=[tracker])

# æ‰¹é‡å¤„ç†
inputs = [{"text": f"æ–‡æœ¬{i}"} for i in range(100)]
results = chain.batch(inputs, config=config)

# æŸ¥çœ‹æˆæœ¬
tracker.report()
```

---

## é”™è¯¯å¤„ç†

### å•ä¸ªä»»åŠ¡å¤±è´¥

```python
from langchain_core.runnables import RunnableLambda

def risky_process(x: str) -> str:
    if x == "error":
        raise ValueError("å¤„ç†å¤±è´¥")
    return x.upper()

runnable = RunnableLambda(risky_process)

# âŒ å•ä¸ªå¤±è´¥å¯¼è‡´æ•´ä¸ª batch å¤±è´¥
try:
    results = runnable.batch(["a", "error", "c"])
except ValueError as e:
    print(f"Batch å¤±è´¥: {e}")

# âœ… ä½¿ç”¨ try-except åŒ…è£…
def safe_process(x: str) -> str:
    try:
        return risky_process(x)
    except Exception as e:
        return f"ERROR: {e}"

safe_runnable = RunnableLambda(safe_process)
results = safe_runnable.batch(["a", "error", "c"])
print(results)  # ["A", "ERROR: å¤„ç†å¤±è´¥", "C"]
```

---

## 2025-2026 æœ€ä½³å®è·µ

### 1. ä½¿ç”¨ batch è€Œéå¾ªç¯

```python
# âŒ ä¸æ¨è
results = [chain.invoke(input) for input in inputs]

# âœ… æ¨è
results = chain.batch(inputs)
```

### 2. åˆç†è®¾ç½®å¹¶å‘æ•°

```python
# âœ… æ ¹æ® API é™åˆ¶è®¾ç½®
config = RunnableConfig(max_concurrency=5)
results = chain.batch(inputs, config=config)
```

### 3. ç›‘æ§æˆæœ¬

```python
# âœ… ä½¿ç”¨å›è°ƒè¿½è¸ªæˆæœ¬
tracker = CostTracker()
config = RunnableConfig(callbacks=[tracker])
results = chain.batch(inputs, config=config)
tracker.report()
```

### 4. è€ƒè™‘ langasync

```python
# âœ… éå®æ—¶åœºæ™¯ä½¿ç”¨ langasync
if not real_time_required:
    async_chain = wrap_chain(chain, batch_size=10)
    results = await async_chain.abatch(inputs)  # 50% æˆæœ¬èŠ‚çœ
```

---

## æ€»ç»“

### batch æ–¹æ³•çš„æ ¸å¿ƒä»·å€¼

1. **å¹¶å‘æ‰§è¡Œ**: è‡ªåŠ¨å¹¶è¡Œå¤„ç†å¤šä¸ªè¾“å…¥
2. **æˆæœ¬ä¼˜åŒ–**: ç»“åˆ langasync é™ä½ 50% æˆæœ¬
3. **é¡ºåºä¿è¯**: è¾“å‡ºé¡ºåºä¸è¾“å…¥ä¸€è‡´
4. **ç®€å•æ˜“ç”¨**: æ— éœ€æ‰‹åŠ¨ç®¡ç†çº¿ç¨‹æ± 

### ä½•æ—¶ä½¿ç”¨ batch

- âœ… æ‰¹é‡å¤„ç†å¤šä¸ªç‹¬ç«‹ä»»åŠ¡
- âœ… è¯„ä¼°å’Œæµ‹è¯•åœºæ™¯
- âœ… æˆæœ¬æ•æ„Ÿçš„ç¦»çº¿ä»»åŠ¡
- âŒ å•æ¬¡æŸ¥è¯¢ï¼ˆç”¨ invokeï¼‰
- âŒ å®æ—¶æµå¼è¾“å‡ºï¼ˆç”¨ streamï¼‰

---

## å‚è€ƒèµ„æ–™

[^1]: [Runnable batch Reference](https://reference.langchain.com/python/langchain_core/runnables) - LangChain, 2025-2026
[^2]: [langasync GitHub](https://github.com/langasync/langasync) - 50% Cost Savings, 2025-2026
[^3]: [LangChain Batch Processing Cost Optimization](https://medium.com/@vinodkrane/langchain-in-production-performance-security-and-cost-optimization-d5e0b44a26fd) - Medium, 2025

---

**ä¸‹ä¸€æ­¥**: é˜…è¯» [03_æ ¸å¿ƒæ¦‚å¿µ_04_streamæ–¹æ³•.md](./03_æ ¸å¿ƒæ¦‚å¿µ_04_streamæ–¹æ³•.md) å­¦ä¹ æµå¼è¾“å‡º
