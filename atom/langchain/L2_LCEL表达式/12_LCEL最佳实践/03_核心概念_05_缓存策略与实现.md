# 核心概念：缓存策略与实现

> **本节目标**：掌握LCEL的缓存机制，实现65-90%成本降低和85%延迟优化。

---

## 一、为什么需要缓存？

### 1.1 成本与性能挑战

**LLM调用成本高**：
- GPT-4：$0.03/1K tokens（输入）
- 重复查询：浪费成本
- 延迟：1-3秒/请求

**缓存收益**：
- **成本降低**：65-90%
- **延迟优化**：85%
- **吞吐量提升**：10-20x

### 1.2 适用场景

| 场景 | 缓存类型 | 收益 |
|------|----------|------|
| 重复查询 | 请求-响应缓存 | 90%成本降低 |
| 相似查询 | 语义缓存 | 65%成本降低 |
| 固定Prompt | Prompt缓存 | 85%延迟降低 |
| RAG上下文 | 上下文缓存 | 80%成本降低 |

---

## 二、Prompt缓存（Amazon Bedrock）

### 2.1 核心原理

**Prompt缓存**：缓存重复的prompt前缀，避免重复处理。

**工作机制**：
```
第1次请求：
  System Prompt (1000 tokens) → 处理 → 缓存
  User Query (50 tokens) → 处理
  总成本：1050 tokens

第2次请求（相同System Prompt）：
  System Prompt (1000 tokens) → 从缓存读取（90%折扣）
  User Query (50 tokens) → 处理
  总成本：150 tokens（85%降低）
```

### 2.2 基础用法

```python
from langchain_openai import ChatOpenAI

# 启用Prompt缓存
llm = ChatOpenAI(
    model="gpt-4o",
    cache=True  # 自动缓存system prompt
)

# 固定的system prompt会被缓存
system_prompt = """你是一个专业的客服助手。
以下是公司的FAQ知识库：
[大量FAQ内容...]
"""

# 第1次调用：完整处理
response1 = llm.invoke([
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "如何退货？"}
])

# 第2次调用：system prompt从缓存读取
response2 = llm.invoke([
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "如何换货？"}
])
# 成本降低90%，延迟降低85%
```

### 2.3 最佳实践

**适用场景**：
- System prompts（固定指令）
- Few-shot examples（示例数据）
- RAG上下文（检索结果）
- 长文档分析

**优化技巧**：
```python
# ✅ 将固定内容放在前面
messages = [
    {"role": "system", "content": fixed_instructions},  # 缓存
    {"role": "user", "content": variable_query}         # 不缓存
]

# ❌ 固定内容在后面（无法缓存）
messages = [
    {"role": "user", "content": variable_query},
    {"role": "system", "content": fixed_instructions}
]
```

---

## 三、请求-响应缓存

### 3.1 核心原理

**请求-响应缓存**：存储完整的请求-响应对，完全跳过LLM调用。

**工作机制**：
```
第1次请求：
  Query → LLM → Response → 存入缓存
  成本：100%

第2次相同请求：
  Query → 缓存命中 → Response
  成本：0%（100%降低）
```

### 3.2 InMemoryCache（本地缓存）

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# 启用内存缓存
set_llm_cache(InMemoryCache())

llm = ChatOpenAI(model="gpt-4o")

# 第1次调用：完整LLM请求
response1 = llm.invoke("什么是LCEL？")
# 耗时：2秒，成本：$0.01

# 第2次调用：从缓存返回
response2 = llm.invoke("什么是LCEL？")
# 耗时：<1ms，成本：$0（100%降低）
```

**特点**：
- ✅ 极快（微秒级）
- ✅ 零配置
- ❌ 不持久（重启丢失）
- ❌ 不共享（单进程）

### 3.3 Redis缓存（分布式）

```python
from langchain.cache import RedisCache
from redis import Redis

# 配置Redis缓存
redis_client = Redis(host="localhost", port=6379)
set_llm_cache(RedisCache(redis_client))

llm = ChatOpenAI(model="gpt-4o")

# 多个服务器共享缓存
response = llm.invoke("什么是LCEL？")
# 所有服务器都能命中缓存
```

**特点**：
- ✅ 持久化
- ✅ 多服务器共享
- ✅ TTL支持
- ❌ 需要Redis服务

### 3.4 TTL与缓存失效

```python
from langchain.cache import RedisCache
from redis import Redis

# 设置TTL（生存时间）
redis_client = Redis(host="localhost", port=6379)
cache = RedisCache(redis_client, ttl=3600)  # 1小时后过期

# 添加随机抖动（防止雪崩）
import random

def get_ttl_with_jitter(base_ttl: int) -> int:
    """添加±10%的随机抖动"""
    jitter = random.uniform(-0.1, 0.1)
    return int(base_ttl * (1 + jitter))

# 使用
ttl = get_ttl_with_jitter(3600)  # 3240-3960秒
```

---

## 四、语义缓存

### 4.1 核心原理

**语义缓存**：基于语义相似度匹配，处理查询变体。

**工作机制**：
```
缓存：
  "如何退货？" → Response A
  "退货流程是什么？" → 语义相似 → 返回 Response A
  "怎么退款？" → 语义相似 → 返回 Response A
```

### 4.2 实现示例

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.cache import SemanticCache

# 配置语义缓存
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)
semantic_cache = SemanticCache(
    vectorstore=vectorstore,
    similarity_threshold=0.85  # 相似度阈值
)

set_llm_cache(semantic_cache)

llm = ChatOpenAI(model="gpt-4o")

# 第1次查询
response1 = llm.invoke("如何退货？")
# 完整LLM调用

# 第2次查询（语义相似）
response2 = llm.invoke("退货流程是什么？")
# 从缓存返回（相似度 > 0.85）

# 第3次查询（语义不同）
response3 = llm.invoke("如何充值？")
# 完整LLM调用（相似度 < 0.85）
```

### 4.3 性能权衡

**收益**：
- 处理查询变体
- 65%成本降低

**成本**：
- Embedding计算开销
- 向量检索延迟

**适用场景**：
- 用户查询多样化
- 语义相似度高
- Embedding成本 < LLM成本

---

## 五、多层缓存架构

### 5.1 架构设计

```
请求 → L1: InMemoryCache（微秒级）
       ↓ 未命中
       → L2: RedisCache（毫秒级）
       ↓ 未命中
       → L3: SemanticCache（100ms级）
       ↓ 未命中
       → LLM调用（秒级）
```

### 5.2 实现示例

```python
from langchain.cache import InMemoryCache, RedisCache, SemanticCache
from redis import Redis

class MultiLayerCache:
    """多层缓存"""

    def __init__(self):
        # L1: 内存缓存
        self.l1_cache = InMemoryCache()

        # L2: Redis缓存
        redis_client = Redis(host="localhost", port=6379)
        self.l2_cache = RedisCache(redis_client, ttl=3600)

        # L3: 语义缓存
        self.l3_cache = SemanticCache(
            vectorstore=vectorstore,
            similarity_threshold=0.85
        )

    def get(self, key: str):
        # 尝试L1
        result = self.l1_cache.get(key)
        if result:
            return result

        # 尝试L2
        result = self.l2_cache.get(key)
        if result:
            self.l1_cache.set(key, result)  # 回填L1
            return result

        # 尝试L3
        result = self.l3_cache.get(key)
        if result:
            self.l2_cache.set(key, result)  # 回填L2
            self.l1_cache.set(key, result)  # 回填L1
            return result

        return None

    def set(self, key: str, value: str):
        """写入所有层"""
        self.l1_cache.set(key, value)
        self.l2_cache.set(key, value)
        self.l3_cache.set(key, value)
```

---

## 六、缓存失效策略

### 6.1 TTL-based（时间过期）

```python
# 自动过期
cache = RedisCache(redis_client, ttl=3600)  # 1小时

# 添加抖动
def get_ttl_with_jitter(base_ttl: int) -> int:
    jitter = random.uniform(-0.1, 0.1)
    return int(base_ttl * (1 + jitter))
```

### 6.2 Proactive（主动删除）

```python
# 数据更新时主动删除缓存
def update_faq(faq_id: str, new_content: str):
    # 更新数据
    db.update(faq_id, new_content)

    # 删除相关缓存
    cache.delete(f"faq:{faq_id}")
    cache.delete_pattern("faq:*")  # 删除所有FAQ缓存
```

### 6.3 Predictive（预测性更新）

```python
# 预测性预加载
async def preload_cache():
    """预加载热门查询"""
    hot_queries = get_hot_queries()  # 获取热门查询

    for query in hot_queries:
        if not cache.exists(query):
            # 预先生成响应
            response = await llm.ainvoke(query)
            cache.set(query, response)
```

---

## 七、监控与优化

### 7.1 缓存指标

```python
class CacheMonitor:
    """缓存监控"""

    def __init__(self):
        self.hits = 0
        self.misses = 0

    def record_hit(self):
        self.hits += 1

    def record_miss(self):
        self.misses += 1

    def get_hit_rate(self) -> float:
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    def report(self):
        hit_rate = self.get_hit_rate()
        print(f"缓存命中率: {hit_rate:.2%}")
        print(f"命中次数: {self.hits}")
        print(f"未命中次数: {self.misses}")
```

### 7.2 优化建议

**目标命中率**：
- 生产环境：>60%
- 开发环境：>40%

**优化策略**：
- 命中率低（<40%）：考虑语义缓存
- 命中率高（>80%）：增加TTL
- 内存不足：减少缓存大小或使用LRU

---

## 八、最佳实践

### 8.1 选择合适的缓存

| 场景 | 推荐缓存 | 原因 |
|------|----------|------|
| 开发环境 | InMemoryCache | 简单快速 |
| 生产环境 | RedisCache | 持久化+共享 |
| 查询多样 | SemanticCache | 处理变体 |
| 固定Prompt | Prompt缓存 | 最大收益 |

### 8.2 避免的陷阱

**陷阱1：缓存所有内容**
```python
# ❌ 缓存低频查询（浪费内存）
cache.set(rare_query, response)

# ✅ 只缓存热门查询
if query_frequency > threshold:
    cache.set(query, response)
```

**陷阱2：忽略缓存失效**
```python
# ❌ 永久缓存（数据过期）
cache.set(key, value)

# ✅ 设置TTL
cache.set(key, value, ttl=3600)
```

**陷阱3：缓存雪崩**
```python
# ❌ 所有缓存同时过期
ttl = 3600

# ✅ 添加随机抖动
ttl = get_ttl_with_jitter(3600)
```

---

## 九、总结

### 核心要点

1. **Prompt缓存**：85%延迟降低，90%成本降低
2. **请求-响应缓存**：100%成本降低（命中时）
3. **语义缓存**：处理查询变体，65%成本降低
4. **多层缓存**：L1（微秒）→ L2（毫秒）→ L3（100ms）→ LLM（秒）

### 缓存选择路径

```
固定Prompt？ → Prompt缓存
  ↓ 否
重复查询？ → 请求-响应缓存
  ↓ 否
查询多样？ → 语义缓存
  ↓ 否
不缓存
```

### 实施建议

- ✅ 从简单缓存开始（InMemoryCache）
- ✅ 监控命中率（目标>60%）
- ✅ 添加TTL和抖动
- ✅ 多层缓存提升性能
- ✅ 定期审查缓存策略

---

**下一步**：学习【监控与调试体系】，掌握生产环境可观测性。
