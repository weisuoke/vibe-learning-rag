# 核心概念2：重试策略

## 概述

**重试策略是 LCEL 错误处理的核心，通过智能的重试机制让 AI 应用在面对临时故障时仍能稳定运行。**

本文深入讲解：
- `with_retry()` 方法的完整配置
- 重试策略的设计原则
- 2025-2026 新特性
- 生产级重试实践

---

## 1. with_retry() 方法详解

### 1.1 基础用法

**最简单的重试：**

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

# 添加重试：最多重试 3 次
llm_with_retry = llm.with_retry(stop_after_attempt=3)

# 使用
response = llm_with_retry.invoke("你好")
```

**执行流程：**
```
第1次尝试 → 失败
第2次尝试 → 失败
第3次尝试 → 成功 ✅
```

### 1.2 完整参数列表

**with_retry() 的所有参数：**

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError, Timeout

llm = ChatOpenAI(model="gpt-4").with_retry(
    # ===== 基础参数 =====
    stop_after_attempt=3,              # 最大重试次数（包括首次尝试）

    # ===== 等待策略 =====
    wait_exponential_jitter=True,      # 指数退避 + 随机抖动
    # 等待时间 = base * 2^attempt ± jitter

    # ===== 异常过滤 =====
    retry_if_exception_type=(          # 只重试这些异常
        RateLimitError,
        APIError,
        Timeout
    ),

    # ===== 高级参数（基于 tenacity 库）=====
    # 这些参数会覆盖上面的简化参数
    # stop=stop_after_attempt(3),
    # wait=wait_exponential(multiplier=1, min=1, max=60),
    # retry=retry_if_exception_type((RateLimitError,)),
    # before_sleep=before_sleep_log(logger, logging.WARNING),
    # after=after_log(logger, logging.INFO),
)
```

### 1.3 参数详解

#### 1.3.1 stop_after_attempt：最大重试次数

```python
# 重试 3 次（总共 3 次尝试）
llm = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

# 执行流程：
# 第1次：尝试
# 第2次：重试
# 第3次：重试
# 如果仍失败：抛出异常
```

**选择重试次数的原则：**
- **临时故障**：3-5 次通常足够
- **关键服务**：5-7 次
- **非关键服务**：2-3 次

#### 1.3.2 wait_exponential_jitter：指数退避

```python
# 启用指数退避 + 随机抖动
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=4,
    wait_exponential_jitter=True
)

# 等待时间（近似）：
# 第1次失败：等待 ~1 秒（1 ± 0.2）
# 第2次失败：等待 ~2 秒（2 ± 0.4）
# 第3次失败：等待 ~4 秒（4 ± 0.8）
```

**为什么需要指数退避？**
1. **避免雪崩效应**：所有客户端不会同时重试
2. **给服务器恢复时间**：等待时间递增
3. **符合最佳实践**：大多数 API 推荐的策略

**为什么需要 jitter（抖动）？**
```python
# 没有 jitter：
# 时间 0s：1000 个请求失败
# 时间 1s：1000 个请求同时重试 → 服务器再次过载 ❌

# 有 jitter：
# 时间 0s：1000 个请求失败
# 时间 0.8-1.2s：请求分散重试 → 服务器逐渐恢复 ✅
```

#### 1.3.3 retry_if_exception_type：异常过滤

```python
from openai import RateLimitError, APIError, AuthenticationError

# 只重试特定异常
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    retry_if_exception_type=(RateLimitError, APIError)
)

# 场景1：RateLimitError → 重试 ✅
# 场景2：APIError → 重试 ✅
# 场景3：AuthenticationError → 立即失败 ❌
```

**常见的可重试异常：**
```python
RETRYABLE_ERRORS = (
    RateLimitError,        # 速率限制
    APIError,              # API 服务器错误
    Timeout,               # 超时
    APIConnectionError,    # 连接错误
)
```

**常见的不可重试异常：**
```python
NON_RETRYABLE_ERRORS = (
    AuthenticationError,   # 认证错误
    InvalidRequestError,   # 请求参数错误
    NotFoundError,         # 资源不存在
)
```

---

## 2. 重试配置参数深度解析

### 2.1 基于 tenacity 库的高级配置

**LangChain 的 with_retry() 基于 tenacity 库实现，支持更高级的配置：**

```python
from langchain_openai import ChatOpenAI
from tenacity import (
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
    after_log
)
from openai import RateLimitError, APIError
import logging

logger = logging.getLogger(__name__)

llm = ChatOpenAI(model="gpt-4").with_retry(
    # 停止条件
    stop=stop_after_attempt(3),

    # 等待策略
    wait=wait_exponential(
        multiplier=1,  # 基础乘数
        min=1,         # 最小等待时间（秒）
        max=60         # 最大等待时间（秒）
    ),

    # 重试条件
    retry=retry_if_exception_type((RateLimitError, APIError)),

    # 重试前回调
    before_sleep=before_sleep_log(logger, logging.WARNING),

    # 重试后回调
    after=after_log(logger, logging.INFO)
)
```

### 2.2 等待策略详解

**指数退避的数学公式：**
```
wait_time = min(multiplier * 2^attempt, max_wait)
```

**示例：**
```python
# multiplier=1, min=1, max=60
# 第1次失败：wait = min(1 * 2^0, 60) = 1 秒
# 第2次失败：wait = min(1 * 2^1, 60) = 2 秒
# 第3次失败：wait = min(1 * 2^2, 60) = 4 秒
# 第4次失败：wait = min(1 * 2^3, 60) = 8 秒
# 第5次失败：wait = min(1 * 2^4, 60) = 16 秒
# 第6次失败：wait = min(1 * 2^5, 60) = 32 秒
# 第7次失败：wait = min(1 * 2^6, 60) = 60 秒（达到上限）
```

**其他等待策略：**

```python
from tenacity import wait_fixed, wait_random, wait_combine

# 固定等待
wait=wait_fixed(2)  # 每次等待 2 秒

# 随机等待
wait=wait_random(min=1, max=5)  # 随机等待 1-5 秒

# 组合等待
wait=wait_combine(
    wait_fixed(1),      # 固定 1 秒
    wait_random(0, 2)   # + 随机 0-2 秒
)  # 总等待时间：1-3 秒
```

### 2.3 停止条件详解

**除了 stop_after_attempt，还有其他停止条件：**

```python
from tenacity import stop_after_delay, stop_never

# 在指定时间后停止
stop=stop_after_delay(30)  # 30 秒后停止重试

# 组合停止条件
from tenacity import stop_any, stop_all

stop=stop_any(
    stop_after_attempt(5),   # 最多 5 次
    stop_after_delay(60)     # 或 60 秒后
)  # 满足任一条件就停止
```

### 2.4 重试回调

**before_sleep：重试前执行**

```python
from tenacity import before_sleep_log
import logging

logger = logging.getLogger(__name__)

llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    before_sleep=before_sleep_log(logger, logging.WARNING)
)

# 输出示例：
# WARNING:__main__:Retrying in 1.0 seconds as it raised RateLimitError: Rate limit exceeded
```

**自定义回调：**

```python
def custom_before_sleep(retry_state):
    """自定义重试前回调"""
    attempt = retry_state.attempt_number
    exception = retry_state.outcome.exception()
    print(f"第 {attempt} 次重试，原因: {exception}")

llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    before_sleep=custom_before_sleep
)
```

---

## 3. 2025-2026 新特性

### 3.1 ModelRetryMiddleware（实验性）

**背景：** 2025 年，LangChain 引入了 ModelRetryMiddleware，提供更细粒度的重试控制

**GitHub Issue:** #33983

```python
# 注意：这是实验性功能，API 可能变化
from langchain_core.runnables import ModelRetryMiddleware
from langchain_openai import ChatOpenAI

# 创建中间件
retry_middleware = ModelRetryMiddleware(
    max_retries=3,
    backoff_factor=2,
    retry_on_status_codes=[429, 500, 502, 503, 504]
)

# 应用到模型
llm = ChatOpenAI(model="gpt-4")
llm_with_middleware = retry_middleware.wrap(llm)
```

**与 with_retry() 的区别：**
- **ModelRetryMiddleware**：更底层，可以拦截 HTTP 请求
- **with_retry()**：更高层，基于异常类型重试

### 3.2 LangGraph RetryPolicy

**背景：** 2026 年，LangGraph 引入了节点级的 RetryPolicy

**GitHub Issue:** #6170

```python
from langgraph.graph import StateGraph
from langgraph.retry import RetryPolicy

# 定义重试策略
retry_policy = RetryPolicy(
    max_attempts=3,
    backoff_type="exponential",
    initial_delay=1.0,
    max_delay=60.0
)

# 应用到节点
graph = StateGraph()
graph.add_node(
    "llm_call",
    llm_node,
    retry_policy=retry_policy
)
```

**适用场景：**
- 多步工作流中的单个节点重试
- 需要状态持久化的重试
- 复杂的重试逻辑（如条件重试）

---

## 4. 生产级重试策略

### 4.1 场景1：处理速率限制

**问题：** OpenAI API 有每分钟请求限制（TPM/RPM）

**策略：**

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError
from tenacity import wait_exponential, before_sleep_log
import logging

logger = logging.getLogger(__name__)

# 速率限制专用重试策略
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,              # 速率限制可能需要更多重试
    wait=wait_exponential(
        multiplier=2,                  # 更激进的退避
        min=2,                         # 最少等待 2 秒
        max=120                        # 最多等待 2 分钟
    ),
    retry_if_exception_type=(RateLimitError,),
    before_sleep=before_sleep_log(logger, logging.WARNING)
)

# 批量处理时的速率控制
import time

def batch_process_with_rate_limit(items, llm, delay=1.0):
    """批量处理，带速率控制"""
    results = []
    for i, item in enumerate(items):
        try:
            result = llm.invoke(item)
            results.append(result)
            logger.info(f"成功处理第 {i+1}/{len(items)} 个")

            # 主动延迟，避免触发速率限制
            if i < len(items) - 1:
                time.sleep(delay)

        except RateLimitError as e:
            logger.error(f"速率限制: {e}")
            results.append(None)

    return results
```

### 4.2 场景2：处理网络超时

**问题：** 网络不稳定或请求耗时过长

**策略：**

```python
from langchain_openai import ChatOpenAI
from openai import Timeout, APIConnectionError
from tenacity import wait_exponential

# 网络超时专用重试策略
llm = ChatOpenAI(
    model="gpt-4",
    timeout=30,                        # 设置超时时间
    max_retries=0                      # 禁用 OpenAI SDK 的内置重试
).with_retry(
    stop_after_attempt=3,
    wait=wait_exponential(
        multiplier=1,
        min=1,
        max=10                         # 网络问题不需要等太久
    ),
    retry_if_exception_type=(Timeout, APIConnectionError)
)

# 自适应超时
def adaptive_timeout_invoke(llm, prompt, base_timeout=30):
    """根据重试次数调整超时时间"""
    for attempt in range(3):
        timeout = base_timeout * (attempt + 1)  # 逐渐增加超时时间
        try:
            llm_with_timeout = ChatOpenAI(
                model="gpt-4",
                timeout=timeout
            )
            return llm_with_timeout.invoke(prompt)
        except Timeout:
            if attempt < 2:
                print(f"超时（{timeout}秒），增加超时时间重试...")
            else:
                raise
```

### 4.3 场景3：断路器模式

**问题：** 避免对已知故障的服务持续重试

**策略：**

```python
from langchain_openai import ChatOpenAI
from circuitbreaker import circuit
import time

# 使用断路器装饰器
@circuit(
    failure_threshold=5,               # 5 次失败后打开断路器
    recovery_timeout=60,               # 60 秒后尝试恢复
    expected_exception=Exception
)
def call_llm_with_circuit_breaker(llm, prompt):
    """带断路器的 LLM 调用"""
    return llm.invoke(prompt)

# 使用
llm = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

try:
    response = call_llm_with_circuit_breaker(llm, "你好")
    print(response.content)
except Exception as e:
    print(f"断路器打开或调用失败: {e}")
```

**断路器状态机：**
```
正常状态（Closed）
    ↓ 连续 5 次失败
断开状态（Open）
    ↓ 60 秒后
半开状态（Half-Open）
    ↓ 测试成功
正常状态（Closed）
```

### 4.4 场景4：多层重试策略

**问题：** 不同组件需要不同的重试策略

**策略：**

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from openai import RateLimitError, APIError, Timeout

# 第1层：Embedding（快速失败）
embeddings = OpenAIEmbeddings().with_retry(
    stop_after_attempt=2,              # 只重试 2 次
    wait_exponential_jitter=True,
    retry_if_exception_type=(Timeout,)
)

# 第2层：LLM（关键组件，更多重试）
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,              # 重试 5 次
    wait_exponential_jitter=True,
    retry_if_exception_type=(RateLimitError, APIError, Timeout)
)

# 第3层：Parser（基础重试）
parser = JsonOutputParser().with_retry(
    stop_after_attempt=2
)

# 组合
prompt = ChatPromptTemplate.from_template("以 JSON 格式返回: {query}")
chain = prompt | llm | parser
```

### 4.5 场景5：自适应重试

**问题：** 根据错误类型动态调整重试策略

**策略：**

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError, Timeout
import time

def adaptive_retry_invoke(llm, prompt, max_attempts=5):
    """自适应重试策略"""
    for attempt in range(max_attempts):
        try:
            return llm.invoke(prompt)

        except RateLimitError as e:
            # 速率限制：长时间等待
            wait_time = min(2 ** attempt * 5, 120)
            print(f"速率限制，等待 {wait_time} 秒...")
            time.sleep(wait_time)

        except Timeout as e:
            # 超时：短时间等待
            wait_time = 2 ** attempt
            print(f"超时，等待 {wait_time} 秒...")
            time.sleep(wait_time)

        except APIError as e:
            # API 错误：中等等待
            wait_time = 2 ** attempt * 2
            print(f"API 错误，等待 {wait_time} 秒...")
            time.sleep(wait_time)

        except Exception as e:
            # 其他错误：立即失败
            raise

    raise Exception("达到最大重试次数")

# 使用
llm = ChatOpenAI(model="gpt-4")
response = adaptive_retry_invoke(llm, "你好")
```

---

## 5. 渐进式示例

### 示例1：基础重试

```python
from langchain_openai import ChatOpenAI

# 最简单的重试
llm = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

response = llm.invoke("你好")
print(response.content)
```

### 示例2：指数退避

```python
from langchain_openai import ChatOpenAI

# 添加指数退避
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
)

response = llm.invoke("你好")
```

### 示例3：异常过滤

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError

# 只重试特定异常
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True,
    retry_if_exception_type=(RateLimitError, APIError)
)

response = llm.invoke("你好")
```

### 示例4：添加日志

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError
from tenacity import before_sleep_log
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 添加重试日志
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True,
    retry_if_exception_type=(RateLimitError, APIError),
    before_sleep=before_sleep_log(logger, logging.WARNING)
)

response = llm.invoke("你好")
```

### 示例5：自定义等待策略

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError
from tenacity import wait_exponential

# 自定义等待策略
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,
    wait=wait_exponential(
        multiplier=2,  # 更激进的退避
        min=2,         # 最少等待 2 秒
        max=120        # 最多等待 2 分钟
    ),
    retry_if_exception_type=(RateLimitError,)
)

response = llm.invoke("你好")
```

### 示例6：生产级配置

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError, Timeout
from tenacity import wait_exponential, before_sleep_log, after_log
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 生产级重试配置
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,
    wait=wait_exponential(
        multiplier=1,
        min=1,
        max=60
    ),
    retry_if_exception_type=(RateLimitError, APIError, Timeout),
    before_sleep=before_sleep_log(logger, logging.WARNING),
    after=after_log(logger, logging.INFO)
)

# 使用
try:
    response = llm.invoke("你好")
    print(response.content)
except Exception as e:
    logger.error(f"最终失败: {e}")
```

---

## 6. 最佳实践总结

### 6.1 重试次数选择

| 场景 | 推荐次数 | 原因 |
|------|----------|------|
| 速率限制 | 5-7 次 | 可能需要较长等待 |
| 网络超时 | 3-5 次 | 网络问题通常短暂 |
| API 错误 | 3-5 次 | 服务器问题通常短暂 |
| 解析错误 | 2-3 次 | LLM 输出不稳定 |
| 非关键服务 | 2-3 次 | 快速失败 |

### 6.2 等待策略选择

| 场景 | 推荐策略 | 配置 |
|------|----------|------|
| 速率限制 | 指数退避 | multiplier=2, max=120 |
| 网络超时 | 指数退避 | multiplier=1, max=10 |
| API 错误 | 指数退避 | multiplier=1, max=60 |
| 通用场景 | 指数退避 + jitter | wait_exponential_jitter=True |

### 6.3 异常过滤原则

**可重试的异常：**
```python
RETRYABLE_ERRORS = (
    RateLimitError,        # 速率限制
    APIError,              # API 错误
    Timeout,               # 超时
    APIConnectionError,    # 连接错误
)
```

**不可重试的异常：**
```python
NON_RETRYABLE_ERRORS = (
    AuthenticationError,   # 认证错误
    InvalidRequestError,   # 参数错误
    NotFoundError,         # 资源不存在
)
```

### 6.4 生产级模板

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError, Timeout
from tenacity import wait_exponential, before_sleep_log
import logging

logger = logging.getLogger(__name__)

# 生产级重试模板
def create_production_llm(model="gpt-4", max_retries=5):
    """创建生产级 LLM"""
    return ChatOpenAI(model=model).with_retry(
        stop_after_attempt=max_retries,
        wait=wait_exponential(
            multiplier=1,
            min=1,
            max=60
        ),
        retry_if_exception_type=(RateLimitError, APIError, Timeout),
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )

# 使用
llm = create_production_llm()
```

---

**记住：重试策略不是"越多越好"，而是"恰到好处"。**
