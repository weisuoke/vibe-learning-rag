# æ ¸å¿ƒæ¦‚å¿µ3ï¼šEmbeddingå‘é‡ç¼“å­˜

## æ¦‚è¿°

Embedding APIè°ƒç”¨è™½ç„¶æ¯”LLMä¾¿å®œï¼Œä½†åœ¨é«˜é¢‘åœºæ™¯ä¸‹ä»ä¼šäº§ç”Ÿå¯è§‚æˆæœ¬ã€‚ç¼“å­˜Embeddingå‘é‡å¯ä»¥æ˜¾è‘—é™ä½æˆæœ¬å’Œå»¶è¿Ÿã€‚

---

## 1. ä¸ºä»€ä¹ˆéœ€è¦ç¼“å­˜Embeddingï¼Ÿ

### æˆæœ¬åˆ†æ

```python
# Embedding APIæˆæœ¬ï¼ˆOpenAI text-embedding-3-smallï¼‰
# ä»·æ ¼ï¼š$0.00002 / 1K tokens

# åœºæ™¯ï¼šè¯­ä¹‰ç¼“å­˜ç³»ç»Ÿï¼Œæ¯å¤©10000ä¸ªæŸ¥è¯¢
daily_queries = 10000
avg_tokens_per_query = 50  # å¹³å‡æ¯ä¸ªæŸ¥è¯¢50 tokens

# æ²¡æœ‰ç¼“å­˜çš„æˆæœ¬
cost_per_query = (avg_tokens_per_query / 1000) * 0.00002
daily_cost_no_cache = daily_queries * cost_per_query
monthly_cost_no_cache = daily_cost_no_cache * 30

print(f"æ¯æ—¥æˆæœ¬ï¼ˆæ— ç¼“å­˜ï¼‰: ${daily_cost_no_cache:.4f}")
print(f"æ¯æœˆæˆæœ¬ï¼ˆæ— ç¼“å­˜ï¼‰: ${monthly_cost_no_cache:.2f}")

# æœ‰ç¼“å­˜çš„æˆæœ¬ï¼ˆå‡è®¾80%å‘½ä¸­ç‡ï¼‰
hit_rate = 0.8
api_calls_with_cache = daily_queries * (1 - hit_rate)
daily_cost_with_cache = api_calls_with_cache * cost_per_query
monthly_cost_with_cache = daily_cost_with_cache * 30

print(f"æ¯æ—¥æˆæœ¬ï¼ˆæœ‰ç¼“å­˜ï¼‰: ${daily_cost_with_cache:.4f}")
print(f"æ¯æœˆæˆæœ¬ï¼ˆæœ‰ç¼“å­˜ï¼‰: ${monthly_cost_with_cache:.2f}")
print(f"æ¯æœˆèŠ‚çœ: ${monthly_cost_no_cache - monthly_cost_with_cache:.2f}")
```

**è¾“å‡ºï¼š**
```
æ¯æ—¥æˆæœ¬ï¼ˆæ— ç¼“å­˜ï¼‰: $0.0100
æ¯æœˆæˆæœ¬ï¼ˆæ— ç¼“å­˜ï¼‰: $0.30
æ¯æ—¥æˆæœ¬ï¼ˆæœ‰ç¼“å­˜ï¼‰: $0.0020
æ¯æœˆæˆæœ¬ï¼ˆæœ‰ç¼“å­˜ï¼‰: $0.06
æ¯æœˆèŠ‚çœ: $0.24
```

---

## 2. Embeddingç¼“å­˜å®ç°

### åŸºç¡€å®ç°

```python
import hashlib
import json
from typing import List, Optional
import redis
from openai import OpenAI

class EmbeddingCache:
    """Embeddingå‘é‡ç¼“å­˜"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.openai_client = OpenAI()

    def _generate_cache_key(self, text: str, model: str) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        content = f"{model}:{text}"
        return f"emb:{hashlib.md5(content.encode()).hexdigest()}"

    def get_embedding(
        self,
        text: str,
        model: str = "text-embedding-3-small"
    ) -> List[float]:
        """è·å–Embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = self._generate_cache_key(text, model)

        # 1. æŸ¥è¯¢ç¼“å­˜
        cached = self.redis.get(cache_key)
        if cached:
            print(f"âœ… Embeddingç¼“å­˜å‘½ä¸­")
            return json.loads(cached)

        # 2. è°ƒç”¨API
        print(f"ğŸ¤– è°ƒç”¨Embedding API")
        response = self.openai_client.embeddings.create(
            model=model,
            input=text
        )
        embedding = response.data[0].embedding

        # 3. ç¼“å­˜ç»“æœï¼ˆ24å°æ—¶ï¼‰
        self.redis.setex(cache_key, 86400, json.dumps(embedding))

        return embedding

    def get_embeddings_batch(
        self,
        texts: List[str],
        model: str = "text-embedding-3-small"
    ) -> List[List[float]]:
        """æ‰¹é‡è·å–Embedding"""
        results = []
        uncached_texts = []
        uncached_indices = []

        # 1. æ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            cache_key = self._generate_cache_key(text, model)
            cached = self.redis.get(cache_key)

            if cached:
                results.append(json.loads(cached))
            else:
                results.append(None)
                uncached_texts.append(text)
                uncached_indices.append(i)

        # 2. æ‰¹é‡è°ƒç”¨APIï¼ˆæœªå‘½ä¸­çš„ï¼‰
        if uncached_texts:
            print(f"ğŸ¤– æ‰¹é‡è°ƒç”¨API: {len(uncached_texts)}ä¸ªæ–‡æœ¬")
            response = self.openai_client.embeddings.create(
                model=model,
                input=uncached_texts
            )

            # 3. ç¼“å­˜å¹¶å¡«å……ç»“æœ
            for i, embedding_data in enumerate(response.data):
                embedding = embedding_data.embedding
                original_index = uncached_indices[i]
                text = uncached_texts[i]

                # ç¼“å­˜
                cache_key = self._generate_cache_key(text, model)
                self.redis.setex(cache_key, 86400, json.dumps(embedding))

                # å¡«å……ç»“æœ
                results[original_index] = embedding

        print(f"âœ… ç¼“å­˜å‘½ä¸­: {len(texts) - len(uncached_texts)}/{len(texts)}")
        return results

# ä½¿ç”¨ç¤ºä¾‹
cache = EmbeddingCache(redis_client)

# å•ä¸ªæ–‡æœ¬
embedding = cache.get_embedding("Python is a programming language")

# æ‰¹é‡æ–‡æœ¬
texts = [
    "Python is a programming language",
    "JavaScript is used for web development",
    "Rust is a systems programming language"
]
embeddings = cache.get_embeddings_batch(texts)
```

---

## 3. ä½¿ç”¨Hashå­˜å‚¨å‘é‡

### ä¸ºä»€ä¹ˆä½¿ç”¨Hashï¼Ÿ

```python
# Stringæ–¹å¼ï¼šæ¯ä¸ªå‘é‡ä¸€ä¸ªkey
redis_client.setex("emb:abc123", 86400, json.dumps(embedding))
# é—®é¢˜ï¼šå¤§é‡keyï¼Œå†…å­˜å¼€é”€å¤§

# Hashæ–¹å¼ï¼šæ‰€æœ‰å‘é‡å­˜åœ¨ä¸€ä¸ªHashä¸­
redis_client.hset("embeddings", "abc123", json.dumps(embedding))
# ä¼˜ç‚¹ï¼šå‡å°‘keyæ•°é‡ï¼Œé™ä½å†…å­˜å¼€é”€
```

### Hashå®ç°

```python
class HashEmbeddingCache:
    """ä½¿ç”¨Hashå­˜å‚¨Embedding"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.openai_client = OpenAI()
        self.hash_key = "embeddings"  # ç»Ÿä¸€çš„Hash key

    def _generate_field_name(self, text: str, model: str) -> str:
        """ç”ŸæˆHashå­—æ®µå"""
        content = f"{model}:{text}"
        return hashlib.md5(content.encode()).hexdigest()

    def get_embedding(
        self,
        text: str,
        model: str = "text-embedding-3-small"
    ) -> List[float]:
        """è·å–Embedding"""
        field_name = self._generate_field_name(text, model)

        # 1. æŸ¥è¯¢Hash
        cached = self.redis.hget(self.hash_key, field_name)
        if cached:
            print(f"âœ… Hashç¼“å­˜å‘½ä¸­")
            return json.loads(cached)

        # 2. è°ƒç”¨API
        print(f"ğŸ¤– è°ƒç”¨Embedding API")
        response = self.openai_client.embeddings.create(
            model=model,
            input=text
        )
        embedding = response.data[0].embedding

        # 3. å­˜å…¥Hash
        self.redis.hset(self.hash_key, field_name, json.dumps(embedding))

        # 4. è®¾ç½®Hashçš„è¿‡æœŸæ—¶é—´ï¼ˆå¯é€‰ï¼‰
        # æ³¨æ„ï¼šHashçš„TTLæ˜¯æ•´ä¸ªHashï¼Œä¸æ˜¯å•ä¸ªå­—æ®µ
        self.redis.expire(self.hash_key, 86400)

        return embedding

    def get_cache_size(self) -> int:
        """è·å–ç¼“å­˜çš„å‘é‡æ•°é‡"""
        return self.redis.hlen(self.hash_key)

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.redis.delete(self.hash_key)
```

---

## 4. å†…å­˜ä¼˜åŒ–ï¼šå‹ç¼©å­˜å‚¨

### å‘é‡å‹ç¼©

```python
import numpy as np
import struct

class CompressedEmbeddingCache:
    """å‹ç¼©å­˜å‚¨Embedding"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.openai_client = OpenAI()

    def _compress_embedding(self, embedding: List[float]) -> bytes:
        """å‹ç¼©å‘é‡ï¼ˆfloat32ï¼‰"""
        # å°†float64è½¬ä¸ºfloat32ï¼Œå‡å°‘50%å†…å­˜
        arr = np.array(embedding, dtype=np.float32)
        return arr.tobytes()

    def _decompress_embedding(self, data: bytes) -> List[float]:
        """è§£å‹ç¼©å‘é‡"""
        arr = np.frombuffer(data, dtype=np.float32)
        return arr.tolist()

    def get_embedding(
        self,
        text: str,
        model: str = "text-embedding-3-small"
    ) -> List[float]:
        """è·å–Embeddingï¼ˆå‹ç¼©å­˜å‚¨ï¼‰"""
        cache_key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"

        # 1. æŸ¥è¯¢ç¼“å­˜
        cached = self.redis.get(cache_key)
        if cached:
            print(f"âœ… ç¼“å­˜å‘½ä¸­ï¼ˆå‹ç¼©ï¼‰")
            return self._decompress_embedding(cached)

        # 2. è°ƒç”¨API
        print(f"ğŸ¤– è°ƒç”¨Embedding API")
        response = self.openai_client.embeddings.create(
            model=model,
            input=text
        )
        embedding = response.data[0].embedding

        # 3. å‹ç¼©å¹¶ç¼“å­˜
        compressed = self._compress_embedding(embedding)
        self.redis.setex(cache_key, 86400, compressed)

        print(f"ğŸ’¾ å‹ç¼©ç‡: {len(json.dumps(embedding)) / len(compressed):.2f}x")

        return embedding

# å†…å­˜å¯¹æ¯”
embedding = [0.1] * 1536  # text-embedding-3-smallçš„ç»´åº¦

# JSONå­˜å‚¨
json_size = len(json.dumps(embedding))
print(f"JSONå¤§å°: {json_size} bytes")  # ~12KB

# å‹ç¼©å­˜å‚¨ï¼ˆfloat32ï¼‰
compressed_size = len(np.array(embedding, dtype=np.float32).tobytes())
print(f"å‹ç¼©å¤§å°: {compressed_size} bytes")  # ~6KB

print(f"å‹ç¼©ç‡: {json_size / compressed_size:.2f}x")  # ~2x
```

---

## 5. æ··åˆç¼“å­˜ï¼šå†…å­˜ + Redis

### ä¸¤å±‚ç¼“å­˜

```python
from collections import OrderedDict

class TieredEmbeddingCache:
    """ä¸¤å±‚ç¼“å­˜ï¼šå†…å­˜ + Redis"""

    def __init__(self, redis_client: redis.Redis, memory_size: int = 1000):
        self.redis = redis_client
        self.openai_client = OpenAI()
        self.memory_cache = OrderedDict()  # LRUç¼“å­˜
        self.memory_size = memory_size

    def get_embedding(
        self,
        text: str,
        model: str = "text-embedding-3-small"
    ) -> List[float]:
        """ä¸¤å±‚ç¼“å­˜è·å–"""
        cache_key = f"{model}:{text}"

        # 1. æŸ¥è¯¢å†…å­˜ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        if cache_key in self.memory_cache:
            print(f"ğŸ¯ å†…å­˜ç¼“å­˜å‘½ä¸­")
            self.memory_cache.move_to_end(cache_key)  # LRUæ›´æ–°
            return self.memory_cache[cache_key]

        # 2. æŸ¥è¯¢Redisç¼“å­˜
        redis_key = f"emb:{hashlib.md5(cache_key.encode()).hexdigest()}"
        cached = self.redis.get(redis_key)
        if cached:
            print(f"âœ… Redisç¼“å­˜å‘½ä¸­")
            embedding = json.loads(cached)
            self._set_memory_cache(cache_key, embedding)
            return embedding

        # 3. è°ƒç”¨API
        print(f"ğŸ¤– è°ƒç”¨Embedding API")
        response = self.openai_client.embeddings.create(
            model=model,
            input=text
        )
        embedding = response.data[0].embedding

        # 4. åŒæ—¶ç¼“å­˜åˆ°å†…å­˜å’ŒRedis
        self._set_memory_cache(cache_key, embedding)
        self.redis.setex(redis_key, 86400, json.dumps(embedding))

        return embedding

    def _set_memory_cache(self, key: str, embedding: List[float]):
        """è®¾ç½®å†…å­˜ç¼“å­˜ï¼ˆLRUæ·˜æ±°ï¼‰"""
        if len(self.memory_cache) >= self.memory_size:
            self.memory_cache.popitem(last=False)  # åˆ é™¤æœ€æ—§çš„
        self.memory_cache[key] = embedding
```

---

## 6. åœ¨è¯­ä¹‰ç¼“å­˜ä¸­çš„åº”ç”¨

### å®Œæ•´ç¤ºä¾‹

```python
class SemanticCache:
    """è¯­ä¹‰ç¼“å­˜ï¼ˆä½¿ç”¨Embeddingç¼“å­˜ï¼‰"""

    def __init__(
        self,
        redis_client: redis.Redis,
        embedding_cache: EmbeddingCache
    ):
        self.redis = redis_client
        self.embedding_cache = embedding_cache

    def add_cache(
        self,
        query: str,
        response: str,
        ttl: int = 3600
    ):
        """æ·»åŠ è¯­ä¹‰ç¼“å­˜"""
        # 1. è·å–queryçš„Embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰
        query_embedding = self.embedding_cache.get_embedding(query)

        # 2. å­˜å‚¨åˆ°Redis Hash
        cache_data = {
            "query": query,
            "response": response,
            "embedding": query_embedding
        }

        cache_id = hashlib.md5(query.encode()).hexdigest()
        self.redis.hset(
            "semantic_cache",
            cache_id,
            json.dumps(cache_data)
        )
        self.redis.expire("semantic_cache", ttl)

    def lookup(
        self,
        query: str,
        threshold: float = 0.9
    ) -> Optional[str]:
        """æŸ¥è¯¢è¯­ä¹‰ç¼“å­˜"""
        # 1. è·å–queryçš„Embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰
        query_embedding = self.embedding_cache.get_embedding(query)

        # 2. è·å–æ‰€æœ‰ç¼“å­˜
        cached_items = self.redis.hgetall("semantic_cache")

        # 3. è®¡ç®—ç›¸ä¼¼åº¦
        best_match = None
        best_score = 0.0

        for cache_id, cache_data_json in cached_items.items():
            cache_data = json.loads(cache_data_json)
            cached_embedding = cache_data["embedding"]

            similarity = self._cosine_similarity(
                query_embedding,
                cached_embedding
            )

            if similarity > best_score:
                best_score = similarity
                best_match = cache_data

        # 4. åˆ¤æ–­æ˜¯å¦å‘½ä¸­
        if best_score >= threshold:
            print(f"ğŸ¯ è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦={best_score:.3f}")
            return best_match["response"]

        print(f"âŒ è¯­ä¹‰ç¼“å­˜æœªå‘½ä¸­ï¼Œæœ€é«˜ç›¸ä¼¼åº¦={best_score:.3f}")
        return None

    def _cosine_similarity(
        self,
        a: List[float],
        b: List[float]
    ) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        import numpy as np
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

---

## æ€»ç»“

1. **åŸºç¡€ç¼“å­˜**ï¼šä½¿ç”¨Stringç±»å‹å­˜å‚¨JSONæ ¼å¼çš„å‘é‡
2. **Hashå­˜å‚¨**ï¼šå‡å°‘keyæ•°é‡ï¼Œé™ä½å†…å­˜å¼€é”€
3. **å‹ç¼©å­˜å‚¨**ï¼šä½¿ç”¨float32ä»£æ›¿float64ï¼ŒèŠ‚çœ50%å†…å­˜
4. **ä¸¤å±‚ç¼“å­˜**ï¼šå†…å­˜ + Redisï¼Œæå‡æŸ¥è¯¢é€Ÿåº¦
5. **æ‰¹é‡å¤„ç†**ï¼šæ‰¹é‡è·å–Embeddingï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
6. **è¯­ä¹‰ç¼“å­˜é›†æˆ**ï¼šEmbeddingç¼“å­˜æ˜¯è¯­ä¹‰ç¼“å­˜çš„åŸºç¡€

**è®°ä½ï¼š** Embeddingç¼“å­˜çš„TTLå¯ä»¥è®¾ç½®è¾ƒé•¿ï¼ˆ24å°æ—¶ï¼‰ï¼Œå› ä¸ºç›¸åŒæ–‡æœ¬çš„Embeddingæ˜¯ç¡®å®šçš„ã€‚
