# 实战代码 - 场景5: RAG数据管理实战

> 完整的 RAG 知识库管理系统，包括文档导入、检索、更新和删除

---

## 场景描述

**目标**：构建一个完整的 RAG 知识库管理系统，支持文档的全生命周期管理。

**功能需求**：
1. 文档导入和索引
2. 语义检索和混合检索
3. 文档更新和版本管理
4. 文档删除和清理
5. 知识库统计和监控

---

## 完整代码

```python
"""
RAG 知识库管理系统
完整的文档生命周期管理
"""

from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Optional
import time
from datetime import datetime
import hashlib


class RAGKnowledgeBase:
    """RAG 知识库管理器"""

    def __init__(self, collection_name="rag_knowledge_base", dim=384):
        self.collection_name = collection_name
        self.dim = dim
        self.collection = None
        self.model = None

    def connect(self, host="localhost", port="19530"):
        """连接到 Milvus"""
        connections.connect(alias="default", host=host, port=port)
        print(f"✓ 已连接到 Milvus")

    def create_collection(self):
        """创建 Collection"""
        if utility.has_collection(self.collection_name):
            print(f"Collection '{self.collection_name}' 已存在")
            self.collection = Collection(name=self.collection_name)
            return

        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=5000),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="chunk_index", dtype=DataType.INT64),
            FieldSchema(name="version", dtype=DataType.INT64),
            FieldSchema(name="created_at", dtype=DataType.INT64),
            FieldSchema(name="updated_at", dtype=DataType.INT64),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.dim)
        ]

        schema = CollectionSchema(fields=fields, description="RAG 知识库")
        self.collection = Collection(name=self.collection_name, schema=schema)

        # 创建索引
        index_params = {
            "index_type": "IVF_FLAT",
            "metric_type": "L2",
            "params": {"nlist": 128}
        }
        self.collection.create_index(field_name="embedding", index_params=index_params)

        print(f"✓ Collection 创建成功")

    def load_model(self, model_name='all-MiniLM-L6-v2'):
        """加载 Embedding 模型"""
        self.model = SentenceTransformer(model_name)
        print("✓ 模型加载成功")

    def load_collection(self):
        """加载 Collection"""
        self.collection.load()
        print("✓ Collection 已加载")

    # ============================================================
    # 文档导入
    # ============================================================

    def import_document(self, doc_id: str, title: str, content: str,
                       source: str, category: str, chunk_size: int = 500):
        """
        导入文档（自动分块）

        Args:
            doc_id: 文档唯一标识
            title: 文档标题
            content: 文档内容
            source: 文档来源
            category: 文档类别
            chunk_size: 分块大小
        """
        print(f"\n导入文档: {title}")

        # 检查文档是否已存在
        existing = self.collection.query(
            expr=f"doc_id == '{doc_id}'",
            output_fields=["doc_id", "version"]
        )

        version = existing[0]["version"] + 1 if existing else 1

        # 如果文档已存在，先删除旧版本
        if existing:
            self.collection.delete(expr=f"doc_id == '{doc_id}'")
            self.collection.flush()
            print(f"  删除旧版本 (v{existing[0]['version']})")

        # 分块
        chunks = self._chunk_text(content, chunk_size)
        print(f"  分为 {len(chunks)} 个块")

        # 生成 Embedding
        texts = [f"{title} {chunk}" for chunk in chunks]
        embeddings = self.model.encode(texts).tolist()

        # 准备数据
        timestamp = int(time.time())
        data = [
            [doc_id] * len(chunks),
            [title] * len(chunks),
            chunks,
            [source] * len(chunks),
            [category] * len(chunks),
            list(range(len(chunks))),
            [version] * len(chunks),
            [timestamp] * len(chunks),
            [timestamp] * len(chunks),
            embeddings
        ]

        # 插入
        self.collection.insert(data)
        self.collection.flush()

        print(f"✓ 文档导入成功 (v{version}, {len(chunks)} 个块)")

    def batch_import_documents(self, documents: List[Dict], chunk_size: int = 500):
        """批量导入文档"""
        print(f"\n批量导入 {len(documents)} 个文档")

        for i, doc in enumerate(documents):
            print(f"\n[{i+1}/{len(documents)}]", end=" ")
            self.import_document(
                doc_id=doc["doc_id"],
                title=doc["title"],
                content=doc["content"],
                source=doc.get("source", "unknown"),
                category=doc.get("category", "general"),
                chunk_size=chunk_size
            )

        print(f"\n✓ 批量导入完成")

    def _chunk_text(self, text: str, chunk_size: int) -> List[str]:
        """文本分块"""
        chunks = []
        words = text.split()

        current_chunk = []
        current_length = 0

        for word in words:
            current_chunk.append(word)
            current_length += len(word) + 1

            if current_length >= chunk_size:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_length = 0

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks

    # ============================================================
    # 文档检索
    # ============================================================

    def search(self, query: str, top_k: int = 5, category: Optional[str] = None):
        """
        语义检索

        Args:
            query: 查询文本
            top_k: 返回 Top-K 结果
            category: 可选的类别过滤
        """
        print(f"\n检索: {query}")

        # 生成查询向量
        query_embedding = self.model.encode([query])[0].tolist()

        # 构建过滤条件
        expr = None
        if category:
            expr = f"category == '{category}'"
            print(f"  过滤: {expr}")

        # 检索
        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=top_k,
            expr=expr,
            output_fields=["doc_id", "title", "content", "source", "chunk_index"]
        )

        print(f"✓ 找到 {len(results[0])} 个相关块")

        # 格式化结果
        formatted_results = []
        for hit in results[0]:
            formatted_results.append({
                "doc_id": hit.entity.get("doc_id"),
                "title": hit.entity.get("title"),
                "content": hit.entity.get("content"),
                "source": hit.entity.get("source"),
                "chunk_index": hit.entity.get("chunk_index"),
                "distance": hit.distance
            })

        return formatted_results

    def search_with_rerank(self, query: str, top_k: int = 5, rerank_top_k: int = 3):
        """
        带重排序的检索

        Args:
            query: 查询文本
            top_k: 初始检索数量
            rerank_top_k: 重排序后返回数量
        """
        print(f"\n检索（带重排序）: {query}")

        # 初始检索
        results = self.search(query, top_k=top_k * 2)

        # 简单的重排序（基于内容相关性）
        query_lower = query.lower()
        for result in results:
            content_lower = result["content"].lower()
            # 计算查询词在内容中的出现次数
            relevance_score = sum(word in content_lower for word in query_lower.split())
            result["relevance_score"] = relevance_score

        # 按相关性排序
        results.sort(key=lambda x: (x["relevance_score"], -x["distance"]), reverse=True)

        print(f"✓ 重排序完成，返回 Top-{rerank_top_k}")

        return results[:rerank_top_k]

    def get_document_chunks(self, doc_id: str):
        """获取文档的所有块"""
        results = self.collection.query(
            expr=f"doc_id == '{doc_id}'",
            output_fields=["doc_id", "title", "content", "chunk_index", "version"]
        )

        # 按 chunk_index 排序
        results.sort(key=lambda x: x["chunk_index"])

        return results

    # ============================================================
    # 文档更新
    # ============================================================

    def update_document(self, doc_id: str, title: Optional[str] = None,
                       content: Optional[str] = None, category: Optional[str] = None):
        """
        更新文档

        Args:
            doc_id: 文档ID
            title: 新标题（可选）
            content: 新内容（可选）
            category: 新类别（可选）
        """
        print(f"\n更新文档: {doc_id}")

        # 查询现有文档
        existing = self.collection.query(
            expr=f"doc_id == '{doc_id}'",
            output_fields=["doc_id", "title", "content", "source", "category", "version"]
        )

        if not existing:
            print(f"✗ 文档 {doc_id} 不存在")
            return

        # 获取原始信息
        original = existing[0]
        new_title = title if title else original["title"]
        new_content = content if content else original["content"]
        new_category = category if category else original["category"]

        # 重新导入文档
        self.import_document(
            doc_id=doc_id,
            title=new_title,
            content=new_content,
            source=original["source"],
            category=new_category
        )

    # ============================================================
    # 文档删除
    # ============================================================

    def delete_document(self, doc_id: str):
        """删除文档"""
        print(f"\n删除文档: {doc_id}")

        # 检查文档是否存在
        existing = self.collection.query(
            expr=f"doc_id == '{doc_id}'",
            output_fields=["doc_id"]
        )

        if not existing:
            print(f"✗ 文档 {doc_id} 不存在")
            return

        # 删除
        self.collection.delete(expr=f"doc_id == '{doc_id}'")
        self.collection.flush()

        print(f"✓ 文档已删除 ({len(existing)} 个块)")

    def delete_by_category(self, category: str):
        """按类别删除文档"""
        print(f"\n删除类别: {category}")

        # 查询该类别的文档
        results = self.collection.query(
            expr=f"category == '{category}'",
            output_fields=["doc_id"]
        )

        if not results:
            print(f"✗ 类别 {category} 下没有文档")
            return

        # 删除
        self.collection.delete(expr=f"category == '{category}'")
        self.collection.flush()

        # 统计删除的文档数
        doc_ids = set(r["doc_id"] for r in results)
        print(f"✓ 已删除 {len(doc_ids)} 个文档 ({len(results)} 个块)")

    def cleanup_old_versions(self, keep_versions: int = 3):
        """清理旧版本"""
        print(f"\n清理旧版本（保留最新 {keep_versions} 个版本）")

        # 查询所有文档
        all_docs = self.collection.query(
            expr="id >= 0",
            output_fields=["doc_id", "version"],
            limit=10000
        )

        # 按 doc_id 分组
        doc_versions = {}
        for doc in all_docs:
            doc_id = doc["doc_id"]
            version = doc["version"]
            if doc_id not in doc_versions:
                doc_versions[doc_id] = []
            doc_versions[doc_id].append(version)

        # 找出需要删除的版本
        to_delete = []
        for doc_id, versions in doc_versions.items():
            versions = sorted(set(versions), reverse=True)
            if len(versions) > keep_versions:
                old_versions = versions[keep_versions:]
                for version in old_versions:
                    to_delete.append((doc_id, version))

        if not to_delete:
            print("✓ 没有需要清理的旧版本")
            return

        # 删除旧版本
        for doc_id, version in to_delete:
            self.collection.delete(expr=f"doc_id == '{doc_id}' and version == {version}")

        self.collection.flush()

        print(f"✓ 已清理 {len(to_delete)} 个旧版本")

    # ============================================================
    # 统计和监控
    # ============================================================

    def get_stats(self):
        """获取统计信息"""
        print("\n知识库统计:")

        # 总块数
        total_chunks = self.collection.num_entities
        print(f"  总块数: {total_chunks}")

        # 文档数
        all_docs = self.collection.query(
            expr="id >= 0",
            output_fields=["doc_id"],
            limit=10000
        )
        unique_docs = len(set(d["doc_id"] for d in all_docs))
        print(f"  文档数: {unique_docs}")

        # 类别统计
        categories = {}
        for doc in all_docs:
            # 需要查询完整信息
            pass

        return {
            "total_chunks": total_chunks,
            "total_documents": unique_docs
        }

    def list_documents(self, limit: int = 10):
        """列出文档"""
        print(f"\n文档列表 (最多 {limit} 个):")

        results = self.collection.query(
            expr="chunk_index == 0",  # 只显示第一个块
            output_fields=["doc_id", "title", "category", "version", "created_at"],
            limit=limit
        )

        for i, doc in enumerate(results):
            created = datetime.fromtimestamp(doc["created_at"]).strftime("%Y-%m-%d %H:%M")
            print(f"  {i+1}. {doc['title']}")
            print(f"     ID: {doc['doc_id']}, 类别: {doc['category']}, 版本: v{doc['version']}, 创建: {created}")

        return results

    # ============================================================
    # RAG 问答
    # ============================================================

    def answer_question(self, question: str, top_k: int = 3):
        """
        RAG 问答

        Args:
            question: 问题
            top_k: 检索 Top-K 文档
        """
        print(f"\n问题: {question}")

        # 检索相关文档
        results = self.search(question, top_k=top_k)

        if not results:
            return "抱歉，没有找到相关信息。"

        # 构建上下文
        context = "\n\n".join([
            f"[{i+1}] {r['title']}\n{r['content']}"
            for i, r in enumerate(results)
        ])

        print(f"\n检索到 {len(results)} 个相关文档块")
        print(f"\n上下文:\n{context[:500]}...")

        # 这里应该调用 LLM 生成答案
        # 为了演示，我们只返回上下文
        answer = f"基于以下文档:\n{context}\n\n[这里应该是 LLM 生成的答案]"

        return answer


def main():
    """主函数"""
    print("=" * 60)
    print("RAG 知识库管理系统")
    print("=" * 60)

    # 初始化
    kb = RAGKnowledgeBase()
    kb.connect()
    kb.create_collection()
    kb.load_model()
    kb.load_collection()

    # 场景 1: 导入文档
    print("\n" + "=" * 60)
    print("场景 1: 导入文档")
    print("=" * 60)

    documents = [
        {
            "doc_id": "doc_001",
            "title": "Python 编程基础",
            "content": "Python 是一种高级编程语言，以其简洁的语法和强大的功能而闻名。Python 支持多种编程范式，包括面向对象、命令式、函数式和过程式编程。Python 拥有丰富的标准库和第三方库，可以用于 Web 开发、数据分析、机器学习、自动化脚本等多个领域。",
            "source": "python_tutorial.pdf",
            "category": "编程"
        },
        {
            "doc_id": "doc_002",
            "title": "机器学习入门",
            "content": "机器学习是人工智能的一个分支，它使计算机能够从数据中学习并做出预测或决策，而无需明确编程。机器学习算法可以分为监督学习、无监督学习和强化学习三大类。监督学习使用标记的训练数据，无监督学习从未标记的数据中发现模式，强化学习通过与环境交互来学习最优策略。",
            "source": "ml_basics.pdf",
            "category": "AI"
        },
        {
            "doc_id": "doc_003",
            "title": "深度学习与神经网络",
            "content": "深度学习是机器学习的一个子领域，它使用多层神经网络来学习数据的复杂表示。深度学习在图像识别、自然语言处理、语音识别等领域取得了突破性进展。常见的深度学习架构包括卷积神经网络（CNN）、循环神经网络（RNN）和 Transformer。",
            "source": "deep_learning.pdf",
            "category": "AI"
        }
    ]

    kb.batch_import_documents(documents, chunk_size=200)

    # 场景 2: 文档检索
    print("\n" + "=" * 60)
    print("场景 2: 文档检索")
    print("=" * 60)

    results = kb.search("什么是机器学习？", top_k=3)
    for i, result in enumerate(results):
        print(f"\n结果 {i+1}:")
        print(f"  文档: {result['title']}")
        print(f"  内容: {result['content'][:100]}...")
        print(f"  距离: {result['distance']:.4f}")

    # 场景 3: 带重排序的检索
    print("\n" + "=" * 60)
    print("场景 3: 带重排序的检索")
    print("=" * 60)

    results = kb.search_with_rerank("Python 编程", top_k=5, rerank_top_k=2)
    for i, result in enumerate(results):
        print(f"\n结果 {i+1}:")
        print(f"  文档: {result['title']}")
        print(f"  相关性分数: {result['relevance_score']}")

    # 场景 4: 文档更新
    print("\n" + "=" * 60)
    print("场景 4: 文档更新")
    print("=" * 60)

    kb.update_document(
        doc_id="doc_001",
        content="Python 是一种高级编程语言，以其简洁优雅的语法和强大的功能而闻名。Python 3.x 是当前的主流版本，提供了更好的 Unicode 支持和性能优化。"
    )

    # 场景 5: 文档删除
    print("\n" + "=" * 60)
    print("场景 5: 文档删除")
    print("=" * 60)

    # 导入一个测试文档
    kb.import_document(
        doc_id="doc_test",
        title="测试文档",
        content="这是一个测试文档，用于演示删除功能。",
        source="test.txt",
        category="测试"
    )

    # 删除测试文档
    kb.delete_document("doc_test")

    # 场景 6: 统计信息
    print("\n" + "=" * 60)
    print("场景 6: 统计信息")
    print("=" * 60)

    kb.get_stats()
    kb.list_documents(limit=5)

    # 场景 7: RAG 问答
    print("\n" + "=" * 60)
    print("场景 7: RAG 问答")
    print("=" * 60)

    answer = kb.answer_question("Python 有哪些特点？", top_k=2)

    print("\n" + "=" * 60)
    print("示例完成！")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

---

## 核心功能

### 1. 文档分块

```python
def _chunk_text(self, text: str, chunk_size: int) -> List[str]:
    """按词数分块"""
    chunks = []
    words = text.split()

    current_chunk = []
    current_length = 0

    for word in words:
        current_chunk.append(word)
        current_length += len(word) + 1

        if current_length >= chunk_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks
```

### 2. 版本管理

```python
# 检查现有版本
existing = collection.query(expr=f"doc_id == '{doc_id}'")
version = existing[0]["version"] + 1 if existing else 1

# 删除旧版本
if existing:
    collection.delete(expr=f"doc_id == '{doc_id}'")
```

### 3. 重排序

```python
# 计算相关性分数
for result in results:
    relevance_score = sum(word in result["content"].lower()
                         for word in query.lower().split())
    result["relevance_score"] = relevance_score

# 排序
results.sort(key=lambda x: (x["relevance_score"], -x["distance"]), reverse=True)
```

---

## 最佳实践

### 1. 文档分块策略

- **固定大小分块**：简单但可能切断语义
- **句子分块**：保持语义完整性
- **段落分块**：适合长文档
- **滑动窗口**：增加上下文重叠

### 2. 版本管理

- 每次更新增加版本号
- 保留最近 N 个版本
- 定期清理旧版本

### 3. 检索优化

- 使用混合检索（向量 + 标量）
- 实现重排序提升准确率
- 缓存热门查询结果

### 4. 性能优化

- 批量导入文档
- 异步处理大文件
- 定期 Compaction

---

## 扩展功能

### 1. 多模态支持

```python
def import_image_document(self, doc_id: str, image_path: str, caption: str):
    """导入图片文档"""
    # 使用 CLIP 等模型生成图片 Embedding
    pass
```

### 2. 增量更新

```python
def incremental_update(self, doc_id: str, new_content: str):
    """增量更新（只更新变化的部分）"""
    pass
```

### 3. 文档去重

```python
def deduplicate_documents(self):
    """去除重复文档"""
    pass
```

---

## 总结

### 核心要点

1. **文档分块**：合理的分块策略
2. **版本管理**：追踪文档变更
3. **语义检索**：向量检索 + 重排序
4. **生命周期管理**：导入、更新、删除
5. **统计监控**：了解知识库状态

### 最佳实践

1. **合理分块**：保持语义完整性
2. **版本控制**：便于回滚和审计
3. **批量操作**：提升性能
4. **定期清理**：释放存储空间
5. **监控统计**：了解使用情况

---

**下一步**: 学习 [14_面试必问.md](./14_面试必问.md) 准备面试
