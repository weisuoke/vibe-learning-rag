# 最小可用知识

> 掌握以下内容，就能开始使用向量存储构建RAG系统

---

## 核心理念

**20%的核心知识解决80%的问题**

你不需要理解HNSW的所有数学细节，也不需要掌握IVF的所有优化技巧。只需要掌握以下5个核心知识点，就能构建生产级RAG系统。

---

## 4.1 向量相似度计算

### 核心概念

**向量相似度 = 衡量两个向量有多"接近"**

最常用的三种度量：

1. **余弦相似度**（最常用）
2. **欧氏距离**
3. **内积**

### 代码实现

```python
import numpy as np

def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    """余弦相似度：范围[-1, 1]，越接近1越相似"""
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

def euclidean_distance(v1: np.ndarray, v2: np.ndarray) -> float:
    """欧氏距离：越小越相似"""
    return np.linalg.norm(v1 - v2)

def inner_product(v1: np.ndarray, v2: np.ndarray) -> float:
    """内积：越大越相似（需要归一化向量）"""
    return np.dot(v1, v2)

# 示例
v1 = np.array([0.1, 0.9, 0.2])
v2 = np.array([0.2, 0.8, 0.3])

print(f"余弦相似度: {cosine_similarity(v1, v2):.3f}")  # 0.998
print(f"欧氏距离: {euclidean_distance(v1, v2):.3f}")    # 0.173
print(f"内积: {inner_product(v1, v2):.3f}")             # 0.800
```

### RAG应用

```python
# OpenAI Embedding默认使用余弦相似度
from openai import OpenAI

client = OpenAI()

# 查询向量
query_embedding = client.embeddings.create(
    input="如何优化RAG系统？",
    model="text-embedding-3-small"
).data[0].embedding

# 文档向量
doc_embedding = client.embeddings.create(
    input="RAG系统优化的10个技巧",
    model="text-embedding-3-small"
).data[0].embedding

# 计算相似度
similarity = cosine_similarity(
    np.array(query_embedding),
    np.array(doc_embedding)
)
print(f"相似度: {similarity:.3f}")  # 高相似度表示相关
```

**记住**：
- ✅ 余弦相似度：最常用，适合归一化向量
- ✅ 欧氏距离：适合未归一化向量
- ✅ OpenAI Embedding默认使用余弦相似度

---

## 4.2 使用ChromaDB构建向量存储

### 核心概念

**ChromaDB = 轻量级向量数据库，5分钟上手**

特点：
- 零配置（无需安装数据库）
- 自动持久化
- 支持过滤
- 适合中小规模（<100万向量）

### 最小可用代码

```python
import chromadb
from chromadb.config import Settings

# 1. 创建客户端（持久化到本地）
client = chromadb.PersistentClient(path="./chroma_db")

# 2. 创建或获取集合
collection = client.get_or_create_collection(
    name="my_documents",
    metadata={"hnsw:space": "cosine"}  # 使用余弦相似度
)

# 3. 添加文档（自动Embedding）
collection.add(
    documents=[
        "Python是一门编程语言",
        "机器学习需要大量数据",
        "RAG系统结合检索和生成"
    ],
    ids=["doc1", "doc2", "doc3"],
    metadatas=[
        {"source": "tutorial"},
        {"source": "course"},
        {"source": "blog"}
    ]
)

# 4. 查询
results = collection.query(
    query_texts=["如何学习编程？"],
    n_results=2
)

print("最相关的文档:")
for doc, distance in zip(results['documents'][0], results['distances'][0]):
    print(f"- {doc} (距离: {distance:.3f})")
```

**输出：**
```
最相关的文档:
- Python是一门编程语言 (距离: 0.234)
- 机器学习需要大量数据 (距离: 0.456)
```

### RAG集成

```python
from openai import OpenAI
import chromadb

# 初始化
openai_client = OpenAI()
chroma_client = chromadb.PersistentClient(path="./rag_db")
collection = chroma_client.get_or_create_collection("knowledge_base")

# 添加知识库文档
def add_documents(texts: list[str]):
    """添加文档到向量库"""
    embeddings = []
    for text in texts:
        emb = openai_client.embeddings.create(
            input=text,
            model="text-embedding-3-small"
        ).data[0].embedding
        embeddings.append(emb)

    collection.add(
        documents=texts,
        embeddings=embeddings,
        ids=[f"doc_{i}" for i in range(len(texts))]
    )

# 检索相关文档
def retrieve(query: str, top_k: int = 3) -> list[str]:
    """检索最相关的文档"""
    query_emb = openai_client.embeddings.create(
        input=query,
        model="text-embedding-3-small"
    ).data[0].embedding

    results = collection.query(
        query_embeddings=[query_emb],
        n_results=top_k
    )

    return results['documents'][0]

# 使用
add_documents([
    "RAG系统包含检索和生成两个阶段",
    "向量存储用于高效检索相关文档",
    "HNSW是最流行的向量索引算法"
])

context_docs = retrieve("什么是RAG？")
print("检索到的上下文:", context_docs)
```

**记住**：
- ✅ ChromaDB适合快速原型和中小规模
- ✅ 自动持久化，无需手动保存
- ✅ 支持自定义Embedding或使用内置模型

---

## 4.3 理解HNSW核心参数

### 核心概念

**HNSW有3个关键参数，影响性能和精度**

1. **M**：每个节点的最大连接数（默认16）
2. **efConstruction**：构建时的搜索宽度（默认200）
3. **efSearch**：查询时的搜索宽度（默认10）

### 参数调优规则

```python
# Milvus示例
from pymilvus import Collection, connections

connections.connect(host="localhost", port="19530")

# 创建HNSW索引
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",
    "params": {
        "M": 16,              # ↑ 提高召回率，↑ 内存
        "efConstruction": 200  # ↑ 提高召回率，↑ 构建时间
    }
}

collection.create_index(
    field_name="embedding",
    index_params=index_params
)

# 查询参数
search_params = {
    "metric_type": "COSINE",
    "params": {
        "ef": 64  # ↑ 提高召回率，↑ 查询延迟
    }
}

results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param=search_params,
    limit=10
)
```

### 参数选择指南

| 场景 | M | efConstruction | efSearch |
|------|---|----------------|----------|
| 快速原型 | 16 | 100 | 10 |
| 平衡性能 | 16 | 200 | 64 |
| 高召回率 | 32 | 500 | 128 |
| 低延迟 | 8 | 100 | 16 |

**记住**：
- ✅ M越大，召回率越高，但内存占用越大
- ✅ efConstruction只影响构建时间，不影响查询
- ✅ efSearch是查询时最重要的参数

---

## 4.4 向量数据库选型

### 核心概念

**不同场景选择不同的向量数据库**

### 选型决策树

```
数据规模？
├─ <10万向量 → ChromaDB（轻量、零配置）
├─ 10万-1000万 → Qdrant（高性能、易部署）
├─ >1000万 → Milvus（分布式、可扩展）
└─ 需要混合搜索 → Weaviate（Vector+关键词）
```

### 快速对比

| 数据库 | 适用规模 | 部署难度 | 特色功能 |
|--------|----------|----------|----------|
| **ChromaDB** | <100万 | ⭐ 极简 | 零配置、自动持久化 |
| **Qdrant** | 100万-1000万 | ⭐⭐ 简单 | 高性能过滤、Rust实现 |
| **Milvus** | >1000万 | ⭐⭐⭐ 中等 | 分布式、GPU加速 |
| **Weaviate** | 任意 | ⭐⭐⭐ 中等 | 混合搜索、GraphQL |

### 代码示例

```python
# ChromaDB（最简单）
import chromadb
client = chromadb.PersistentClient(path="./db")
collection = client.get_or_create_collection("docs")

# Qdrant（高性能）
from qdrant_client import QdrantClient
client = QdrantClient(host="localhost", port=6333)
client.create_collection(
    collection_name="docs",
    vectors_config={"size": 1536, "distance": "Cosine"}
)

# Milvus（大规模）
from pymilvus import connections, Collection
connections.connect(host="localhost", port="19530")
collection = Collection("docs")
```

**记住**：
- ✅ 原型阶段：ChromaDB
- ✅ 生产中小规模：Qdrant
- ✅ 生产大规模：Milvus
- ✅ 需要混合搜索：Weaviate

---

## 4.5 混合检索策略

### 核心概念

**混合检索 = 向量检索（语义） + 关键词检索（精确）**

为什么需要混合？
- 向量检索：理解语义，但可能遗漏关键词
- 关键词检索：精确匹配，但不理解语义
- 混合检索：两者优势互补

### 最小可用实现

```python
from rank_bm25 import BM25Okapi
import numpy as np

class HybridRetriever:
    """混合检索器：Vector + BM25"""

    def __init__(self, vector_store, documents):
        self.vector_store = vector_store
        self.documents = documents

        # 构建BM25索引
        tokenized_docs = [doc.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)

    def search(self, query: str, top_k: int = 5, alpha: float = 0.6):
        """
        混合检索
        alpha: 向量检索权重（0.6 = 60%向量 + 40%关键词）
        """
        # 1. 向量检索
        vector_results = self.vector_store.query(
            query_texts=[query],
            n_results=top_k * 2  # 多检索一些候选
        )
        vector_scores = {
            doc: 1 - dist  # 距离转相似度
            for doc, dist in zip(
                vector_results['documents'][0],
                vector_results['distances'][0]
            )
        }

        # 2. BM25关键词检索
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        bm25_dict = {
            self.documents[i]: score
            for i, score in enumerate(bm25_scores)
        }

        # 3. 融合（归一化后加权）
        all_docs = set(vector_scores.keys()) | set(bm25_dict.keys())

        # 归一化
        max_vector = max(vector_scores.values()) if vector_scores else 1
        max_bm25 = max(bm25_dict.values()) if bm25_dict else 1

        hybrid_scores = {}
        for doc in all_docs:
            v_score = vector_scores.get(doc, 0) / max_vector
            b_score = bm25_dict.get(doc, 0) / max_bm25
            hybrid_scores[doc] = alpha * v_score + (1 - alpha) * b_score

        # 4. 排序返回Top-K
        sorted_docs = sorted(
            hybrid_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]

        return [doc for doc, score in sorted_docs]

# 使用示例
documents = [
    "Python是一门编程语言",
    "机器学习需要Python",
    "RAG系统使用向量检索"
]

retriever = HybridRetriever(vector_store, documents)
results = retriever.search("Python编程", top_k=2, alpha=0.6)
print("混合检索结果:", results)
```

**输出：**
```
混合检索结果: ['Python是一门编程语言', '机器学习需要Python']
```

### 权重调优

```python
# 不同场景的alpha值
alpha_configs = {
    "语义为主": 0.8,  # 80%向量 + 20%关键词
    "平衡": 0.6,      # 60%向量 + 40%关键词（推荐）
    "精确为主": 0.4   # 40%向量 + 60%关键词
}

# 实验不同权重
for name, alpha in alpha_configs.items():
    results = retriever.search(query, alpha=alpha)
    print(f"{name} (alpha={alpha}): {results[0]}")
```

**记住**：
- ✅ 混合检索召回率比单一方法高30%
- ✅ alpha=0.6是经验最优值
- ✅ 使用RRF（Reciprocal Rank Fusion）融合更稳定

---

## 这些知识足以

掌握以上5个核心知识点，你已经可以：

### ✅ 构建基础RAG系统
- 使用ChromaDB存储文档向量
- 实现语义检索
- 集成OpenAI Embedding

### ✅ 优化检索性能
- 调整HNSW参数提升召回率
- 使用混合检索提升精度
- 选择合适的向量数据库

### ✅ 部署生产系统
- 使用Qdrant/Milvus处理大规模数据
- 监控查询延迟和召回率
- 实现增量更新

---

## 下一步学习

完成最小可用知识后，建议深入学习：

1. **HNSW算法原理** (`03_核心概念_03_HNSW算法原理.md`)
   - 理解分层导航小世界图
   - 掌握构建和搜索过程

2. **IVF倒排索引** (`03_核心概念_05_IVF倒排索引原理.md`)
   - 理解聚类分区
   - 掌握量化压缩

3. **2025-2026最新优化** (`03_核心概念_04_HNSW_2025年最新优化.md`)
   - HNSW++、Ada-ef、ACORN-1
   - GPU加速、DiskANN

4. **实战代码** (`07_实战代码_场景2_ChromaDB向量存储构建.md`)
   - 完整可运行的RAG系统
   - 性能优化技巧

---

## 快速参考卡

### 向量相似度
```python
# 余弦相似度（最常用）
similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
```

### ChromaDB
```python
import chromadb
client = chromadb.PersistentClient(path="./db")
collection = client.get_or_create_collection("docs")
collection.add(documents=[...], ids=[...])
results = collection.query(query_texts=[...], n_results=5)
```

### HNSW参数
```python
# 平衡配置
{"M": 16, "efConstruction": 200, "ef": 64}
```

### 混合检索
```python
# 60%向量 + 40%关键词
hybrid_score = 0.6 * vector_score + 0.4 * bm25_score
```

### 数据库选型
```
<10万 → ChromaDB
10万-1000万 → Qdrant
>1000万 → Milvus
```

---

**记住**：这5个知识点是基础，但足以构建生产级RAG系统。深入学习可以进一步优化性能。
