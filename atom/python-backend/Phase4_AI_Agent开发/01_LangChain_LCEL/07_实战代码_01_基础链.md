# 实战代码 01 - 基础链

> 完整可运行的基础链实现，从简单问答到复杂应用

---

## 场景1：最简单的问答链

### 需求

构建一个最基础的问答链：用户提问 → LLM 回答。

---

### 完整代码

```python
"""
场景1：最简单的问答链
演示：Prompt → LLM → Parser 的基础链
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 加载环境变量
load_dotenv()

# ===== 1. 定义组件 =====
print("=== 定义组件 ===")

# Prompt 模板
prompt = ChatPromptTemplate.from_template("回答：{question}")
print(f"Prompt 模板: {prompt}")

# LLM
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
)
print(f"LLM: {llm.model_name}")

# 输出解析器
parser = StrOutputParser()
print(f"Parser: {parser}")

# ===== 2. 组合成链 =====
print("\n=== 组合成链 ===")

chain = prompt | llm | parser
print(f"Chain: {chain}")
print(f"Chain 类型: {type(chain)}")

# ===== 3. 调用链 =====
print("\n=== 调用链 ===")

question = "什么是 LangChain LCEL？"
print(f"问题: {question}")

answer = chain.invoke({"question": question})
print(f"\n回答: {answer}")

# ===== 4. 批量调用 =====
print("\n=== 批量调用 ===")

questions = [
    "什么是 LCEL？",
    "什么是 Runnable？",
    "什么是管道操作符？"
]

answers = chain.batch([{"question": q} for q in questions])

for q, a in zip(questions, answers):
    print(f"\nQ: {q}")
    print(f"A: {a[:100]}...")  # 只显示前100个字符
```

**运行输出示例**：
```
=== 定义组件 ===
Prompt 模板: input_variables=['question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='回答：{question}'))]
LLM: gpt-3.5-turbo
Parser: StrOutputParser()

=== 组合成链 ===
Chain: first=ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='回答：{question}'))]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x...>, model_name='gpt-3.5-turbo', temperature=0.7)] last=StrOutputParser()
Chain 类型: <class 'langchain_core.runnables.base.RunnableSequence'>

=== 调用链 ===
问题: 什么是 LangChain LCEL？

回答: LangChain LCEL（LangChain Expression Language）是 LangChain 框架中的一种表达式语言，用于构建和组合复杂的语言模型应用...

=== 批量调用 ===

Q: 什么是 LCEL？
A: LCEL 是 LangChain Expression Language 的缩写，是 LangChain 框架中用于构建和组合语言模型应用的表达式语言...

Q: 什么是 Runnable？
A: Runnable 是 LangChain 中的核心接口，所有可以被执行的组件都实现了这个接口...

Q: 什么是管道操作符？
A: 管道操作符 | 是 LCEL 中用于连接多个组件的语法，让数据从左到右依次流动...
```

---

### 代码解析

**1. 组件定义**：
- `ChatPromptTemplate`：格式化用户输入
- `ChatOpenAI`：调用 LLM
- `StrOutputParser`：解析输出为字符串

**2. 链式组合**：
- 用 `|` 连接三个组件
- 创建 `RunnableSequence` 对象

**3. 调用方式**：
- `invoke()`：单次调用
- `batch()`：批量调用

---

## 场景2：带系统消息的问答链

### 需求

添加系统消息，让 LLM 扮演特定角色。

---

### 完整代码

```python
"""
场景2：带系统消息的问答链
演示：多消息模板 + 角色设定
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ===== 1. 定义多消息模板 =====
print("=== 定义多消息模板 ===")

prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个{role}，擅长{skill}。请用简洁的语言回答问题。"),
    ("human", "{question}")
])

print(f"Prompt 模板: {prompt}")

# ===== 2. 组合成链 =====
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
parser = StrOutputParser()

chain = prompt | llm | parser

# ===== 3. 调用链（指定角色和技能） =====
print("\n=== Python 专家回答 ===")

answer = chain.invoke({
    "role": "Python 专家",
    "skill": "异步编程和 Web 开发",
    "question": "什么是 asyncio？"
})

print(f"回答: {answer}")

# ===== 4. 调用链（不同角色） =====
print("\n=== 前端专家回答 ===")

answer = chain.invoke({
    "role": "前端专家",
    "skill": "React 和 TypeScript",
    "question": "什么是 React Hooks？"
})

print(f"回答: {answer}")

# ===== 5. 部分变量填充 =====
print("\n=== 部分变量填充 ===")

# 固定角色和技能
python_expert_prompt = prompt.partial(
    role="Python 专家",
    skill="异步编程和 Web 开发"
)

python_expert_chain = python_expert_prompt | llm | parser

# 只需要提供问题
answer = python_expert_chain.invoke({
    "question": "什么是装饰器？"
})

print(f"回答: {answer}")
```

**运行输出示例**：
```
=== 定义多消息模板 ===
Prompt 模板: input_variables=['question', 'role', 'skill'] messages=[SystemMessagePromptTemplate(...), HumanMessagePromptTemplate(...)]

=== Python 专家回答 ===
回答: asyncio 是 Python 的异步 I/O 库，用于编写并发代码。它基于事件循环和协程，让你可以用 async/await 语法编写非阻塞的异步程序...

=== 前端专家回答 ===
回答: React Hooks 是 React 16.8 引入的特性，让你可以在函数组件中使用状态和其他 React 特性。常用的 Hooks 包括 useState、useEffect、useContext 等...

=== 部分变量填充 ===
回答: 装饰器是 Python 中用于修改函数或类行为的语法糖。它本质上是一个接受函数作为参数并返回新函数的高阶函数...
```

---

### 代码解析

**1. 多消息模板**：
- `("system", "...")`: 系统消息，设定角色
- `("human", "...")`: 用户消息，提出问题

**2. 部分变量填充**：
- `prompt.partial()`: 固定部分变量
- 复用相同的角色设定

---

## 场景3：结构化输出链

### 需求

让 LLM 输出结构化数据（JSON 格式），并自动解析为 Python 对象。

---

### 完整代码

```python
"""
场景3：结构化输出链
演示：Pydantic 输出解析器 + 类型安全
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

load_dotenv()

# ===== 1. 定义输出结构 =====
print("=== 定义输出结构 ===")

class PersonInfo(BaseModel):
    """人物信息"""
    name: str = Field(description="人名")
    age: int = Field(description="年龄")
    occupation: str = Field(description="职业")
    skills: list[str] = Field(description="技能列表")

print(f"输出结构: {PersonInfo}")

# ===== 2. 创建解析器 =====
parser = PydanticOutputParser(pydantic_object=PersonInfo)

# 获取格式说明
format_instructions = parser.get_format_instructions()
print(f"\n格式说明:\n{format_instructions}")

# ===== 3. 创建 Prompt =====
prompt = ChatPromptTemplate.from_template("""
从以下文本中提取人物信息：

{text}

{format_instructions}
""")

# ===== 4. 组合成链 =====
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
chain = prompt | llm | parser

# ===== 5. 调用链 =====
print("\n=== 提取人物信息 ===")

text = """
Alice 今年 30 岁，是一名资深软件工程师。
她精通 Python、JavaScript 和 Go 语言，
擅长后端开发和系统架构设计。
"""

result = chain.invoke({
    "text": text,
    "format_instructions": format_instructions
})

print(f"\n提取结果:")
print(f"类型: {type(result)}")
print(f"姓名: {result.name}")
print(f"年龄: {result.age}")
print(f"职业: {result.occupation}")
print(f"技能: {result.skills}")

# ===== 6. 类型安全的访问 =====
print("\n=== 类型安全 ===")

# Pydantic 对象支持类型检查
print(f"年龄是否大于 25: {result.age > 25}")
print(f"技能数量: {len(result.skills)}")
print(f"是否会 Python: {'Python' in result.skills}")

# ===== 7. 批量提取 =====
print("\n=== 批量提取 ===")

texts = [
    "Bob 今年 25 岁，是一名前端工程师，擅长 React 和 Vue。",
    "Carol 今年 35 岁，是一名数据科学家，精通 Python 和 R 语言。"
]

results = chain.batch([
    {"text": t, "format_instructions": format_instructions}
    for t in texts
])

for person in results:
    print(f"\n{person.name} ({person.age}岁) - {person.occupation}")
    print(f"技能: {', '.join(person.skills)}")
```

**运行输出示例**：
```
=== 定义输出结构 ===
输出结构: <class '__main__.PersonInfo'>

格式说明:
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"name": {"title": "Name", "description": "人名", "type": "string"}, "age": {"title": "Age", "description": "年龄", "type": "integer"}, "occupation": {"title": "Occupation", "description": "职业", "type": "string"}, "skills": {"title": "Skills", "description": "技能列表", "type": "array", "items": {"type": "string"}}}, "required": ["name", "age", "occupation", "skills"]}
```

=== 提取人物信息 ===

提取结果:
类型: <class '__main__.PersonInfo'>
姓名: Alice
年龄: 30
职业: 软件工程师
技能: ['Python', 'JavaScript', 'Go', '后端开发', '系统架构设计']

=== 类型安全 ===
年龄是否大于 25: True
技能数量: 5
是否会 Python: True

=== 批量提取 ===

Bob (25岁) - 前端工程师
技能: React, Vue

Carol (35岁) - 数据科学家
技能: Python, R
```

---

### 代码解析

**1. Pydantic 模型**：
- 定义输出结构
- 自动类型验证

**2. PydanticOutputParser**：
- 生成格式说明
- 自动解析 JSON 为 Pydantic 对象

**3. 类型安全**：
- 可以直接访问属性
- 支持类型检查

---

## 场景4：异步调用链

### 需求

在 FastAPI 中使用异步调用，提升性能。

---

### 完整代码

```python
"""
场景4：异步调用链
演示：ainvoke() 异步调用 + FastAPI 集成
"""

import os
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ===== 1. 定义链 =====
prompt = ChatPromptTemplate.from_template("回答：{question}")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
parser = StrOutputParser()

chain = prompt | llm | parser

# ===== 2. 同步调用（阻塞） =====
print("=== 同步调用 ===")

import time

start = time.time()
answer1 = chain.invoke({"question": "什么是 LCEL？"})
answer2 = chain.invoke({"question": "什么是 Runnable？"})
print(f"同步调用耗时: {time.time() - start:.1f}秒")
print(f"回答1: {answer1[:50]}...")
print(f"回答2: {answer2[:50]}...")

# ===== 3. 异步调用（非阻塞） =====
print("\n=== 异步调用 ===")

async def async_qa():
    start = time.time()

    # 并发调用
    answer1, answer2 = await asyncio.gather(
        chain.ainvoke({"question": "什么是 LCEL？"}),
        chain.ainvoke({"question": "什么是 Runnable？"})
    )

    print(f"异步调用耗时: {time.time() - start:.1f}秒")
    print(f"回答1: {answer1[:50]}...")
    print(f"回答2: {answer2[:50]}...")

asyncio.run(async_qa())

# ===== 4. FastAPI 集成 =====
print("\n=== FastAPI 集成示例 ===")

# 以下是 FastAPI 集成代码（需要单独运行）
fastapi_code = '''
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

# 定义请求体
class Question(BaseModel):
    text: str

# 定义链
chain = prompt | llm | parser

@app.post("/ask")
async def ask(question: Question):
    """异步问答端点"""
    answer = await chain.ainvoke({"question": question.text})
    return {"answer": answer}

# 运行：uvicorn main:app --reload
'''

print(fastapi_code)

# ===== 5. 异步批量调用 =====
print("\n=== 异步批量调用 ===")

async def async_batch():
    questions = [
        "什么是 LCEL？",
        "什么是 Runnable？",
        "什么是管道操作符？"
    ]

    start = time.time()

    # 异步批量调用（并行）
    answers = await chain.abatch([{"question": q} for q in questions])

    print(f"异步批量调用耗时: {time.time() - start:.1f}秒")

    for q, a in zip(questions, answers):
        print(f"\nQ: {q}")
        print(f"A: {a[:50]}...")

asyncio.run(async_batch())
```

**运行输出示例**：
```
=== 同步调用 ===
同步调用耗时: 4.2秒
回答1: LCEL 是 LangChain Expression Language 的缩写...
回答2: Runnable 是 LangChain 中的核心接口...

=== 异步调用 ===
异步调用耗时: 2.1秒
回答1: LCEL 是 LangChain Expression Language 的缩写...
回答2: Runnable 是 LangChain 中的核心接口...

=== FastAPI 集成示例 ===
from fastapi import FastAPI
from pydantic import BaseModel
...

=== 异步批量调用 ===
异步批量调用耗时: 2.3秒

Q: 什么是 LCEL？
A: LCEL 是 LangChain Expression Language 的缩写...

Q: 什么是 Runnable？
A: Runnable 是 LangChain 中的核心接口...

Q: 什么是管道操作符？
A: 管道操作符 | 是 LCEL 中用于连接多个组件的语法...
```

---

### 代码解析

**1. 同步 vs 异步**：
- 同步：串行执行，耗时 = 单次耗时 × 次数
- 异步：并行执行，耗时 = 单次耗时（最慢的）

**2. FastAPI 集成**：
- 使用 `async def` 定义端点
- 使用 `await chain.ainvoke()` 异步调用

**3. 性能提升**：
- 异步调用比同步快约 2 倍（2个请求并行）

---

## 场景5：链的调试和监控

### 需求

在开发过程中调试链的执行过程，查看中间结果。

---

### 完整代码

```python
"""
场景5：链的调试和监控
演示：打印中间结果 + 性能监控
"""

import os
import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

load_dotenv()

# ===== 1. 定义调试函数 =====
def debug_print(step_name: str):
    """创建调试函数"""
    def _debug(x):
        print(f"\n[{step_name}]")
        print(f"类型: {type(x)}")
        if hasattr(x, 'content'):
            print(f"内容: {x.content[:100]}...")
        else:
            print(f"内容: {str(x)[:100]}...")
        return x
    return _debug

# ===== 2. 定义性能监控函数 =====
def timer(step_name: str):
    """创建性能监控函数"""
    def _timer(x):
        start = time.time()
        # 这里只是记录时间，实际处理在下一步
        print(f"\n[{step_name}] 开始执行...")
        return x
    return _timer

# ===== 3. 构建带调试的链 =====
print("=== 构建带调试的链 ===")

prompt = ChatPromptTemplate.from_template("回答：{question}")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
parser = StrOutputParser()

# 在每个步骤之间插入调试函数
chain = (
    RunnableLambda(timer("开始"))
    | prompt
    | RunnableLambda(debug_print("Prompt 输出"))
    | llm
    | RunnableLambda(debug_print("LLM 输出"))
    | parser
    | RunnableLambda(debug_print("Parser 输出"))
)

# ===== 4. 调用链并查看调试信息 =====
print("\n=== 调用链 ===")

result = chain.invoke({"question": "什么是 LCEL？"})

print(f"\n=== 最终结果 ===")
print(result)

# ===== 5. 简化版调试：只打印关键步骤 =====
print("\n\n=== 简化版调试 ===")

def simple_debug(x):
    """简化的调试函数"""
    if hasattr(x, 'content'):
        print(f"→ {x.content[:80]}...")
    else:
        print(f"→ {str(x)[:80]}...")
    return x

simple_chain = (
    prompt
    | RunnableLambda(simple_debug)
    | llm
    | RunnableLambda(simple_debug)
    | parser
)

result = simple_chain.invoke({"question": "什么是 Runnable？"})

print(f"\n最终结果: {result[:100]}...")
```

**运行输出示例**：
```
=== 构建带调试的链 ===

=== 调用链 ===

[开始] 开始执行...

[Prompt 输出]
类型: <class 'langchain_core.prompt_values.ChatPromptValue'>
内容: [HumanMessage(content='回答：什么是 LCEL？')]

[LLM 输出]
类型: <class 'langchain_core.messages.ai.AIMessage'>
内容: LCEL 是 LangChain Expression Language 的缩写，是 LangChain 框架中用于构建和组合语言模型应用的表达式语言...

[Parser 输出]
类型: <class 'str'>
内容: LCEL 是 LangChain Expression Language 的缩写，是 LangChain 框架中用于构建和组合语言模型应用的表达式语言...

=== 最终结果 ===
LCEL 是 LangChain Expression Language 的缩写，是 LangChain 框架中用于构建和组合语言模型应用的表达式语言...


=== 简化版调试 ===
→ [HumanMessage(content='回答：什么是 Runnable？')]
→ Runnable 是 LangChain 中的核心接口，所有可以被执行的组件都实现了这个接口...

最终结果: Runnable 是 LangChain 中的核心接口，所有可以被执行的组件都实现了这个接口...
```

---

### 代码解析

**1. RunnableLambda**：
- 将普通函数包装成 Runnable
- 可以插入到链中

**2. 调试策略**：
- 打印中间结果的类型和内容
- 监控每个步骤的执行时间

**3. 生产环境**：
- 使用 LangSmith 自动追踪
- 不需要手动插入调试代码

---

## 总结

### 五个场景对比

| 场景 | 核心特性 | 适用场景 | 关键代码 |
|------|---------|---------|---------|
| **场景1** | 最简单的链 | 快速原型 | `prompt \| llm \| parser` |
| **场景2** | 系统消息 | 角色设定 | `ChatPromptTemplate.from_messages()` |
| **场景3** | 结构化输出 | 数据提取 | `PydanticOutputParser` |
| **场景4** | 异步调用 | FastAPI 集成 | `await chain.ainvoke()` |
| **场景5** | 调试监控 | 开发调试 | `RunnableLambda(debug_fn)` |

---

### 学习检查清单

完成本实战代码后，你应该能够：

- [ ] 构建最简单的问答链（Prompt → LLM → Parser）
- [ ] 使用多消息模板设定系统角色
- [ ] 使用 Pydantic 输出解析器提取结构化数据
- [ ] 使用异步调用提升性能
- [ ] 在 FastAPI 中集成 LCEL 链
- [ ] 使用 RunnableLambda 调试链的执行过程
- [ ] 理解同步调用和异步调用的性能差异
- [ ] 使用批量调用处理多个请求

---

### 下一步

掌握了基础链后，继续学习：

- **07_实战代码_02_RAG链.md** - RAG 检索链的完整实现
- **07_实战代码_03_多链组合.md** - 并行执行、条件分支的复杂链
- **07_实战代码_04_流式输出.md** - 实时返回 LLM 生成内容
- **07_实战代码_05_错误处理.md** - 重试、降级策略

---

**版本**: v1.0
**最后更新**: 2026-02-12
