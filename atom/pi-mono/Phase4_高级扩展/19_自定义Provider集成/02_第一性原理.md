# 第一性原理

## 基础定义

**自定义Provider集成**是pi-mono扩展系统的核心能力之一，允许开发者将任何LLM服务（无论是云端API、本地模型还是企业内部服务）集成到pi-coding-agent中，实现统一的开发体验。

## 为什么需要自定义Provider？

### 问题1：LLM服务的多样性

**现实场景：**
- 云端服务：OpenAI、Anthropic、Google、xAI等
- 本地模型：Ollama、vLLM、llama.cpp等
- 企业服务：内部部署的LLM、需要OAuth认证的服务
- 特殊需求：代理路由、自定义端点、非标准API

**核心矛盾：**
每个LLM服务都有不同的：
- API格式（OpenAI格式、Anthropic格式、Google格式等）
- 认证方式（API Key、OAuth、SSO等）
- 流式协议（SSE、WebSocket、自定义格式等）
- 端点地址（云端、本地、企业内网等）

**如果没有统一抽象：**
```typescript
// 糟糕的方式：为每个Provider写不同的代码
if (provider === 'openai') {
  const response = await openai.chat.completions.create({...})
} else if (provider === 'anthropic') {
  const response = await anthropic.messages.create({...})
} else if (provider === 'ollama') {
  const response = await fetch('http://localhost:11434/v1/chat/completions', {...})
} else if (provider === 'company-llm') {
  // 需要OAuth认证
  const token = await getOAuthToken()
  const response = await fetch('https://ai.company.com/v1/chat', {
    headers: { 'Authorization': `Bearer ${token}` }
  })
}
// 每增加一个Provider都要修改核心代码
```

### 问题2：扩展性与维护性

**核心挑战：**
- **闭源困境**：如果只支持内置Provider，用户无法使用新的LLM服务
- **维护负担**：每个新Provider都要修改核心代码，增加测试负担
- **企业需求**：企业内部LLM服务无法集成到开源工具中

**理想状态：**
- 用户可以自己添加Provider，无需修改pi-mono源码
- 新Provider的集成不影响现有功能
- 支持从简单配置到完全自定义的各种场景

## 三层价值

### 第一层：使用者价值（简单配置）

**场景：** 使用OpenAI兼容的本地模型（如Ollama）

**价值：**
- 零代码配置，仅需编辑JSON文件
- 数据隐私（完全本地运行）
- 无API调用成本

**实现：**
```json
// ~/.pi/agent/models.json
{
  "providers": [
    {
      "id": "ollama",
      "name": "Ollama",
      "apiKeyEnvVar": null,
      "baseUrl": "http://localhost:11434/v1"
    }
  ],
  "models": [
    {
      "id": "ollama/llama3.2",
      "name": "Llama 3.2",
      "providerId": "ollama",
      "contextWindow": 128000,
      "maxOutputTokens": 4096
    }
  ]
}
```

**核心思想：** 如果LLM服务提供OpenAI兼容API，只需配置端点地址即可。

### 第二层：开发者价值（Extension扩展）

**场景：** 集成企业内部LLM服务（需要OAuth认证）

**价值：**
- 支持复杂认证流程（OAuth、SSO）
- 自定义API适配（非OpenAI格式）
- 完全控制请求/响应处理

**实现：**
```typescript
// ~/.pi/agent/extensions/company-llm/index.ts
export default function(pi: ExtensionAPI) {
  pi.registerProvider('company-llm', {
    baseUrl: 'https://ai.company.com/v1',
    api: 'openai-completions',
    models: [{ id: 'company-gpt-4', name: 'Company GPT-4', ... }],
    oauth: {
      name: 'Company AI (SSO)',
      async login(callbacks) {
        // 实现OAuth Device Code Flow
        const deviceCode = await requestDeviceCode()
        callbacks.onDeviceCode({
          userCode: deviceCode.user_code,
          verificationUri: deviceCode.verification_uri
        })
        const token = await pollForToken(deviceCode.device_code)
        return { access: token.access_token, refresh: token.refresh_token, expires: ... }
      },
      async refreshToken(credentials) { /* 刷新Token */ },
      getApiKey(credentials) { return credentials.access }
    }
  })
}
```

**核心思想：** 通过Extension API，开发者可以实现任何复杂的集成逻辑。

### 第三层：架构价值（统一抽象）

**核心设计：**
```typescript
// pi-mono的统一抽象
interface Provider {
  id: string
  baseUrl: string
  api: Api  // 'openai-completions' | 'anthropic-messages' | ...
  models: Model[]
  oauth?: OAuthProviderInterface
  streamSimple?: StreamFunction
}

// 所有Provider都通过统一接口调用
const stream = await pi.ai.stream(model, context, options)
// 无论是OpenAI、Anthropic、Ollama还是企业内部服务
// 都返回统一的AssistantMessageEventStream
```

**架构优势：**
1. **解耦**：核心代码不依赖具体Provider实现
2. **可测试**：每个Provider可以独立测试
3. **可扩展**：新Provider不影响现有功能
4. **可维护**：Provider实现与核心逻辑分离

## 推导过程

### 第一性原理推导

**起点：** AI Agent需要调用LLM API

**推导链：**

1. **多样性是必然的**
   - 不同公司提供不同的LLM服务
   - 不同场景需要不同的部署方式（云端、本地、企业内网）
   - 新的LLM服务会不断出现

2. **统一抽象是必要的**
   - Agent核心逻辑不应该关心具体Provider
   - 用户切换Provider不应该修改代码
   - 新Provider的添加不应该影响现有功能

3. **分层设计是合理的**
   - **简单场景**：OpenAI兼容API → 配置文件即可
   - **中等复杂度**：需要OAuth认证 → Extension + OAuth接口
   - **高级场景**：非标准API → Extension + 自定义streamSimple

4. **插件化是最佳方案**
   - Extension系统提供注册机制
   - Provider实现与核心代码分离
   - 用户可以自己开发和分享Extension

### 设计决策

**决策1：两种集成方式**
- **models.json配置**：适用于OpenAI兼容API，零代码
- **Extension开发**：适用于复杂场景，完全可控

**决策2：统一流式接口**
- 所有Provider都返回`AssistantMessageEventStream`
- 统一的事件格式：`start`, `text_delta`, `toolcall_start`, `done`等
- 核心代码只需处理统一的事件流

**决策3：OAuth标准化**
- `OAuthProviderInterface`定义标准接口
- `login()`, `refreshToken()`, `getApiKey()`三个核心方法
- 凭证存储在`~/.pi/agent/auth.json`

**决策4：API类型抽象**
- 预定义常见API类型：`openai-completions`, `anthropic-messages`等
- 每种API类型有对应的流式处理实现
- 自定义API可以实现`streamSimple`函数

## 核心洞察

### 洞察1：配置优于代码

**原则：** 能用配置解决的问题，不要写代码。

**体现：**
- 80%的场景（OpenAI兼容API）只需配置models.json
- 只有20%的场景（OAuth、自定义API）才需要写Extension

### 洞察2：抽象的边界

**原则：** 抽象要足够通用，但不要过度抽象。

**体现：**
- 统一的流式事件格式（通用）
- 但保留API类型的差异（不过度抽象）
- 允许自定义streamSimple（灵活性）

### 洞察3：渐进式复杂度

**原则：** 简单场景简单，复杂场景可控。

**体现：**
- 简单：编辑JSON文件
- 中等：实现OAuth接口
- 复杂：实现完整的streamSimple

### 洞察4：安全性内置

**原则：** 安全机制应该内置在架构中。

**体现：**
- OAuth凭证存储在系统Keychain（macOS）或加密文件
- Token自动刷新机制
- 环境变量管理API Key

## 总结

自定义Provider集成的第一性原理是：**通过统一抽象和插件化设计，让AI Agent能够无缝集成任何LLM服务，同时保持核心代码的简洁性和可维护性。**

**核心要素：**
1. **统一抽象**：AssistantMessageEventStream统一流式接口
2. **分层设计**：配置文件 → Extension → 自定义streamSimple
3. **插件化**：Extension系统实现解耦
4. **标准化**：OAuth接口、API类型、事件格式

**价值体现：**
- **使用者**：零代码集成本地模型
- **开发者**：完全控制复杂集成
- **架构**：可扩展、可维护、可测试
