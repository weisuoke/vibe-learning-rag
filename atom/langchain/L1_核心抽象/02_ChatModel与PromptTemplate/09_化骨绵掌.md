# 化骨绵掌

> **本文目标**: 通过 10 个知识卡片，构建 ChatModel 与 PromptTemplate 的完整知识体系

---

## 概述

化骨绵掌是将知识拆分成 10 个独立的 2 分钟卡片，每个卡片都可以单独理解，但组合起来形成完整的知识体系。

---

## 卡片1: ChatModel 的本质

**一句话**: ChatModel 是消息列表到消息的映射函数，专为对话场景设计。

**核心公式**:
```
ChatModel: List[BaseMessage] → AIMessage
```

**关键特征**:
- 输入：消息列表（不是字符串）
- 输出：AIMessage 对象（不是字符串）
- 角色感知：理解 System/Human/AI 的语义差异
- Runnable 协议：统一的 invoke/batch/stream 接口

**类比**:
- 前端：Fetch API（结构化请求/响应）
- 日常：打电话咨询专家（角色明确的对话）

**应用**: 所有现代 AI 对话应用的基础，从聊天机器人到 Agent 系统。

**记忆点**: "消息进，消息出，角色清晰，接口统一"

---

## 卡片2: ChatModel vs LLM

**一句话**: ChatModel 是消息驱动的高级抽象，LLM 是字符串驱动的低级抽象。

**核心对比**:

| 维度 | LLM | ChatModel |
|------|-----|-----------|
| 输入 | 字符串 | 消息列表 |
| 输出 | 字符串 | AIMessage |
| 角色 | 手动管理 | 原生支持 |
| 工具调用 | 不支持 | 支持（2025+） |

**举例**:
```python
# LLM - 手动格式化
llm.invoke("你是助手。\n\n用户: 你好\n助手:")

# ChatModel - 结构化
chatmodel.invoke([
    SystemMessage("你是助手"),
    HumanMessage("你好")
])
```

**应用**: 2025-2026 最佳实践推荐优先使用 ChatModel。

**记忆点**: "ChatModel = LLM + 结构化 + 角色感知"

---

## 卡片3: 消息类型系统

**一句话**: 不同消息类型表达不同的角色和语义，是对话的"语法"。

**四种核心消息**:
1. **SystemMessage**: 定义行为规则（"宪法"）
2. **HumanMessage**: 用户输入（"问题"）
3. **AIMessage**: AI 回复（"答案"）
4. **ToolMessage**: 工具结果（"证据"，2025+）

**使用规则**:
- SystemMessage：每个对话一条，定义全局行为
- HumanMessage：用户的每次输入
- AIMessage：保存历史回复，维护上下文
- ToolMessage：Agent 工具调用的返回值

**举例**:
```python
[
    SystemMessage("你是Python专家"),  # 角色定义
    HumanMessage("什么是装饰器？"),    # 用户问题
    AIMessage("装饰器是..."),         # 历史回复
    HumanMessage("能举例吗？")        # 新问题
]
```

**应用**: 多轮对话、Agent 推理、工具调用的基础。

**记忆点**: "System 定规则，Human 提问题，AI 给答案，Tool 提供证据"

---

## 卡片4: PromptTemplate 的价值

**一句话**: PromptTemplate 不是字符串格式化，而是 Runnable 生态的一等公民。

**四大价值**:
1. **Runnable 协议**: 支持 invoke/batch/stream，可用 `|` 组合
2. **类型安全**: 缺少变量会报错，不是运行时才发现
3. **工程化**: 可序列化、版本控制、A/B 测试
4. **可观测性**: 与 LangSmith 集成，追踪使用情况

**对比**:
```python
# ❌ 字符串格式化
prompt = f"你是{role}"  # 无法组合，无法验证

# ✅ PromptTemplate
template = ChatPromptTemplate.from_messages([
    ("system", "你是{role}")
])
chain = template | model  # 可组合
```

**应用**: 企业级提示词管理、模板版本控制、性能优化。

**记忆点**: "PromptTemplate = 字符串格式化 + Runnable + 工程化"

---

## 卡片5: MessagesPlaceholder 的通用性

**一句话**: MessagesPlaceholder 是动态消息注入的通用机制，不只是对话历史。

**四大用途**:
1. **对话历史**: 注入不定长度的历史消息
2. **Few-shot 学习**: 注入示例消息对
3. **工具调用**: 注入 ToolMessage 结果（2025+）
4. **多步推理**: 注入中间推理步骤

**核心特性**:
- 不定长度：可以注入任意数量的消息
- 类型灵活：可以注入任何类型的消息
- 可选性：`optional=True` 支持可选注入

**举例**:
```python
template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    MessagesPlaceholder("history"),    # 对话历史
    MessagesPlaceholder("examples"),   # Few-shot 示例
    MessagesPlaceholder("tool_results"), # 工具结果
    ("human", "{question}")
])
```

**应用**: 有记忆的聊天、Few-shot 学习、Agent 工具调用。

**记忆点**: "MessagesPlaceholder = 动态插槽，不只是历史"

---

## 卡片6: Runnable 三剑客

**一句话**: invoke/stream/batch 代表三种执行策略，选择标准是性能、成本和体验。

**三种方法**:

| 方法 | 特点 | 适用场景 | 关键指标 |
|------|------|----------|----------|
| **invoke** | 等待完整响应 | 后端 API、测试 | 简单直接 |
| **stream** | 逐 token 输出 | 聊天界面 | 首字时间 0.3s |
| **batch** | 并发处理 | 批量评估 | 快 5-10 倍 |

**性能对比**（10 个请求）:
- 循环 invoke: 50 秒
- batch: 10 秒（5 倍提升）
- stream: 总时间不变，但感知延迟降低 90%

**决策树**:
```
用户界面？→ stream（降低感知延迟）
多个请求？→ batch（性能 + 成本）
其他场景？→ invoke（简单直接）
```

**应用**: 聊天应用用 stream，评估系统用 batch，API 用 invoke。

**记忆点**: "invoke 等完整，stream 降延迟，batch 提效率"

---

## 卡片7: 对话历史管理

**一句话**: 历史管理需要平衡记忆完整性和 token 效率。

**三大挑战**:
1. **长度限制**: 历史无限增长会超过 context window
2. **持久化**: 用户刷新后历史丢失
3. **性能优化**: 每次传输完整历史浪费 tokens

**三种策略**:
1. **固定长度**: 保留最近 N 条（简单有效）
2. **Token 限制**: 基于实际 token 数量（精确控制）
3. **定期总结**: 用 LLM 总结历史，清空详细记录（最优）

**最佳实践**:
```python
MAX_HISTORY = 20  # 固定长度
MAX_TOKENS = 2000  # Token 限制
SUMMARY_THRESHOLD = 10  # 总结阈值

# 混合策略：最近消息 + 历史总结
history = recent_messages[-MAX_HISTORY:] + [summary]
```

**应用**: 所有有记忆的聊天应用。

**记忆点**: "限长度、控 token、定期总结"

---

## 卡片8: 结构化输出

**一句话**: 用 Pydantic 让 LLM 输出类型安全、易于处理。

**核心方法**:
```python
from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int

structured_model = model.with_structured_output(Person)
result = structured_model.invoke(messages)
# result 是 Person 对象，类型安全
```

**三大优势**:
1. **类型安全**: 编译时检查，不是运行时才发现错误
2. **易于处理**: 直接访问属性，不需要解析字符串
3. **验证机制**: Pydantic 验证器确保数据质量

**应用场景**:
- 信息提取：从文本中提取结构化信息
- 分类任务：返回类别和置信度
- 问答系统：返回答案、来源、相关问题

**应用**: 所有需要结构化输出的场景。

**记忆点**: "with_structured_output = 类型安全 + 易处理"

---

## 卡片9: 2025-2026 安全实践

**一句话**: 2025 年修复了多个严重漏洞，必须了解并防范。

**四大安全风险**:

| CVE | 风险 | 防范 |
|-----|------|------|
| **CVE-2025-65106** | 模板注入 | 永不信任用户输入的模板 |
| **CVE-2025-68664** | 序列化注入 | 使用 `allowed_objects=['core']` |
| **CVE-2025-68664** | 秘密泄露 | `secretsFromEnv=False`（默认） |
| **CVE-2025-6984** | XXE 攻击 | 更新 langchain-community |

**安全检查清单**:
- [ ] 使用 LangChain 0.3.x+
- [ ] 永不信任用户输入的模板
- [ ] 显式指定 `allowed_objects`
- [ ] 使用专门的秘密管理工具
- [ ] 定期更新依赖

**应用**: 所有生产环境的 LangChain 应用。

**记忆点**: "2025 安全年，四大 CVE 要记牢"

---

## 卡片10: 成本优化策略

**一句话**: 通过模型选择、批处理、缓存和 Prompt 优化降低成本 70%。

**四大策略**:

| 策略 | 方法 | 效果 |
|------|------|------|
| **模型选择** | 简单任务用 mini | -40% |
| **批处理 API** | batch_mode=True（2025+） | -50% |
| **缓存** | InMemoryCache | -30% |
| **Prompt 优化** | 历史总结、减少上下文 | -40% |

**实际案例**:
- 初始成本：$5000/月
- 优化后：$1500/月（降低 70%）
- ROI：2100%

**批处理 API 示例**:
```python
model = ChatOpenAI(
    model="gpt-4o-mini",
    batch_mode=True  # 成本降低 50%
)
responses = model.batch(inputs)
```

**应用**: 所有关注成本的生产环境。

**记忆点**: "选模型、用批处理、加缓存、优 Prompt"

---

## 知识体系总结

### 核心架构

```
ChatModel（消息驱动）
    ↓
消息类型系统（角色语义）
    ↓
PromptTemplate（模板管理）
    ↓
MessagesPlaceholder（动态注入）
    ↓
Runnable 方法（执行策略）
    ↓
实际应用（对话、Agent、RAG）
```

### 关键数字

- **性能**: batch 比循环快 5-10 倍
- **体验**: stream 降低感知延迟 90%
- **成本**: 批处理 API 降低成本 50%
- **优化**: 综合策略降低成本 70%
- **历史**: 保留最近 10-20 条消息
- **Token**: 限制在 2000 以内
- **安全**: 2025 年修复 4 个 CVE

### 最佳实践速查

**开发环境**:
- 使用 ChatModel（不是 LLM）
- 添加 SystemMessage
- 用 PromptTemplate 管理模板
- invoke 快速测试

**生产环境**:
- 必须添加 SystemMessage
- 限制历史长度（10-20 条）
- 基于 Token 限制（2000）
- 定期总结历史
- 聊天用 stream
- 批量用 batch
- 启用缓存
- 监控成本

**安全要求**:
- LangChain 0.3.x+
- 永不信任用户模板
- 使用 `allowed_objects`
- 秘密管理工具
- 定期更新依赖

---

## 学习路径建议

### 快速掌握（1 小时）

1. **卡片 1-2**: 理解 ChatModel 本质和与 LLM 的区别
2. **卡片 3**: 掌握消息类型系统
3. **卡片 4**: 理解 PromptTemplate 的价值
4. **卡片 6**: 学会选择 invoke/stream/batch

### 深入理解（4 小时）

5. **卡片 5**: 掌握 MessagesPlaceholder 的通用性
6. **卡片 7**: 学习对话历史管理
7. **卡片 8**: 实现结构化输出

### 生产就绪（8 小时）

8. **卡片 9**: 了解安全实践
9. **卡片 10**: 掌握成本优化
10. **实战练习**: 构建完整应用

---

## 记忆口诀

**ChatModel 核心**:
> 消息进出角色清，Runnable 接口统一行

**PromptTemplate 价值**:
> 不只格式化，Runnable 一等公民

**MessagesPlaceholder**:
> 动态插槽通用强，历史示例工具装

**Runnable 三剑客**:
> invoke 等完整，stream 降延迟，batch 提效率

**历史管理**:
> 限长度控 token，定期总结最优选

**结构化输出**:
> Pydantic 保安全，类型检查在编译

**安全实践**:
> 2025 四大 CVE，模板注入要防范

**成本优化**:
> 选模型用批处理，加缓存优 Prompt

---

## 自测题

完成学习后，测试你的掌握程度：

1. [ ] 能用一句话解释 ChatModel 的本质
2. [ ] 能说出 ChatModel 和 LLM 的三个核心区别
3. [ ] 能列举四种消息类型及其作用
4. [ ] 能解释 PromptTemplate 的四大价值
5. [ ] 能说出 MessagesPlaceholder 的四大用途
6. [ ] 能根据场景选择 invoke/stream/batch
7. [ ] 能设计对话历史管理方案
8. [ ] 能使用 Pydantic 实现结构化输出
9. [ ] 能说出 2025 年的四个安全 CVE
10. [ ] 能提供四种成本优化策略

**通过标准**: 10 题全对

---

## 延伸学习

### 下一步知识点

- **L1-03**: OutputParser 与结构化输出
- **L1-04**: 链式组合（管道操作符）
- **L2**: LCEL 表达式
- **L4**: Agent 系统

### 实战项目

1. **聊天机器人**: 使用 stream + 历史管理
2. **批量评估**: 使用 batch + 结构化输出
3. **RAG 应用**: 使用 ChatModel + PromptTemplate
4. **Agent 系统**: 使用 MessagesPlaceholder + 工具调用

### 深入资源

- LangChain 官方文档
- LangChain 1.0 发布说明（2025 年 10 月）
- 2025 年安全公告（CVE 列表）
- LangSmith 可观测性指南

---

**恭喜你完成了 ChatModel 与 PromptTemplate 的完整学习！**

这 10 个知识卡片构成了一个完整的知识体系，从基础概念到高级应用，从开发实践到生产优化。

**记住**: 知识的价值在于应用。现在就开始构建你的第一个 LangChain 应用吧！

---

**版本信息**: 基于 LangChain 0.3.x (2025-2026 标准)
**最后更新**: 2026-02-18
**维护者**: Claude Code
