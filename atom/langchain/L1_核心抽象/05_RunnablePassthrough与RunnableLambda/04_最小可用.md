# 04_最小可用

> **学习目标**：掌握 20% 核心知识，解决 80% 实际问题，快速上手 RunnablePassthrough 与 RunnableLambda

---

## 核心理念

**最小可用知识 = 能立即应用到实际项目的核心技能**

本文档聚焦：
- ✅ 最常用的 API 和模式
- ✅ 典型应用场景的标准解决方案
- ✅ 必须避免的关键陷阱
- ❌ 不包含高级特性和边界情况

---

## 20% 核心知识

### 核心 1：RunnablePassthrough 基础透传

**用途**：在链中保持输入不变地传递

**最小 API**：
```python
from langchain_core.runnables import RunnablePassthrough

# 创建透传器
passthrough = RunnablePassthrough()

# 使用
result = passthrough.invoke("hello")  # 输出: "hello"
```

**典型场景**：
```python
# 在并行处理中保留原始输入
from langchain_core.runnables import RunnableParallel

chain = RunnableParallel({
    "original": RunnablePassthrough(),  # 保留原始输入
    "processed": some_processor          # 处理后的结果
})
```

**记忆要点**：
- 输入 = 输出（identity function）
- 不做任何转换
- 用于数据保留

---

### 核心 2：RunnablePassthrough.assign() 扩展数据

**用途**：保留原始输入的同时添加新字段（RAG 必备）

**最小 API**：
```python
# 添加单个字段
chain = RunnablePassthrough.assign(
    new_field=lambda x: compute(x)
)

# 添加多个字段
chain = RunnablePassthrough.assign(
    field1=lambda x: compute1(x),
    field2=lambda x: compute2(x)
)
```

**典型场景 - RAG 数据保留**：
```python
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 假设已有 retriever
retriever = ...  # 你的检索器

# RAG 链：保留查询 + 添加上下文
rag_chain = (
    RunnablePassthrough.assign(context=retriever)  # 添加 context 字段
    | prompt
    | llm
)

# 使用
result = rag_chain.invoke({"query": "什么是 LangChain?"})
# 内部数据流：
# 1. 输入: {"query": "什么是 LangChain?"}
# 2. assign 后: {"query": "什么是 LangChain?", "context": "检索结果..."}
# 3. 传给 prompt 和 llm
```

**记忆要点**：
- assign() 是扩展，不是替换
- 原始输入保留，新字段添加
- RAG 中 99% 用这个模式

**关键陷阱**：
```python
# ❌ 错误理解：以为 assign 会替换整个输入
# 实际：assign 只是添加/覆盖字段

input_data = {"query": "hello", "user": "alice"}
result = RunnablePassthrough.assign(context="retrieved").invoke(input_data)
# 输出: {"query": "hello", "user": "alice", "context": "retrieved"}
# 注意：query 和 user 都保留了！
```

---

### 核心 3：RunnableLambda 包装自定义函数

**用途**：将任意 Python 函数转换为 Runnable，集成到 LCEL 链中

**最小 API**：
```python
from langchain_core.runnables import RunnableLambda

# 包装同步函数
def my_function(x):
    return x.upper()

lambda_runnable = RunnableLambda(my_function)

# 使用
result = lambda_runnable.invoke("hello")  # 输出: "HELLO"
```

**典型场景 1 - 查询预处理**：
```python
# 查询改写
def rewrite_query(input_dict):
    query = input_dict["query"]
    # 添加上下文或改写
    rewritten = f"请详细解释：{query}"
    return {"query": rewritten}

# 集成到链中
chain = (
    RunnableLambda(rewrite_query)  # 预处理
    | RunnablePassthrough.assign(context=retriever)  # 检索
    | prompt
    | llm
)
```

**典型场景 2 - 数据后处理**：
```python
# 提取关键信息
def extract_answer(llm_output):
    # 从 LLM 输出中提取答案
    return llm_output.content.split("\n")[0]

# 集成到链中
chain = (
    prompt
    | llm
    | RunnableLambda(extract_answer)  # 后处理
)
```

**记忆要点**：
- 任何函数都能包装
- 自动适配 LCEL 接口
- 用于自定义转换逻辑

---

### 核心 4：异步函数支持

**用途**：在异步链中使用异步函数，避免阻塞

**最小 API**：
```python
import asyncio
from langchain_core.runnables import RunnableLambda

# 异步函数
async def async_process(x):
    await asyncio.sleep(1)  # 模拟异步操作
    return x.upper()

# 包装
async_runnable = RunnableLambda(async_process)

# 使用
result = await async_runnable.ainvoke("hello")  # 输出: "HELLO"
```

**典型场景 - 调用外部 API**：
```python
import httpx

async def call_external_api(input_dict):
    query = input_dict["query"]
    async with httpx.AsyncClient() as client:
        response = await client.get(f"https://api.example.com/search?q={query}")
        return {"query": query, "api_result": response.json()}

# 集成到异步链中
chain = (
    RunnableLambda(call_external_api)  # 自动识别为异步
    | RunnablePassthrough.assign(context=retriever)
    | prompt
    | llm
)

# 异步调用
result = await chain.ainvoke({"query": "hello"})
```

**记忆要点**：
- 异步函数自动识别
- 使用 ainvoke 调用
- 不会阻塞事件循环

**关键陷阱**：
```python
# ❌ 同步函数会阻塞异步链
import time

def slow_sync(x):
    time.sleep(5)  # 阻塞！
    return x

# ✅ 使用异步函数
async def fast_async(x):
    await asyncio.sleep(5)  # 不阻塞
    return x
```

---

### 核心 5：错误处理与重试

**用途**：处理不稳定的操作（如 API 调用），自动重试

**最小 API**：
```python
from langchain_core.runnables import RunnableLambda

def risky_function(x):
    # 可能失败的操作
    if random.random() < 0.5:
        raise Exception("Random failure")
    return x

# 添加重试
safe_runnable = RunnableLambda(risky_function).with_retry(
    stop_after_attempt=3  # 最多重试 3 次
)
```

**典型场景 - API 调用重试**：
```python
import httpx

async def call_api(input_dict):
    async with httpx.AsyncClient() as client:
        response = await client.get("https://api.example.com/data")
        response.raise_for_status()  # 可能抛出异常
        return response.json()

# 添加重试机制
safe_api_call = RunnableLambda(call_api).with_retry(
    stop_after_attempt=3,
    wait_exponential_multiplier=1,  # 指数退避
    wait_exponential_max=10
)

# 集成到链中
chain = (
    safe_api_call  # 自动重试
    | RunnablePassthrough.assign(context=retriever)
    | llm
)
```

**记忆要点**：
- with_retry() 自动重试
- 支持指数退避
- 适用于不稳定的外部调用

---

## 80% 实际场景

### 场景 1：标准 RAG 管道（最常用）

**需求**：构建一个问答系统，保留用户查询并添加检索上下文

**完整代码**：
```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_chroma import Chroma

# 1. 初始化组件
llm = ChatOpenAI(model="gpt-4o-mini")
embeddings = OpenAIEmbeddings()

# 2. 创建向量存储和检索器
vectorstore = Chroma(
    collection_name="docs",
    embedding_function=embeddings
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 3. 创建 Prompt
prompt = ChatPromptTemplate.from_template("""
根据以下上下文回答问题：

上下文：
{context}

问题：{query}

答案：
""")

# 4. 构建 RAG 链
rag_chain = (
    RunnablePassthrough.assign(
        context=lambda x: retriever.invoke(x["query"])  # 检索上下文
    )
    | prompt
    | llm
    | StrOutputParser()
)

# 5. 使用
answer = rag_chain.invoke({"query": "什么是 LangChain?"})
print(answer)
```

**数据流动**：
```
输入: {"query": "什么是 LangChain?"}
  ↓
RunnablePassthrough.assign(context=retriever)
  ↓
{"query": "什么是 LangChain?", "context": [doc1, doc2, doc3]}
  ↓
prompt（填充模板）
  ↓
llm（生成答案）
  ↓
StrOutputParser()（提取文本）
  ↓
输出: "LangChain 是一个用于构建 LLM 应用的框架..."
```

---

### 场景 2：查询改写 + RAG

**需求**：在检索前改写用户查询，提高检索质量

**完整代码**：
```python
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# 1. 定义查询改写函数
def rewrite_query(input_dict):
    """将简短查询扩展为更详细的检索查询"""
    query = input_dict["query"]

    # 简单的改写逻辑（实际可以用 LLM）
    if len(query) < 10:
        rewritten = f"请详细解释：{query}，包括定义、用途和示例"
    else:
        rewritten = query

    return {"query": rewritten, "original_query": query}

# 2. 构建链
chain = (
    RunnableLambda(rewrite_query)  # 改写查询
    | RunnablePassthrough.assign(
        context=lambda x: retriever.invoke(x["query"])  # 用改写后的查询检索
    )
    | prompt
    | llm
    | StrOutputParser()
)

# 3. 使用
answer = chain.invoke({"query": "RAG"})
# 内部流程：
# 1. "RAG" → 改写为 "请详细解释：RAG，包括定义、用途和示例"
# 2. 用改写后的查询检索
# 3. 生成答案
```

---

### 场景 3：并行数据增强

**需求**：同时添加多个字段（如检索结果、元数据、时间戳）

**完整代码**：
```python
from datetime import datetime
from langchain_core.runnables import RunnablePassthrough

# 1. 定义辅助函数
def get_metadata(input_dict):
    """获取元数据"""
    return {
        "timestamp": datetime.now().isoformat(),
        "user": input_dict.get("user", "anonymous")
    }

def get_query_stats(input_dict):
    """获取查询统计"""
    query = input_dict["query"]
    return {
        "query_length": len(query),
        "word_count": len(query.split())
    }

# 2. 构建链：同时添加多个字段
chain = (
    RunnablePassthrough.assign(
        context=lambda x: retriever.invoke(x["query"]),  # 检索上下文
        metadata=get_metadata,                            # 元数据
        stats=get_query_stats                             # 统计信息
    )
    | prompt
    | llm
)

# 3. 使用
result = chain.invoke({
    "query": "什么是 LangChain?",
    "user": "alice"
})

# 内部数据：
# {
#     "query": "什么是 LangChain?",
#     "user": "alice",
#     "context": [doc1, doc2, doc3],
#     "metadata": {"timestamp": "2026-02-18T...", "user": "alice"},
#     "stats": {"query_length": 15, "word_count": 3}
# }
```

---

### 场景 4：条件处理

**需求**：根据输入条件执行不同的处理逻辑

**完整代码**：
```python
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# 1. 定义条件处理函数
def conditional_process(input_dict):
    """根据查询类型选择不同的处理"""
    query = input_dict["query"]

    if "代码" in query or "示例" in query:
        # 代码相关查询：使用代码检索器
        return {
            **input_dict,
            "retriever_type": "code",
            "search_type": "similarity"
        }
    else:
        # 普通查询：使用文档检索器
        return {
            **input_dict,
            "retriever_type": "docs",
            "search_type": "mmr"
        }

# 2. 定义检索函数
def smart_retrieve(input_dict):
    """根据类型选择检索器"""
    if input_dict["retriever_type"] == "code":
        return code_retriever.invoke(input_dict["query"])
    else:
        return doc_retriever.invoke(input_dict["query"])

# 3. 构建链
chain = (
    RunnableLambda(conditional_process)  # 条件处理
    | RunnablePassthrough.assign(
        context=smart_retrieve  # 智能检索
    )
    | prompt
    | llm
)
```

---

### 场景 5：外部 API 集成

**需求**：在 RAG 管道中集成外部 API（如天气、股票等）

**完整代码**：
```python
import httpx
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# 1. 定义 API 调用函数
async def fetch_weather(input_dict):
    """获取天气信息"""
    query = input_dict["query"]

    # 简单的城市提取（实际应该用 NER）
    city = "北京"  # 简化示例

    async with httpx.AsyncClient() as client:
        response = await client.get(
            f"https://api.weather.com/v1/current?city={city}"
        )
        weather_data = response.json()

    return {
        **input_dict,
        "weather": weather_data
    }

# 2. 添加重试机制
safe_weather_fetch = RunnableLambda(fetch_weather).with_retry(
    stop_after_attempt=3
)

# 3. 构建链
chain = (
    safe_weather_fetch  # 获取天气
    | RunnablePassthrough.assign(
        context=lambda x: retriever.invoke(x["query"])  # 检索相关信息
    )
    | prompt
    | llm
)

# 4. 使用
answer = await chain.ainvoke({"query": "北京今天天气怎么样？"})
```

---

## 常见陷阱与解决方案

### 陷阱 1：误以为 assign 会替换输入

**错误理解**：
```python
# ❌ 认为 assign 会替换整个输入
input_data = {"query": "hello", "user": "alice"}
result = RunnablePassthrough.assign(query="new").invoke(input_data)
# 期望: {"query": "new"}
# 实际: {"query": "new", "user": "alice"}  # user 仍然保留！
```

**正确理解**：
- assign() 是**扩展**，不是替换
- 原始输入的所有字段都保留
- 只有同名字段会被覆盖

**解决方案**：
```python
# ✅ 如果需要完全替换，使用 RunnableLambda
def replace_input(x):
    return {"query": "new"}

chain = RunnableLambda(replace_input) | ...
```

---

### 陷阱 2：Lambda 中忘记错误处理

**错误代码**：
```python
# ❌ 没有错误处理
def risky_func(x):
    return x["key"]  # 如果 key 不存在会崩溃

chain = RunnableLambda(risky_func) | llm
# 一旦出错，整个链崩溃！
```

**解决方案 1：函数内部处理**：
```python
# ✅ 添加错误处理
def safe_func(x):
    try:
        return x.get("key", "default_value")
    except Exception as e:
        print(f"Error: {e}")
        return "fallback_value"

chain = RunnableLambda(safe_func) | llm
```

**解决方案 2：使用 with_retry**：
```python
# ✅ 自动重试
safe_chain = RunnableLambda(risky_func).with_retry(
    stop_after_attempt=3
)
```

---

### 陷阱 3：同步函数阻塞异步链

**错误代码**：
```python
# ❌ 同步函数阻塞
import time

def slow_sync(x):
    time.sleep(5)  # 阻塞整个事件循环！
    return x

chain = RunnableLambda(slow_sync) | llm
await chain.ainvoke("input")  # 会阻塞 5 秒
```

**解决方案**：
```python
# ✅ 使用异步函数
import asyncio

async def fast_async(x):
    await asyncio.sleep(5)  # 不阻塞事件循环
    return x

chain = RunnableLambda(fast_async) | llm
await chain.ainvoke("input")  # 不会阻塞其他任务
```

---

### 陷阱 4：assign 中的 lambda 访问错误的键

**错误代码**：
```python
# ❌ 假设输入一定有某个键
chain = RunnablePassthrough.assign(
    result=lambda x: x["query"].upper()  # 如果没有 query 键会崩溃
)
```

**解决方案**：
```python
# ✅ 使用 get 方法
chain = RunnablePassthrough.assign(
    result=lambda x: x.get("query", "").upper()
)

# ✅ 或者添加类型检查
def safe_upper(x):
    if isinstance(x, dict) and "query" in x:
        return x["query"].upper()
    return ""

chain = RunnablePassthrough.assign(result=safe_upper)
```

---

## 快速决策树

```
需要在链中处理数据？
│
├─ 需要保留原始输入？
│  │
│  ├─ 只需透传，不添加字段
│  │  └─ 使用 RunnablePassthrough()
│  │
│  └─ 需要添加新字段（如 RAG 上下文）
│     └─ 使用 RunnablePassthrough.assign(key=func)
│
└─ 需要自定义转换？
   │
   ├─ 简单的数据转换
   │  └─ 使用 RunnableLambda(func)
   │
   ├─ 需要调用外部 API
   │  └─ 使用 RunnableLambda(async_func).with_retry()
   │
   └─ 需要条件处理
      └─ 使用 RunnableLambda(conditional_func)
```

---

## 最小工具箱

### 必备导入

```python
from langchain_core.runnables import (
    RunnablePassthrough,
    RunnableLambda,
    RunnableParallel
)
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
```

### 必备模式

```python
# 模式 1：RAG 数据保留
rag_chain = (
    RunnablePassthrough.assign(context=retriever)
    | prompt
    | llm
)

# 模式 2：查询预处理
chain = (
    RunnableLambda(preprocess)
    | RunnablePassthrough.assign(context=retriever)
    | llm
)

# 模式 3：错误重试
safe_chain = RunnableLambda(risky_func).with_retry(
    stop_after_attempt=3
)

# 模式 4：并行处理
parallel = RunnableParallel({
    "original": RunnablePassthrough(),
    "processed": RunnableLambda(process)
})
```

---

## 学习检查点

### 基础检查（必须全部掌握）

- [ ] 能解释 RunnablePassthrough 的作用
- [ ] 能写出 assign() 的基础代码
- [ ] 能解释 RunnableLambda 的作用
- [ ] 能包装一个简单函数为 Runnable
- [ ] 理解 assign() 是扩展而非替换

### 实战检查（能独立完成）

- [ ] 能构建标准 RAG 管道
- [ ] 能实现查询预处理
- [ ] 能添加错误重试机制
- [ ] 能使用异步函数
- [ ] 能同时添加多个字段

**如果以上都能完成，说明已掌握最小可用知识；否则需要重点复习相关部分。**

---

## 下一步

### 如果你已掌握本文档

- **深入理解**：阅读 `02_第一性原理.md` 了解底层设计
- **详细机制**：学习 `03_核心概念_*.md` 系列
- **高级用法**：查看 `07_实战代码_02_Lambda自定义处理.md`

### 如果还有疑问

- **重新阅读**：本文档的相关部分
- **运行示例**：`07_实战代码_01_基础透传与assign.md`
- **查看类比**：`05_双重类比.md` 建立直觉

---

## 总结：记住这 5 个核心

1. **RunnablePassthrough()** = 透传输入，保持不变
2. **RunnablePassthrough.assign()** = 保留输入 + 添加字段（RAG 必备）
3. **RunnableLambda(func)** = 包装函数为 Runnable
4. **async 函数** = 避免阻塞，用于 API 调用
5. **with_retry()** = 自动重试，处理不稳定操作

**记住这一句话**：
> RunnablePassthrough.assign() 是 RAG 的标准模式，RunnableLambda 是自定义逻辑的桥梁。

---

**参考资料**：
- [LangChain RunnablePassthrough 官方文档](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) (2025)
- [Building Production-Ready AI Pipelines](https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557) (2026)
- [Master LangChain in 2025](https://towardsai.net/p/machine-learning/master-langchain-in-2025-from-rag-to-tools-complete-guide) (2025)

---

**版本**：v1.0
**最后更新**：2026-02-18
**预计学习时间**：2-3 小时
