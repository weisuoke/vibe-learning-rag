# 核心概念2：LLM响应缓存策略

## 概述

LLM API调用是AI Agent开发中最昂贵和最慢的操作之一。通过缓存LLM响应，可以显著降低成本和延迟。本文介绍精确缓存和语义缓存两种策略。

---

## 1. 精确缓存（Exact Match Cache）

### 定义

**精确缓存使用prompt的hash作为key，缓存LLM的完整响应，只有完全相同的prompt才能命中缓存。**

### 核心原理

```python
import hashlib
import json
from typing import Optional

def generate_cache_key(prompt: str, model: str = "gpt-4") -> str:
    """生成缓存key"""
    # 将prompt和model组合后计算hash
    content = f"{model}:{prompt}"
    return f"llm:{hashlib.md5(content.encode()).hexdigest()}"

# 示例
key1 = generate_cache_key("What is Python?", "gpt-4")
key2 = generate_cache_key("What is Python?", "gpt-4")
key3 = generate_cache_key("What is Python?", "gpt-3.5-turbo")

print(key1 == key2)  # True（相同prompt和model）
print(key1 == key3)  # False（不同model）
```

### 完整实现

```python
import redis
import hashlib
from typing import Optional
from openai import OpenAI

class ExactLLMCache:
    """精确LLM缓存"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.openai_client = OpenAI()

    def get_cached_response(
        self,
        prompt: str,
        model: str = "gpt-4",
        system_prompt: Optional[str] = None
    ) -> Optional[str]:
        """获取缓存的LLM响应"""
        # 生成缓存key（包含system_prompt）
        cache_content = f"{model}:{system_prompt or ''}:{prompt}"
        cache_key = f"llm:{hashlib.md5(cache_content.encode()).hexdigest()}"

        # 查询缓存
        cached = self.redis.get(cache_key)
        if cached:
            print(f"✅ 缓存命中: {cache_key[:30]}...")
            return cached

        print(f"❌ 缓存未命中: {cache_key[:30]}...")
        return None

    def set_cached_response(
        self,
        prompt: str,
        response: str,
        model: str = "gpt-4",
        system_prompt: Optional[str] = None,
        ttl: int = 3600
    ):
        """设置LLM响应缓存"""
        cache_content = f"{model}:{system_prompt or ''}:{prompt}"
        cache_key = f"llm:{hashlib.md5(cache_content.encode()).hexdigest()}"

        # 存储缓存
        self.redis.setex(cache_key, ttl, response)
        print(f"💾 缓存已保存: {cache_key[:30]}..., TTL={ttl}秒")

    async def get_or_generate(
        self,
        prompt: str,
        model: str = "gpt-4",
        system_prompt: Optional[str] = None,
        ttl: int = 3600
    ) -> str:
        """获取缓存或生成新响应"""
        # 1. 尝试从缓存获取
        cached = self.get_cached_response(prompt, model, system_prompt)
        if cached:
            return cached

        # 2. 调用LLM API
        print(f"🤖 调用LLM API: {model}")
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = self.openai_client.chat.completions.create(
            model=model,
            messages=messages
        )
        answer = response.choices[0].message.content

        # 3. 缓存响应
        self.set_cached_response(prompt, answer, model, system_prompt, ttl)

        return answer

# 使用示例
cache = ExactLLMCache(redis_client)

# 第一次调用：未命中缓存，调用API
response1 = await cache.get_or_generate("What is Python?")
print(response1)

# 第二次调用：命中缓存，直接返回
response2 = await cache.get_or_generate("What is Python?")
print(response2)

# 不同prompt：未命中缓存
response3 = await cache.get_or_generate("What is JavaScript?")
```

### 优点

1. **实现简单**：只需要计算hash和存储字符串
2. **查询快速**：O(1)时间复杂度
3. **100%准确**：不会返回错误的响应
4. **内存占用小**：只存储响应文本

### 缺点

1. **命中率低**：表达方式稍有不同就无法命中
2. **无法处理同义问题**：
   - "What is Python?" ❌ 无法命中
   - "Python是什么？" ❌ 无法命中
   - "请介绍一下Python" ❌ 无法命中

### 适用场景

- **FAQ系统**：问题固定，用户按照模板提问
- **API文档查询**：查询格式统一
- **代码生成**：prompt格式化后再缓存
- **内部工具**：用户熟悉固定的提问方式

---

## 2. 缓存key设计策略

### 策略1：只缓存prompt

```python
def simple_cache_key(prompt: str) -> str:
    """最简单的缓存key"""
    return f"llm:{hashlib.md5(prompt.encode()).hexdigest()}"

# 问题：不同model的响应会互相覆盖
```

### 策略2：缓存prompt + model

```python
def model_aware_cache_key(prompt: str, model: str) -> str:
    """包含model的缓存key"""
    content = f"{model}:{prompt}"
    return f"llm:{hashlib.md5(content.encode()).hexdigest()}"

# 优点：不同model的响应分开存储
```

### 策略3：缓存prompt + model + system_prompt

```python
def full_cache_key(
    prompt: str,
    model: str,
    system_prompt: Optional[str] = None
) -> str:
    """完整的缓存key"""
    content = f"{model}:{system_prompt or ''}:{prompt}"
    return f"llm:{hashlib.md5(content.encode()).hexdigest()}"

# 优点：考虑了system_prompt的影响
```

### 策略4：缓存所有参数

```python
def comprehensive_cache_key(
    prompt: str,
    model: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 1000
) -> str:
    """包含所有参数的缓存key"""
    content = json.dumps({
        "model": model,
        "system_prompt": system_prompt,
        "prompt": prompt,
        "temperature": temperature,
        "max_tokens": max_tokens
    }, sort_keys=True)
    return f"llm:{hashlib.md5(content.encode()).hexdigest()}"

# 优点：最精确，考虑了所有影响响应的参数
# 缺点：命中率更低
```

### 推荐策略

```python
def recommended_cache_key(
    prompt: str,
    model: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7
) -> str:
    """推荐的缓存key策略"""
    # 只有temperature=0时才缓存（确定性响应）
    if temperature > 0:
        return None  # 不缓存非确定性响应

    content = f"{model}:{system_prompt or ''}:{prompt}"
    return f"llm:{hashlib.md5(content.encode()).hexdigest()}"

# 原因：temperature>0时，相同prompt可能返回不同响应
```

---

## 3. TTL策略

### 基础TTL设置

```python
# 不同场景的TTL配置
TTL_CONFIG = {
    "faq": 86400,           # FAQ：24小时（问题和答案都不变）
    "general": 3600,        # 通用问答：1小时（平衡新鲜度和命中率）
    "news": 600,            # 新闻相关：10分钟（内容可能更新）
    "realtime": 0,          # 实时数据：不缓存
}

def get_ttl(prompt: str) -> int:
    """根据prompt类型返回TTL"""
    if any(keyword in prompt.lower() for keyword in ["今天", "最新", "现在"]):
        return TTL_CONFIG["news"]
    elif any(keyword in prompt.lower() for keyword in ["什么是", "如何", "为什么"]):
        return TTL_CONFIG["faq"]
    else:
        return TTL_CONFIG["general"]
```

### 动态TTL策略

```python
class DynamicTTLCache:
    """动态TTL缓存"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.access_count = {}  # 记录访问次数

    def set_with_dynamic_ttl(
        self,
        cache_key: str,
        response: str,
        base_ttl: int = 3600
    ):
        """根据访问频率动态调整TTL"""
        # 获取访问次数
        access_count = self.access_count.get(cache_key, 0)

        # 高频访问：延长TTL
        if access_count > 100:
            ttl = base_ttl * 4  # 4小时
        elif access_count > 50:
            ttl = base_ttl * 2  # 2小时
        else:
            ttl = base_ttl      # 1小时

        self.redis.setex(cache_key, ttl, response)
        print(f"💾 缓存已保存，访问次数={access_count}, TTL={ttl}秒")

    def get_and_track(self, cache_key: str) -> Optional[str]:
        """获取缓存并记录访问"""
        cached = self.redis.get(cache_key)
        if cached:
            # 记录访问次数
            self.access_count[cache_key] = self.access_count.get(cache_key, 0) + 1
        return cached
```

---

## 4. 缓存预热

### 定义

**缓存预热是在系统启动时或低峰期，提前加载热点数据到缓存中。**

### 实现

```python
from typing import List

class CacheWarmer:
    """缓存预热器"""

    def __init__(self, cache: ExactLLMCache):
        self.cache = cache

    async def warmup_faq(self, faq_list: List[dict]):
        """预热FAQ缓存"""
        print("🔥 开始预热FAQ缓存...")

        for faq in faq_list:
            question = faq["question"]
            answer = faq["answer"]

            # 直接设置缓存，不调用LLM
            self.cache.set_cached_response(
                prompt=question,
                response=answer,
                ttl=86400  # 24小时
            )

        print(f"✅ 预热完成，共{len(faq_list)}条FAQ")

    async def warmup_from_logs(self, log_file: str, top_n: int = 100):
        """从日志中提取热门问题并预热"""
        print("🔥 从日志中提取热门问题...")

        # 1. 分析日志，统计问题频率
        question_count = {}
        with open(log_file, 'r') as f:
            for line in f:
                # 假设日志格式：timestamp|question|response
                parts = line.strip().split('|')
                if len(parts) >= 2:
                    question = parts[1]
                    question_count[question] = question_count.get(question, 0) + 1

        # 2. 获取top N热门问题
        hot_questions = sorted(
            question_count.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_n]

        # 3. 预热缓存
        for question, count in hot_questions:
            # 调用LLM生成答案并缓存
            await self.cache.get_or_generate(question, ttl=7200)  # 2小时
            print(f"  预热: {question[:50]}... (访问{count}次)")

        print(f"✅ 预热完成，共{len(hot_questions)}个热门问题")

# 使用示例
warmer = CacheWarmer(cache)

# 方式1：从FAQ列表预热
faq_list = [
    {"question": "What is Python?", "answer": "Python is a programming language..."},
    {"question": "How to install Python?", "answer": "You can download Python from..."},
]
await warmer.warmup_faq(faq_list)

# 方式2：从日志预热
await warmer.warmup_from_logs("access.log", top_n=50)
```

---

## 5. 缓存失效策略

### 主动失效

```python
class CacheInvalidator:
    """缓存失效管理器"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client

    def invalidate_by_pattern(self, pattern: str):
        """按模式失效缓存"""
        # 查找匹配的key
        keys = self.redis.keys(pattern)
        if keys:
            self.redis.delete(*keys)
            print(f"🗑️ 已删除{len(keys)}个缓存")

    def invalidate_by_tag(self, tag: str):
        """按标签失效缓存"""
        # 获取该标签下的所有cache_key
        cache_keys = self.redis.smembers(f"tag:{tag}")
        if cache_keys:
            self.redis.delete(*cache_keys)
            self.redis.delete(f"tag:{tag}")
            print(f"🗑️ 已删除标签'{tag}'下的{len(cache_keys)}个缓存")

    def set_with_tag(
        self,
        cache_key: str,
        response: str,
        tags: List[str],
        ttl: int = 3600
    ):
        """设置缓存并关联标签"""
        # 存储缓存
        self.redis.setex(cache_key, ttl, response)

        # 关联标签
        for tag in tags:
            self.redis.sadd(f"tag:{tag}", cache_key)
            self.redis.expire(f"tag:{tag}", ttl)

# 使用示例
invalidator = CacheInvalidator(redis_client)

# 设置带标签的缓存
invalidator.set_with_tag(
    cache_key="llm:abc123",
    response="Python is...",
    tags=["python", "programming"],
    ttl=3600
)

# 失效所有Python相关的缓存
invalidator.invalidate_by_tag("python")

# 失效所有LLM缓存
invalidator.invalidate_by_pattern("llm:*")
```

---

## 6. 监控和统计

### 缓存命中率监控

```python
class CacheMonitor:
    """缓存监控器"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.hits = 0
        self.misses = 0

    def record_hit(self):
        """记录缓存命中"""
        self.hits += 1
        self.redis.incr("cache:stats:hits")

    def record_miss(self):
        """记录缓存未命中"""
        self.misses += 1
        self.redis.incr("cache:stats:misses")

    def get_hit_rate(self) -> float:
        """获取缓存命中率"""
        total = self.hits + self.misses
        if total == 0:
            return 0.0
        return self.hits / total

    def get_stats(self) -> dict:
        """获取统计信息"""
        hits = int(self.redis.get("cache:stats:hits") or 0)
        misses = int(self.redis.get("cache:stats:misses") or 0)
        total = hits + misses

        return {
            "hits": hits,
            "misses": misses,
            "total": total,
            "hit_rate": hits / total if total > 0 else 0.0
        }

    def reset_stats(self):
        """重置统计"""
        self.redis.delete("cache:stats:hits", "cache:stats:misses")
        self.hits = 0
        self.misses = 0

# 使用示例
monitor = CacheMonitor(redis_client)

# 在缓存查询时记录
cached = cache.get_cached_response(prompt)
if cached:
    monitor.record_hit()
else:
    monitor.record_miss()

# 查看统计
stats = monitor.get_stats()
print(f"缓存命中率: {stats['hit_rate']:.2%}")
print(f"总请求数: {stats['total']}")
```

---

## 7. 成本分析

### 缓存前后对比

```python
def calculate_cost_savings(
    total_requests: int,
    hit_rate: float,
    cost_per_request: float = 0.01  # $0.01/请求
) -> dict:
    """计算缓存节省的成本"""

    # 没有缓存的成本
    cost_without_cache = total_requests * cost_per_request

    # 有缓存的成本（只有未命中的请求需要调用API）
    api_calls = total_requests * (1 - hit_rate)
    cost_with_cache = api_calls * cost_per_request

    # 节省的成本
    savings = cost_without_cache - cost_with_cache
    savings_rate = savings / cost_without_cache if cost_without_cache > 0 else 0

    return {
        "total_requests": total_requests,
        "hit_rate": hit_rate,
        "api_calls_without_cache": total_requests,
        "api_calls_with_cache": int(api_calls),
        "cost_without_cache": cost_without_cache,
        "cost_with_cache": cost_with_cache,
        "savings": savings,
        "savings_rate": savings_rate
    }

# 示例：1000个请求，80%命中率
result = calculate_cost_savings(
    total_requests=1000,
    hit_rate=0.8,
    cost_per_request=0.01
)

print(f"总请求数: {result['total_requests']}")
print(f"缓存命中率: {result['hit_rate']:.0%}")
print(f"API调用次数: {result['api_calls_without_cache']} → {result['api_calls_with_cache']}")
print(f"成本: ${result['cost_without_cache']:.2f} → ${result['cost_with_cache']:.2f}")
print(f"节省: ${result['savings']:.2f} ({result['savings_rate']:.0%})")
```

**输出：**
```
总请求数: 1000
缓存命中率: 80%
API调用次数: 1000 → 200
成本: $10.00 → $2.00
节省: $8.00 (80%)
```

---

## 8. 最佳实践

### 1. 只缓存确定性响应

```python
def should_cache(temperature: float, model: str) -> bool:
    """判断是否应该缓存"""
    # temperature=0时，响应是确定的
    if temperature == 0:
        return True

    # 某些model即使temperature>0也相对稳定
    stable_models = ["gpt-4", "gpt-3.5-turbo"]
    if model in stable_models and temperature < 0.3:
        return True

    return False
```

### 2. 规范化prompt

```python
def normalize_prompt(prompt: str) -> str:
    """规范化prompt，提高命中率"""
    # 1. 去除多余空格
    prompt = " ".join(prompt.split())

    # 2. 转换为小写（如果不区分大小写）
    # prompt = prompt.lower()

    # 3. 去除标点符号（可选）
    # prompt = prompt.strip(".,!?;:")

    return prompt

# 使用
normalized = normalize_prompt("  What   is  Python?  ")
# "What is Python?"
```

### 3. 分层缓存

```python
class TieredCache:
    """分层缓存：内存 + Redis"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.memory_cache = {}  # 内存缓存（最快）
        self.max_memory_size = 100  # 内存缓存最大条目数

    def get(self, cache_key: str) -> Optional[str]:
        """分层获取缓存"""
        # 1. 先查内存缓存
        if cache_key in self.memory_cache:
            print("🎯 内存缓存命中")
            return self.memory_cache[cache_key]

        # 2. 再查Redis缓存
        cached = self.redis.get(cache_key)
        if cached:
            print("🎯 Redis缓存命中")
            # 提升到内存缓存
            self._set_memory_cache(cache_key, cached)
            return cached

        print("❌ 缓存未命中")
        return None

    def set(self, cache_key: str, response: str, ttl: int = 3600):
        """分层设置缓存"""
        # 同时设置内存和Redis缓存
        self._set_memory_cache(cache_key, response)
        self.redis.setex(cache_key, ttl, response)

    def _set_memory_cache(self, cache_key: str, response: str):
        """设置内存缓存（LRU淘汰）"""
        if len(self.memory_cache) >= self.max_memory_size:
            # 删除最旧的条目
            oldest_key = next(iter(self.memory_cache))
            del self.memory_cache[oldest_key]

        self.memory_cache[cache_key] = response
```

---

## 总结

1. **精确缓存**：使用prompt hash作为key，简单高效但命中率低
2. **缓存key设计**：包含model、system_prompt等影响响应的参数
3. **TTL策略**：根据数据特性设置不同TTL，高频数据延长TTL
4. **缓存预热**：提前加载热点数据，提升命中率
5. **缓存失效**：支持按模式和标签失效缓存
6. **监控统计**：跟踪命中率，计算成本节省
7. **最佳实践**：只缓存确定性响应、规范化prompt、分层缓存

**记住：** 精确缓存适合FAQ等固定问题场景，对于开放式问答需要结合语义缓存提升命中率。
