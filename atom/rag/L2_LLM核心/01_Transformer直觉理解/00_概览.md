# Transformer 直觉理解

> 理解大模型的工作原理，知其所以然

---

## 为什么要学这个？

Transformer 是所有现代大语言模型（GPT、Claude、Llama）的核心架构。理解它的工作原理，能帮助你：

- **理解 RAG 为什么有效**：知道 LLM 如何"阅读"你注入的上下文
- **写出更好的 Prompt**：明白为什么信息的位置和结构很重要
- **理解 Token 限制**：知道 Context Window 的本质是什么
- **调试 RAG 系统**：当输出不理想时，知道从哪里找原因

---

## 本知识点包含

| 文档 | 内容 | 阅读时间 |
|-----|------|---------|
| [01_30字核心](./01_30字核心.md) | 一句话说清 Transformer 的本质 | 1分钟 |
| [02_第一性原理](./02_第一性原理.md) | 从根本思考：为什么需要 Transformer | 5分钟 |
| [03_核心概念](./03_核心概念.md) | 自注意力、多头注意力、位置编码 | 10分钟 |
| [04_最小可用](./04_最小可用.md) | 20%核心知识解决80%问题 | 5分钟 |
| [05_双重类比](./05_双重类比.md) | 前端开发 + 日常生活类比 | 5分钟 |
| [06_反直觉点](./06_反直觉点.md) | 3个最常见的误区 | 5分钟 |
| [07_实战代码](./07_实战代码.md) | 可运行的注意力可视化示例 | 10分钟 |
| [08_面试必问](./08_面试必问.md) | 如何答出彩 | 5分钟 |
| [09_化骨绵掌](./09_化骨绵掌.md) | 10个2分钟知识卡片 | 20分钟 |
| [10_一句话总结](./10_一句话总结.md) | 最终总结 | 1分钟 |

---

## 学习建议

### 快速入门路径（15分钟）

```
01_30字核心 → 04_最小可用 → 05_双重类比 → 10_一句话总结
```

适合：想快速了解概念，马上开始 RAG 开发的同学

### 完整学习路径（1小时）

```
01_30字核心 → 02_第一性原理 → 03_核心概念 → 05_双重类比
    → 06_反直觉点 → 07_实战代码 → 09_化骨绵掌 → 10_一句话总结
```

适合：想深入理解原理，打好基础的同学

### 面试准备路径（20分钟）

```
01_30字核心 → 03_核心概念 → 06_反直觉点 → 08_面试必问
```

适合：准备面试的同学

---

## 前置知识

- ✅ L1_NLP基础（特别是 Embedding 的概念）
- ✅ 基本的 Python 编程能力

## 后续学习

- → [02_大模型API调用](../02_大模型API调用.md)
- → [03_Token与Context_Window](../03_Token与Context_Window.md)

---

## 核心要点预览

学完本知识点，你将理解：

1. **Transformer 的核心是注意力机制**：让模型能"看到"所有输入，决定关注哪些部分
2. **注意力是并行的**：不像 RNN 按顺序处理，Transformer 同时处理所有词
3. **位置编码告诉模型顺序**：因为注意力本身不知道词的先后顺序
4. **这就是 RAG 能工作的原因**：LLM 通过注意力机制"阅读"你注入的上下文

---

**版本：** v1.0
**最后更新：** 2026-02-05
