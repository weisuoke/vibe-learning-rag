# Token与Context Window - 面试必问

关于 Token 和 Context Window 的高频面试问题及出彩回答。

---

## 问题1："什么是 Token？为什么 LLM 用 Token 而不是字符？"

### 普通回答（❌ 不出彩）

"Token 是 LLM 处理文本的单位，比字符大一点。用 Token 是因为效率更高。"

### 出彩回答（✅ 推荐）

> **Token 有三层含义：**
>
> 1. **本质定义**：Token 是 LLM 处理文本的最小语义单位，由分词算法（如 BPE）自动学习得到，介于字符和单词之间。
>
> 2. **为什么不用字符**：
>    - 字符粒度太细，序列太长，计算量大（Transformer 是 O(n²)）
>    - 字符缺乏语义信息，"h-e-l-l-o" 不如 "hello" 有意义
>
> 3. **为什么不用单词**：
>    - 无法处理新词（OOV 问题）
>    - 无法处理中文等非空格分隔的语言
>    - 词表会非常大
>
> **Token 的优势**：
> - 平衡了效率和灵活性
> - 通过 BPE 算法自动学习最优切分
> - 高频词是单个 Token，罕见词拆成多个 Token
>
> **在 RAG 开发中的应用**：
> - Token 数量直接决定 API 调用成本
> - Context Window 以 Token 为单位，决定能注入多少检索内容
> - 不同语言的 Token 效率不同，中文约 1.5 字符/Token，英文约 4 字符/Token

### 为什么这个回答出彩？

1. ✅ 从三个层面解释（定义、对比、优势）
2. ✅ 解释了"为什么"而不只是"是什么"
3. ✅ 联系了实际应用（RAG、成本）
4. ✅ 展示了对底层原理的理解（BPE、O(n²)）

---

## 问题2："Context Window 是什么？在 RAG 系统中如何处理 Context Window 限制？"

### 普通回答（❌ 不出彩）

"Context Window 是模型能处理的最大长度。RAG 中如果内容太长就截断。"

### 出彩回答（✅ 推荐）

> **Context Window 有两层理解：**
>
> 1. **技术定义**：Context Window 是 LLM 一次能处理的最大 Token 数量，包括输入和输出。这个限制源于 Transformer 注意力机制的 O(n²) 复杂度——Token 数翻倍，计算量翻四倍。
>
> 2. **实际意义**：它决定了模型能"看到"多少上下文，是 RAG 设计的核心约束。
>
> **在 RAG 中处理 Context Window 限制的策略：**
>
> | 策略 | 说明 |
> |------|------|
> | **Token 预算分配** | 合理分配：系统提示 + 检索内容 + 用户问题 + 输出预留 |
> | **检索数量控制** | Top-K 检索，只取最相关的 5-10 篇文档 |
> | **智能截断** | 按相关性排序，优先保留高相关文档 |
> | **文档压缩** | 用 LLM 先摘要长文档，再注入 Context |
> | **分块策略** | Chunking 时控制块大小，避免单块过大 |
>
> **一个实际的 Token 预算分配示例（128K 模型）：**
> ```
> 系统提示：500 Token
> 用户问题：200 Token
> 检索内容：~117K Token（主要空间）
> 输出预留：4K Token
> 安全边际：~6K Token
> ```
>
> **需要注意的陷阱：**
> - Context 越大不一定越好，有"Lost in the Middle"问题
> - 必须预留输出空间，不能把 Context 全用于输入
> - 不同模型的 Token 计算方式可能不同

### 为什么这个回答出彩？

1. ✅ 解释了技术原理（O(n²) 复杂度）
2. ✅ 给出了多种处理策略，展示全面性
3. ✅ 提供了具体的数字示例
4. ✅ 指出了常见陷阱，展示实战经验

---

## 加分话题：主动延伸

如果面试官追问，可以延伸到：

### 延伸1：不同模型的 Context Window 对比

```
GPT-4o: 128K
Claude 3.5: 200K
Gemini 1.5 Pro: 1M+

但更大不一定更好：
- 成本更高
- 延迟更长
- 注意力稀释
```

### 延伸2：Lost in the Middle 问题

```
研究发现：
- 开头信息：准确率 ~80%
- 中间信息：准确率 ~50%
- 结尾信息：准确率 ~70%

应对策略：
- 关键信息放开头或结尾
- 使用 ReRank 重排序
- 控制 Context 长度
```

### 延伸3：Token 计费与成本优化

```
成本 = 输入 Token × 输入价格 + 输出 Token × 输出价格

优化策略：
- 精简系统提示
- 控制检索数量
- 使用更便宜的模型做初筛
- 缓存常见问答
```

---

**上一节：** [07_实战代码.md](./07_实战代码.md)
**下一节：** [09_化骨绵掌.md](./09_化骨绵掌.md)
