# 长任务处理 - 最小可用知识

> 掌握以下内容，就能开始处理超过30秒的AI Agent任务

---

## 核心认知

**长任务处理 = 任务队列 + 状态管理 + 进度通知**

掌握这20%的核心知识，就能解决80%的长任务处理问题。

---

## 4.1 识别长任务场景

### 什么时候需要长任务处理？

**判断标准**：任务执行时间 > 30秒

```python
# ❌ 不需要长任务处理（<30秒）
@app.post("/quick-query")
async def quick_query(question: str):
    # LLM单次调用，通常5-15秒
    response = await llm.ainvoke(question)
    return {"answer": response}

# ✅ 需要长任务处理（>30秒）
@app.post("/batch-process")
async def batch_process(files: List[UploadFile]):
    # 处理100个文件，每个30秒 = 50分钟
    for file in files:
        parse_and_embed(file)  # 这会超时！
    return {"status": "done"}
```

### AI Agent开发中的典型长任务

| 场景 | 预计耗时 | 是否需要长任务处理 |
|------|---------|------------------|
| 单次LLM调用 | 5-15秒 | ❌ 不需要 |
| 3步Agent推理 | 30-60秒 | ✅ 需要 |
| 解析10个PDF | 5-10分钟 | ✅ 需要 |
| 批量生成100篇摘要 | 30-60分钟 | ✅ 需要 |
| 向量库全量更新 | 1-2小时 | ✅ 需要 |

---

## 4.2 最简单的任务队列：Celery

### 安装与配置

```bash
# 1. 安装依赖
uv add celery redis

# 2. 启动Redis（作为消息代理）
redis-server

# 3. 启动Celery Worker
celery -A app.tasks worker --loglevel=info
```

### 定义任务

```python
# app/tasks.py
from celery import Celery

# 创建Celery应用
app = Celery(
    'tasks',
    broker='redis://localhost:6379/0',      # 消息队列
    backend='redis://localhost:6379/1'      # 结果存储
)

# 定义异步任务
@app.task
def process_document(file_path: str) -> dict:
    """处理单个文档（耗时操作）"""
    import time

    # 模拟耗时操作
    print(f"开始处理: {file_path}")
    time.sleep(60)  # 假设需要1分钟

    return {
        "file": file_path,
        "status": "completed",
        "word_count": 1000
    }
```

**关键点**：
- `@app.task` 装饰器：标记为异步任务
- `broker`：消息队列地址（Redis）
- `backend`：结果存储地址（Redis）

### 在FastAPI中调用任务

```python
# app/main.py
from fastapi import FastAPI, UploadFile
from app.tasks import process_document

app = FastAPI()

@app.post("/upload")
async def upload_file(file: UploadFile):
    # 保存文件
    file_path = f"/tmp/{file.filename}"
    with open(file_path, "wb") as f:
        f.write(await file.read())

    # 异步调用任务（立即返回）
    result = process_document.delay(file_path)

    # 返回任务ID
    return {
        "task_id": result.id,
        "status": "processing",
        "message": "任务已提交，请稍后查询结果"
    }

@app.get("/tasks/{task_id}")
async def get_task_status(task_id: str):
    # 查询任务状态
    from celery.result import AsyncResult
    result = AsyncResult(task_id, app=process_document.app)

    return {
        "task_id": task_id,
        "status": result.status,  # PENDING, STARTED, SUCCESS, FAILURE
        "result": result.result if result.ready() else None
    }
```

**关键点**：
- `.delay()`：异步调用任务
- `result.id`：任务ID，用于后续查询
- `AsyncResult`：查询任务状态和结果

---

## 4.3 任务状态管理

### 任务状态机

```
PENDING (待处理)
   ↓
STARTED (执行中)
   ↓
SUCCESS (成功) / FAILURE (失败)
```

### 数据库模型设计

```python
# app/models/task.py
from sqlalchemy import Column, Integer, String, Float, DateTime, Text
from sqlalchemy.sql import func
from app.core.database import Base

class Task(Base):
    __tablename__ = "tasks"

    id = Column(Integer, primary_key=True, index=True)
    task_id = Column(String, unique=True, index=True)  # Celery任务ID
    task_type = Column(String)  # 任务类型：document_process, batch_generate
    status = Column(String, default="pending")  # pending, running, completed, failed
    progress = Column(Float, default=0.0)  # 进度：0.0-100.0
    result = Column(Text, nullable=True)  # JSON格式的结果
    error = Column(Text, nullable=True)  # 错误信息
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, onupdate=func.now())
```

**关键字段**：
- `task_id`：Celery任务ID
- `status`：任务状态
- `progress`：进度百分比
- `result`：任务结果（JSON）

### 更新任务状态

```python
# app/tasks.py
from app.models.task import Task
from app.core.database import SessionLocal

@app.task(bind=True)  # bind=True 可以访问 self
def process_document(self, file_path: str, db_task_id: int):
    """处理文档并更新进度"""
    db = SessionLocal()

    try:
        # 更新状态为运行中
        task = db.query(Task).filter(Task.id == db_task_id).first()
        task.status = "running"
        task.progress = 0.0
        db.commit()

        # 步骤1：解析文档（33%）
        content = parse_pdf(file_path)
        task.progress = 33.0
        db.commit()

        # 步骤2：生成Embedding（66%）
        embedding = generate_embedding(content)
        task.progress = 66.0
        db.commit()

        # 步骤3：保存到向量库（100%）
        save_to_vectordb(embedding)
        task.progress = 100.0
        task.status = "completed"
        task.result = json.dumps({"file": file_path, "chunks": len(embedding)})
        db.commit()

        return {"status": "success"}

    except Exception as e:
        # 更新状态为失败
        task.status = "failed"
        task.error = str(e)
        db.commit()
        raise

    finally:
        db.close()
```

**关键点**：
- `bind=True`：访问任务实例（self）
- 分步更新进度：0% → 33% → 66% → 100%
- 异常处理：失败时记录错误

---

## 4.4 实时进度推送：SSE

### 为什么用SSE而不是WebSocket？

| 特性 | SSE | WebSocket |
|------|-----|-----------|
| **通信方向** | 单向（服务器→客户端） | 双向 |
| **协议** | HTTP | WebSocket协议 |
| **自动重连** | ✅ 浏览器自动重连 | ❌ 需要手动实现 |
| **实现复杂度** | 简单 | 复杂 |
| **适用场景** | 进度推送、通知 | 聊天、实时协作 |

**结论**：进度推送只需要单向通信，SSE更简单。

### SSE实现

```python
# app/api/tasks.py
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import asyncio
import json

app = FastAPI()

async def task_progress_stream(task_id: str):
    """SSE流：推送任务进度"""
    from app.models.task import Task
    from app.core.database import SessionLocal

    db = SessionLocal()

    try:
        while True:
            # 查询任务状态
            task = db.query(Task).filter(Task.task_id == task_id).first()

            if not task:
                yield f"data: {json.dumps({'error': 'Task not found'})}\n\n"
                break

            # 推送进度
            data = {
                "task_id": task_id,
                "status": task.status,
                "progress": task.progress,
                "result": task.result
            }
            yield f"data: {json.dumps(data)}\n\n"

            # 任务完成或失败，停止推送
            if task.status in ["completed", "failed"]:
                break

            # 每秒查询一次
            await asyncio.sleep(1)

    finally:
        db.close()

@app.get("/tasks/{task_id}/stream")
async def stream_task_progress(task_id: str):
    """SSE端点：实时推送任务进度"""
    return StreamingResponse(
        task_progress_stream(task_id),
        media_type="text/event-stream"
    )
```

**关键点**：
- `StreamingResponse`：流式响应
- `media_type="text/event-stream"`：SSE格式
- `yield f"data: {json.dumps(data)}\n\n"`：SSE数据格式
- 每秒查询一次数据库

### 前端连接SSE

```javascript
// 前端代码（JavaScript）
const eventSource = new EventSource(`/tasks/${taskId}/stream`);

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log(`进度: ${data.progress}%`);

    // 更新进度条
    document.getElementById('progress').value = data.progress;

    // 任务完成
    if (data.status === 'completed') {
        console.log('任务完成:', data.result);
        eventSource.close();
    }
};

eventSource.onerror = (error) => {
    console.error('SSE错误:', error);
    eventSource.close();
};
```

**关键点**：
- `EventSource`：浏览器原生SSE客户端
- `onmessage`：接收服务器推送的数据
- `eventSource.close()`：关闭连接

---

## 4.5 错误处理与重试

### 自动重试

```python
# app/tasks.py
@app.task(
    bind=True,
    max_retries=3,           # 最多重试3次
    default_retry_delay=60   # 重试间隔60秒
)
def process_document(self, file_path: str):
    try:
        # 耗时操作
        result = parse_and_embed(file_path)
        return result

    except Exception as e:
        # 重试（指数退避）
        raise self.retry(exc=e, countdown=60 * (2 ** self.request.retries))
```

**关键点**：
- `max_retries=3`：最多重试3次
- `default_retry_delay=60`：重试间隔60秒
- `countdown=60 * (2 ** self.request.retries)`：指数退避（60秒、120秒、240秒）

### 任务超时

```python
@app.task(
    time_limit=300,      # 硬超时：5分钟
    soft_time_limit=270  # 软超时：4.5分钟（抛出异常）
)
def process_document(file_path: str):
    # 任务逻辑
    pass
```

**关键点**：
- `time_limit`：硬超时，强制终止
- `soft_time_limit`：软超时，抛出异常（可捕获）

---

## 这些知识足以

掌握以上内容，你就能：

✅ **识别长任务场景**：判断是否需要长任务处理
✅ **使用Celery**：定义任务、异步调用、查询结果
✅ **管理任务状态**：设计数据库模型、更新进度
✅ **实时推送进度**：使用SSE推送任务进度
✅ **处理错误**：自动重试、超时控制

---

## 完整示例：文档处理系统

```python
# ===== 1. 定义任务 (app/tasks.py) =====
from celery import Celery

app = Celery('tasks', broker='redis://localhost:6379/0')

@app.task(bind=True, max_retries=3)
def process_document(self, file_path: str, db_task_id: int):
    from app.models.task import Task
    from app.core.database import SessionLocal

    db = SessionLocal()
    task = db.query(Task).filter(Task.id == db_task_id).first()

    try:
        # 步骤1：解析
        task.status = "running"
        task.progress = 0.0
        db.commit()

        content = parse_pdf(file_path)
        task.progress = 50.0
        db.commit()

        # 步骤2：生成Embedding
        embedding = generate_embedding(content)
        task.progress = 100.0
        task.status = "completed"
        db.commit()

        return {"status": "success"}

    except Exception as e:
        task.status = "failed"
        task.error = str(e)
        db.commit()
        raise self.retry(exc=e)

    finally:
        db.close()

# ===== 2. FastAPI端点 (app/main.py) =====
from fastapi import FastAPI, UploadFile
from fastapi.responses import StreamingResponse
from app.tasks import process_document
from app.models.task import Task
from app.core.database import SessionLocal

app = FastAPI()

@app.post("/upload")
async def upload_file(file: UploadFile):
    # 保存文件
    file_path = f"/tmp/{file.filename}"
    with open(file_path, "wb") as f:
        f.write(await file.read())

    # 创建任务记录
    db = SessionLocal()
    db_task = Task(task_type="document_process", status="pending")
    db.add(db_task)
    db.commit()
    db.refresh(db_task)

    # 异步调用Celery任务
    celery_result = process_document.delay(file_path, db_task.id)

    # 更新Celery任务ID
    db_task.task_id = celery_result.id
    db.commit()
    db.close()

    return {
        "task_id": celery_result.id,
        "status": "processing"
    }

@app.get("/tasks/{task_id}/stream")
async def stream_progress(task_id: str):
    async def generate():
        db = SessionLocal()
        while True:
            task = db.query(Task).filter(Task.task_id == task_id).first()

            data = {
                "status": task.status,
                "progress": task.progress
            }
            yield f"data: {json.dumps(data)}\n\n"

            if task.status in ["completed", "failed"]:
                break

            await asyncio.sleep(1)

        db.close()

    return StreamingResponse(generate(), media_type="text/event-stream")

# ===== 3. 启动服务 =====
# Terminal 1: 启动Redis
# redis-server

# Terminal 2: 启动Celery Worker
# celery -A app.tasks worker --loglevel=info

# Terminal 3: 启动FastAPI
# uvicorn app.main:app --reload
```

---

## 下一步学习

掌握最小可用知识后，建议深入学习：

1. **WebSocket实时通信**：双向通信场景
2. **任务取消机制**：用户主动取消任务
3. **死信队列（DLQ）**：处理失败任务
4. **监控与告警**：Flower监控Celery
5. **性能优化**：并发控制、资源限制

---

**记住**：长任务处理的核心是**解耦**，让HTTP请求立即返回，任务在后台异步执行。
