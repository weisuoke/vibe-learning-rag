# 化骨绵掌

10 个 2 分钟知识卡片，全面掌握 RAG Prompt Engineering 基础。

---

## 卡片1：RAG Prompt 的本质

**一句话：** RAG Prompt 是让 LLM "基于给定资料回答问题"的指令。

**类比：**
- 普通 Prompt = 闭卷考试（凭记忆答题）
- RAG Prompt = 开卷考试（基于参考资料答题）

**核心区别：**
```python
# 普通 Prompt
"什么是量子计算？"  # LLM 用自己的知识回答

# RAG Prompt
"根据以下文档回答：什么是量子计算？"  # LLM 基于文档回答
```

**应用：** RAG 系统的输出质量很大程度取决于 Prompt 设计。

---

## 卡片2：系统提示词 - 角色设定

**一句话：** 告诉 LLM "你是谁"，建立回答的基调和风格。

**模板：**
```python
"你是一个[角色]，专门[职责]。"

# 示例
"你是一个技术文档助手，专门回答 API 使用问题。"
"你是一个客服助手，专门解答产品相关疑问。"
```

**为什么重要：**
- 角色影响回答风格（专业 vs 友好）
- 角色限定回答范围（只回答相关问题）

**应用：** 不同场景使用不同角色，客服用友好风格，技术文档用专业风格。

---

## 卡片3：系统提示词 - 行为约束

**一句话：** 告诉 LLM "什么能做、什么不能做"。

**三个核心约束：**
```python
system_prompt = """
1. 只基于提供的文档回答（信息来源约束）
2. 不要编造文档中没有的信息（禁止幻觉）
3. 无法回答时明确告知（边界处理）
"""
```

**为什么重要：**
- 没有约束，LLM 会"自由发挥"
- 约束是防止幻觉的第一道防线

**应用：** 每个 RAG 系统都应该有这三个基本约束。

---

## 卡片4：上下文注入 - 基本格式

**一句话：** 把检索到的文档结构化地放入 Prompt。

**最简格式：**
```python
context = """
【文档1】
{doc1_content}

【文档2】
{doc2_content}
"""
```

**关键点：**
- 用明显的分隔符区分文档
- 给文档编号，方便引用
- 保持格式一致

**应用：** 这是最基础的注入方式，适用于大多数场景。

---

## 卡片5：上下文注入 - 元数据标注

**一句话：** 在文档内容旁边标注来源信息。

**带元数据的格式：**
```python
context = """
【文档1】
来源：产品手册.pdf
页码：第15页
内容：
{doc1_content}
"""
```

**元数据的价值：**
- 让 LLM 可以引用来源
- 方便用户验证信息
- 增加回答的可信度

**应用：** 需要引用来源的场景（如法律、医疗、学术）必须使用。

---

## 卡片6：用户问题 - 格式化技巧

**一句话：** 把用户的原始问题包装成更清晰的格式。

**格式化示例：**
```python
# 原始问题
"退款"

# 格式化后
"""
问题：如何申请退款？
期望回答：请提供具体的操作步骤
"""
```

**格式化的好处：**
- 补充问题的上下文
- 明确期望的输出格式
- 减少歧义

**应用：** 对于简短或模糊的问题，格式化能显著提升回答质量。

---

## 卡片7：处理"无相关内容"

**一句话：** 明确告诉 LLM 找不到答案时该怎么回复。

**标准处理：**
```python
instruction = """
如果文档中没有相关信息，请回复：
"根据提供的文档，我无法找到关于这个问题的信息。"
不要尝试用自己的知识补充。
"""
```

**为什么重要：**
- 防止 LLM 编造答案
- 给用户明确的反馈
- 保持系统的可信度

**应用：** 这是防止幻觉的关键措施，必须在系统提示词中包含。

---

## 卡片8：输出格式控制

**一句话：** 指定 LLM 回答的结构和格式。

**常用格式指令：**
```python
# 简洁回答
"请用一句话回答。"

# 分步骤
"请分步骤回答，每步单独一行。"

# 结构化
"请按以下格式回答：【答案】... 【来源】..."

# JSON
"请用 JSON 格式回答。"
```

**格式指令的位置：**
- 放在系统提示词中（全局生效）
- 或放在用户问题后（单次生效）

**应用：** 根据下游需求选择格式，API 接口用 JSON，用户界面用 Markdown。

---

## 卡片9：Token 预算分配

**一句话：** 合理分配 Context Window 给系统提示词、上下文和问题。

**典型分配：**
```
总 Token 预算：4000 tokens（示例）
├── 系统提示词：200 tokens（5%）
├── 上下文文档：3000 tokens（75%）
├── 用户问题：300 tokens（7.5%）
└── 预留回答：500 tokens（12.5%）
```

**优化原则：**
- 系统提示词要精简
- 上下文是大头，但不要塞太多
- 预留足够的回答空间

**应用：** 根据模型的 Context Window 大小调整分配比例。

---

## 卡片10：RAG Prompt 调试技巧

**一句话：** 当回答不理想时，按顺序排查问题。

**调试清单：**
```
1. 检索问题？
   → 检索到的文档是否包含答案？

2. 注入问题？
   → 文档是否被正确格式化注入？

3. Prompt 问题？
   → 系统提示词是否清晰？
   → 是否有冲突的指令？

4. 模型问题？
   → 换个模型试试？
   → 调整 temperature？
```

**快速调试方法：**
```python
# 打印完整 Prompt 检查
print("=== System ===")
print(system_prompt)
print("=== User ===")
print(user_message)
```

**应用：** 遇到问题时，先打印完整 Prompt，80%的问题能直接看出来。

---

## 知识卡片总结

| 卡片 | 核心要点 |
|------|---------|
| 1. 本质 | RAG Prompt = 开卷考试指令 |
| 2. 角色设定 | 告诉 LLM "你是谁" |
| 3. 行为约束 | 告诉 LLM "能做什么、不能做什么" |
| 4. 基本注入 | 用分隔符和编号组织文档 |
| 5. 元数据 | 标注来源便于引用 |
| 6. 问题格式化 | 补充上下文、明确期望 |
| 7. 无答案处理 | 明确告知，不要编造 |
| 8. 输出格式 | 指定回答的结构 |
| 9. Token 预算 | 合理分配，预留回答空间 |
| 10. 调试 | 打印 Prompt，按顺序排查 |

---

**下一步：** [10_一句话总结](./10_一句话总结.md) - 最终总结
