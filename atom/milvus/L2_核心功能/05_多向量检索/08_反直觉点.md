# 反直觉点

> 多向量检索的3个常见误区

---

## 误区1：多向量字段越多越好 ❌

### 为什么错？

**错误观点：** "我应该为每个可能的维度都创建一个向量字段，这样检索会更准确"

**正确理解：**
- 向量字段过多会**显著降低检索性能**
- 每个向量字段都需要单独的索引和检索操作
- 融合算法的复杂度随字段数量增加
- 维护成本（存储、更新）成倍增长

**实际影响：**
```python
# ❌ 过度设计：10个向量字段
fields = [
    FieldSchema(name="text_vector", dtype=DataType.FLOAT_VECTOR, dim=1536),
    FieldSchema(name="title_vector", dtype=DataType.FLOAT_VECTOR, dim=768),
    FieldSchema(name="summary_vector", dtype=DataType.FLOAT_VECTOR, dim=512),
    FieldSchema(name="keyword_vector", dtype=DataType.FLOAT_VECTOR, dim=256),
    FieldSchema(name="image_vector", dtype=DataType.FLOAT_VECTOR, dim=512),
    FieldSchema(name="audio_vector", dtype=DataType.FLOAT_VECTOR, dim=128),
    FieldSchema(name="video_vector", dtype=DataType.FLOAT_VECTOR, dim=256),
    FieldSchema(name="metadata_vector", dtype=DataType.FLOAT_VECTOR, dim=64),
    FieldSchema(name="tag_vector", dtype=DataType.FLOAT_VECTOR, dim=32),
    FieldSchema(name="category_vector", dtype=DataType.FLOAT_VECTOR, dim=16),
]

# 检索时需要10次ANN搜索 + 融合 → 性能灾难！

# ✅ 合理设计：2-3个核心向量字段
fields = [
    FieldSchema(name="text_vector", dtype=DataType.FLOAT_VECTOR, dim=1536),
    FieldSchema(name="image_vector", dtype=DataType.FLOAT_VECTOR, dim=512),
]

# 检索时只需2次ANN搜索 + 融合 → 性能可控
```

**性能对比：**
| 向量字段数 | 检索延迟 | 存储成本 | 维护复杂度 |
|-----------|---------|---------|-----------|
| 2个 | ~50ms | 1x | 低 |
| 5个 | ~150ms | 2.5x | 中 |
| 10个 | ~400ms | 5x | 高 |

### 为什么人们容易这样错？

**心理原因：**
- **完美主义陷阱**：觉得"多就是好"，想覆盖所有可能的维度
- **过度工程**：担心未来需要某个维度，提前预留
- **忽视成本**：只看到"更全面"的好处，忽视性能和维护成本

**类比：**
就像做菜时加调料，不是越多越好。盐、糖、醋、酱油都加一点，反而会串味。**抓住2-3个核心调料就够了**。

### 正确做法

**原则：** 只保留**最有区分度**的向量字段

```python
# 决策流程
def should_add_vector_field(field_name):
    """是否应该添加这个向量字段？"""
    questions = [
        "这个字段能提供独特的信息吗？",  # 与现有字段不重复
        "这个字段对检索准确率有显著提升吗？",  # 实测提升>10%
        "这个字段的维护成本可接受吗？",  # 更新频率、计算成本
    ]
    # 三个问题都是"是"才添加
    return all(questions)

# 示例：文档检索系统
# ✅ 保留：text_vector（核心内容）
# ✅ 保留：image_vector（图表信息）
# ❌ 删除：title_vector（标题已包含在text中）
# ❌ 删除：summary_vector（与text高度相关）
# ❌ 删除：keyword_vector（可以用标量字段存储）
```

---

## 误区2：RRF融合一定比加权平均好 ❌

### 为什么错？

**错误观点：** "RRF是更先进的算法，应该总是用RRF而不是加权平均"

**正确理解：**
- RRF和加权平均**适用场景不同**
- RRF适合：不同度量方式（COSINE vs L2）、不同模型（OpenAI vs BGE）
- 加权平均适合：相同度量方式、需要精细控制权重

**实际对比：**
```python
# 场景1：不同度量方式 → RRF更好
# text_vector用COSINE（0-1），image_vector用L2（0-∞）
# 分数范围不同，直接加权平均会失真

req1 = AnnSearchRequest(
    data=[text_embedding],
    anns_field="text_vector",
    param={"metric_type": "COSINE"},  # 分数范围：0-1
    limit=10
)

req2 = AnnSearchRequest(
    data=[image_embedding],
    anns_field="image_vector",
    param={"metric_type": "L2"},  # 分数范围：0-∞
    limit=10
)

# ✅ 使用RRF（只看排名，不看分数）
results = collection.hybrid_search(reqs=[req1, req2], rerank=RRFRanker(), limit=5)

# ---

# 场景2：相同度量方式 + 需要精细控制 → 加权平均更好
# 两个字段都用COSINE，且需要text权重70%，image权重30%

req1 = AnnSearchRequest(
    data=[text_embedding],
    anns_field="text_vector",
    param={"metric_type": "COSINE"},
    limit=10
)

req2 = AnnSearchRequest(
    data=[image_embedding],
    anns_field="image_vector",
    param={"metric_type": "COSINE"},
    limit=10
)

# ❌ RRF无法精确控制权重比例（只能通过limit间接影响）
# ✅ 手动加权平均（精确控制70:30）
def weighted_fusion(results1, results2, w1=0.7, w2=0.3):
    scores = {}
    for hit in results1[0]:
        scores[hit.id] = hit.distance * w1
    for hit in results2[0]:
        scores[hit.id] = scores.get(hit.id, 0) + hit.distance * w2
    return sorted(scores.items(), key=lambda x: x[1], reverse=True)
```

**选择指南：**
| 场景 | 推荐算法 | 原因 |
|------|---------|------|
| 不同度量方式（COSINE vs L2） | RRF | 分数范围不同，排名更公平 |
| 不同Embedding模型 | RRF | 分数分布不同 |
| 相同度量方式 + 需要精细权重 | 加权平均 | 可精确控制权重比例 |
| 快速原型 | RRF | 无需调参，开箱即用 |
| 生产优化 | 加权平均 | 可根据A/B测试调优权重 |

### 为什么人们容易这样错？

**心理原因：**
- **新技术崇拜**：RRF是新算法，觉得一定比传统方法好
- **懒惰思维**：RRF不需要调参，省事
- **忽视场景**：没有分析具体场景的特点

**类比：**
就像炒菜，**大火爆炒**和**小火慢炖**都是好方法，但要看食材。青菜用大火，排骨用小火。**没有绝对的"更好"，只有"更适合"**。

### 正确做法

**决策流程：**
```python
def choose_fusion_algorithm(vector_fields):
    """选择融合算法"""
    # 1. 检查度量方式是否一致
    metrics = [field.metric_type for field in vector_fields]
    if len(set(metrics)) > 1:
        return "RRF"  # 不同度量方式 → RRF

    # 2. 检查是否需要精细权重控制
    if need_precise_weights():
        return "WeightedAverage"  # 需要精细控制 → 加权平均

    # 3. 默认使用RRF（简单、鲁棒）
    return "RRF"

# 实际应用
if choose_fusion_algorithm(fields) == "RRF":
    results = collection.hybrid_search(reqs=[req1, req2], rerank=RRFRanker(), limit=5)
else:
    results = weighted_fusion(results1, results2, w1=0.7, w2=0.3)
```

---

## 误区3：多向量检索可以替代标量过滤 ❌

### 为什么错？

**错误观点：** "我可以把所有过滤条件都转成向量字段，这样就不需要标量过滤了"

**正确理解：**
- 向量检索是**语义相似度匹配**（模糊匹配）
- 标量过滤是**精确条件匹配**（硬约束）
- 两者**互补**，不能互相替代

**实际问题：**
```python
# ❌ 错误做法：把价格范围转成向量
# 用户需求：找5000-8000元的手机
price_embedding = encode_price_range(5000, 8000)  # 把价格转成向量？

# 问题1：价格是精确数值，不是语义概念
# 问题2：向量检索是Top-K，无法保证"必须在5000-8000"
# 问题3：浪费向量字段和计算资源

results = collection.search(
    data=[price_embedding],
    anns_field="price_vector",  # ❌ 不应该存在这个字段
    limit=10
)
# 结果可能包含4999元或8001元的手机（不符合硬约束）

# ✅ 正确做法：标量过滤 + 向量检索
results = collection.search(
    data=[text_embedding],
    anns_field="text_vector",
    param={"metric_type": "COSINE"},
    limit=10,
    expr="price >= 5000 and price <= 8000"  # ✅ 硬约束用标量过滤
)
# 保证所有结果都在5000-8000元范围内
```

**向量 vs 标量对比：**
| 维度 | 向量检索 | 标量过滤 |
|------|---------|---------|
| **用途** | 语义相似度匹配 | 精确条件匹配 |
| **匹配方式** | 模糊匹配（Top-K） | 硬约束（必须满足） |
| **适用场景** | 文本、图片、音频 | 价格、日期、类别 |
| **示例** | "找相似的文章" | "价格<1000" |
| **性能** | ANN算法（近似） | 索引查找（精确） |

### 为什么人们容易这样错？

**心理原因：**
- **工具单一化**：学会了向量检索，就想用它解决所有问题
- **概念混淆**：把"多维度"等同于"多向量"
- **忽视本质**：没有理解向量检索的本质是语义匹配

**类比：**
就像用**放大镜**和**尺子**测量物体。放大镜看细节（语义相似度），尺子量长度（精确数值）。**不能用放大镜量长度，也不能用尺子看细节**。

### 正确做法

**混合使用：向量检索 + 标量过滤**

```python
# 场景：电商搜索 "红色连衣裙，价格1000以下，有货"

# 1. 向量检索（语义匹配）
text_embedding = get_embedding("红色连衣裙")
image_embedding = get_embedding(user_sketch)

req1 = AnnSearchRequest(
    data=[text_embedding],
    anns_field="text_vector",
    param={"metric_type": "COSINE"},
    limit=50  # 先召回50个候选
)

req2 = AnnSearchRequest(
    data=[image_embedding],
    anns_field="image_vector",
    param={"metric_type": "COSINE"},
    limit=50
)

# 2. 标量过滤（硬约束）
results = collection.hybrid_search(
    reqs=[req1, req2],
    rerank=RRFRanker(),
    limit=10,
    # ✅ 硬约束：价格、库存、类别
    expr="price < 1000 and stock > 0 and category == 'dress'"
)

# 最终结果：
# - 语义上相似（红色、连衣裙）
# - 满足硬约束（价格<1000、有货）
```

**决策指南：**
```python
def should_use_vector_or_scalar(field_name, field_type):
    """判断应该用向量还是标量"""
    # 向量字段：语义信息
    vector_fields = [
        "text", "title", "description",  # 文本
        "image", "video", "audio",       # 多媒体
        "embedding", "features"          # 已有向量
    ]

    # 标量字段：精确信息
    scalar_fields = [
        "price", "date", "timestamp",    # 数值、时间
        "category", "status", "type",    # 枚举、分类
        "id", "user_id", "order_id"      # 标识符
    ]

    if field_name in vector_fields:
        return "vector"
    elif field_name in scalar_fields:
        return "scalar"
    else:
        # 默认：能用标量就用标量（更快、更准确）
        return "scalar"
```

---

## 误区总结

| 误区 | 错误观点 | 正确理解 |
|------|---------|---------|
| **误区1** | 向量字段越多越好 | 2-3个核心字段即可，过多影响性能 |
| **误区2** | RRF一定比加权平均好 | 根据场景选择，各有优势 |
| **误区3** | 多向量可以替代标量过滤 | 向量是语义匹配，标量是硬约束，互补 |

---

## 避免误区的原则

1. **性能优先**：每增加一个向量字段，都要评估性能影响
2. **场景驱动**：根据实际场景选择融合算法，不盲目追新
3. **工具互补**：向量检索 + 标量过滤 = 最佳组合
4. **实测验证**：通过A/B测试验证效果，不凭直觉

---

**继续学习：** [09_实战代码_场景1_多字段向量定义.md](./09_实战代码_场景1_多字段向量定义.md)
