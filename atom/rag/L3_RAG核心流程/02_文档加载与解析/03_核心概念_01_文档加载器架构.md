# 核心概念1：文档加载器架构

> 理解Loader模式的设计原理和标准化接口

---

## 什么是文档加载器架构？

**文档加载器架构**是RAG系统中处理多格式文档的标准化设计模式，通过统一的接口和可插拔的解析器实现格式无关的文档处理。

**核心思想：**
```
多种格式 → 统一接口 → 标准输出
(PDF/DOCX/HTML) → (Loader) → (Document对象)
```

---

## 1. Loader模式的设计原理

### 1.1 为什么需要Loader模式？

**问题：** 不同格式的文档需要不同的处理方式

```python
# ❌ 没有Loader模式：代码重复、难以维护
def process_pdf(file_path):
    # PDF特定的处理逻辑
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text

def process_docx(file_path):
    # DOCX特定的处理逻辑
    doc = Document(file_path)
    text = ""
    for para in doc.paragraphs:
        text += para.text
    return text

def process_html(file_path):
    # HTML特定的处理逻辑
    with open(file_path) as f:
        soup = BeautifulSoup(f, 'html.parser')
        text = soup.get_text()
    return text

# 使用时需要判断格式
if file_path.endswith('.pdf'):
    text = process_pdf(file_path)
elif file_path.endswith('.docx'):
    text = process_docx(file_path)
elif file_path.endswith('.html'):
    text = process_html(file_path)
```

**解决方案：** Loader模式提供统一接口

```python
# ✅ 使用Loader模式：统一接口、易于扩展
from langchain.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredHTMLLoader
)

# 所有Loader都实现相同的接口
loaders = {
    '.pdf': PyPDFLoader,
    '.docx': Docx2txtLoader,
    '.html': UnstructuredHTMLLoader
}

# 统一的使用方式
def load_document(file_path):
    ext = os.path.splitext(file_path)[1]
    loader_class = loaders.get(ext)
    if loader_class:
        loader = loader_class(file_path)
        return loader.load()  # 统一的load()方法
    raise ValueError(f"不支持的格式: {ext}")

# 使用时不需要关心格式
docs = load_document("report.pdf")  # 返回List[Document]
docs = load_document("proposal.docx")  # 返回List[Document]
docs = load_document("article.html")  # 返回List[Document]
```

### 1.2 Loader模式的核心组件

```python
from abc import ABC, abstractmethod
from typing import List
from langchain.schema import Document

class BaseLoader(ABC):
    """所有Loader的基类"""

    def __init__(self, file_path: str):
        self.file_path = file_path

    @abstractmethod
    def load(self) -> List[Document]:
        """
        加载文档的核心方法
        返回: Document对象列表
        """
        pass

    def lazy_load(self):
        """
        流式加载（可选）
        用于大文件的逐步处理
        """
        yield from self.load()

# 具体实现示例
class CustomPDFLoader(BaseLoader):
    """自定义PDF加载器"""

    def load(self) -> List[Document]:
        from pypdf import PdfReader

        reader = PdfReader(self.file_path)
        documents = []

        for page_num, page in enumerate(reader.pages):
            text = page.extract_text()
            doc = Document(
                page_content=text,
                metadata={
                    "source": self.file_path,
                    "page": page_num,
                    "total_pages": len(reader.pages)
                }
            )
            documents.append(doc)

        return documents
```

**关键设计原则：**
1. **统一接口**：所有Loader都实现`load()`方法
2. **返回标准对象**：返回`List[Document]`
3. **元数据保留**：每个Document包含metadata
4. **可扩展性**：新格式只需实现BaseLoader

---

## 2. Connector vs Parser 分离

### 2.1 概念区分

**Connector（连接器）**：负责获取原始数据
- 从文件系统读取
- 从网络下载
- 从数据库查询
- 从API获取

**Parser（解析器）**：负责解析数据格式
- PDF解析
- HTML解析
- JSON解析
- XML解析

### 2.2 分离的好处

```python
# ===== Connector层 =====
class FileConnector:
    """文件系统连接器"""
    def fetch(self, path: str) -> bytes:
        with open(path, 'rb') as f:
            return f.read()

class URLConnector:
    """网络连接器"""
    def fetch(self, url: str) -> bytes:
        import requests
        response = requests.get(url)
        return response.content

class S3Connector:
    """S3存储连接器"""
    def fetch(self, s3_path: str) -> bytes:
        # 从S3下载
        pass

# ===== Parser层 =====
class PDFParser:
    """PDF解析器"""
    def parse(self, data: bytes) -> List[Document]:
        from pypdf import PdfReader
        from io import BytesIO

        reader = PdfReader(BytesIO(data))
        # 解析逻辑...
        return documents

class HTMLParser:
    """HTML解析器"""
    def parse(self, data: bytes) -> List[Document]:
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(data, 'html.parser')
        # 解析逻辑...
        return documents

# ===== 组合使用 =====
class UniversalLoader:
    """通用加载器：组合Connector和Parser"""

    def __init__(self, connector, parser):
        self.connector = connector
        self.parser = parser

    def load(self, source: str) -> List[Document]:
        # 1. 使用Connector获取数据
        data = self.connector.fetch(source)

        # 2. 使用Parser解析数据
        documents = self.parser.parse(data)

        return documents

# 灵活组合
# 从文件加载PDF
loader1 = UniversalLoader(FileConnector(), PDFParser())
docs1 = loader1.load("report.pdf")

# 从URL加载PDF
loader2 = UniversalLoader(URLConnector(), PDFParser())
docs2 = loader2.load("https://example.com/report.pdf")

# 从S3加载PDF
loader3 = UniversalLoader(S3Connector(), PDFParser())
docs3 = loader3.load("s3://bucket/report.pdf")
```

**分离的优势：**
- **复用性**：同一个Parser可以配合不同的Connector
- **可测试性**：可以单独测试Connector和Parser
- **灵活性**：可以自由组合不同的Connector和Parser

---

## 3. 元数据Schema设计

### 3.1 标准元数据字段

```python
from typing import Optional, List
from datetime import datetime
from pydantic import BaseModel

class DocumentMetadata(BaseModel):
    """标准化的文档元数据schema"""

    # ===== 必需字段（核心元数据） =====
    source: str              # 文件路径或URL
    page: Optional[int] = None  # 页码（如果适用）

    # ===== 推荐字段（增强检索） =====
    title: Optional[str] = None        # 文档标题
    author: Optional[str] = None       # 作者
    created_date: Optional[datetime] = None  # 创建时间
    modified_date: Optional[datetime] = None  # 修改时间
    language: Optional[str] = "zh"     # 语言

    # ===== 结构元数据 =====
    section: Optional[str] = None      # 章节
    heading: Optional[str] = None      # 标题
    page_count: Optional[int] = None   # 总页数

    # ===== 技术元数据 =====
    file_size: Optional[int] = None    # 文件大小（字节）
    file_type: Optional[str] = None    # 文件类型
    encoding: Optional[str] = None     # 编码

    # ===== 业务元数据（可扩展） =====
    department: Optional[str] = None   # 部门
    category: Optional[str] = None     # 分类
    tags: Optional[List[str]] = None   # 标签
    access_level: Optional[str] = "public"  # 访问级别

    # ===== 系统元数据 =====
    loaded_at: Optional[datetime] = None  # 加载时间
    loader_version: Optional[str] = None  # 加载器版本
```

### 3.2 元数据提取策略

```python
from langchain.document_loaders import PyPDFLoader
from datetime import datetime
import os

class EnhancedPDFLoader:
    """增强的PDF加载器，提取完整元数据"""

    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Document]:
        # 1. 基础加载
        loader = PyPDFLoader(self.file_path)
        docs = loader.load()

        # 2. 提取文件级元数据
        file_metadata = self._extract_file_metadata()

        # 3. 提取PDF元数据
        pdf_metadata = self._extract_pdf_metadata()

        # 4. 合并元数据到每个Document
        for doc in docs:
            doc.metadata.update(file_metadata)
            doc.metadata.update(pdf_metadata)

        return docs

    def _extract_file_metadata(self) -> dict:
        """提取文件系统元数据"""
        stat = os.stat(self.file_path)
        return {
            "file_name": os.path.basename(self.file_path),
            "file_size": stat.st_size,
            "file_type": "pdf",
            "created_date": datetime.fromtimestamp(stat.st_ctime).isoformat(),
            "modified_date": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "loaded_at": datetime.now().isoformat()
        }

    def _extract_pdf_metadata(self) -> dict:
        """提取PDF内部元数据"""
        from pypdf import PdfReader

        reader = PdfReader(self.file_path)
        metadata = reader.metadata

        return {
            "title": metadata.get("/Title", ""),
            "author": metadata.get("/Author", ""),
            "subject": metadata.get("/Subject", ""),
            "creator": metadata.get("/Creator", ""),
            "producer": metadata.get("/Producer", ""),
            "page_count": len(reader.pages)
        }

# 使用示例
loader = EnhancedPDFLoader("report.pdf")
docs = loader.load()

print(docs[0].metadata)
# {
#   'source': 'report.pdf',
#   'page': 0,
#   'file_name': 'report.pdf',
#   'file_size': 1024000,
#   'file_type': 'pdf',
#   'created_date': '2025-01-15T10:30:00',
#   'modified_date': '2025-01-20T15:45:00',
#   'loaded_at': '2026-02-15T12:00:00',
#   'title': 'RAG技术报告',
#   'author': '张三',
#   'page_count': 50
# }
```

---

## 4. LangChain的Loader架构

### 4.1 LangChain Loader层次结构

```
BaseLoader (抽象基类)
├── FileBasedLoader (文件加载器基类)
│   ├── PyPDFLoader
│   ├── Docx2txtLoader
│   ├── TextLoader
│   └── UnstructuredFileLoader
├── WebBasedLoader (网络加载器基类)
│   ├── WebBaseLoader
│   ├── GitHubLoader
│   └── NotionLoader
└── DirectoryLoader (目录加载器)
```

### 4.2 实际使用示例

```python
from langchain.document_loaders import (
    PyPDFLoader,
    DirectoryLoader,
    WebBaseLoader
)

# ===== 1. 单文件加载 =====
pdf_loader = PyPDFLoader("report.pdf")
docs = pdf_loader.load()

# ===== 2. 目录批量加载 =====
dir_loader = DirectoryLoader(
    "docs/",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader,
    show_progress=True  # 显示进度条
)
all_docs = dir_loader.load()

# ===== 3. 网络加载 =====
web_loader = WebBaseLoader("https://example.com/article")
web_docs = web_loader.load()

# ===== 4. 流式加载（大文件） =====
for doc in pdf_loader.lazy_load():
    # 逐个处理文档
    process_document(doc)
```

---

## 5. 在RAG中的应用

### 5.1 构建知识库

```python
from langchain.document_loaders import DirectoryLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# 1. 加载所有文档
loader = DirectoryLoader(
    "knowledge_base/",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader
)
documents = loader.load()
print(f"加载了 {len(documents)} 个文档")

# 2. 分块
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = splitter.split_documents(documents)
print(f"分块后得到 {len(chunks)} 个文本块")

# 3. 向量化并存储
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    chunks,
    embeddings,
    persist_directory="./chroma_db"
)

# 4. 检索
query = "RAG的核心流程是什么？"
results = vectorstore.similarity_search(query, k=3)

# 5. 显示结果和来源
for i, doc in enumerate(results, 1):
    print(f"\n结果 {i}:")
    print(f"内容: {doc.page_content[:100]}...")
    print(f"来源: {doc.metadata['source']} (第{doc.metadata['page']}页)")
```

### 5.2 自定义Loader

```python
from langchain.document_loaders.base import BaseLoader
from langchain.schema import Document
from typing import List

class CustomMarkdownLoader(BaseLoader):
    """自定义Markdown加载器，提取标题层级"""

    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Document]:
        with open(self.file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # 按标题分割
        sections = self._split_by_headers(content)

        documents = []
        for section in sections:
            doc = Document(
                page_content=section['content'],
                metadata={
                    "source": self.file_path,
                    "heading": section['heading'],
                    "level": section['level']
                }
            )
            documents.append(doc)

        return documents

    def _split_by_headers(self, content: str) -> List[dict]:
        """按Markdown标题分割"""
        import re

        sections = []
        current_section = {"heading": "", "level": 0, "content": ""}

        for line in content.split('\n'):
            # 检测标题
            match = re.match(r'^(#{1,6})\s+(.+)$', line)
            if match:
                # 保存上一个section
                if current_section['content']:
                    sections.append(current_section)

                # 开始新section
                level = len(match.group(1))
                heading = match.group(2)
                current_section = {
                    "heading": heading,
                    "level": level,
                    "content": ""
                }
            else:
                current_section['content'] += line + '\n'

        # 保存最后一个section
        if current_section['content']:
            sections.append(current_section)

        return sections

# 使用自定义Loader
loader = CustomMarkdownLoader("guide.md")
docs = loader.load()

for doc in docs:
    print(f"标题: {doc.metadata['heading']} (级别: {doc.metadata['level']})")
    print(f"内容: {doc.page_content[:50]}...\n")
```

---

## 总结

**文档加载器架构的核心价值：**

1. **统一接口**：所有格式通过相同的`load()`方法处理
2. **标准输出**：返回统一的`Document`对象
3. **元数据保留**：完整保留文档的上下文信息
4. **可扩展性**：易于添加新格式支持
5. **分离关注点**：Connector和Parser职责清晰

**在RAG中的作用：**
- 提供高质量的文档输入
- 保留元数据用于检索过滤
- 支持多种数据源（文件、网络、数据库）
- 为后续的分块和向量化奠定基础

---

## 参考来源

> **参考来源：**
> - [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) - 官方文档 (2025)
> - [LangChain BaseLoader Source](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/document_loaders/base.py) - 源码 (2025)
> - [Design Patterns: Strategy Pattern](https://refactoring.guru/design-patterns/strategy) - 策略模式

---

**版本：** v1.0
**最后更新：** 2026-02-15
**下一步：** 阅读 [03_核心概念_02_多格式解析策略.md](./03_核心概念_02_多格式解析策略.md)
