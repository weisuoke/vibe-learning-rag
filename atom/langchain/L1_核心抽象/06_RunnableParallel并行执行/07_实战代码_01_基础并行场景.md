# RunnableParallel并行执行 - 实战代码1：基础并行场景

> 通过完整可运行的代码掌握基础并行场景

---

## 场景1：多模型并行调用

### 场景描述

在 AI Agent 开发中，经常需要同时调用多个 LLM 模型进行结果对比或投票决策。

**业务需求**：
- 同时调用 GPT-4、Claude、Gemini 三个模型
- 对比三个模型的输出
- 计算一致性和置信度
- 响应时间要求：< 2秒

**技术挑战**：
- 串行调用耗时 4.5秒（1.5s × 3）
- 需要并行执行降低到 1.5秒
- 需要处理部分模型失败的情况

---

### 完整代码

```python
"""
场景1：多模型并行调用
演示：同时调用多个 LLM 模型进行结果对比
"""

import os
import asyncio
from typing import Dict, Any
from collections import Counter
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnableParallel
from langchain_core.prompts import ChatPromptTemplate

# 加载环境变量
load_dotenv()

# ===== 1. 定义多个模型 =====
print("=== 初始化模型 ===")

# GPT-4o
gpt4 = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    api_key=os.getenv("OPENAI_API_KEY")
)

# Claude 3.5 Sonnet
claude = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0,
    api_key=os.getenv("ANTHROPIC_API_KEY")
)

# Gemini（通过 OpenAI 兼容接口）
gemini = ChatOpenAI(
    model="gemini-2.0-flash-exp",
    temperature=0,
    base_url=os.getenv("GEMINI_BASE_URL", "https://generativelanguage.googleapis.com/v1beta/openai/"),
    api_key=os.getenv("GEMINI_API_KEY")
)

print("✓ 模型初始化完成")

# ===== 2. 创建并行执行链 =====
print("\n=== 创建并行执行链 ===")

# 定义提示词模板
prompt = ChatPromptTemplate.from_template(
    "请用一句话回答：{question}\n"
    "要求：简洁、准确、客观"
)

# 创建三个模型链
gpt4_chain = prompt | gpt4
claude_chain = prompt | claude
gemini_chain = prompt | gemini

# 并行执行
parallel = RunnableParallel(
    gpt4=gpt4_chain,
    claude=claude_chain,
    gemini=gemini_chain
)

print("✓ 并行链创建完成")

# ===== 3. 执行并行调用 =====
async def multi_model_query(question: str) -> Dict[str, Any]:
    """多模型并行查询"""
    print(f"\n=== 查询问题：{question} ===")

    import time
    start = time.time()

    # 并行调用三个模型
    results = await parallel.ainvoke({"question": question})

    elapsed = time.time() - start
    print(f"✓ 并行执行完成，耗时：{elapsed:.2f}秒")

    # 提取回答
    answers = {
        "gpt4": results["gpt4"].content,
        "claude": results["claude"].content,
        "gemini": results["gemini"].content
    }

    return answers

# ===== 4. 结果分析 =====
def analyze_results(answers: Dict[str, str]) -> Dict[str, Any]:
    """分析多模型结果"""
    print("\n=== 结果分析 ===")

    # 打印每个模型的回答
    for model, answer in answers.items():
        print(f"\n{model.upper()}:")
        print(f"  {answer}")

    # 计算一致性（简单的文本相似度）
    answer_list = list(answers.values())
    counter = Counter(answer_list)
    most_common = counter.most_common(1)[0]

    consensus = {
        "most_common_answer": most_common[0],
        "agreement_count": most_common[1],
        "total_models": len(answers),
        "consensus_rate": most_common[1] / len(answers)
    }

    print(f"\n一致性分析:")
    print(f"  最常见回答：{consensus['most_common_answer'][:50]}...")
    print(f"  同意数量：{consensus['agreement_count']}/{consensus['total_models']}")
    print(f"  一致率：{consensus['consensus_rate'] * 100:.1f}%")

    return consensus

# ===== 5. 主函数 =====
async def main():
    """主函数"""
    # 测试问题
    questions = [
        "什么是量子计算？",
        "Python 和 JavaScript 的主要区别是什么？",
        "如何优化 LLM 应用的成本？"
    ]

    for question in questions:
        # 并行查询
        answers = await multi_model_query(question)

        # 分析结果
        consensus = analyze_results(answers)

        print("\n" + "="*60)

# 运行
if __name__ == "__main__":
    asyncio.run(main())
```

---

### 运行结果

```
=== 初始化模型 ===
✓ 模型初始化完成

=== 创建并行执行链 ===
✓ 并行链创建完成

=== 查询问题：什么是量子计算？ ===
✓ 并行执行完成，耗时：1.52秒

=== 结果分析 ===

GPT4:
  量子计算是利用量子力学原理（如叠加和纠缠）进行信息处理的计算方式，能够在特定问题上实现指数级加速。

CLAUDE:
  量子计算是利用量子力学原理（叠加态和纠缠）进行信息处理的新型计算范式，在特定问题上具有超越经典计算机的潜力。

GEMINI:
  量子计算利用量子力学原理（叠加和纠缠）处理信息，在特定问题上比传统计算机更快。

一致性分析:
  最常见回答：量子计算是利用量子力学原理（如叠加和纠缠）进行信息处理...
  同意数量：1/3
  一致率：33.3%

============================================================

=== 查询问题：Python 和 JavaScript 的主要区别是什么？ ===
✓ 并行执行完成，耗时：1.48秒

=== 结果分析 ===

GPT4:
  Python 是后端语言，强类型，适合数据科学；JavaScript 是前端语言，弱类型，适合 Web 开发。

CLAUDE:
  Python 主要用于后端和数据科学，语法简洁；JavaScript 主要用于前端和全栈开发，基于事件驱动。

GEMINI:
  Python 用于后端和数据科学，JavaScript 用于前端和全栈，语法和应用场景不同。

一致性分析:
  最常见回答：Python 是后端语言，强类型，适合数据科学；JavaScript...
  同意数量：1/3
  一致率：33.3%

============================================================
```

**性能对比**：
- 串行执行：4.5秒（1.5s × 3）
- 并行执行：1.5秒
- **提升：3倍**

---

### 最佳实践

#### 1. 添加容错机制

```python
# 为每个模型添加 fallback
gpt4_with_fallback = gpt4_chain.with_fallback([
    ChatOpenAI(model="gpt-3.5-turbo") | prompt
])

claude_with_fallback = claude_chain.with_fallback([
    ChatAnthropic(model="claude-3-haiku-20240307") | prompt
])

# 并行执行（带容错）
parallel = RunnableParallel(
    gpt4=gpt4_with_fallback,
    claude=claude_with_fallback,
    gemini=gemini_chain
)
```

#### 2. 添加超时控制

```python
# 设置超时
gpt4_with_timeout = gpt4_chain.with_config({"timeout": 5})
claude_with_timeout = claude_chain.with_config({"timeout": 5})
gemini_with_timeout = gemini_chain.with_config({"timeout": 5})

parallel = RunnableParallel(
    gpt4=gpt4_with_timeout,
    claude=claude_with_timeout,
    gemini=gemini_with_timeout
)
```

#### 3. 添加成本追踪

```python
from langsmith import trace

@trace
async def tracked_multi_model_query(question: str):
    """带成本追踪的多模型查询"""
    results = await parallel.ainvoke({"question": question})
    # LangSmith 自动记录：
    # - 每个模型的 token 使用量
    # - 预估成本
    # - 执行时间
    return results
```

---

## 场景2：多数据源并行检索

### 场景描述

在 RAG 系统中，需要同时从多个数据源检索相关信息。

**业务需求**：
- 同时从向量数据库、知识图谱、搜索引擎检索
- 合并去重检索结果
- 按相关性排序
- 响应时间要求：< 2秒

---

### 完整代码

```python
"""
场景2：多数据源并行检索
演示：RAG 系统中的多路并行检索
"""

import asyncio
from typing import List, Dict, Any
from dataclasses import dataclass

from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_core.documents import Document

# ===== 1. 定义文档数据结构 =====
@dataclass
class SearchResult:
    """检索结果"""
    content: str
    source: str
    score: float
    metadata: Dict[str, Any]

# ===== 2. 模拟检索器 =====
async def vector_search(query: str) -> List[SearchResult]:
    """向量数据库检索（模拟）"""
    print(f"  [向量检索] 查询：{query}")
    await asyncio.sleep(1)  # 模拟网络延迟

    return [
        SearchResult(
            content="LangChain 是一个用于构建 LLM 应用的框架",
            source="vector_db",
            score=0.95,
            metadata={"doc_id": "v1"}
        ),
        SearchResult(
            content="RunnableParallel 支持并行执行多个任务",
            source="vector_db",
            score=0.88,
            metadata={"doc_id": "v2"}
        )
    ]

async def kg_search(query: str) -> List[SearchResult]:
    """知识图谱检索（模拟）"""
    print(f"  [知识图谱] 查询：{query}")
    await asyncio.sleep(1)

    return [
        SearchResult(
            content="LangChain 由 Harrison Chase 创建于 2022 年",
            source="knowledge_graph",
            score=0.82,
            metadata={"entity": "LangChain"}
        )
    ]

async def web_search(query: str) -> List[SearchResult]:
    """搜索引擎检索（模拟）"""
    print(f"  [搜索引擎] 查询：{query}")
    await asyncio.sleep(1)

    return [
        SearchResult(
            content="LangChain 是一个用于构建 LLM 应用的框架",  # 重复
            source="web_search",
            score=0.75,
            metadata={"url": "https://langchain.com"}
        ),
        SearchResult(
            content="LangChain 支持多种 LLM 模型",
            source="web_search",
            score=0.70,
            metadata={"url": "https://docs.langchain.com"}
        )
    ]

# ===== 3. 创建并行检索链 =====
print("=== 创建并行检索链 ===")

parallel_retrieval = RunnableParallel(
    vector=RunnableLambda(vector_search),
    kg=RunnableLambda(kg_search),
    web=RunnableLambda(web_search)
)

print("✓ 并行检索链创建完成")

# ===== 4. 结果合并与去重 =====
def merge_and_deduplicate(results: Dict[str, List[SearchResult]]) -> List[SearchResult]:
    """合并去重检索结果"""
    print("\n=== 合并去重 ===")

    # 1. 收集所有结果
    all_results = []
    for source, docs in results.items():
        all_results.extend(docs)

    print(f"  原始结果数：{len(all_results)}")

    # 2. 去重（基于内容）
    seen = set()
    unique_results = []

    for result in all_results:
        content_hash = hash(result.content)
        if content_hash not in seen:
            seen.add(content_hash)
            unique_results.append(result)

    print(f"  去重后结果数：{len(unique_results)}")

    # 3. 按分数排序
    unique_results.sort(key=lambda r: r.score, reverse=True)

    return unique_results

# ===== 5. 主函数 =====
async def main():
    """主函数"""
    query = "LangChain 是什么？"

    print(f"\n=== 查询：{query} ===")

    import time
    start = time.time()

    # 并行检索
    results = await parallel_retrieval.ainvoke(query)

    elapsed = time.time() - start
    print(f"\n✓ 并行检索完成，耗时：{elapsed:.2f}秒")

    # 合并去重
    merged_results = merge_and_deduplicate(results)

    # 打印结果
    print("\n=== 最终结果（Top 3）===")
    for i, result in enumerate(merged_results[:3], 1):
        print(f"\n{i}. [{result.source}] (分数: {result.score})")
        print(f"   {result.content}")

# 运行
if __name__ == "__main__":
    asyncio.run(main())
```

---

### 运行结果

```
=== 创建并行检索链 ===
✓ 并行检索链创建完成

=== 查询：LangChain 是什么？ ===
  [向量检索] 查询：LangChain 是什么？
  [知识图谱] 查询：LangChain 是什么？
  [搜索引擎] 查询：LangChain 是什么？

✓ 并行检索完成，耗时：1.01秒

=== 合并去重 ===
  原始结果数：5
  去重后结果数：4

=== 最终结果（Top 3）===

1. [vector_db] (分数: 0.95)
   LangChain 是一个用于构建 LLM 应用的框架

2. [vector_db] (分数: 0.88)
   RunnableParallel 支持并行执行多个任务

3. [knowledge_graph] (分数: 0.82)
   LangChain 由 Harrison Chase 创建于 2022 年
```

**性能对比**：
- 串行检索：3秒（1s × 3）
- 并行检索：1秒
- **提升：3倍**

---

### 最佳实践

#### 1. 加权合并

```python
def weighted_merge(results: Dict[str, List[SearchResult]]) -> List[SearchResult]:
    """加权合并"""
    weights = {
        "vector": 1.0,   # 向量检索权重最高
        "kg": 0.8,       # 知识图谱次之
        "web": 0.6       # 搜索引擎最低
    }

    all_results = []
    for source, docs in results.items():
        weight = weights.get(source, 0.5)
        for doc in docs:
            doc.score *= weight  # 应用权重
            all_results.append(doc)

    # 按加权分数排序
    all_results.sort(key=lambda r: r.score, reverse=True)
    return all_results
```

#### 2. 部分失败容错

```python
# 为每个检索器添加 fallback
parallel_retrieval = RunnableParallel(
    vector=RunnableLambda(vector_search).with_fallback([
        RunnableLambda(lambda _: [])  # 失败返回空列表
    ]),
    kg=RunnableLambda(kg_search).with_fallback([
        RunnableLambda(lambda _: [])
    ]),
    web=RunnableLambda(web_search).with_fallback([
        RunnableLambda(lambda _: [])
    ])
)

# 即使某个数据源失败，也能返回其他结果
```

---

## 场景3：并行数据处理

### 场景描述

批量处理数据时，需要对每条数据进行多维度分析。

**业务需求**：
- 对文本进行情感分析、主题提取、实体识别
- 批量处理 100 条数据
- 每条数据的三个分析任务并行执行
- 总耗时要求：< 60秒

---

### 完整代码

```python
"""
场景3：并行数据处理
演示：批量数据的多维度并行分析
"""

import asyncio
from typing import List, Dict, Any
from langchain_core.runnables import RunnableParallel, RunnableLambda

# ===== 1. 定义分析函数 =====
async def analyze_sentiment(text: str) -> str:
    """情感分析（模拟）"""
    await asyncio.sleep(0.5)
    # 简单规则：包含"好"、"棒"为积极
    if any(word in text for word in ["好", "棒", "优秀"]):
        return "positive"
    elif any(word in text for word in ["差", "烂", "糟糕"]):
        return "negative"
    else:
        return "neutral"

async def extract_topic(text: str) -> str:
    """主题提取（模拟）"""
    await asyncio.sleep(0.5)
    # 简单规则：提取关键词
    if "LangChain" in text:
        return "LangChain"
    elif "Python" in text:
        return "Python"
    else:
        return "General"

async def extract_entities(text: str) -> List[str]:
    """实体识别（模拟）"""
    await asyncio.sleep(0.5)
    # 简单规则：提取大写词
    words = text.split()
    entities = [w for w in words if w[0].isupper() and len(w) > 1]
    return entities

# ===== 2. 创建并行分析链 =====
parallel_analysis = RunnableParallel(
    sentiment=RunnableLambda(analyze_sentiment),
    topic=RunnableLambda(extract_topic),
    entities=RunnableLambda(extract_entities)
)

# ===== 3. 批量处理 =====
async def batch_process(texts: List[str], batch_size: int = 10) -> List[Dict[str, Any]]:
    """批量处理文本"""
    print(f"=== 批量处理 {len(texts)} 条数据 ===")

    results = []
    import time
    start = time.time()

    # 分批处理
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        print(f"\n处理批次 {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1}")

        # 并行处理当前批次
        batch_tasks = [
            parallel_analysis.ainvoke(text)
            for text in batch
        ]
        batch_results = await asyncio.gather(*batch_tasks)

        results.extend(batch_results)

    elapsed = time.time() - start
    print(f"\n✓ 批量处理完成，耗时：{elapsed:.2f}秒")
    print(f"  平均耗时：{elapsed / len(texts):.2f}秒/条")
    print(f"  吞吐量：{len(texts) / elapsed:.2f} 条/秒")

    return results

# ===== 4. 主函数 =====
async def main():
    """主函数"""
    # 测试数据
    texts = [
        "LangChain 是一个很棒的框架",
        "Python 是一门优秀的语言",
        "这个产品很差劲",
        "今天天气不错",
        "OpenAI 发布了新模型"
    ] * 20  # 100 条数据

    # 批量处理
    results = await batch_process(texts, batch_size=10)

    # 统计结果
    print("\n=== 结果统计 ===")
    sentiments = [r["sentiment"] for r in results]
    topics = [r["topic"] for r in results]

    from collections import Counter
    print(f"情感分布：{dict(Counter(sentiments))}")
    print(f"主题分布：{dict(Counter(topics))}")

# 运行
if __name__ == "__main__":
    asyncio.run(main())
```

---

### 运行结果

```
=== 批量处理 100 条数据 ===

处理批次 1/10

处理批次 2/10

...

处理批次 10/10

✓ 批量处理完成，耗时：5.12秒
  平均耗时：0.05秒/条
  吞吐量：19.53 条/秒

=== 结果统计 ===
情感分布：{'positive': 40, 'negative': 20, 'neutral': 40}
主题分布：{'LangChain': 20, 'Python': 20, 'General': 60}
```

**性能对比**：
- 串行处理：150秒（1.5s × 100）
- 并行处理：5秒（批处理 + 并行）
- **提升：30倍**

---

### 最佳实践

#### 1. 进度追踪

```python
from tqdm.asyncio import tqdm

async def batch_process_with_progress(texts: List[str], batch_size: int = 10):
    """带进度条的批量处理"""
    results = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Processing"):
        batch = texts[i:i + batch_size]
        batch_tasks = [parallel_analysis.ainvoke(text) for text in batch]
        batch_results = await asyncio.gather(*batch_tasks)
        results.extend(batch_results)

    return results
```

#### 2. 错误处理

```python
async def safe_batch_process(texts: List[str], batch_size: int = 10):
    """安全的批量处理（容错）"""
    results = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]

        # 使用 return_exceptions=True 容错
        batch_tasks = [parallel_analysis.ainvoke(text) for text in batch]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

        # 过滤异常
        for result in batch_results:
            if isinstance(result, Exception):
                print(f"错误: {result}")
                results.append(None)
            else:
                results.append(result)

    return results
```

---

## 学习检查清单

- [ ] 能够实现多模型并行调用
- [ ] 能够实现多数据源并行检索
- [ ] 能够实现批量数据并行处理
- [ ] 理解并行执行的性能优势
- [ ] 掌握容错和超时控制
- [ ] 能够添加进度追踪和监控

---

## 参考资料

[来源: Using RunnableParallel in LangChain LCEL for Concurrent LLM Workflows - https://www.jellyfishtechnologies.com/using-runnableparallel-langchain-lcel-concurrent-llm-workflows, 访问日期: 2026-02-18]

[来源: Building Production-Ready AI Pipelines with LangChain Runnables - https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557, 访问日期: 2026-02-18]

---

**版本**: v1.0
**最后更新**: 2026-02-18
**场景数量**: 3个基础场景
