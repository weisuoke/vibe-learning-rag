# 实战代码 - 场景 3：分布式部署

> **场景定位**：在分布式环境中部署和监控 Milvus 集群
> **难度等级**：进阶级
> **预计时间**：30 分钟

---

## 1. 场景描述

### 1.1 业务场景

RAG 系统需要处理大规模数据和高并发查询，单机部署无法满足需求：
- 数据量：1 亿+ 向量
- QPS：1000+ 查询/秒
- 可用性：99.9% SLA
- 扩展性：支持水平扩展

### 1.2 技术目标

1. 部署分布式 Milvus 集群
2. 配置多节点架构
3. 实现负载均衡
4. 监控集群状态
5. 实现故障转移

### 1.3 在 RAG 中的应用

- **高可用**：多节点部署，避免单点故障
- **高性能**：负载分散，提升查询吞吐量
- **可扩展**：按需增加节点，支持业务增长
- **成本优化**：资源按需分配，降低运营成本

---

## 2. 环境准备

### 2.1 系统要求

```bash
# 最小配置（3 节点集群）
- 3 台服务器
- 每台：8 核 CPU, 32GB 内存, 500GB SSD
- 网络：10 Gbps 内网带宽
```

### 2.2 安装 Docker 和 Docker Compose

```bash
# 在每台服务器上安装 Docker
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# 安装 Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

### 2.3 准备配置文件

```bash
# 下载 Milvus 分布式部署配置
wget https://github.com/milvus-io/milvus/releases/download/v2.3.0/milvus-cluster-docker-compose.yml
```

---

## 3. 核心代码实现

### 3.1 分布式部署配置

```yaml
# docker-compose-cluster.yml
version: '3.5'

services:
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd

  minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  pulsar:
    image: apachepulsar/pulsar:2.11.0
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/pulsar:/pulsar/data
    environment:
      - PULSAR_MEM=-Xms512m -Xmx512m -XX:MaxDirectMemorySize=1g
    command: bin/pulsar standalone

  rootcoord:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "rootcoord"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - etcd
      - minio
      - pulsar

  datacoord:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "datacoord"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - etcd
      - minio
      - pulsar
      - rootcoord

  querycoord:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "querycoord"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - etcd
      - minio
      - pulsar
      - rootcoord

  indexcoord:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "indexcoord"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - etcd
      - minio
      - pulsar
      - rootcoord

  proxy:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "proxy"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - etcd
      - minio
      - pulsar
      - rootcoord
      - querycoord
      - datacoord
      - indexcoord

  querynode1:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "querynode"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - querycoord

  querynode2:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "querynode"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - querycoord

  datanode1:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "datanode"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - datacoord

  datanode2:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "datanode"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - datacoord

  indexnode1:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "indexnode"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - indexcoord

  indexnode2:
    image: milvusdb/milvus:v2.3.0
    command: ["milvus", "run", "indexnode"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
      PULSAR_ADDRESS: pulsar://pulsar:6650
    depends_on:
      - indexcoord

networks:
  default:
    name: milvus
```

### 3.2 启动集群

```bash
# 启动集群
docker-compose -f docker-compose-cluster.yml up -d

# 查看服务状态
docker-compose -f docker-compose-cluster.yml ps

# 查看日志
docker-compose -f docker-compose-cluster.yml logs -f proxy
```

### 3.3 集群监控脚本

```python
"""
分布式集群监控脚本
"""

from pymilvus import connections, utility, Collection
from typing import Dict, List
import time
from datetime import datetime
from tabulate import tabulate

class DistributedClusterMonitor:
    """分布式集群监控器"""

    def __init__(self, host: str = "localhost", port: str = "19530"):
        self.host = host
        self.port = port
        self.alias = "default"

    def connect(self):
        """连接到集群"""
        connections.connect(alias=self.alias, host=self.host, port=self.port)
        print(f"Connected to Milvus cluster at {self.host}:{self.port}")

    def disconnect(self):
        """断开连接"""
        connections.disconnect(alias=self.alias)

    def get_cluster_info(self) -> Dict:
        """获取集群基本信息"""
        return {
            "version": utility.get_server_version(),
            "collections": len(utility.list_collections())
        }

    def get_node_distribution(self, collection_name: str) -> Dict:
        """获取节点分布信息"""
        segments = utility.get_query_segment_info(collection_name)

        node_stats = {}
        for seg in segments:
            node_id = seg.nodeID
            if node_id not in node_stats:
                node_stats[node_id] = {
                    "segment_count": 0,
                    "total_rows": 0,
                    "states": {}
                }

            node_stats[node_id]["segment_count"] += 1
            node_stats[node_id]["total_rows"] += seg.num_rows

            state = seg.state
            if state not in node_stats[node_id]["states"]:
                node_stats[node_id]["states"][state] = 0
            node_stats[node_id]["states"][state] += 1

        return node_stats

    def check_load_balance(self, collection_name: str) -> Dict:
        """检查负载均衡状态"""
        node_stats = self.get_node_distribution(collection_name)

        if not node_stats:
            return {"balanced": True, "variance": 0}

        row_counts = [stats["total_rows"] for stats in node_stats.values()]
        avg_rows = sum(row_counts) / len(row_counts)
        variance = sum((x - avg_rows) ** 2 for x in row_counts) / len(row_counts)
        std_dev = variance ** 0.5

        # 如果标准差小于平均值的 20%，认为负载均衡
        balanced = std_dev < avg_rows * 0.2

        return {
            "balanced": balanced,
            "variance": variance,
            "std_dev": std_dev,
            "avg_rows": avg_rows
        }

    def print_cluster_status(self, collection_name: str):
        """打印集群状态"""
        print("\n" + "="*70)
        print(f"Distributed Cluster Status - {datetime.now()}")
        print("="*70)

        # 集群信息
        cluster_info = self.get_cluster_info()
        print(f"\n[Cluster Info]")
        print(f"  Version: {cluster_info['version']}")
        print(f"  Collections: {cluster_info['collections']}")

        # 节点分布
        node_stats = self.get_node_distribution(collection_name)
        print(f"\n[Node Distribution] ({len(node_stats)} nodes)")

        table_data = []
        for node_id, stats in node_stats.items():
            table_data.append([
                node_id,
                stats["segment_count"],
                f"{stats['total_rows']:,}",
                ", ".join(f"{k}:{v}" for k, v in stats["states"].items())
            ])

        print(tabulate(
            table_data,
            headers=["Node ID", "Segments", "Rows", "States"],
            tablefmt="grid"
        ))

        # 负载均衡状态
        balance_info = self.check_load_balance(collection_name)
        print(f"\n[Load Balance]")
        print(f"  Status: {'Balanced' if balance_info['balanced'] else 'Unbalanced'}")
        print(f"  Avg Rows/Node: {balance_info['avg_rows']:,.0f}")
        print(f"  Std Dev: {balance_info['std_dev']:,.0f}")

        print("\n" + "="*70)

    def continuous_monitoring(self, collection_name: str, interval: int = 60):
        """持续监控"""
        print(f"Starting continuous monitoring (interval: {interval}s)")
        print("Press Ctrl+C to stop")

        try:
            while True:
                self.print_cluster_status(collection_name)
                time.sleep(interval)
        except KeyboardInterrupt:
            print("\nMonitoring stopped")


def main():
    """主函数"""
    monitor = DistributedClusterMonitor()

    try:
        monitor.connect()
        monitor.print_cluster_status("your_collection_name")
        # monitor.continuous_monitoring("your_collection_name", interval=60)
    finally:
        monitor.disconnect()


if __name__ == "__main__":
    main()
```

### 3.4 负载测试脚本

```python
"""
分布式集群负载测试
"""

from pymilvus import connections, Collection
import random
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict

class LoadTester:
    """负载测试器"""

    def __init__(self, host: str = "localhost", port: str = "19530"):
        self.host = host
        self.port = port

    def connect(self):
        """连接到集群"""
        connections.connect(host=self.host, port=self.port)

    def disconnect(self):
        """断开连接"""
        connections.disconnect()

    def generate_vectors(self, dim: int, count: int) -> List[List[float]]:
        """生成随机向量"""
        return [[random.random() for _ in range(dim)] for _ in range(count)]

    def insert_batch(self, collection_name: str, batch_size: int, dim: int) -> Dict:
        """插入一批数据"""
        collection = Collection(collection_name)

        # 生成数据
        start_id = int(time.time() * 1000000)
        ids = list(range(start_id, start_id + batch_size))
        vectors = self.generate_vectors(dim, batch_size)

        # 插入
        start = time.time()
        collection.insert([ids, vectors])
        duration = time.time() - start

        return {
            "batch_size": batch_size,
            "duration": duration,
            "throughput": batch_size / duration
        }

    def query_batch(self, collection_name: str, query_count: int, dim: int) -> Dict:
        """执行一批查询"""
        collection = Collection(collection_name)
        collection.load()

        # 生成查询向量
        query_vectors = self.generate_vectors(dim, query_count)

        # 查询
        start = time.time()
        for vector in query_vectors:
            collection.search(
                data=[vector],
                anns_field="embedding",
                param={"metric_type": "L2", "params": {"nprobe": 10}},
                limit=10
            )
        duration = time.time() - start

        return {
            "query_count": query_count,
            "duration": duration,
            "qps": query_count / duration
        }

    def concurrent_insert_test(self, collection_name: str,
                               total_batches: int, batch_size: int,
                               dim: int, workers: int = 10) -> Dict:
        """并发插入测试"""
        print(f"\nConcurrent Insert Test:")
        print(f"  Total batches: {total_batches}")
        print(f"  Batch size: {batch_size}")
        print(f"  Workers: {workers}")

        start = time.time()
        results = []

        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [
                executor.submit(self.insert_batch, collection_name, batch_size, dim)
                for _ in range(total_batches)
            ]

            for future in as_completed(futures):
                results.append(future.result())

        total_duration = time.time() - start
        total_rows = total_batches * batch_size
        avg_throughput = total_rows / total_duration

        print(f"\nResults:")
        print(f"  Total rows: {total_rows:,}")
        print(f"  Total duration: {total_duration:.2f}s")
        print(f"  Avg throughput: {avg_throughput:.0f} rows/s")

        return {
            "total_rows": total_rows,
            "total_duration": total_duration,
            "avg_throughput": avg_throughput
        }

    def concurrent_query_test(self, collection_name: str,
                             total_queries: int, dim: int,
                             workers: int = 10) -> Dict:
        """并发查询测试"""
        print(f"\nConcurrent Query Test:")
        print(f"  Total queries: {total_queries}")
        print(f"  Workers: {workers}")

        queries_per_worker = total_queries // workers

        start = time.time()
        results = []

        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [
                executor.submit(self.query_batch, collection_name, queries_per_worker, dim)
                for _ in range(workers)
            ]

            for future in as_completed(futures):
                results.append(future.result())

        total_duration = time.time() - start
        avg_qps = total_queries / total_duration

        print(f"\nResults:")
        print(f"  Total queries: {total_queries:,}")
        print(f"  Total duration: {total_duration:.2f}s")
        print(f"  Avg QPS: {avg_qps:.0f}")

        return {
            "total_queries": total_queries,
            "total_duration": total_duration,
            "avg_qps": avg_qps
        }


def main_load_test():
    """负载测试主函数"""
    tester = LoadTester()

    try:
        tester.connect()

        # 插入测试
        tester.concurrent_insert_test(
            collection_name="your_collection_name",
            total_batches=100,
            batch_size=1000,
            dim=128,
            workers=10
        )

        # 查询测试
        tester.concurrent_query_test(
            collection_name="your_collection_name",
            total_queries=1000,
            dim=128,
            workers=10
        )

    finally:
        tester.disconnect()


if __name__ == "__main__":
    main_load_test()
```

---

## 4. 在 RAG 中的应用

### 4.1 高可用架构

```python
def setup_high_availability():
    """配置高可用架构"""

    # 多 Proxy 负载均衡
    proxy_endpoints = [
        "proxy1.example.com:19530",
        "proxy2.example.com:19530",
        "proxy3.example.com:19530"
    ]

    # 使用负载均衡器（如 Nginx）
    nginx_config = """
    upstream milvus_cluster {
        server proxy1.example.com:19530;
        server proxy2.example.com:19530;
        server proxy3.example.com:19530;
    }

    server {
        listen 19530;
        proxy_pass milvus_cluster;
    }
    """

    print("High availability configured with 3 Proxy nodes")
```

### 4.2 水平扩展

```python
def scale_cluster(component: str, replicas: int):
    """水平扩展集群"""

    if component == "querynode":
        print(f"Scaling QueryNode to {replicas} replicas")
        # docker-compose scale querynode={replicas}

    elif component == "datanode":
        print(f"Scaling DataNode to {replicas} replicas")
        # docker-compose scale datanode={replicas}

    elif component == "indexnode":
        print(f"Scaling IndexNode to {replicas} replicas")
        # docker-compose scale indexnode={replicas}

    print(f"Cluster scaled successfully")
```

### 4.3 故障转移

```python
def handle_node_failure(failed_node_id: int, collection_name: str):
    """处理节点故障"""

    print(f"Detected failure on node {failed_node_id}")

    # 1. 检查受影响的 Segment
    segments = utility.get_query_segment_info(collection_name)
    affected_segments = [seg for seg in segments if seg.nodeID == failed_node_id]

    print(f"Affected segments: {len(affected_segments)}")

    # 2. 触发重新分配
    collection = Collection(collection_name)
    collection.release()
    collection.load()

    print("Segments redistributed to healthy nodes")
```

---

## 5. 最佳实践

### 5.1 资源规划

```python
# 根据数据量和 QPS 规划资源
def calculate_resources(vector_count: int, qps: int) -> Dict:
    """计算所需资源"""

    # QueryNode 规划（每个节点处理 1000 万向量）
    querynode_count = max(2, (vector_count // 10000000) + 1)

    # DataNode 规划（每个节点处理 500 QPS 写入）
    datanode_count = max(2, (qps // 500) + 1)

    # IndexNode 规划（每个节点构建 1000 万向量索引）
    indexnode_count = max(2, (vector_count // 10000000) + 1)

    return {
        "querynode": querynode_count,
        "datanode": datanode_count,
        "indexnode": indexnode_count
    }

# 示例
resources = calculate_resources(vector_count=50000000, qps=1000)
print(f"Recommended resources: {resources}")
```

### 5.2 监控告警

```python
# 设置监控告警阈值
ALERT_THRESHOLDS = {
    "node_down": 1,              # 节点宕机数量
    "load_imbalance": 0.3,       # 负载不均衡阈值（30%）
    "query_latency_p99": 500,    # P99 查询延迟（ms）
    "insert_throughput": 5000    # 最小插入吞吐量（rows/s）
}

def check_cluster_health(monitor: DistributedClusterMonitor,
                        collection_name: str) -> List[str]:
    """检查集群健康状态"""
    alerts = []

    # 检查负载均衡
    balance_info = monitor.check_load_balance(collection_name)
    if not balance_info["balanced"]:
        alerts.append(f"Load imbalance detected: std_dev={balance_info['std_dev']:.0f}")

    return alerts
```

---

## 6. 总结

### 6.1 核心要点

1. **分布式架构**：多节点部署，提升性能和可用性
2. **负载均衡**：数据和查询均匀分布到各节点
3. **水平扩展**：按需增加节点，支持业务增长
4. **故障转移**：自动检测和恢复节点故障

### 6.2 部署检查清单

- [ ] 配置文件正确
- [ ] 所有服务启动成功
- [ ] 节点间网络连通
- [ ] 负载均衡正常
- [ ] 监控告警配置完成

### 6.3 下一步

- **场景 4**：RAG 架构集成 - 将 Milvus 集群集成到 RAG 系统

---

**完整代码**：`examples/milvus_distributed_deployment.py`
