# 实战代码：检索质量评估

> 手写实现检索评估指标，评估 RAG 系统的检索质量

---

## 环境准备

```bash
# 纯 Python 实现，无需额外安装
# 仅使用标准库 math，无需 numpy 或其他第三方库
python eval_retrieval.py
```

---

## 完整代码

```python
"""
检索质量评估实战
演示：手写实现 Precision@K, Recall@K, MRR, NDCG, Hit Rate
并对模拟的 RAG 检索结果进行批量评估
"""

import math
from typing import List, Set, Dict


# ===== 1. 评估指标实现 =====
print("=" * 60)
print("1. 检索评估指标实现")
print("=" * 60)


def precision_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
    """
    Precision@K：前 K 个结果中相关文档的比例
    衡量："找到的文档里，有多少是真正有用的？"

    公式：Precision@K = |retrieved[:K] ∩ relevant| / K
    """
    if k <= 0:
        return 0.0
    retrieved_k = set(retrieved[:k])
    return len(retrieved_k & relevant) / k


def recall_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
    """
    Recall@K：前 K 个结果覆盖了多少相关文档
    衡量："所有有用的文档，找到了多少？"

    公式：Recall@K = |retrieved[:K] ∩ relevant| / |relevant|
    """
    if not relevant:
        return 0.0
    retrieved_k = set(retrieved[:k])
    return len(retrieved_k & relevant) / len(relevant)


def reciprocal_rank(retrieved: List[str], relevant: Set[str]) -> float:
    """
    Reciprocal Rank（倒数排名）：第一个相关结果排名的倒数
    衡量："最好的结果排在第几？"

    公式：RR = 1 / rank_of_first_relevant_doc
    """
    for i, doc in enumerate(retrieved):
        if doc in relevant:
            return 1.0 / (i + 1)
    return 0.0


def ndcg_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
    """
    NDCG@K：归一化折损累积增益
    衡量："相关文档的排序质量如何？越靠前越好"

    公式：
      DCG@K  = Σ rel_i / log2(i + 2)    (i 从 0 开始)
      IDCG@K = 理想排序下的 DCG@K
      NDCG@K = DCG@K / IDCG@K
    """
    # 计算 DCG：实际排序的累积增益
    dcg = 0.0
    for i in range(min(k, len(retrieved))):
        rel = 1.0 if retrieved[i] in relevant else 0.0
        dcg += rel / math.log2(i + 2)  # i+2 因为 log2(1)=0

    # 计算 IDCG：理想排序（所有相关文档排在最前面）
    ideal_count = min(k, len(relevant))
    idcg = sum(1.0 / math.log2(i + 2) for i in range(ideal_count))

    return dcg / idcg if idcg > 0 else 0.0


def hit_rate(retrieved: List[str], relevant: Set[str], k: int) -> int:
    """
    Hit Rate（命中率）：前 K 个结果中是否至少有一个相关文档
    衡量："有没有命中？"

    返回：1（命中）或 0（未命中）
    """
    return 1 if set(retrieved[:k]) & relevant else 0


# ===== 2. 单个查询评估示例 =====
print("\n" + "=" * 60)
print("2. 单个查询评估示例")
print("=" * 60)

# 模拟一次检索：返回了 5 个文档，其中 doc_1, doc_3, doc_2 是相关的
retrieved_docs = ["doc_1", "doc_5", "doc_3", "doc_8", "doc_2"]
relevant_docs = {"doc_1", "doc_2", "doc_3", "doc_4"}

print(f"\n检索结果: {retrieved_docs}")
print(f"相关文档: {relevant_docs}")
print()

# 分别计算 K=1, 3, 5 时的指标
for k in [1, 3, 5]:
    p = precision_at_k(retrieved_docs, relevant_docs, k)
    r = recall_at_k(retrieved_docs, relevant_docs, k)
    n = ndcg_at_k(retrieved_docs, relevant_docs, k)
    h = hit_rate(retrieved_docs, relevant_docs, k)
    print(f"K={k}: Precision={p:.3f}  Recall={r:.3f}  NDCG={n:.3f}  Hit={h}")

rr = reciprocal_rank(retrieved_docs, relevant_docs)
print(f"\nReciprocal Rank = {rr:.3f}")
print("（第一个相关文档 doc_1 排在第 1 位，所以 RR = 1/1 = 1.000）")


# ===== 3. 批量评估多个查询（模拟 RAG 场景） =====
print("\n" + "=" * 60)
print("3. 批量评估多个查询（模拟 RAG 场景）")
print("=" * 60)

# 模拟 4 个用户查询的检索结果
queries_data = [
    {
        "query": "什么是向量数据库？",
        "retrieved": ["vec_db_intro", "sql_basics", "vec_db_compare",
                      "nosql_guide", "vec_db_usage"],
        "relevant": {"vec_db_intro", "vec_db_compare",
                     "vec_db_usage", "vec_db_perf"},
    },
    {
        "query": "Python 异步编程怎么用？",
        "retrieved": ["asyncio_tutorial", "threading_guide", "await_syntax",
                      "multiprocess", "async_patterns"],
        "relevant": {"asyncio_tutorial", "await_syntax", "async_patterns"},
    },
    {
        "query": "如何部署 Docker 容器？",
        "retrieved": ["k8s_intro", "docker_basics", "docker_compose",
                      "vm_setup", "docker_deploy"],
        "relevant": {"docker_basics", "docker_compose", "docker_deploy"},
    },
    {
        "query": "机器学习入门推荐？",
        "retrieved": ["ml_roadmap", "dl_course", "ml_books",
                      "data_science", "ml_projects"],
        "relevant": {"ml_roadmap", "ml_books", "ml_projects", "ml_basics"},
    },
]

# 批量计算所有指标
k = 5
all_precision = []
all_recall = []
all_rr = []
all_ndcg = []
all_hits = []

header = f"{'查询':<20} {'P@5':<10} {'R@5':<10} {'RR':<10} {'NDCG@5':<10} {'Hit':<5}"
print(f"\n{header}")
print("-" * 65)

for data in queries_data:
    p = precision_at_k(data["retrieved"], data["relevant"], k)
    r = recall_at_k(data["retrieved"], data["relevant"], k)
    rr_val = reciprocal_rank(data["retrieved"], data["relevant"])
    n = ndcg_at_k(data["retrieved"], data["relevant"], k)
    h = hit_rate(data["retrieved"], data["relevant"], k)

    all_precision.append(p)
    all_recall.append(r)
    all_rr.append(rr_val)
    all_ndcg.append(n)
    all_hits.append(h)

    # 截断过长的查询名
    q = data["query"]
    q_display = q[:17] + "..." if len(q) > 17 else q
    print(f"{q_display:<20} {p:<10.3f} {r:<10.3f} {rr_val:<10.3f} {n:<10.3f} {h:<5}")

# 汇总统计：平均值
print("-" * 65)
avg_p = sum(all_precision) / len(all_precision)
avg_r = sum(all_recall) / len(all_recall)
mrr = sum(all_rr) / len(all_rr)
avg_n = sum(all_ndcg) / len(all_ndcg)
avg_h = sum(all_hits) / len(all_hits)
print(f"{'平均值':<20} {avg_p:<10.3f} {avg_r:<10.3f} {mrr:<10.3f} {avg_n:<10.3f} {avg_h:<5.3f}")


# ===== 4. F1@K 指标（Precision 和 Recall 的调和平均） =====
print("\n" + "=" * 60)
print("4. F1@K 指标")
print("=" * 60)


def f1_at_k(retrieved: List[str], relevant: Set[str], k: int) -> float:
    """
    F1@K：Precision@K 和 Recall@K 的调和平均
    衡量："精确率和召回率的综合表现"
    """
    p = precision_at_k(retrieved, relevant, k)
    r = recall_at_k(retrieved, relevant, k)
    if p + r == 0:
        return 0.0
    return 2 * p * r / (p + r)


for data in queries_data:
    f1 = f1_at_k(data["retrieved"], data["relevant"], k)
    q = data["query"]
    q_display = q[:17] + "..." if len(q) > 17 else q
    print(f"{q_display:<20} F1@{k} = {f1:.3f}")


# ===== 5. 诊断与优化建议 =====
print("\n" + "=" * 60)
print("5. 诊断与优化建议")
print("=" * 60)


def diagnose(avg_p: float, avg_r: float, mrr: float,
             avg_n: float, avg_h: float, k: int) -> None:
    """根据评估指标输出诊断建议"""
    print(f"\n--- 检索质量诊断报告 ---")

    # Precision 诊断
    status = "OK" if avg_p >= 0.6 else "偏低"
    advice = "" if avg_p >= 0.6 else " -> 建议：添加 ReRank 重排序，过滤无关文档"
    print(f"  Precision@{k}: {avg_p:.3f} [{status}]{advice}")

    # Recall 诊断
    status = "OK" if avg_r >= 0.7 else "偏低"
    advice = "" if avg_r >= 0.7 else " -> 建议：增大 K 值，或优化 Embedding 模型"
    print(f"  Recall@{k}:    {avg_r:.3f} [{status}]{advice}")

    # MRR 诊断
    status = "OK" if mrr >= 0.5 else "偏低"
    advice = "" if mrr >= 0.5 else " -> 建议：优化排序策略，确保最相关文档排在前面"
    print(f"  MRR:           {mrr:.3f} [{status}]{advice}")

    # NDCG 诊断
    status = "OK" if avg_n >= 0.6 else "偏低"
    advice = "" if avg_n >= 0.6 else " -> 建议：引入 ReRank 或调整相似度算法"
    print(f"  NDCG@{k}:      {avg_n:.3f} [{status}]{advice}")

    # Hit Rate 诊断
    status = "OK" if avg_h >= 0.8 else "偏低"
    advice = "" if avg_h >= 0.8 else " -> 建议：检查 Embedding 质量，部分查询完全未命中"
    print(f"  Hit Rate@{k}:  {avg_h:.3f} [{status}]{advice}")

    print()


diagnose(avg_p, avg_r, mrr, avg_n, avg_h, k)


# ===== 6. 对比实验：不同 K 值的影响 =====
print("=" * 60)
print("6. 对比实验：不同 K 值的影响")
print("=" * 60)

print(f"\n{'K':<5} {'Avg P@K':<12} {'Avg R@K':<12} {'Avg NDCG@K':<12}")
print("-" * 41)

for test_k in [1, 3, 5, 10]:
    p_list = []
    r_list = []
    n_list = []
    for data in queries_data:
        # 如果 K 超过检索结果数量，用最大长度
        p_list.append(precision_at_k(data["retrieved"], data["relevant"], test_k))
        r_list.append(recall_at_k(data["retrieved"], data["relevant"], test_k))
        n_list.append(ndcg_at_k(data["retrieved"], data["relevant"], test_k))

    avg_pk = sum(p_list) / len(p_list)
    avg_rk = sum(r_list) / len(r_list)
    avg_nk = sum(n_list) / len(n_list)
    print(f"{test_k:<5} {avg_pk:<12.3f} {avg_rk:<12.3f} {avg_nk:<12.3f}")

print("\n结论：K 越大，Recall 越高但 Precision 越低——需要根据场景权衡。")
print("  - 问答场景：K=3~5，优先 Precision（答案要准）")
print("  - 综述场景：K=10~20，优先 Recall（信息要全）")
```

---

## 运行输出示例

```
============================================================
1. 检索评估指标实现
============================================================

============================================================
2. 单个查询评估示例
============================================================

检索结果: ['doc_1', 'doc_5', 'doc_3', 'doc_8', 'doc_2']
相关文档: {'doc_1', 'doc_2', 'doc_3', 'doc_4'}

K=1: Precision=1.000  Recall=0.250  NDCG=1.000  Hit=1
K=3: Precision=0.667  Recall=0.500  NDCG=0.773  Hit=1
K=5: Precision=0.600  Recall=0.750  NDCG=0.741  Hit=1

Reciprocal Rank = 1.000
（第一个相关文档 doc_1 排在第 1 位，所以 RR = 1/1 = 1.000）

============================================================
3. 批量评估多个查询（模拟 RAG 场景）
============================================================

查询                  P@5        R@5        RR         NDCG@5     Hit
-----------------------------------------------------------------
什么是向量数据库？...  0.600      0.750      1.000      0.741      1
Python 异步编程...     0.600      1.000      1.000      0.831      1
如何部署 Docker...     0.600      1.000      0.500      0.717      1
机器学习入门推荐？...  0.600      0.750      1.000      0.741      1
-----------------------------------------------------------------
平均值                0.600      0.875      0.875      0.757      1.000

============================================================
4. F1@K 指标
============================================================
什么是向量数据库？...  F1@5 = 0.667
Python 异步编程...     F1@5 = 0.750
如何部署 Docker...     F1@5 = 0.750
机器学习入门推荐？...  F1@5 = 0.667

============================================================
5. 诊断与优化建议
============================================================

--- 检索质量诊断报告 ---
  Precision@5: 0.600 [OK]
  Recall@5:    0.875 [OK]
  MRR:         0.875 [OK]
  NDCG@5:      0.757 [OK]
  Hit Rate@5:  1.000 [OK]

============================================================
6. 对比实验：不同 K 值的影响
============================================================

K     Avg P@K      Avg R@K      Avg NDCG@K
-----------------------------------------
1     0.750        0.214        0.750
3     0.500        0.548        0.651
5     0.600        0.875        0.757
10    0.600        0.875        0.757

结论：K 越大，Recall 越高但 Precision 越低——需要根据场景权衡。
  - 问答场景：K=3~5，优先 Precision（答案要准）
  - 综述场景：K=10~20，优先 Recall（信息要全）
```

---

## 一句话记住

**检索评估不需要复杂框架——5 个函数就能全面诊断检索质量，关键是构建好评估数据集（query + relevant_docs + retrieved_docs）。**
