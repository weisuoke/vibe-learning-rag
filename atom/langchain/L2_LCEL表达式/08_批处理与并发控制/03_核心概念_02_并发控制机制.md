# 核心概念_02_并发控制机制

> 深入理解 max_concurrency 参数和线程池管理机制

---

## 并发控制概述

并发控制是批处理系统的核心机制，用于限制同时执行的任务数量，避免资源耗尽和 API 限流。

### 核心参数

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    max_concurrency=10,  # 最大并发数
)

results = chain.batch(inputs, config=config)
```

---

## max_concurrency 工作原理

### 1. ThreadPoolExecutor 实现

```python
from concurrent.futures import ThreadPoolExecutor

class BatchProcessor:
    def batch(self, inputs, max_concurrency=10):
        """使用 ThreadPoolExecutor 实现并发控制"""
        # 创建线程池，max_workers 即为 max_concurrency
        with ThreadPoolExecutor(max_workers=max_concurrency) as executor:
            # 提交所有任务
            futures = [
                executor.submit(self.process, inp)
                for inp in inputs
            ]

            # 收集结果
            results = [f.result() for f in futures]

        return results
```

**关键机制**：
- ThreadPoolExecutor 维护一个固定大小的线程池
- max_workers 参数决定线程池大小
- 任务队列：超过 max_workers 的任务会排队等待

**参考来源**：[LangChain Runnables 文档](https://reference.langchain.com/python/langchain_core/runnables/)

---

### 2. 任务调度流程

```
输入任务队列: [Task1, Task2, ..., Task100]
                    ↓
            ThreadPoolExecutor
         (max_workers=10)
                    ↓
    ┌─────────────────────────────┐
    │  Thread 1: Task1  → Task11  │
    │  Thread 2: Task2  → Task12  │
    │  Thread 3: Task3  → Task13  │
    │  ...                        │
    │  Thread 10: Task10 → Task20 │
    └─────────────────────────────┘
                    ↓
            结果收集器
                    ↓
    [Result1, Result2, ..., Result100]
```

**执行流程**：
1. 创建 10 个工作线程
2. 前 10 个任务立即分配给线程
3. 线程完成任务后，从队列取下一个任务
4. 重复直到所有任务完成

---

### 3. 并发数与性能关系

```python
import time
import matplotlib.pyplot as plt

def benchmark_concurrency(chain, inputs, max_range=50):
    """测试不同并发数的性能"""
    results = {}

    for conc in range(1, max_range + 1, 5):
        start = time.time()
        chain.batch(inputs[:50], config={"max_concurrency": conc})
        duration = time.time() - start
        results[conc] = duration

    # 绘制性能曲线
    plt.plot(results.keys(), results.values())
    plt.xlabel("并发数")
    plt.ylabel("执行时间（秒）")
    plt.title("并发数与性能关系")
    plt.show()

    return results
```

**典型性能曲线**：
```
执行时间
  ^
  |  \
  |   \___
  |       \____
  |            \______
  |                   \_________
  +--------------------------------> 并发数
  1   5   10   20   30   40   50
```

**关键观察**：
- 1-10：性能快速提升
- 10-20：性能继续提升但速度放缓
- 20+：性能提升不明显，甚至下降

**参考来源**：[LangGraph 并行节点最佳实践](https://forum.langchain.com/t/best-practices-for-parallel-nodes-fanouts/1900)

---

## 并发限制的三个维度

### 1. API 速率限制

**OpenAI 速率限制（2025-2026）**：

| Tier | RPM | TPM | 推荐 max_concurrency |
|------|-----|-----|---------------------|
| Free | 3 | 40K | 1 |
| Tier 1 | 500 | 200K | 5-10 |
| Tier 2 | 5000 | 2M | 10-50 |
| Tier 3+ | 10000+ | 10M+ | 50-100 |

**计算公式**：
```python
# 根据 RPM 计算最大并发数
rpm = 500  # 每分钟请求数
avg_request_time = 2  # 平均请求时间（秒）

max_concurrency = (rpm / 60) * avg_request_time
# 500 / 60 * 2 = 16.7 ≈ 16
```

**实际建议**：
- 保守估计：max_concurrency = RPM / 60
- OpenAI Tier 1：max_concurrency = 5-10
- OpenAI Tier 2+：max_concurrency = 10-50

---

### 2. 系统资源限制

**内存限制**：
```python
# 每个线程的内存占用
thread_memory = 1-8  # MB（取决于任务）
available_memory = 2000  # MB

max_concurrency = available_memory / thread_memory
# 2000 / 8 = 250（理论上限）
```

**文件描述符限制**：
```python
import resource

# 查看系统限制
soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
print(f"文件描述符限制: {soft} (soft), {hard} (hard)")

# 每个 HTTP 连接占用 1 个文件描述符
max_concurrency = soft // 2  # 保守估计
```

**CPU 限制**：
```python
import os

cpu_count = os.cpu_count()
# 对于 IO 密集型任务，可以超过 CPU 核心数
max_concurrency = cpu_count * 2  # 经验值
```

---

### 3. 成本限制

**成本控制**：
```python
class CostAwareConcurrency:
    def __init__(self, budget_per_hour=10.0):
        self.budget_per_hour = budget_per_hour
        self.cost_per_request = 0.01  # 假设每次请求 $0.01

    def calculate_max_concurrency(self):
        """根据预算计算最大并发数"""
        # 每小时最多请求数
        max_requests_per_hour = self.budget_per_hour / self.cost_per_request

        # 假设每个请求 2 秒
        avg_request_time = 2

        # 最大并发数
        max_concurrency = (max_requests_per_hour / 3600) * avg_request_time

        return int(max_concurrency)

# 示例
controller = CostAwareConcurrency(budget_per_hour=10.0)
max_conc = controller.calculate_max_concurrency()
print(f"推荐并发数: {max_conc}")
```

---

## 动态并发控制

### 1. 自适应并发调整

```python
from collections import deque
import time

class AdaptiveConcurrencyController:
    def __init__(self, initial_concurrency=10):
        self.concurrency = initial_concurrency
        self.min_concurrency = 5
        self.max_concurrency = 50

        # 记录最近的成功率和延迟
        self.success_rate = deque(maxlen=100)
        self.latencies = deque(maxlen=100)

    def adjust(self):
        """根据成功率和延迟动态调整并发数"""
        if len(self.success_rate) < 10:
            return

        recent_success = sum(self.success_rate) / len(self.success_rate)
        avg_latency = sum(self.latencies) / len(self.latencies)

        # 成功率高且延迟低：增加并发
        if recent_success > 0.95 and avg_latency < 3.0:
            self.concurrency = min(
                self.concurrency + 2,
                self.max_concurrency
            )

        # 成功率低或延迟高：降低并发
        elif recent_success < 0.80 or avg_latency > 5.0:
            self.concurrency = max(
                self.concurrency - 2,
                self.min_concurrency
            )

    def record_result(self, success, latency):
        """记录执行结果"""
        self.success_rate.append(1 if success else 0)
        self.latencies.append(latency)
        self.adjust()

    def get_concurrency(self):
        """获取当前推荐的并发数"""
        return self.concurrency
```

**使用示例**：
```python
controller = AdaptiveConcurrencyController(initial_concurrency=10)

for batch in batches:
    start = time.time()
    try:
        results = chain.batch(
            batch,
            config={"max_concurrency": controller.get_concurrency()}
        )
        latency = time.time() - start
        controller.record_result(success=True, latency=latency)
    except Exception as e:
        latency = time.time() - start
        controller.record_result(success=False, latency=latency)
```

**参考来源**：[LangChain 最佳实践](https://www.swarnendu.de/blog/langchain-best-practices/)

---

### 2. 基于令牌桶的限流

```python
import time
import threading

class TokenBucket:
    def __init__(self, rate, capacity):
        """
        rate: 每秒生成的令牌数
        capacity: 桶的容量
        """
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()
        self.lock = threading.Lock()

    def consume(self, tokens=1):
        """消费令牌"""
        with self.lock:
            now = time.time()
            # 补充令牌
            elapsed = now - self.last_update
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now

            # 检查是否有足够的令牌
            if self.tokens >= tokens:
                self.tokens -= tokens
                return True
            return False

    def wait_for_token(self, tokens=1):
        """等待直到有足够的令牌"""
        while not self.consume(tokens):
            time.sleep(0.1)

# 使用示例
bucket = TokenBucket(rate=10, capacity=50)  # 每秒10个请求

def rate_limited_batch(chain, inputs):
    """限流的批处理"""
    results = []
    for inp in inputs:
        bucket.wait_for_token()  # 等待令牌
        result = chain.invoke(inp)
        results.append(result)
    return results
```

---

## 异步并发控制

### 1. asyncio.Semaphore

```python
import asyncio

class AsyncConcurrencyController:
    def __init__(self, max_concurrency=10):
        self.semaphore = asyncio.Semaphore(max_concurrency)

    async def process_with_limit(self, chain, inp):
        """带并发限制的异步处理"""
        async with self.semaphore:
            return await chain.ainvoke(inp)

    async def batch(self, chain, inputs):
        """异步批处理"""
        tasks = [
            self.process_with_limit(chain, inp)
            for inp in inputs
        ]
        return await asyncio.gather(*tasks)

# 使用示例
async def main():
    controller = AsyncConcurrencyController(max_concurrency=10)
    results = await controller.batch(chain, inputs)

asyncio.run(main())
```

**Semaphore vs ThreadPoolExecutor**：

| 特性 | Semaphore | ThreadPoolExecutor |
|------|-----------|-------------------|
| 并发模型 | 协程 | 线程 |
| 内存占用 | 低（KB级） | 高（MB级） |
| 切换开销 | 极低 | 较高 |
| 适用场景 | 高并发 | 中低并发 |
| 最大并发数 | 10000+ | 100-500 |

---

### 2. 混合并发控制

```python
class HybridConcurrencyController:
    def __init__(self, max_concurrency=10):
        self.max_concurrency = max_concurrency

    def batch_sync(self, chain, inputs):
        """同步批处理（ThreadPoolExecutor）"""
        return chain.batch(
            inputs,
            config={"max_concurrency": self.max_concurrency}
        )

    async def batch_async(self, chain, inputs):
        """异步批处理（Semaphore）"""
        semaphore = asyncio.Semaphore(self.max_concurrency)

        async def process_one(inp):
            async with semaphore:
                return await chain.ainvoke(inp)

        tasks = [process_one(inp) for inp in inputs]
        return await asyncio.gather(*tasks)

    def batch_auto(self, chain, inputs):
        """自动选择最优方式"""
        if len(inputs) < 50:
            # 低并发：使用同步
            return self.batch_sync(chain, inputs)
        else:
            # 高并发：使用异步
            return asyncio.run(self.batch_async(chain, inputs))
```

---

## 监控和调优

### 1. 并发监控

```python
import threading
import time
from collections import defaultdict

class ConcurrencyMonitor:
    def __init__(self):
        self.metrics = defaultdict(list)
        self.lock = threading.Lock()

    def record_start(self, task_id):
        """记录任务开始"""
        with self.lock:
            self.metrics["active_tasks"].append(task_id)
            self.metrics["start_times"][task_id] = time.time()

    def record_end(self, task_id):
        """记录任务结束"""
        with self.lock:
            self.metrics["active_tasks"].remove(task_id)
            start_time = self.metrics["start_times"].pop(task_id)
            duration = time.time() - start_time
            self.metrics["durations"].append(duration)

    def get_stats(self):
        """获取统计信息"""
        with self.lock:
            return {
                "active_tasks": len(self.metrics["active_tasks"]),
                "avg_duration": sum(self.metrics["durations"]) / len(self.metrics["durations"]) if self.metrics["durations"] else 0,
                "total_completed": len(self.metrics["durations"])
            }

# 使用示例
monitor = ConcurrencyMonitor()

def monitored_batch(chain, inputs, max_concurrency=10):
    """带监控的批处理"""
    with ThreadPoolExecutor(max_workers=max_concurrency) as executor:
        futures = {}
        for i, inp in enumerate(inputs):
            monitor.record_start(i)
            future = executor.submit(chain.invoke, inp)
            futures[future] = i

        results = []
        for future in futures:
            task_id = futures[future]
            result = future.result()
            monitor.record_end(task_id)
            results.append(result)

        # 打印统计信息
        stats = monitor.get_stats()
        print(f"统计: {stats}")

        return results
```

---

### 2. 性能调优建议

**调优流程**：
```python
def tune_concurrency(chain, inputs):
    """自动调优并发数"""
    best_concurrency = 10
    best_time = float('inf')

    # 测试不同的并发数
    for conc in [1, 5, 10, 20, 30, 40, 50]:
        start = time.time()
        try:
            chain.batch(
                inputs[:20],  # 使用样本数据
                config={"max_concurrency": conc}
            )
            duration = time.time() - start

            if duration < best_time:
                best_time = duration
                best_concurrency = conc

            print(f"并发数 {conc}: {duration:.2f}秒")
        except Exception as e:
            print(f"并发数 {conc} 失败: {e}")
            break

    print(f"\n推荐并发数: {best_concurrency}")
    return best_concurrency
```

**调优建议**：
1. 从小到大测试并发数
2. 监控成功率和延迟
3. 找到性能拐点
4. 留有安全余量（-20%）

**参考来源**：[LangGraph 并行节点最佳实践](https://forum.langchain.com/t/best-practices-for-parallel-nodes-fanouts/1900)

---

## 2025-2026 最佳实践

### 1. 根据 API Tier 设置并发数

```python
class TierBasedConcurrency:
    TIER_CONFIGS = {
        "free": {"max_concurrency": 1, "rpm": 3},
        "tier1": {"max_concurrency": 8, "rpm": 500},
        "tier2": {"max_concurrency": 40, "rpm": 5000},
        "tier3": {"max_concurrency": 80, "rpm": 10000},
    }

    def __init__(self, tier="tier1"):
        self.config = self.TIER_CONFIGS[tier]

    def get_concurrency(self):
        return self.config["max_concurrency"]
```

### 2. 监控资源使用

```python
import psutil

def check_system_resources():
    """检查系统资源"""
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()

    print(f"CPU 使用率: {cpu_percent}%")
    print(f"内存使用率: {memory.percent}%")

    # 根据资源使用调整并发数
    if cpu_percent > 80 or memory.percent > 80:
        return "降低并发数"
    elif cpu_percent < 50 and memory.percent < 50:
        return "可以增加并发数"
    else:
        return "保持当前并发数"
```

### 3. 实现断路器模式

```python
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half_open

    def call(self, func, *args, **kwargs):
        """带断路器的调用"""
        if self.state == "open":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "half_open"
            else:
                raise Exception("断路器打开，拒绝请求")

        try:
            result = func(*args, **kwargs)
            if self.state == "half_open":
                self.state = "closed"
                self.failures = 0
            return result
        except Exception as e:
            self.failures += 1
            self.last_failure_time = time.time()

            if self.failures >= self.failure_threshold:
                self.state = "open"

            raise
```

---

## 总结

并发控制机制的核心要点：

1. **max_concurrency 原理**：
   - 通过 ThreadPoolExecutor 限制线程数
   - 任务队列管理超额任务
   - 结果按输入顺序返回

2. **三个限制维度**：
   - API 速率限制（最常见）
   - 系统资源限制（内存、CPU、文件描述符）
   - 成本限制（预算控制）

3. **动态并发控制**：
   - 自适应调整并发数
   - 令牌桶限流
   - 断路器模式

4. **异步并发**：
   - asyncio.Semaphore
   - 适合高并发场景
   - 内存占用更小

5. **2025-2026 最佳实践**：
   - 根据 API Tier 设置并发数
   - 监控系统资源
   - 实现断路器保护

---

## 参考来源

1. [LangChain Runnables 文档](https://reference.langchain.com/python/langchain_core/runnables/) - max_concurrency API
2. [LangGraph 并行节点最佳实践](https://forum.langchain.com/t/best-practices-for-parallel-nodes-fanouts/1900) - 2025 最佳实践
3. [LangChain 最佳实践](https://www.swarnendu.de/blog/langchain-best-practices/) - 并发控制策略
4. [LangChain 并发编程指南](https://medium.com/@oscar.f.agreda/unlocking-pythons-potential-concurrency-with-langchain-and-beyond-876149aaf475) - ThreadPoolExecutor vs asyncio

---

**下一步**：阅读 `03_核心概念_03_批量优化策略.md` 了解性能优化和成本优化的高级技巧
