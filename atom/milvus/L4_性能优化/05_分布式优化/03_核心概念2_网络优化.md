# 核心概念2：网络优化

## 什么是网络优化？

**网络优化**是减少分布式系统中节点间通信开销的技术，包括数据压缩、连接池管理、超时配置和数据本地化。

在 Milvus 分布式集群中，网络通信发生在：
- **Proxy ↔ QueryNode**：查询请求和结果传输
- **QueryNode ↔ DataNode**：数据加载和同步
- **Coordinator ↔ Worker**：元数据和任务调度
- **Worker ↔ Object Storage**：数据持久化

---

## 1. gRPC 压缩

### 1.1 工作原理

Milvus 使用 gRPC 进行节点间通信，支持多种压缩算法：

| 压缩算法 | 压缩率 | CPU 开销 | 适用场景 |
|----------|--------|----------|----------|
| None | 0% | 无 | 内网高速网络 |
| Gzip | 60-70% | 中等 | 公网或跨地域 |
| Snappy | 40-50% | 低 | 平衡性能和压缩率 |
| LZ4 | 50-60% | 很低 | 高吞吐量场景 |

### 1.2 压缩效果分析

**向量数据压缩**：
```python
# 示例：1024维浮点向量
vector_size = 1024 * 4  # 4KB (float32)

# 不压缩：4KB
# Gzip 压缩：约 1.2KB (70% 压缩率)
# Snappy 压缩：约 2KB (50% 压缩率)
```

**压缩收益计算**：
```
网络时间节省 = 原始传输时间 × 压缩率
CPU 开销 = 压缩时间 + 解压时间

净收益 = 网络时间节省 - CPU 开销
```

### 1.3 配置示例

```yaml
# milvus.yaml
grpc:
  # 服务端配置
  server:
    compressionEnabled: true
    compressionAlgorithm: "gzip"  # gzip, snappy, lz4

  # 客户端配置
  client:
    compressionEnabled: true
    compressionAlgorithm: "gzip"

  # 压缩阈值（小于此大小不压缩）
  compressionThreshold: 1024  # 1KB
```

### 1.4 Python 代码示例

```python
from pymilvus import Collection, connections
import time
import numpy as np

connections.connect(alias="default", host="localhost", port="19530")

collection = Collection("compression_test")
collection.load()

# 测试不同数据量的压缩效果
for dim in [128, 512, 1024, 2048]:
    # 生成查询向量
    query_vectors = np.random.rand(100, dim).tolist()

    # 查询并计时
    start = time.time()
    results = collection.search(
        data=query_vectors,
        anns_field="embedding",
        param={"metric_type": "L2", "params": {"nprobe": 10}},
        limit=100
    )
    elapsed = time.time() - start

    # 计算数据量
    data_size = 100 * dim * 4 / 1024  # KB
    print(f"维度 {dim}: 数据量 {data_size:.1f}KB, 耗时 {elapsed*1000:.1f}ms")

# 预期结果（启用压缩 vs 不启用）：
# 维度 128: 50KB, 20ms vs 25ms (压缩后更快)
# 维度 512: 200KB, 45ms vs 80ms (压缩后更快)
# 维度 1024: 400KB, 85ms vs 150ms (压缩后更快)
# 维度 2048: 800KB, 160ms vs 280ms (压缩后更快)
```

### 1.5 决策树

```
是否启用压缩？
├─ 网络带宽 < 1Gbps？
│  └─ 是 → 启用 Gzip
│  └─ 否 → 继续判断
├─ 向量维度 > 512？
│  └─ 是 → 启用 Snappy
│  └─ 否 → 继续判断
├─ CPU 资源紧张？
│  └─ 是 → 不启用
│  └─ 否 → 启用 LZ4
```

---

## 2. 连接池管理

### 2.1 什么是连接池？

**连接池**：预先创建并维护一组 gRPC 连接，避免每次请求都建立新连接。

**问题**：
- 建立 gRPC 连接需要 TCP 三次握手 + TLS 握手（约 50-100ms）
- 高并发场景下，频繁建立连接会成为瓶颈

**解决方案**：
- 使用连接池复用连接
- 减少连接建立开销

### 2.2 连接池参数

```yaml
# milvus.yaml
grpc:
  client:
    # 连接池大小
    maxConnectionNum: 100  # 每个目标节点的最大连接数

    # 连接空闲超时（秒）
    maxIdleTime: 300  # 5分钟

    # 连接保活
    keepAliveTime: 60  # 60秒发送一次心跳
    keepAliveTimeout: 10  # 心跳超时时间

  server:
    # 最大并发流
    maxConcurrentStreams: 1000

    # 最大接收消息大小（MB）
    maxRecvMsgSize: 100
```

### 2.3 连接池大小计算

```python
# 连接池大小 = 并发请求数 / 平均请求时间

# 示例：
# - 并发请求：1000 QPS
# - 平均请求时间：50ms
# - 连接池大小 = 1000 * 0.05 = 50

# 建议：
# - 低并发（< 100 QPS）：10-20 连接
# - 中等并发（100-1000 QPS）：50-100 连接
# - 高并发（> 1000 QPS）：100-200 连接
```

### 2.4 Python 代码示例

```python
from pymilvus import connections
import time
import concurrent.futures

# 配置连接池
connections.connect(
    alias="default",
    host="localhost",
    port="19530",
    pool_size=50  # 连接池大小
)

# 并发查询测试
def query_task(task_id):
    from pymilvus import Collection
    collection = Collection("connection_pool_test")

    start = time.time()
    results = collection.search(
        data=[[0.1] * 128],
        anns_field="embedding",
        param={"metric_type": "L2", "params": {"nprobe": 10}},
        limit=10
    )
    return time.time() - start

# 测试不同并发度
for concurrency in [10, 50, 100]:
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
        futures = [executor.submit(query_task, i) for i in range(100)]
        results = [f.result() for f in futures]

    avg_latency = sum(results) / len(results) * 1000
    print(f"并发度 {concurrency}: 平均延迟 {avg_latency:.1f}ms")

# 预期结果：
# 并发度 10: 平均延迟 25ms
# 并发度 50: 平均延迟 30ms (连接池充足)
# 并发度 100: 平均延迟 60ms (连接池不足，需要等待)
```

---

## 3. 超时配置

### 3.1 超时类型

| 超时类型 | 说明 | 默认值 | 推荐值 |
|----------|------|--------|--------|
| 连接超时 | 建立连接的最大时间 | 5s | 3-5s |
| 请求超时 | 单个请求的最大时间 | 30s | 10-30s |
| 空闲超时 | 连接空闲多久后关闭 | 300s | 60-300s |
| 心跳超时 | 心跳响应的最大时间 | 10s | 5-10s |

### 3.2 配置示例

```yaml
# milvus.yaml
grpc:
  client:
    # 连接超时（毫秒）
    dialTimeout: 5000  # 5秒

    # 请求超时（毫秒）
    requestTimeout: 30000  # 30秒

    # 空闲超时（秒）
    maxIdleTime: 300  # 5分钟

  server:
    # 服务端请求超时（毫秒）
    serverMaxRecvMsgSize: 100  # 100MB
    serverMaxSendMsgSize: 100  # 100MB
```

### 3.3 超时策略

**场景1：低延迟要求（实时推荐）**
```yaml
grpc:
  client:
    requestTimeout: 5000  # 5秒（快速失败）
```

**场景2：大数据量查询（批量处理）**
```yaml
grpc:
  client:
    requestTimeout: 60000  # 60秒（容忍长时间查询）
```

**场景3：不稳定网络（跨地域）**
```yaml
grpc:
  client:
    dialTimeout: 10000  # 10秒（容忍慢速连接）
    requestTimeout: 60000  # 60秒
```

### 3.4 Python 代码示例

```python
from pymilvus import connections, Collection
import time

# 配置超时
connections.connect(
    alias="default",
    host="localhost",
    port="19530",
    timeout=10  # 连接超时 10秒
)

collection = Collection("timeout_test")
collection.load()

# 测试超时
try:
    start = time.time()
    results = collection.search(
        data=[[0.1] * 128],
        anns_field="embedding",
        param={"metric_type": "L2", "params": {"nprobe": 10}},
        limit=10,
        timeout=5  # 查询超时 5秒
    )
    print(f"查询成功: {(time.time() - start)*1000:.1f}ms")
except Exception as e:
    print(f"查询超时: {e}")
```

---

## 4. 数据本地化

### 4.1 什么是数据本地化？

**数据本地化**：让查询尽量在本地节点完成，减少跨节点数据传输。

**问题**：
- 跨节点查询需要传输大量向量数据
- 网络延迟和带宽成为瓶颈

**解决方案**：
- 合理的数据分片策略
- 将相关数据放在同一节点

### 4.2 分片策略

#### 策略1：按用户 ID 分片

```python
# 适用场景：用户个人数据查询
# 好处：同一用户的数据在同一节点，查询不需要跨节点

# 示例：用户画像检索
user_id = "user_123"
shard_key = hash(user_id) % num_shards

# 查询时，只需要访问一个分片
```

#### 策略2：按时间分片

```python
# 适用场景：时间序列数据
# 好处：查询最近数据时，只需要访问最新的分片

# 示例：文档检索（按上传时间）
upload_time = "2024-01-01"
shard_key = get_shard_by_time(upload_time)

# 查询最近7天的数据，只需要访问最新的分片
```

#### 策略3：按类别分片

```python
# 适用场景：分类数据
# 好处：查询特定类别时，只需要访问对应的分片

# 示例：商品推荐（按类别）
category = "electronics"
shard_key = get_shard_by_category(category)

# 查询电子产品时，只需要访问电子产品分片
```

### 4.3 Milvus 分区实现

```python
from pymilvus import Collection, Partition

collection = Collection("localized_data")

# 创建分区（按类别）
partition_electronics = Partition(collection, "electronics")
partition_books = Partition(collection, "books")
partition_clothing = Partition(collection, "clothing")

# 插入数据到对应分区
electronics_data = [...]
partition_electronics.insert(electronics_data)

# 查询时只搜索特定分区（数据本地化）
results = collection.search(
    data=[[0.1] * 128],
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=10,
    partition_names=["electronics"]  # 只搜索电子产品分区
)

# 好处：
# - 减少搜索范围（只搜索 1/3 的数据）
# - 减少跨节点通信（分区可能在同一节点）
# - 提升查询速度（3倍）
```

### 4.4 数据本地化效果

| 场景 | 跨节点查询 | 本地化查询 | 性能提升 |
|------|------------|------------|----------|
| 用户画像 | 50ms | 15ms | 3.3x |
| 文档检索 | 80ms | 25ms | 3.2x |
| 商品推荐 | 60ms | 20ms | 3.0x |

---

## 5. 批量处理

### 5.1 批量查询

**问题**：
- 单个查询的网络开销固定（约 5-10ms）
- 高并发场景下，网络开销占比大

**解决方案**：
- 批量发送多个查询
- 分摊网络开销

```python
from pymilvus import Collection

collection = Collection("batch_query_test")
collection.load()

# 单个查询（不推荐）
for i in range(100):
    results = collection.search(
        data=[[0.1] * 128],
        anns_field="embedding",
        param={"metric_type": "L2", "params": {"nprobe": 10}},
        limit=10
    )
# 总耗时：100 * (5ms 网络 + 10ms 计算) = 1500ms

# 批量查询（推荐）
batch_vectors = [[0.1] * 128 for _ in range(100)]
results = collection.search(
    data=batch_vectors,  # 一次发送 100 个查询
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=10
)
# 总耗时：5ms 网络 + 100 * 10ms 计算 = 1005ms
# 性能提升：1.5x
```

### 5.2 批量插入

```python
from pymilvus import Collection

collection = Collection("batch_insert_test")

# 单条插入（不推荐）
for i in range(10000):
    collection.insert([[i] + [0.1] * 127])
# 总耗时：10000 * 5ms = 50秒

# 批量插入（推荐）
batch_data = [[i] + [0.1] * 127 for i in range(10000)]
collection.insert(batch_data)
# 总耗时：5ms + 插入时间 = 约 2秒
# 性能提升：25x
```

---

## 6. 网络监控指标

### 6.1 关键指标

| 指标 | 说明 | 目标值 |
|------|------|--------|
| `milvus_grpc_request_duration` | gRPC 请求延迟 | P99 < 100ms |
| `milvus_grpc_request_size` | 请求数据大小 | < 10MB |
| `milvus_grpc_response_size` | 响应数据大小 | < 10MB |
| `milvus_grpc_connection_num` | 活跃连接数 | < 连接池大小 |
| `milvus_network_bandwidth` | 网络带宽使用 | < 80% |

### 6.2 监控示例

```python
import requests

# 查询 Prometheus 指标
metrics_url = "http://localhost:9091/metrics"
response = requests.get(metrics_url)

# 解析 gRPC 指标
for line in response.text.split('\n'):
    if 'milvus_grpc_request_duration' in line:
        print(line)

# 示例输出：
# milvus_grpc_request_duration_sum{method="Search"} 1250.5
# milvus_grpc_request_duration_count{method="Search"} 1000
# → 平均延迟 = 1250.5 / 1000 = 1.25ms
```

---

## 7. 网络优化最佳实践

### 7.1 内网部署

```yaml
# 高速内网（10Gbps）
grpc:
  compressionEnabled: false  # 不启用压缩
  client:
    maxConnectionNum: 50
    requestTimeout: 10000  # 10秒
```

### 7.2 跨地域部署

```yaml
# 公网或跨地域（< 1Gbps）
grpc:
  compressionEnabled: true  # 启用压缩
  compressionAlgorithm: "gzip"
  client:
    maxConnectionNum: 100
    requestTimeout: 60000  # 60秒
    dialTimeout: 10000  # 10秒
```

### 7.3 混合部署

```python
# 根据目标节点动态选择策略
def get_grpc_config(target_host):
    if is_local_network(target_host):
        return {
            'compression': False,
            'timeout': 10
        }
    else:
        return {
            'compression': True,
            'timeout': 60
        }
```

---

## 8. 在 RAG 系统中的应用

### 8.1 文档检索场景

**需求**：
- 1 亿篇文档
- 1000 并发用户
- P99 延迟 < 100ms

**网络优化策略**：
```yaml
grpc:
  compressionEnabled: true  # 启用压缩（文档向量维度高）
  compressionAlgorithm: "snappy"  # 平衡压缩率和 CPU
  client:
    maxConnectionNum: 100
    requestTimeout: 30000
```

```python
# 使用分区实现数据本地化
collection.create_partition("recent_docs")  # 最近文档
collection.create_partition("archive_docs")  # 归档文档

# 查询时只搜索最近文档
results = collection.search(
    data=query_vector,
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=10,
    partition_names=["recent_docs"]  # 数据本地化
)
```

### 8.2 实时推荐场景

**需求**：
- 1000 万用户画像
- 10000 QPS
- P99 延迟 < 50ms

**网络优化策略**：
```yaml
grpc:
  compressionEnabled: false  # 不启用压缩（低延迟优先）
  client:
    maxConnectionNum: 200
    requestTimeout: 5000  # 快速失败
```

```python
# 批量查询
batch_vectors = [user_vectors[i] for i in user_ids]
results = collection.search(
    data=batch_vectors,  # 批量查询
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=10
)
```

---

## 总结

**网络优化的核心要点**：

1. **压缩**：根据网络速度和数据特征选择
2. **连接池**：复用连接，减少建立开销
3. **超时**：合理配置，避免长时间等待
4. **数据本地化**：减少跨节点通信
5. **批量处理**：分摊网络开销

**记住**：网络优化需要根据实际环境（内网/公网）和业务需求（延迟/吞吐量）权衡。
