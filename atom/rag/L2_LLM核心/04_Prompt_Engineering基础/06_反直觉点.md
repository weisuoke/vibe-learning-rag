# 反直觉点

RAG Prompt 设计中最常见的 3 个误区。

---

## 误区1：Prompt 越详细越好 ❌

**错误观点**："我应该把所有可能的情况都写进系统提示词"

### 为什么错？

- **Token 浪费**：冗长的系统提示词占用宝贵的 Context Window
- **指令冲突**：规则太多容易相互矛盾
- **注意力分散**：LLM 可能忽略真正重要的指令

### 为什么人们容易这样错？

因为在人类沟通中，详细说明通常能减少误解。但 LLM 不是人：
- LLM 会"平等对待"所有指令，不会自动判断优先级
- 过多指令会稀释每条指令的"权重"

### 正确理解

```python
# ❌ 过度详细的系统提示词
bad_prompt = """
你是一个专业的客服助手。你需要友好、专业、耐心地回答用户问题。
你应该使用简体中文回答。你的回答应该简洁明了。
你不能回答与产品无关的问题。你不能透露公司内部信息。
你不能使用粗鲁的语言。你应该在回答末尾询问是否还有其他问题。
如果用户情绪激动，你应该先安抚用户情绪。
如果问题涉及退款，你应该引导用户查看退款政策。
如果问题涉及技术问题，你应该建议用户联系技术支持。
...（继续写了20条规则）
"""

# ✅ 简洁有效的系统提示词
good_prompt = """
你是客服助手。基于提供的文档回答问题。
无法回答时说"建议联系人工客服"。不要编造信息。
"""
```

**原则**：系统提示词应该像代码一样——能用 3 行解决的，不要用 30 行。

---

## 误区2：检索到的内容越多越好 ❌

**错误观点**："我应该把检索到的所有相关文档都塞进 Prompt"

### 为什么错？

- **噪声干扰**：不太相关的内容会干扰 LLM 的判断
- **Token 限制**：Context Window 是有限的
- **Lost in the Middle**：LLM 对中间位置的内容关注度较低

### 为什么人们容易这样错？

因为在人类阅读中，更多参考资料通常意味着更全面的答案。但 LLM 的注意力机制不是这样工作的：
- LLM 会尝试"综合"所有内容，包括不相关的
- 过多内容会导致关键信息被"淹没"

### 正确理解

```python
# ❌ 注入过多文档
def bad_context_injection(docs: list) -> str:
    # 把检索到的 20 个文档全部注入
    return "\n\n".join([doc["content"] for doc in docs[:20]])

# ✅ 精选最相关的文档
def good_context_injection(docs: list, max_docs: int = 3) -> str:
    # 只注入 top-3 最相关的文档
    top_docs = sorted(docs, key=lambda x: x["score"], reverse=True)[:max_docs]
    return "\n\n".join([doc["content"] for doc in top_docs])
```

**原则**：质量 > 数量。3 个高度相关的文档 > 10 个一般相关的文档。

---

## 误区3：LLM 会自动忽略无关内容 ❌

**错误观点**："就算注入了一些不相关的内容，LLM 也会自动忽略它们"

### 为什么错？

- **LLM 没有"忽略"能力**：它会尝试理解和使用所有输入
- **无关内容会影响输出**：可能导致回答偏离主题
- **可能引入错误信息**：LLM 可能从无关内容中"找到"似是而非的答案

### 为什么人们容易这样错？

因为人类阅读时可以快速跳过无关内容。但 LLM 的注意力机制会"看到"所有内容：
- 每个 Token 都会参与注意力计算
- 无关内容会分散对相关内容的注意力

### 正确理解

```python
# ❌ 假设 LLM 会自动过滤
def bad_approach(query: str) -> str:
    # 检索了很多文档，包括不相关的
    all_docs = retrieve(query, top_k=20)
    # 直接全部注入，期望 LLM 自己判断
    return build_prompt(all_docs, query)

# ✅ 主动过滤无关内容
def good_approach(query: str) -> str:
    # 检索文档
    docs = retrieve(query, top_k=10)
    # 过滤掉相关性低的文档
    relevant_docs = [d for d in docs if d["score"] > 0.7]
    # 只注入相关文档
    return build_prompt(relevant_docs, query)
```

**原则**：垃圾进，垃圾出。在注入前就要确保内容的相关性。

---

## 额外误区：一些常见的小错误

### 误区4：格式指令放在最后

```python
# ❌ 格式指令放在最后，容易被忽略
bad = f"""
{context}
{query}
请用 JSON 格式回答。
"""

# ✅ 格式指令放在开头或系统提示词中
good_system = "你是助手。请始终用 JSON 格式回答。"
good_user = f"""
请用 JSON 格式回答以下问题。

参考文档：
{context}

问题：{query}
"""
```

### 误区5：没有处理"无答案"情况

```python
# ❌ 没有告诉 LLM 无答案时怎么办
bad = "根据文档回答问题。"

# ✅ 明确处理无答案情况
good = """根据文档回答问题。
如果文档中没有相关信息，回复"文档中未找到相关信息"。"""
```

### 误区6：期望 LLM 记住之前的对话

```python
# ❌ 假设 LLM 记得之前的对话
bad = "继续回答上一个问题。"

# ✅ 每次都提供完整上下文
good = f"""
之前的问题：{previous_query}
之前的回答：{previous_answer}
用户的追问：{follow_up_query}
"""
```

---

## 误区总结表

| 误区 | 错误做法 | 正确做法 |
|------|---------|---------|
| Prompt 越详细越好 | 写 30 条规则 | 精简到 3-5 条核心规则 |
| 内容越多越好 | 注入 20 个文档 | 精选 3-5 个最相关的 |
| LLM 会自动忽略 | 不过滤直接注入 | 主动过滤低相关性内容 |
| 格式指令放最后 | 在末尾说"用 JSON" | 在开头或系统提示词中说明 |
| 不处理无答案 | 只说"回答问题" | 明确说明无答案时的回复 |

---

**下一步：** [07_实战代码](./07_实战代码.md) - 完整的 RAG Prompt 实现
