# 实战代码:错误处理与重试实战

> **本节目标**:通过完整的弹性系统项目,掌握RunnableRetry和RunnableWithFallbacks的实战应用。

---

## 一、项目场景

**企业级客服系统**:
- 主模型:GPT-4(高质量)
- 备用模型:GPT-3.5(快速)
- 缓存降级:Redis缓存
- 默认响应:保底机制
- 自动重试:临时故障恢复

**可靠性目标**:
- 可用性:99.9%+
- 错误率:<1%
- 自动恢复:无需人工介入

---

## 二、环境准备

### 2.1 安装依赖

```bash
# 安装依赖
uv add langchain langchain-openai redis tenacity

# 配置环境变量
cat > .env << EOF
OPENAI_API_KEY=your_api_key_here
REDIS_HOST=localhost
REDIS_PORT=6379
EOF
```

### 2.2 项目结构

```
resilient_system/
├── chains/
│   ├── __init__.py
│   ├── primary.py          # 主链
│   ├── fallback.py         # 降级链
│   └── cache.py            # 缓存链
├── utils/
│   ├── __init__.py
│   └── retry.py            # 重试配置
├── main.py                 # 主程序
└── .env                    # 环境变量
```

---

## 三、重试机制实现

### 3.1 基础重试

```python
# chains/primary.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
from openai import RateLimitError, APITimeoutError
import logging

logger = logging.getLogger(__name__)

def create_primary_chain_with_retry():
    """创建带重试的主链

    重试策略:
    - 最多重试3次
    - 指数退避(1s, 2s, 4s)
    - 只重试临时错误(限流、超时)
    """
    prompt = ChatPromptTemplate.from_template(
        """你是一个专业的客服助手。请回答用户的问题。

问题:{question}

回答:"""
    )

    # 主模型:GPT-4
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        max_retries=3  # LLM内置重试
    )

    parser = StrOutputParser()

    # 链级别重试
    chain = (
        prompt
        | llm
        | parser
    ).with_retry(
        stop_after_attempt=3,
        wait_exponential_jitter=True,
        retry_if_exception_type=(
            RateLimitError,
            APITimeoutError,
            ConnectionError
        )
    )

    return chain


# 示例使用
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    chain = create_primary_chain_with_retry()

    try:
        result = chain.invoke({"question": "如何退货?"})
        print(f"成功:{result}")
    except Exception as e:
        logger.error(f"失败:{e}")
```

### 3.2 自定义重试逻辑

```python
# utils/retry.py
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
    after_log
)
import logging

logger = logging.getLogger(__name__)

def create_retry_decorator(
    max_attempts: int = 3,
    min_wait: int = 1,
    max_wait: int = 60
):
    """创建重试装饰器

    Args:
        max_attempts: 最大重试次数
        min_wait: 最小等待时间(秒)
        max_wait: 最大等待时间(秒)
    """
    return retry(
        stop=stop_after_attempt(max_attempts),
        wait=wait_exponential(
            multiplier=1,
            min=min_wait,
            max=max_wait
        ),
        retry=retry_if_exception_type((
            RateLimitError,
            APITimeoutError,
            ConnectionError
        )),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        after=after_log(logger, logging.INFO)
    )


# 使用示例
@create_retry_decorator(max_attempts=5)
def call_llm_with_retry(prompt: str) -> str:
    """带重试的LLM调用"""
    llm = ChatOpenAI(model="gpt-4o")
    return llm.invoke(prompt).content


# 测试
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    try:
        result = call_llm_with_retry("测试问题")
        print(f"结果:{result}")
    except Exception as e:
        logger.error(f"所有重试都失败:{e}")
```

---

## 四、降级机制实现

### 4.1 多层降级链

```python
# chains/fallback.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
import logging

logger = logging.getLogger(__name__)

def create_fallback_chains():
    """创建多层降级链

    降级层次:
    1. 主链:GPT-4(高质量)
    2. 备用1:GPT-3.5(快速)
    3. 备用2:缓存(历史响应)
    4. 默认:保底响应
    """
    parser = StrOutputParser()

    # 主链:GPT-4 + 重试
    primary_prompt = ChatPromptTemplate.from_template(
        "专业回答:{question}"
    )
    primary_chain = (
        primary_prompt
        | ChatOpenAI(model="gpt-4o", temperature=0)
        | parser
    ).with_retry(stop_after_attempt=3)

    # 备用1:GPT-3.5 + 重试
    secondary_prompt = ChatPromptTemplate.from_template(
        "简洁回答:{question}"
    )
    secondary_chain = (
        secondary_prompt
        | ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        | parser
    ).with_retry(stop_after_attempt=2)

    # 备用2:缓存
    def get_cached_response(input_dict: dict) -> str:
        """从缓存获取响应"""
        question = input_dict.get("question", "")

        # 简单缓存(实际应使用Redis)
        cache = {
            "如何退货?": "请在订单页面点击退货按钮,填写退货原因即可。",
            "如何换货?": "请联系客服申请换货,我们会尽快处理。"
        }

        cached = cache.get(question)
        if cached:
            logger.info(f"缓存命中:{question}")
            return cached

        raise ValueError("缓存未命中")

    cache_chain = RunnableLambda(get_cached_response)

    # 默认:保底响应
    def get_default_response(input_dict: dict) -> str:
        """保底响应"""
        logger.warning("所有链都失败,返回默认响应")
        return "抱歉,系统暂时无法处理您的请求。请稍后重试或联系人工客服。"

    default_chain = RunnableLambda(get_default_response)

    # 组合降级链
    resilient_chain = primary_chain.with_fallbacks([
        secondary_chain,
        cache_chain,
        default_chain
    ])

    return resilient_chain


# 示例使用
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    chain = create_fallback_chains()

    # 测试1:正常查询
    result1 = chain.invoke({"question": "什么是LCEL?"})
    print(f"正常查询:{result1}")

    # 测试2:缓存查询
    result2 = chain.invoke({"question": "如何退货?"})
    print(f"缓存查询:{result2}")
```

### 4.2 降级监控

```python
# 降级监控
from langchain_core.callbacks import BaseCallbackHandler

class FallbackMonitor(BaseCallbackHandler):
    """降级监控回调"""

    def __init__(self):
        self.fallback_count = 0
        self.fallback_reasons = []

    def on_chain_error(self, error, **kwargs):
        """链错误时"""
        self.fallback_count += 1
        self.fallback_reasons.append(str(error))
        logger.warning(f"降级触发(第{self.fallback_count}次):{error}")

    def report(self):
        """生成报告"""
        return {
            "fallback_count": self.fallback_count,
            "fallback_reasons": self.fallback_reasons
        }


# 使用
monitor = FallbackMonitor()

chain = create_fallback_chains().with_config({
    "callbacks": [monitor]
})

# 执行多次查询
for question in questions:
    result = chain.invoke({"question": question})

# 查看报告
report = monitor.report()
print(f"降级次数:{report['fallback_count']}")
print(f"降级原因:{report['fallback_reasons']}")
```

---

## 五、Redis缓存集成

### 5.1 Redis缓存实现

```python
# chains/cache.py
import redis
import json
import hashlib
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class RedisCache:
    """Redis缓存"""

    def __init__(self, host: str = "localhost", port: int = 6379, ttl: int = 3600):
        """初始化Redis缓存

        Args:
            host: Redis主机
            port: Redis端口
            ttl: 缓存过期时间(秒)
        """
        self.client = redis.Redis(host=host, port=port, decode_responses=True)
        self.ttl = ttl

    def _generate_key(self, question: str) -> str:
        """生成缓存键"""
        return f"qa:{hashlib.md5(question.encode()).hexdigest()}"

    def get(self, question: str) -> Optional[str]:
        """获取缓存"""
        key = self._generate_key(question)
        cached = self.client.get(key)

        if cached:
            logger.info(f"缓存命中:{question}")
            return cached

        logger.info(f"缓存未命中:{question}")
        return None

    def set(self, question: str, answer: str):
        """设置缓存"""
        key = self._generate_key(question)
        self.client.setex(key, self.ttl, answer)
        logger.info(f"缓存写入:{question}")

    def delete(self, question: str):
        """删除缓存"""
        key = self._generate_key(question)
        self.client.delete(key)
        logger.info(f"缓存删除:{question}")


def create_cached_chain(cache: RedisCache):
    """创建带缓存的链"""
    from langchain_core.runnables import RunnableLambda

    # 主链
    primary_chain = create_primary_chain_with_retry()

    # 缓存包装
    def cached_invoke(input_dict: dict) -> str:
        """带缓存的调用"""
        question = input_dict.get("question", "")

        # 尝试从缓存获取
        cached = cache.get(question)
        if cached:
            return cached

        # 缓存未命中,调用主链
        result = primary_chain.invoke(input_dict)

        # 写入缓存
        cache.set(question, result)

        return result

    return RunnableLambda(cached_invoke)


# 示例使用
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # 创建缓存
    cache = RedisCache(ttl=3600)

    # 创建带缓存的链
    chain = create_cached_chain(cache)

    # 第1次调用:缓存未命中
    result1 = chain.invoke({"question": "什么是LCEL?"})
    print(f"第1次:{result1}")

    # 第2次调用:缓存命中
    result2 = chain.invoke({"question": "什么是LCEL?"})
    print(f"第2次:{result2}")
```

---

## 六、完整弹性系统

### 6.1 系统架构

```python
# main.py
from dotenv import load_dotenv
from chains.primary import create_primary_chain_with_retry
from chains.fallback import create_fallback_chains
from chains.cache import RedisCache, create_cached_chain
from langchain_core.runnables import RunnableLambda
import logging
import time

# 加载环境变量
load_dotenv()

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ResilientCustomerService:
    """弹性客服系统"""

    def __init__(self, use_cache: bool = True):
        """初始化系统

        Args:
            use_cache: 是否启用缓存
        """
        self.use_cache = use_cache

        # 创建缓存
        if use_cache:
            self.cache = RedisCache(ttl=3600)

        # 创建弹性链
        self.chain = self._create_chain()

        # 统计
        self.stats = {
            "total": 0,
            "success": 0,
            "cache_hit": 0,
            "fallback": 0,
            "error": 0
        }

    def _create_chain(self):
        """创建完整链"""
        # 基础链:主链 + 降级
        base_chain = create_fallback_chains()

        # 如果启用缓存,添加缓存层
        if self.use_cache:
            return create_cached_chain(self.cache)

        return base_chain

    def ask(self, question: str) -> dict:
        """问答

        Returns:
            {
                "answer": str,
                "latency": float,
                "source": str  # "cache" | "primary" | "fallback" | "error"
            }
        """
        self.stats["total"] += 1

        start = time.time()

        try:
            # 检查缓存
            if self.use_cache:
                cached = self.cache.get(question)
                if cached:
                    self.stats["cache_hit"] += 1
                    return {
                        "answer": cached,
                        "latency": time.time() - start,
                        "source": "cache"
                    }

            # 调用链
            answer = self.chain.invoke({"question": question})

            self.stats["success"] += 1

            return {
                "answer": answer,
                "latency": time.time() - start,
                "source": "primary"
            }

        except Exception as e:
            self.stats["error"] += 1
            logger.error(f"系统错误:{e}")

            return {
                "answer": "系统错误,请稍后重试。",
                "latency": time.time() - start,
                "source": "error"
            }

    def get_stats(self) -> dict:
        """获取统计"""
        total = self.stats["total"]
        if total == 0:
            return self.stats

        return {
            **self.stats,
            "success_rate": self.stats["success"] / total * 100,
            "cache_hit_rate": self.stats["cache_hit"] / total * 100,
            "error_rate": self.stats["error"] / total * 100
        }


def main():
    """主程序"""
    # 创建系统
    system = ResilientCustomerService(use_cache=True)

    # 测试查询
    questions = [
        "如何退货?",
        "如何换货?",
        "什么是LCEL?",
        "如何退货?",  # 重复查询(测试缓存)
        "LCEL有哪些优势?",
    ]

    print("=" * 60)
    print("弹性客服系统")
    print("=" * 60)

    for question in questions:
        print(f"\n问题:{question}")

        result = system.ask(question)

        print(f"答案:{result['answer']}")
        print(f"延迟:{result['latency']:.3f}秒")
        print(f"来源:{result['source']}")
        print("-" * 60)

    # 统计报告
    stats = system.get_stats()
    print("\n" + "=" * 60)
    print("统计报告")
    print("=" * 60)
    print(f"总请求数:{stats['total']}")
    print(f"成功率:{stats['success_rate']:.1f}%")
    print(f"缓存命中率:{stats['cache_hit_rate']:.1f}%")
    print(f"错误率:{stats['error_rate']:.1f}%")


if __name__ == "__main__":
    main()
```

---

## 七、错误注入测试

### 7.1 模拟故障

```python
# test_resilience.py
import random
from langchain_core.runnables import RunnableLambda
from openai import RateLimitError, APITimeoutError

class FaultInjector:
    """故障注入器"""

    def __init__(self, failure_rate: float = 0.3):
        """初始化

        Args:
            failure_rate: 故障率(0-1)
        """
        self.failure_rate = failure_rate
        self.call_count = 0

    def __call__(self, input_dict: dict) -> str:
        """模拟LLM调用"""
        self.call_count += 1

        # 随机故障
        if random.random() < self.failure_rate:
            # 随机选择错误类型
            error_type = random.choice([
                RateLimitError,
                APITimeoutError,
                ConnectionError
            ])
            raise error_type(f"模拟故障(第{self.call_count}次调用)")

        # 成功响应
        return f"成功响应(第{self.call_count}次调用)"


def test_resilience():
    """测试弹性"""
    # 创建故障注入链
    faulty_chain = RunnableLambda(FaultInjector(failure_rate=0.5))

    # 添加重试
    resilient_chain = faulty_chain.with_retry(
        stop_after_attempt=5,
        wait_exponential_jitter=True
    )

    # 测试
    success_count = 0
    total_count = 10

    for i in range(total_count):
        try:
            result = resilient_chain.invoke({"test": i})
            success_count += 1
            print(f"✓ 测试{i+1}:成功 - {result}")
        except Exception as e:
            print(f"✗ 测试{i+1}:失败 - {e}")

    print(f"\n成功率:{success_count}/{total_count} ({success_count/total_count*100:.1f}%)")


if __name__ == "__main__":
    test_resilience()
```

---

## 八、监控与告警

### 8.1 实时监控

```python
# monitoring.py
from dataclasses import dataclass
from datetime import datetime
from typing import List
import logging

logger = logging.getLogger(__name__)

@dataclass
class ErrorEvent:
    """错误事件"""
    timestamp: datetime
    error_type: str
    error_message: str
    retry_count: int

class ErrorMonitor:
    """错误监控"""

    def __init__(self, alert_threshold: int = 5):
        """初始化

        Args:
            alert_threshold: 告警阈值(错误数)
        """
        self.alert_threshold = alert_threshold
        self.errors: List[ErrorEvent] = []

    def record_error(self, error: Exception, retry_count: int = 0):
        """记录错误"""
        event = ErrorEvent(
            timestamp=datetime.now(),
            error_type=type(error).__name__,
            error_message=str(error),
            retry_count=retry_count
        )

        self.errors.append(event)

        # 检查告警
        if len(self.errors) >= self.alert_threshold:
            self._trigger_alert()

    def _trigger_alert(self):
        """触发告警"""
        logger.error(f"⚠️ 告警:错误数达到阈值({len(self.errors)}/{self.alert_threshold})")

        # 统计错误类型
        error_types = {}
        for event in self.errors:
            error_types[event.error_type] = error_types.get(event.error_type, 0) + 1

        logger.error(f"错误分布:{error_types}")

    def get_report(self) -> dict:
        """获取报告"""
        if not self.errors:
            return {"total_errors": 0}

        # 统计
        error_types = {}
        total_retries = 0

        for event in self.errors:
            error_types[event.error_type] = error_types.get(event.error_type, 0) + 1
            total_retries += event.retry_count

        return {
            "total_errors": len(self.errors),
            "error_types": error_types,
            "avg_retries": total_retries / len(self.errors),
            "first_error": self.errors[0].timestamp,
            "last_error": self.errors[-1].timestamp
        }


# 使用示例
monitor = ErrorMonitor(alert_threshold=3)

# 模拟错误
for i in range(5):
    try:
        # 模拟调用
        if random.random() < 0.5:
            raise RateLimitError("限流")
    except Exception as e:
        monitor.record_error(e, retry_count=i)

# 查看报告
report = monitor.get_report()
print(f"错误报告:{report}")
```

---

## 九、性能测试

### 9.1 压力测试

```python
# benchmark.py
import asyncio
import time
from typing import List

async def benchmark_resilience(
    chain,
    questions: List[str],
    concurrency: int = 10
):
    """弹性基准测试

    Args:
        chain: 测试链
        questions: 测试问题列表
        concurrency: 并发数
    """
    start = time.time()

    # 并发执行
    tasks = [
        chain.ainvoke({"question": q})
        for q in questions
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    elapsed = time.time() - start

    # 统计
    success_count = sum(1 for r in results if not isinstance(r, Exception))
    error_count = len(results) - success_count

    print(f"总请求数:{len(questions)}")
    print(f"并发数:{concurrency}")
    print(f"总耗时:{elapsed:.2f}秒")
    print(f"吞吐量:{len(questions)/elapsed:.2f} req/s")
    print(f"成功率:{success_count/len(questions)*100:.1f}%")
    print(f"错误率:{error_count/len(questions)*100:.1f}%")


# 运行测试
if __name__ == "__main__":
    chain = create_fallback_chains()

    # 生成测试数据
    questions = [f"测试问题{i}" for i in range(100)]

    # 运行基准测试
    asyncio.run(benchmark_resilience(chain, questions, concurrency=10))
```

---

## 十、总结

### 核心要点

1. **RunnableRetry**:自动重试临时故障,指数退避
2. **RunnableWithFallbacks**:多层降级,保证可用性
3. **Redis缓存**:减少LLM调用,提升性能
4. **监控告警**:实时追踪错误,及时响应

### 弹性架构

```
请求 → 缓存检查
       ↓ 未命中
       → 主链(GPT-4) + 重试3次
       ↓ 失败
       → 备用链(GPT-3.5) + 重试2次
       ↓ 失败
       → 缓存降级
       ↓ 失败
       → 默认响应(保底)
```

### 实战收获

- ✅ 99.9%+可用性
- ✅ 自动故障恢复
- ✅ 多层降级保护
- ✅ 完整监控体系
- ✅ 生产级弹性系统

---

**下一步**:学习【配置管理实战】,实现多环境部署。
