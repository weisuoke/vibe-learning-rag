# å®æˆ˜ä»£ç 2ï¼šè‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›å®Œæ•´çš„è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨å®æˆ˜ä»£ç ï¼Œå±•ç¤ºå¦‚ä½•è®°å½•æ—¥å¿—ã€è¿½è¸ªæ€§èƒ½ã€å¤„ç†é”™è¯¯ã€‚

**æ¶µç›–åœºæ™¯**ï¼š
1. æ–‡ä»¶æ—¥å¿—å›è°ƒå¤„ç†å™¨ - JSON Linesæ ¼å¼
2. æ€§èƒ½è¿½è¸ªå¤„ç†å™¨ - å»¶è¿Ÿå’ŒTokenç»Ÿè®¡
3. é”™è¯¯å¤„ç†å’Œåˆ†ç±» - é”™è¯¯åˆ†ç±»å’Œå‘Šè­¦
4. ç»„åˆå¤šä¸ªå¤„ç†å™¨ - ç»¼åˆä½¿ç”¨

**å‰ç½®è¦æ±‚**ï¼š
- Python 3.13+
- LangChain v0.3+
- OpenAI APIå¯†é’¥

---

## åœºæ™¯1ï¼šæ–‡ä»¶æ—¥å¿—å›è°ƒå¤„ç†å™¨

### ç›®æ ‡

å°†é“¾æ‰§è¡Œæ—¥å¿—è®°å½•åˆ°JSON Linesæ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­åˆ†æã€‚

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯1ï¼šæ–‡ä»¶æ—¥å¿—å›è°ƒå¤„ç†å™¨
è®°å½•åˆ°JSON Linesæ–‡ä»¶
"""

from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from datetime import datetime
import json

load_dotenv()


class FileLogHandler(BaseCallbackHandler):
    """è®°å½•åˆ°JSON Linesæ–‡ä»¶"""

    def __init__(self, log_file="chain_logs.jsonl"):
        self.log_file = log_file

    def _write_log(self, event_type, data):
        """å†™å…¥æ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "event": event_type,
            "data": data
        }
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"Failed to write log: {e}")

    def on_chain_start(self, serialized, inputs, **kwargs):
        """é“¾å¼€å§‹æ—¶è®°å½•è¾“å…¥"""
        self._write_log("chain_start", {
            "inputs": inputs,
            "run_id": kwargs.get("run_id")
        })

    def on_chain_end(self, outputs, **kwargs):
        """é“¾ç»“æŸæ—¶è®°å½•è¾“å‡º"""
        self._write_log("chain_end", {
            "outputs": str(outputs)[:200],  # é™åˆ¶é•¿åº¦
            "run_id": kwargs.get("run_id")
        })

    def on_chain_error(self, error, **kwargs):
        """é“¾å‡ºé”™æ—¶è®°å½•é”™è¯¯"""
        self._write_log("chain_error", {
            "error": str(error),
            "error_type": error.__class__.__name__,
            "run_id": kwargs.get("run_id")
        })

    def on_llm_end(self, response, **kwargs):
        """LLMç»“æŸæ—¶è®°å½•Tokenä½¿ç”¨"""
        tokens = response.llm_output.get("token_usage", {})
        self._write_log("llm_end", {
            "tokens": tokens,
            "run_id": kwargs.get("run_id")
        })


# ä½¿ç”¨ç¤ºä¾‹
def main():
    prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model | StrOutputParser()

    # ä½¿ç”¨æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨
    handler = FileLogHandler("my_logs.jsonl")

    result = chain.invoke(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        config={"callbacks": [handler]}
    )

    print(result)
    print("\næ—¥å¿—å·²å†™å…¥ my_logs.jsonl")


if __name__ == "__main__":
    main()
```

### æ—¥å¿—æ–‡ä»¶ç¤ºä¾‹

```json
{"timestamp": "2026-02-20T14:30:00.123", "event": "chain_start", "data": {"inputs": {"question": "ä»€ä¹ˆæ˜¯LCEL?"}, "run_id": "..."}}
{"timestamp": "2026-02-20T14:30:02.456", "event": "llm_end", "data": {"tokens": {"prompt_tokens": 15, "completion_tokens": 120, "total_tokens": 135}, "run_id": "..."}}
{"timestamp": "2026-02-20T14:30:02.789", "event": "chain_end", "data": {"outputs": "LCELæ˜¯...", "run_id": "..."}}
```

---

## åœºæ™¯2ï¼šæ€§èƒ½è¿½è¸ªå¤„ç†å™¨

### ç›®æ ‡

è¿½è¸ªé“¾æ‰§è¡Œçš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å»¶è¿Ÿå’ŒTokenä½¿ç”¨ã€‚

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯2ï¼šæ€§èƒ½è¿½è¸ªå¤„ç†å™¨
è¿½è¸ªå»¶è¿Ÿå’ŒTokenä½¿ç”¨
"""

from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import time
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PerformanceHandler(BaseCallbackHandler):
    """è¿½è¸ªæ€§èƒ½æŒ‡æ ‡"""

    def __init__(self):
        self.start_times = {}
        self.metrics = {
            "chain_duration": 0,
            "llm_duration": 0,
            "total_tokens": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0
        }

    def on_chain_start(self, serialized, inputs, **kwargs):
        """é“¾å¼€å§‹æ—¶è®°å½•æ—¶é—´"""
        run_id = kwargs.get("run_id")
        self.start_times[f"chain_{run_id}"] = time.time()
        logger.info(f"Chain started: {inputs}")

    def on_chain_end(self, outputs, **kwargs):
        """é“¾ç»“æŸæ—¶è®¡ç®—å»¶è¿Ÿ"""
        run_id = kwargs.get("run_id")
        key = f"chain_{run_id}"
        if key in self.start_times:
            duration = time.time() - self.start_times[key]
            self.metrics["chain_duration"] = duration
            logger.info(f"Chain completed in {duration:.2f}s")

    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLMå¼€å§‹æ—¶è®°å½•æ—¶é—´"""
        run_id = kwargs.get("run_id")
        self.start_times[f"llm_{run_id}"] = time.time()

    def on_llm_end(self, response, **kwargs):
        """LLMç»“æŸæ—¶è®¡ç®—å»¶è¿Ÿå’ŒToken"""
        run_id = kwargs.get("run_id")
        key = f"llm_{run_id}"
        if key in self.start_times:
            duration = time.time() - self.start_times[key]
            self.metrics["llm_duration"] = duration

        # è®°å½•Tokenä½¿ç”¨
        tokens = response.llm_output.get("token_usage", {})
        self.metrics["total_tokens"] = tokens.get("total_tokens", 0)
        self.metrics["prompt_tokens"] = tokens.get("prompt_tokens", 0)
        self.metrics["completion_tokens"] = tokens.get("completion_tokens", 0)

        logger.info(f"LLM call: {duration:.2f}s, tokens: {tokens}")

    def get_metrics(self):
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return self.metrics

    def print_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        print("\n=== æ€§èƒ½æŠ¥å‘Š ===")
        print(f"é“¾æ€»å»¶è¿Ÿ: {self.metrics['chain_duration']:.2f}s")
        print(f"LLMå»¶è¿Ÿ: {self.metrics['llm_duration']:.2f}s")
        print(f"æ€»Token: {self.metrics['total_tokens']}")
        print(f"  Prompt: {self.metrics['prompt_tokens']}")
        print(f"  Completion: {self.metrics['completion_tokens']}")

        # è®¡ç®—æˆæœ¬ï¼ˆgpt-4o-miniä»·æ ¼ï¼‰
        prompt_cost = self.metrics['prompt_tokens'] * 0.15 / 1_000_000
        completion_cost = self.metrics['completion_tokens'] * 0.60 / 1_000_000
        total_cost = prompt_cost + completion_cost
        print(f"ä¼°ç®—æˆæœ¬: ${total_cost:.6f}")


# ä½¿ç”¨ç¤ºä¾‹
def main():
    prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model

    # ä½¿ç”¨æ€§èƒ½è¿½è¸ªå¤„ç†å™¨
    handler = PerformanceHandler()

    result = chain.invoke(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        config={"callbacks": [handler]}
    )

    print(result.content)
    handler.print_report()


if __name__ == "__main__":
    main()
```

### è¾“å‡ºç¤ºä¾‹

```
2026-02-20 14:30:00 - __main__ - INFO - Chain started: {'question': 'ä»€ä¹ˆæ˜¯LCEL?'}
2026-02-20 14:30:02 - __main__ - INFO - LLM call: 1.23s, tokens: {'prompt_tokens': 15, 'completion_tokens': 120, 'total_tokens': 135}
2026-02-20 14:30:02 - __main__ - INFO - Chain completed in 1.25s

LCELæ˜¯LangChain Expression Languageçš„ç¼©å†™...

=== æ€§èƒ½æŠ¥å‘Š ===
é“¾æ€»å»¶è¿Ÿ: 1.25s
LLMå»¶è¿Ÿ: 1.23s
æ€»Token: 135
  Prompt: 15
  Completion: 120
ä¼°ç®—æˆæœ¬: $0.000074
```

---

## åœºæ™¯3ï¼šé”™è¯¯å¤„ç†å’Œåˆ†ç±»

### ç›®æ ‡

æ•è·å’Œåˆ†ç±»é”™è¯¯ï¼Œæ–¹ä¾¿åç»­åˆ†æå’Œå‘Šè­¦ã€‚

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯3ï¼šé”™è¯¯å¤„ç†å’Œåˆ†ç±»
æ•è·å’Œåˆ†ç±»é”™è¯¯
"""

from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
from datetime import datetime
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ErrorHandler(BaseCallbackHandler):
    """é”™è¯¯å¤„ç†å’Œåˆ†ç±»"""

    def __init__(self):
        self.errors = []
        self.error_counts = {
            "rate_limit": 0,
            "timeout": 0,
            "auth": 0,
            "model": 0,
            "unknown": 0
        }

    def on_chain_error(self, error, **kwargs):
        """é“¾é”™è¯¯"""
        self._handle_error("chain", error, kwargs)

    def on_llm_error(self, error, **kwargs):
        """LLMé”™è¯¯"""
        self._handle_error("llm", error, kwargs)

    def on_tool_error(self, error, **kwargs):
        """å·¥å…·é”™è¯¯"""
        self._handle_error("tool", error, kwargs)

    def _handle_error(self, error_type, error, kwargs):
        """ç»Ÿä¸€é”™è¯¯å¤„ç†"""
        error_str = str(error).lower()
        error_class = error.__class__.__name__

        # åˆ†ç±»é”™è¯¯
        if "rate limit" in error_str or "429" in error_str:
            category = "rate_limit"
        elif "timeout" in error_str or "timed out" in error_str:
            category = "timeout"
        elif "authentication" in error_str or "401" in error_str or "403" in error_str:
            category = "auth"
        elif "model" in error_str or "invalid" in error_str:
            category = "model"
        else:
            category = "unknown"

        # è®°å½•é”™è¯¯
        error_info = {
            "timestamp": datetime.now().isoformat(),
            "type": error_type,
            "category": category,
            "error": str(error),
            "error_class": error_class,
            "run_id": kwargs.get("run_id")
        }

        self.errors.append(error_info)
        self.error_counts[category] += 1

        # è®°å½•æ—¥å¿—
        logger.error(f"[{category}] {error_type} error: {error}")

        # å‘Šè­¦ï¼ˆç¤ºä¾‹ï¼‰
        if category == "rate_limit":
            logger.warning("âš ï¸ Rate limit reached! Consider implementing backoff.")
        elif category == "auth":
            logger.error("ğŸ”’ Authentication failed! Check API key.")

    def get_errors(self):
        """è·å–é”™è¯¯åˆ—è¡¨"""
        return self.errors

    def get_error_counts(self):
        """è·å–é”™è¯¯ç»Ÿè®¡"""
        return self.error_counts

    def print_report(self):
        """æ‰“å°é”™è¯¯æŠ¥å‘Š"""
        print("\n=== é”™è¯¯æŠ¥å‘Š ===")
        print(f"æ€»é”™è¯¯æ•°: {len(self.errors)}")
        print("\né”™è¯¯åˆ†ç±»:")
        for category, count in self.error_counts.items():
            if count > 0:
                print(f"  {category}: {count}")

        if self.errors:
            print("\næœ€è¿‘çš„é”™è¯¯:")
            for error in self.errors[-5:]:  # åªæ˜¾ç¤ºæœ€è¿‘5ä¸ª
                print(f"  [{error['category']}] {error['error'][:80]}...")


# ä½¿ç”¨ç¤ºä¾‹
def main():
    prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model

    # ä½¿ç”¨é”™è¯¯å¤„ç†å™¨
    handler = ErrorHandler()

    try:
        result = chain.invoke(
            {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
            config={"callbacks": [handler]}
        )
        print(result.content)
    except Exception as e:
        print(f"Chain failed: {e}")

    handler.print_report()


if __name__ == "__main__":
    main()
```

---

## åœºæ™¯4ï¼šç»„åˆå¤šä¸ªå¤„ç†å™¨

### ç›®æ ‡

åŒæ—¶ä½¿ç”¨å¤šä¸ªå›è°ƒå¤„ç†å™¨ï¼Œå®ç°å®Œæ•´çš„ç›‘æ§æ–¹æ¡ˆã€‚

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯4ï¼šç»„åˆå¤šä¸ªå¤„ç†å™¨
åŒæ—¶ä½¿ç”¨æ–‡ä»¶æ—¥å¿—ã€æ€§èƒ½è¿½è¸ªå’Œé”™è¯¯å¤„ç†
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

load_dotenv()

# å¯¼å…¥å‰é¢å®šä¹‰çš„å¤„ç†å™¨
# from scenario1 import FileLogHandler
# from scenario2 import PerformanceHandler
# from scenario3 import ErrorHandler


def main():
    """ç»„åˆä½¿ç”¨å¤šä¸ªå¤„ç†å™¨"""
    prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model | StrOutputParser()

    # åˆ›å»ºå¤šä¸ªå¤„ç†å™¨
    file_handler = FileLogHandler("combined_logs.jsonl")
    perf_handler = PerformanceHandler()
    error_handler = ErrorHandler()

    # ç»„åˆå¤„ç†å™¨
    handlers = [file_handler, perf_handler, error_handler]

    try:
        result = chain.invoke(
            {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
            config={"callbacks": handlers}
        )

        print(result)

        # æ‰“å°æŠ¥å‘Š
        perf_handler.print_report()
        error_handler.print_report()

        print("\næ—¥å¿—å·²å†™å…¥ combined_logs.jsonl")

    except Exception as e:
        print(f"Chain failed: {e}")
        error_handler.print_report()


if __name__ == "__main__":
    main()
```

### è¾“å‡ºç¤ºä¾‹

```
LCELæ˜¯LangChain Expression Languageçš„ç¼©å†™...

=== æ€§èƒ½æŠ¥å‘Š ===
é“¾æ€»å»¶è¿Ÿ: 1.25s
LLMå»¶è¿Ÿ: 1.23s
æ€»Token: 135
  Prompt: 15
  Completion: 120
ä¼°ç®—æˆæœ¬: $0.000074

=== é”™è¯¯æŠ¥å‘Š ===
æ€»é”™è¯¯æ•°: 0

æ—¥å¿—å·²å†™å…¥ combined_logs.jsonl
```

---

## å¼‚æ­¥å›è°ƒå¤„ç†å™¨

### å®Œæ•´ä»£ç 

```python
"""
å¼‚æ­¥å›è°ƒå¤„ç†å™¨
ä¸ä¼šé˜»å¡é“¾æ‰§è¡Œ
"""

from langchain_core.callbacks import AsyncCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import asyncio
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AsyncLogHandler(AsyncCallbackHandler):
    """å¼‚æ­¥æ—¥å¿—å¤„ç†å™¨"""

    async def on_chain_start(self, serialized, inputs, **kwargs):
        """é“¾å¼€å§‹ï¼ˆå¼‚æ­¥ï¼‰"""
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
        logger.info(f"Chain started: {inputs}")

    async def on_chain_end(self, outputs, **kwargs):
        """é“¾ç»“æŸï¼ˆå¼‚æ­¥ï¼‰"""
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
        logger.info(f"Chain completed: {str(outputs)[:50]}...")

    async def on_llm_end(self, response, **kwargs):
        """LLMç»“æŸï¼ˆå¼‚æ­¥ï¼‰"""
        tokens = response.llm_output.get("token_usage", {})
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
        logger.info(f"Tokens: {tokens}")


async def main():
    """å¼‚æ­¥ä½¿ç”¨ç¤ºä¾‹"""
    prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model

    # ä½¿ç”¨å¼‚æ­¥å¤„ç†å™¨
    handler = AsyncLogHandler()

    result = await chain.ainvoke(
        {"question": "ä»€ä¹ˆæ˜¯LCEL?"},
        config={"callbacks": [handler]}
    )

    print(result.content)


if __name__ == "__main__":
    asyncio.run(main())
```

---

## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
def on_chain_start(self, serialized, inputs, **kwargs):
    try:
        self._write_log(inputs)
    except Exception as e:
        logger.error(f"Failed to write log: {e}")
        # ä¸è¦è®©æ—¥å¿—é”™è¯¯å½±å“é“¾æ‰§è¡Œ
```

### 2. é™åˆ¶æ—¥å¿—å¤§å°

```python
def on_chain_end(self, outputs, **kwargs):
    # é™åˆ¶è¾“å‡ºé•¿åº¦
    output_str = str(outputs)[:200]
    self._write_log(output_str)
```

### 3. ä½¿ç”¨å¼‚æ­¥å¤„ç†æ…¢é€Ÿæ“ä½œ

```python
class AsyncDBHandler(AsyncCallbackHandler):
    async def on_chain_start(self, serialized, inputs, **kwargs):
        await self._write_to_database(inputs)
```

### 4. èµ„æºæ¸…ç†

```python
class FileHandler(BaseCallbackHandler):
    def __init__(self):
        self.file = open("logs.txt", "a")

    def __del__(self):
        self.file.close()
```

---

## æ€»ç»“

**æ ¸å¿ƒæŠ€å·§**ï¼š

1. **æ–‡ä»¶æ—¥å¿—**ï¼šä½¿ç”¨JSON Linesæ ¼å¼ï¼Œæ–¹ä¾¿åˆ†æ
2. **æ€§èƒ½è¿½è¸ª**ï¼šè®°å½•å»¶è¿Ÿå’ŒTokenä½¿ç”¨
3. **é”™è¯¯åˆ†ç±»**ï¼šåˆ†ç±»é”™è¯¯ï¼Œæ–¹ä¾¿å‘Šè­¦
4. **ç»„åˆä½¿ç”¨**ï¼šå¤šä¸ªå¤„ç†å™¨ååŒå·¥ä½œ

**æœ€ä½³å®è·µ**ï¼š

- ä½¿ç”¨å¼‚æ­¥å¤„ç†å™¨é¿å…é˜»å¡
- é™åˆ¶æ—¥å¿—å¤§å°
- é”™è¯¯å¤„ç†ä¸å½±å“é“¾æ‰§è¡Œ
- åŠæ—¶æ¸…ç†èµ„æº

---

**ç‰ˆæœ¬ä¿¡æ¯**
- LangChain: v0.3+ (2025-2026)
- Python: 3.13+
- æœ€åæ›´æ–°: 2026-02-20
