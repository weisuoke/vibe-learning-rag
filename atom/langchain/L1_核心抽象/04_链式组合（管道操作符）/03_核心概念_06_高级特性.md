# 核心概念6：高级特性

## 概述

管道操作符支持异步、批处理、流式等高级特性，满足生产环境的复杂需求。

**一句话定义**：管道操作符内置支持异步执行、批量处理、流式输出、配置传递、回调系统等企业级特性。

---

## 1. 异步执行（Async/Await）

### 1.1 异步调用基础

```python
import asyncio
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("讲个笑话")
model = ChatOpenAI()
parser = StrOutputParser()

chain = prompt | model | parser

# 异步调用
async def main():
    result = await chain.ainvoke({"topic": "AI"})
    print(result)

asyncio.run(main())
```

### 1.2 并发执行多个请求

```python
async def process_multiple():
    """并发处理多个请求"""
    tasks = [
        chain.ainvoke({"topic": "AI"}),
        chain.ainvoke({"topic": "Python"}),
        chain.ainvoke({"topic": "数据库"})
    ]

    results = await asyncio.gather(*tasks)
    return results

# 执行
results = asyncio.run(process_multiple())
# 3个请求并发执行，总时间 ≈ 单个请求时间
```

### 1.3 异步流式

```python
async def async_stream():
    """异步流式输出"""
    async for chunk in chain.astream({"topic": "AI"}):
        print(chunk, end="", flush=True)

asyncio.run(async_stream())
```

### 1.4 在 AI Agent 中的应用

```python
# 并发调用多个 Agent
async def multi_agent_system(query):
    """多 Agent 并发执行"""
    results = await asyncio.gather(
        research_agent.ainvoke(query),
        analysis_agent.ainvoke(query),
        summary_agent.ainvoke(query)
    )

    return {
        "research": results[0],
        "analysis": results[1],
        "summary": results[2]
    }
```

---

## 2. 批量处理（Batch Processing）

### 2.1 批量调用基础

```python
chain = prompt | model | parser

# 批量处理
inputs = [
    {"topic": "AI"},
    {"topic": "Python"},
    {"topic": "数据库"}
]

results = chain.batch(inputs)
# 返回: [result1, result2, result3]
```

### 2.2 批量处理的优势

```python
import time

# ❌ 低效：循环调用
start = time.time()
results = []
for input in inputs:
    result = chain.invoke(input)
    results.append(result)
print(f"循环调用耗时: {time.time() - start:.2f}秒")  # 假设 6秒

# ✅ 高效：批量调用
start = time.time()
results = chain.batch(inputs)
print(f"批量调用耗时: {time.time() - start:.2f}秒")  # 假设 2秒
```

**性能提升原因**：
- LLM API 支持批量请求（如 OpenAI Batch API）
- 减少网络往返次数
- 内部优化并行处理

### 2.3 批量配置

```python
# 配置批量大小
results = chain.batch(
    inputs,
    config={
        "max_concurrency": 5  # 最多5个并发请求
    }
)
```

### 2.4 在 AI Agent 中的应用

```python
# 批量评估
test_cases = [
    {"question": "什么是AI？"},
    {"question": "什么是机器学习？"},
    {"question": "什么是深度学习？"},
    # ... 100个测试用例
]

# 批量执行
results = rag_chain.batch(test_cases)

# 批量评估
for i, (test, result) in enumerate(zip(test_cases, results)):
    score = evaluate(test, result)
    print(f"测试 {i+1}: {score}")
```

---

## 3. 流式输出（Streaming）

### 3.1 流式调用基础

```python
chain = prompt | model | parser

# 流式输出
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)

# 输出: A I   是 人 工 智 能 ...（逐字输出）
```

### 3.2 流式的执行机制

```python
# 只有最后一个步骤流式
chain = prompt | model | parser

for chunk in chain.stream(input):
    print(chunk, end="")

# 执行流程:
# 1. prompt.invoke(input)  → 同步执行
# 2. model.invoke(...)     → 同步执行
# 3. parser.stream(...)    → 流式执行（只有最后一步）
```

### 3.3 自定义流式处理

```python
from langchain_core.runnables import RunnableLambda

def custom_stream(input):
    """自定义流式处理"""
    result = process(input)
    # 逐字符返回
    for char in result:
        yield char

stream_processor = RunnableLambda(custom_stream)

chain = prompt | model | parser | stream_processor

for chunk in chain.stream(input):
    print(chunk, end="")
```

### 3.4 在 AI Agent 中的应用

```python
# 实时对话系统
def chat_with_streaming(user_message):
    """流式对话"""
    print("AI: ", end="")
    for chunk in conversation_chain.stream(user_message):
        print(chunk, end="", flush=True)
    print()  # 换行

# 使用
chat_with_streaming("你好")
# 输出: AI: 你好！我是AI助手，有什么可以帮助你的吗？
```

---

## 4. 配置传递（Config Passing）

### 4.1 配置基础

```python
chain = prompt | model | parser

# 传递配置
result = chain.invoke(
    {"topic": "AI"},
    config={
        "tags": ["production", "rag"],
        "metadata": {"user_id": "123", "session_id": "abc"},
        "max_concurrency": 5
    }
)
```

### 4.2 常用配置选项

```python
config = {
    # 标签（用于追踪和过滤）
    "tags": ["production", "rag", "v1"],

    # 元数据（附加信息）
    "metadata": {
        "user_id": "user_123",
        "session_id": "session_abc",
        "environment": "production"
    },

    # 回调（监控和日志）
    "callbacks": [logger, metrics_collector],

    # 并发控制
    "max_concurrency": 5,

    # 运行名称（用于追踪）
    "run_name": "rag_query_20260218"
}

result = chain.invoke(input, config=config)
```

### 4.3 配置继承

```python
# 父链配置会传递给子链
parent_chain = (
    preprocessor
    | child_chain  # 会继承父链的配置
    | postprocessor
)

result = parent_chain.invoke(
    input,
    config={"tags": ["parent"]}
)
# child_chain 也会收到 tags=["parent"]
```

### 4.4 在 AI Agent 中的应用

```python
# 为不同环境配置不同参数
def get_config(environment="production"):
    """根据环境返回配置"""
    if environment == "production":
        return {
            "tags": ["production"],
            "max_concurrency": 10,
            "callbacks": [prod_logger, metrics_collector]
        }
    else:
        return {
            "tags": ["development"],
            "max_concurrency": 2,
            "callbacks": [dev_logger]
        }

# 使用
config = get_config("production")
result = rag_chain.invoke(query, config=config)
```

---

## 5. 回调系统（Callbacks）

### 5.1 回调基础

```python
from langchain.callbacks.base import BaseCallbackHandler

class CustomCallback(BaseCallbackHandler):
    """自定义回调"""

    def on_chain_start(self, serialized, inputs, **kwargs):
        print(f"链开始: {inputs}")

    def on_chain_end(self, outputs, **kwargs):
        print(f"链结束: {outputs}")

    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"LLM 开始: {prompts}")

    def on_llm_end(self, response, **kwargs):
        print(f"LLM 结束: {response}")

# 使用回调
callback = CustomCallback()
result = chain.invoke(
    input,
    config={"callbacks": [callback]}
)
```

### 5.2 内置回调

```python
from langchain.callbacks import StdOutCallbackHandler

# 标准输出回调
stdout_callback = StdOutCallbackHandler()

result = chain.invoke(
    input,
    config={"callbacks": [stdout_callback]}
)
# 自动打印每个步骤的输入输出
```

### 5.3 多个回调

```python
# 同时使用多个回调
result = chain.invoke(
    input,
    config={
        "callbacks": [
            logger_callback,      # 日志
            metrics_callback,     # 指标
            tracing_callback      # 追踪
        ]
    }
)
```

### 5.4 在 AI Agent 中的应用

```python
class AgentMonitor(BaseCallbackHandler):
    """Agent 监控回调"""

    def __init__(self):
        self.steps = []
        self.total_tokens = 0

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.steps.append({
            "type": "llm_start",
            "prompts": prompts,
            "timestamp": time.time()
        })

    def on_llm_end(self, response, **kwargs):
        self.total_tokens += response.llm_output.get("token_usage", {}).get("total_tokens", 0)
        self.steps.append({
            "type": "llm_end",
            "tokens": self.total_tokens,
            "timestamp": time.time()
        })

    def get_report(self):
        return {
            "total_steps": len(self.steps),
            "total_tokens": self.total_tokens,
            "duration": self.steps[-1]["timestamp"] - self.steps[0]["timestamp"]
        }

# 使用
monitor = AgentMonitor()
result = agent_chain.invoke(query, config={"callbacks": [monitor]})
print(monitor.get_report())
```

---

## 6. 错误处理与重试

### 6.1 基础错误处理

```python
try:
    result = chain.invoke(input)
except Exception as e:
    print(f"错误: {e}")
    # 降级处理
    result = fallback_chain.invoke(input)
```

### 6.2 自动重试

```python
from langchain_core.runnables import RunnableLambda

def with_retry(runnable, max_retries=3, delay=1):
    """添加重试功能"""
    def retry_invoke(input):
        for i in range(max_retries):
            try:
                return runnable.invoke(input)
            except Exception as e:
                if i == max_retries - 1:
                    raise
                print(f"重试 {i+1}/{max_retries}: {e}")
                time.sleep(delay)
        return None
    return RunnableLambda(retry_invoke)

# 使用
chain = prompt | with_retry(model, max_retries=3) | parser
```

### 6.3 降级策略

```python
from langchain_core.runnables import RunnableLambda

def with_fallback(primary, fallback):
    """添加降级功能"""
    def fallback_invoke(input):
        try:
            return primary.invoke(input)
        except Exception as e:
            print(f"主路径失败: {e}，使用降级路径")
            return fallback.invoke(input)
    return RunnableLambda(fallback_invoke)

# 使用
expensive_chain = prompt | gpt4 | parser
cheap_chain = prompt | gpt35 | parser

chain = with_fallback(expensive_chain, cheap_chain)
```

### 6.4 在 AI Agent 中的应用

```python
# 多级降级策略
def build_resilient_chain():
    """构建弹性链"""
    # 主路径: GPT-4
    primary = prompt | ChatOpenAI(model="gpt-4") | parser

    # 降级路径1: GPT-3.5
    fallback1 = prompt | ChatOpenAI(model="gpt-3.5-turbo") | parser

    # 降级路径2: 本地模型
    fallback2 = prompt | local_model | parser

    # 组合
    return with_fallback(
        with_retry(primary, max_retries=2),
        with_fallback(fallback1, fallback2)
    )

resilient_chain = build_resilient_chain()
```

---

## 7. 缓存机制

### 7.1 LLM 缓存

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# 启用缓存
set_llm_cache(InMemoryCache())

chain = prompt | model | parser

# 第一次调用（缓存未命中）
result1 = chain.invoke({"topic": "AI"})  # 调用 API

# 第二次调用（缓存命中）
result2 = chain.invoke({"topic": "AI"})  # 直接返回缓存
```

### 7.2 自定义缓存层

```python
from langchain_core.runnables import RunnableLambda

class CacheLayer:
    """自定义缓存层"""

    def __init__(self):
        self.cache = {}

    def get_or_compute(self, key, compute_fn):
        if key in self.cache:
            print(f"缓存命中: {key}")
            return self.cache[key]

        print(f"缓存未命中: {key}")
        result = compute_fn()
        self.cache[key] = result
        return result

# 使用
cache = CacheLayer()

def cached_invoke(input):
    key = str(input)
    return cache.get_or_compute(key, lambda: chain.invoke(input))

cached_chain = RunnableLambda(cached_invoke)
```

### 7.3 在 AI Agent 中的应用

```python
# RAG 系统的多级缓存
class RAGCache:
    """RAG 缓存系统"""

    def __init__(self):
        self.query_cache = {}      # 查询缓存
        self.retrieval_cache = {}  # 检索缓存
        self.generation_cache = {} # 生成缓存

    def get_or_generate(self, query):
        # 检查完整缓存
        if query in self.generation_cache:
            return self.generation_cache[query]

        # 检查检索缓存
        if query in self.retrieval_cache:
            context = self.retrieval_cache[query]
        else:
            context = retriever.invoke(query)
            self.retrieval_cache[query] = context

        # 生成答案
        answer = generation_chain.invoke({
            "context": context,
            "question": query
        })

        # 缓存答案
        self.generation_cache[query] = answer
        return answer
```

---

## 8. 监控与追踪

### 8.1 LangSmith 集成

```python
import os

# 配置 LangSmith
os.environ["LANGSMITH_API_KEY"] = "your_key"
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "my_project"

# 自动追踪
chain = prompt | model | parser
result = chain.invoke(input)

# 在 LangSmith UI 中查看完整追踪
```

### 8.2 自定义指标收集

```python
class MetricsCollector(BaseCallbackHandler):
    """指标收集器"""

    def __init__(self):
        self.metrics = {
            "total_calls": 0,
            "total_tokens": 0,
            "total_cost": 0,
            "avg_latency": 0
        }
        self.start_times = {}

    def on_llm_start(self, serialized, prompts, run_id, **kwargs):
        self.metrics["total_calls"] += 1
        self.start_times[run_id] = time.time()

    def on_llm_end(self, response, run_id, **kwargs):
        # 计算延迟
        latency = time.time() - self.start_times[run_id]
        self.metrics["avg_latency"] = (
            (self.metrics["avg_latency"] * (self.metrics["total_calls"] - 1) + latency)
            / self.metrics["total_calls"]
        )

        # 计算 token 和成本
        usage = response.llm_output.get("token_usage", {})
        tokens = usage.get("total_tokens", 0)
        self.metrics["total_tokens"] += tokens
        self.metrics["total_cost"] += tokens * 0.00002  # 假设价格

    def get_metrics(self):
        return self.metrics

# 使用
metrics = MetricsCollector()
result = chain.invoke(input, config={"callbacks": [metrics]})
print(metrics.get_metrics())
```

### 8.3 在 AI Agent 中的应用

```python
# 生产环境监控
class ProductionMonitor:
    """生产环境监控"""

    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.error_tracker = ErrorTracker()
        self.performance_monitor = PerformanceMonitor()

    def get_callbacks(self):
        return [
            self.metrics_collector,
            self.error_tracker,
            self.performance_monitor
        ]

    def get_dashboard(self):
        return {
            "metrics": self.metrics_collector.get_metrics(),
            "errors": self.error_tracker.get_errors(),
            "performance": self.performance_monitor.get_stats()
        }

# 使用
monitor = ProductionMonitor()
result = rag_chain.invoke(
    query,
    config={"callbacks": monitor.get_callbacks()}
)
print(monitor.get_dashboard())
```

---

## 9. 成本优化

### 9.1 模型选择策略

```python
from langchain_core.runnables import RunnableBranch

# 根据任务复杂度选择模型
model_selector = RunnableBranch(
    (lambda x: len(x) < 100, ChatOpenAI(model="gpt-4o-mini")),  # 简单任务
    (lambda x: len(x) < 500, ChatOpenAI(model="gpt-4o")),       # 中等任务
    ChatOpenAI(model="gpt-4")                                    # 复杂任务
)

chain = prompt | model_selector | parser
```

### 9.2 批处理降低成本

```python
# 使用批处理 API（成本降低 50%）
# 注意：需要等待批处理完成，不适合实时场景

# 收集请求
batch_inputs = []
for user_query in user_queries:
    batch_inputs.append({"question": user_query})

# 批量处理
results = chain.batch(batch_inputs)

# 成本对比:
# 单次调用: 100 个请求 × $0.01 = $1.00
# 批量调用: 1 个批处理 × $0.005 × 100 = $0.50
```

### 9.3 缓存减少调用

```python
# 启用缓存
set_llm_cache(InMemoryCache())

# 相同输入不会重复调用 API
for i in range(10):
    result = chain.invoke({"topic": "AI"})  # 只有第一次调用 API
```

### 9.4 在 AI Agent 中的应用

```python
# 成本优化的 RAG 系统
class CostOptimizedRAG:
    """成本优化的 RAG 系统"""

    def __init__(self):
        self.cache = InMemoryCache()
        self.cheap_model = ChatOpenAI(model="gpt-4o-mini")
        self.expensive_model = ChatOpenAI(model="gpt-4")

    def query(self, question, use_expensive=False):
        # 检查缓存
        cache_key = f"{question}_{use_expensive}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        # 选择模型
        model = self.expensive_model if use_expensive else self.cheap_model

        # 构建链
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | model
            | parser
        )

        # 执行
        result = chain.invoke(question)

        # 缓存结果
        self.cache[cache_key] = result
        return result
```

---

## 10. 高级特性总结

### 10.1 特性对照表

| 特性 | 用途 | 使用场景 | 性能影响 |
|------|------|----------|----------|
| **异步执行** | 并发处理 | 高并发场景 | 提升吞吐量 |
| **批量处理** | 批量调用 | 批量评估、数据标注 | 降低成本 50% |
| **流式输出** | 实时反馈 | 对话系统、实时生成 | 改善用户体验 |
| **配置传递** | 参数控制 | 环境切换、追踪 | 无影响 |
| **回调系统** | 监控日志 | 生产环境监控 | 轻微开销 |
| **错误处理** | 容错降级 | 提高可靠性 | 增加延迟 |
| **缓存机制** | 减少调用 | 重复查询 | 降低成本 90% |
| **监控追踪** | 可观测性 | 调试、优化 | 轻微开销 |

### 10.2 组合使用

```python
# 组合多个高级特性
async def production_ready_chain(query):
    """生产级链"""
    # 配置
    config = {
        "tags": ["production", "rag"],
        "metadata": {"user_id": "123"},
        "callbacks": [
            logger,
            metrics_collector,
            error_tracker
        ]
    }

    # 异步 + 流式 + 缓存 + 重试 + 降级
    chain = (
        cache_layer
        | with_retry(
            with_fallback(
                expensive_model_chain,
                cheap_model_chain
            ),
            max_retries=3
        )
    )

    # 流式输出
    async for chunk in chain.astream(query, config=config):
        yield chunk
```

---

## 11. 学习检查

完成本节后，检查是否掌握：

- [ ] 能使用异步执行提高并发性能
- [ ] 掌握批量处理降低成本
- [ ] 能实现流式输出改善用户体验
- [ ] 理解配置传递机制
- [ ] 能使用回调系统监控执行
- [ ] 掌握错误处理和重试策略
- [ ] 能实现缓存机制减少调用
- [ ] 理解成本优化策略

---

[Source: LangChain Official Docs - https://python.langchain.com/docs/concepts/]
[Source: Medium Production Guide - https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557]
