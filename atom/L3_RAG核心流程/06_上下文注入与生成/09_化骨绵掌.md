# 化骨绵掌

把上下文注入与生成拆成 10 个 2 分钟能看完的知识卡片。

---

## 卡片1：什么是上下文注入？

**一句话：** 上下文注入就是把检索到的文档"塞进"Prompt 里，让 LLM 基于这些资料回答问题。

**类比：**
```
考试时：
- 闭卷考试 = 普通 LLM（靠记忆）
- 开卷考试 = RAG（可以看资料）

上下文注入 = 把参考资料放到考生面前
```

**核心公式：**
```
RAG Prompt = System Prompt + Context（检索结果）+ User Query
```

**应用：** 这是 RAG 区别于普通 LLM 的核心——让模型"有据可查"而非"凭空回答"。

---

## 卡片2：Prompt 三层结构

**一句话：** 一个完整的 RAG Prompt 由 System（角色）、Context（资料）、Query（问题）三部分组成。

**结构图：**
```
┌─────────────────────────┐
│  System Prompt          │  ← 定义角色和规则
│  "你是专业的客服助手..." │
├─────────────────────────┤
│  Context                │  ← 检索到的参考资料
│  "【文档1】根据手册..." │
├─────────────────────────┤
│  User Query             │  ← 用户的问题
│  "年假政策是什么？"     │
└─────────────────────────┘
```

**代码：**
```python
prompt = f"""
{system_prompt}

【参考资料】
{context}

【问题】
{query}
"""
```

**应用：** 这个结构是所有 RAG 系统的基础，先掌握这个再学高级技巧。

---

## 卡片3：System Prompt 四要素

**一句话：** 好的 System Prompt 包含角色定义、行为约束、输出格式、引用规范四个要素。

**模板：**
```
【角色定义】你是{公司}的{角色}，专门回答{领域}问题。

【行为约束】
1. 只基于参考资料回答
2. 不知道就说不知道
3. 不要编造信息

【输出格式】
- 使用简洁的中文
- 重要信息加粗

【引用规范】
回答时标注来源：[来源：文档名]
```

**应用：** System Prompt 是"一次设置，多次复用"的，花时间打磨好它。

---

## 卡片4：Context 格式化

**一句话：** 检索结果不能直接拼接，需要结构化格式化，包含编号和来源信息。

**对比：**
```python
# ❌ 错误：直接拼接
context = doc1 + doc2 + doc3

# ✅ 正确：结构化格式
context = """
【文档1】来源：员工手册.pdf，第15页
员工入职满1年享有5天年假...

---

【文档2】来源：请假制度.docx，第3页
请假需提前3天申请...
"""
```

**好处：**
- LLM 能区分不同文档
- 答案可以引用具体来源
- 减少语义混淆

**应用：** 格式化是小投入大回报的优化，5分钟改进，效果提升明显。

---

## 卡片5：Lost in the Middle

**一句话：** LLM 对长上下文中间部分的注意力会下降，重要内容要放开头和结尾。

**数据：**
```
位置        | 信息利用率
------------|------------
开头 (前10%) | ~90%
中间 (40-60%)| ~50%  ← 注意力低谷
结尾 (后10%) | ~85%
```

**解决方案：**
```
最相关的文档 → 放开头（第1位）
次相关的文档 → 放结尾（最后1位）
其他文档     → 放中间
```

**应用：** 检索后不要直接用，先按相关性重排序。

---

## 卡片6：Token 预算管理

**一句话：** Context Window 有限，需要合理分配给 System、Context、Query 和输出。

**预算分配（以 12K tokens 为例）：**
```
System Prompt:  ~500 tokens  (4%)
Context:        ~8000 tokens (67%)
User Query:     ~500 tokens  (4%)
预留输出:       ~3000 tokens (25%)
```

**代码：**
```python
import tiktoken

def count_tokens(text, model="gpt-3.5-turbo"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# 计算剩余预算
context_budget = 12000 - system_tokens - query_tokens - 3000
```

**应用：** 超出预算会导致输出被截断，一定要预留输出空间。

---

## 卡片7：生成参数 Temperature

**一句话：** Temperature 控制输出的随机性，问答场景用 0，创意场景用 0.7-1.0。

**参数速查：**
```
temperature=0   → 确定性输出，每次一样（推荐问答）
temperature=0.3 → 轻微变化
temperature=0.7 → 平衡创意和准确
temperature=1.0 → 高随机性（创意写作）
```

**代码：**
```python
from langchain_openai import ChatOpenAI

# 问答场景
llm = ChatOpenAI(temperature=0)

# 创意场景
llm = ChatOpenAI(temperature=0.7)
```

**应用：** RAG 问答系统几乎都用 `temperature=0`，保证答案稳定。

---

## 卡片8：流式输出 Streaming

**一句话：** 流式输出让用户实时看到生成过程，提升体验，尤其适合长答案。

**对比：**
```
普通输出：等待3秒... 一次性显示全部答案
流式输出：立即开始，逐字显示，用户感知更快
```

**代码：**
```python
# LangChain 流式输出
for chunk in rag_chain.stream("你的问题"):
    print(chunk, end="", flush=True)

# OpenAI 原生流式
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages,
    stream=True  # 开启流式
)
for chunk in response:
    print(chunk.choices[0].delta.content, end="")
```

**应用：** 生产环境必备，用户体验提升明显。

---

## 卡片9：引用来源标注

**一句话：** 让 LLM 在回答中标注信息来源，便于用户验证，减少幻觉风险。

**Prompt 设计：**
```
【引用规范】
回答时必须标注信息来源，格式：[来源：文档名称，第X页]
如果某个信息来自多个文档，列出所有来源。
```

**输出示例：**
```
根据公司政策，入职满1年的员工享有5天带薪年假 [来源：员工手册.pdf，第15页]。
请假需提前3个工作日申请 [来源：请假制度.docx，第3页]。
```

**应用：** 引用标注是 RAG 的核心优势之一——答案可追溯、可验证。

---

## 卡片10：幻觉防控

**一句话：** 通过检索过滤、Prompt 约束、参数控制三层防线减少 LLM 编造信息。

**三层防线：**
```
┌─────────────────────────────────────┐
│  第1层：检索过滤                     │
│  - 相关性阈值 > 0.7                 │
│  - 宁缺毋滥，不塞无关内容            │
├─────────────────────────────────────┤
│  第2层：Prompt 约束                  │
│  - "只基于资料回答"                 │
│  - "不知道就说不知道"               │
├─────────────────────────────────────┤
│  第3层：参数控制                     │
│  - temperature=0                    │
│  - 要求引用来源                     │
└─────────────────────────────────────┘
```

**应用：** 幻觉是 RAG 的最大敌人，三层防线缺一不可。

---

## 10个卡片的学习路径

```
基础理解（卡片1-4）
├── 卡片1：什么是上下文注入
├── 卡片2：Prompt 三层结构
├── 卡片3：System Prompt 四要素
└── 卡片4：Context 格式化

进阶优化（卡片5-7）
├── 卡片5：Lost in the Middle
├── 卡片6：Token 预算管理
└── 卡片7：生成参数 Temperature

生产实践（卡片8-10）
├── 卡片8：流式输出 Streaming
├── 卡片9：引用来源标注
└── 卡片10：幻觉防控
```

---

**下一步：** [10_一句话总结](./10_一句话总结.md) - 总结与学习检查清单
