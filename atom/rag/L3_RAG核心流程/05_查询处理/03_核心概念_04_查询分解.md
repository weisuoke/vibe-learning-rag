# 核心概念 04：查询分解 (Query Decomposition)

## 概述

查询分解是将复杂查询拆分为多个简单子查询的技术。根据 arXiv 2025 的研究，查询分解在处理多跳推理和复杂问题时可以提升 25-45% 的准确率。

**定义：** 查询分解是将单一复杂查询分解为多个相互关联的子查询，逐步执行并聚合结果的过程。

---

## 为什么需要查询分解？

### 问题：复杂查询的挑战

**核心问题：** 复杂查询包含多个子意图，单次检索难以全面覆盖。

**示例：**

```
复杂查询："对比 LangChain 和 LlamaIndex 在 RAG 开发中的优缺点"

包含的子意图：
1. LangChain 的优势
2. LangChain 的劣势
3. LlamaIndex 的优势
4. LlamaIndex 的劣势
5. 两者的核心差异
```

如果直接用原始查询检索，可能：
- 召回的文档过于宽泛
- 无法针对每个子意图深入检索
- 结果缺乏结构化

---

## 查询分解的核心原理

### 原理 1：分而治之

**目标：** 将复杂问题分解为简单问题，逐个解决

**流程：**

```
复杂查询
    ↓
分解为 N 个子查询
    ↓
逐个执行子查询
    ↓
聚合子查询结果
    ↓
生成最终答案
```

---

### 原理 2：结构化检索

**目标：** 为每个子意图提供专门的检索策略

**示例：**

```
原始查询："RAG 系统的检索、生成和评估方法"

分解后：
1. "RAG 检索优化方法" → 检索相关文档
2. "RAG 生成质量提升" → 检索相关文档
3. "RAG 评估指标体系" → 检索相关文档

每个子查询独立检索，结果更精准
```

---

### 原理 3：依赖关系处理

**目标：** 处理子查询之间的依赖关系

**执行策略：**

1. **并行执行**：子查询相互独立
2. **串行执行**：后续查询依赖前面查询的结果
3. **混合执行**：部分并行，部分串行

---

## 查询分解的实现方法

### 方法 1：基于 LLM 的查询分解

```python
from openai import OpenAI

client = OpenAI()

DECOMPOSE_PROMPT = """你是一个查询分解专家。请将以下复杂查询分解为多个简单的子查询。

要求：
1. 每个子查询应该独立且明确
2. 子查询应该覆盖原始查询的所有方面
3. 子查询之间应该有逻辑关系
4. 每行一个子查询，不要编号

原始查询：{query}

子查询："""

def decompose_query(query: str) -> list[str]:
    """分解复杂查询"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": DECOMPOSE_PROMPT.format(query=query)}
        ],
        temperature=0.5
    )

    # 解析子查询
    sub_queries = response.choices[0].message.content.strip().split('\n')
    sub_queries = [q.strip() for q in sub_queries if q.strip()]

    return sub_queries
```

**测试：**

```python
query = "对比 LangChain 和 LlamaIndex 在 RAG 开发中的优缺点"
sub_queries = decompose_query(query)

print(f"原始查询: {query}\n")
print("分解后的子查询:")
for i, sq in enumerate(sub_queries, 1):
    print(f"{i}. {sq}")
```

**预期输出：**

```
原始查询: 对比 LangChain 和 LlamaIndex 在 RAG 开发中的优缺点

分解后的子查询:
1. LangChain 在 RAG 开发中的优势和特点
2. LangChain 在 RAG 开发中的劣势和限制
3. LlamaIndex 在 RAG 开发中的优势和特点
4. LlamaIndex 在 RAG 开发中的劣势和限制
5. LangChain 和 LlamaIndex 的核心差异对比
```

---

### 方法 2：结构化分解（推荐）

**策略：** 使用结构化输出确保分解质量

```python
import json

STRUCTURED_DECOMPOSE_PROMPT = """你是一个查询分解专家。请将以下复杂查询分解为多个子查询。

原始查询：{query}

请以 JSON 格式输出，格式如下：
{{
  "sub_queries": [
    {{
      "query": "子查询内容",
      "type": "类型（fact/concept/comparison/how-to）",
      "dependency": "依赖的子查询索引（如果独立则为 null）"
    }}
  ]
}}
"""

def decompose_query_structured(query: str) -> list[dict]:
    """结构化查询分解"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": STRUCTURED_DECOMPOSE_PROMPT.format(query=query)}
        ],
        temperature=0.5,
        response_format={"type": "json_object"}
    )

    result = json.loads(response.choices[0].message.content)
    return result.get("sub_queries", [])
```

---

### 方法 3：模板化分解

**适用场景：** 特定类型的查询（对比、因果、多步骤）

```python
# 对比类查询模板
COMPARISON_TEMPLATE = [
    "{A} 的优势",
    "{A} 的劣势",
    "{B} 的优势",
    "{B} 的劣势",
    "{A} 和 {B} 的核心差异"
]

def decompose_comparison(query: str, entity_a: str, entity_b: str) -> list[str]:
    """分解对比类查询"""
    return [
        template.format(A=entity_a, B=entity_b)
        for template in COMPARISON_TEMPLATE
    ]

# 使用
sub_queries = decompose_comparison(
    "对比 LangChain 和 LlamaIndex",
    entity_a="LangChain",
    entity_b="LlamaIndex"
)
```

---

## 子查询执行策略

### 策略 1：并行执行

**适用场景：** 子查询相互独立

```python
import chromadb

chroma_client = chromadb.Client()
collection = chroma_client.create_collection("docs")

def execute_parallel(sub_queries: list[str], k: int = 3) -> dict:
    """并行执行子查询"""
    results = {}

    for i, sub_query in enumerate(sub_queries):
        # 检索
        docs = collection.query(
            query_texts=[sub_query],
            n_results=k
        )

        results[f"sub_query_{i}"] = {
            "query": sub_query,
            "documents": docs['documents'][0]
        }

    return results
```

---

### 策略 2：串行执行

**适用场景：** 后续查询依赖前面查询的结果

```python
def execute_sequential(sub_queries: list[str], k: int = 3) -> dict:
    """串行执行子查询"""
    results = {}
    context = ""  # 累积上下文

    for i, sub_query in enumerate(sub_queries):
        # 如果有上下文，将其添加到查询中
        if context:
            enhanced_query = f"{context}\n\n{sub_query}"
        else:
            enhanced_query = sub_query

        # 检索
        docs = collection.query(
            query_texts=[enhanced_query],
            n_results=k
        )

        # 保存结果
        results[f"sub_query_{i}"] = {
            "query": sub_query,
            "documents": docs['documents'][0]
        }

        # 更新上下文
        context += f"\n{sub_query}: {docs['documents'][0][0][:200]}..."

    return results
```

---

### 策略 3：混合执行

**适用场景：** 部分子查询独立，部分依赖

```python
def execute_hybrid(sub_queries: list[dict], k: int = 3) -> dict:
    """混合执行子查询"""
    results = {}
    completed = set()

    # 按依赖关系排序
    sorted_queries = topological_sort(sub_queries)

    for query_info in sorted_queries:
        query_id = query_info['id']
        query_text = query_info['query']
        dependency = query_info.get('dependency')

        # 如果有依赖，等待依赖完成
        if dependency and dependency not in completed:
            continue

        # 检索
        docs = collection.query(
            query_texts=[query_text],
            n_results=k
        )

        results[query_id] = {
            "query": query_text,
            "documents": docs['documents'][0]
        }

        completed.add(query_id)

    return results
```

---

## 结果聚合策略

### 策略 1：简单拼接

```python
def aggregate_simple(results: dict) -> str:
    """简单拼接子查询结果"""
    aggregated = []

    for key, value in results.items():
        query = value['query']
        docs = value['documents']

        aggregated.append(f"## {query}\n")
        for doc in docs:
            aggregated.append(f"- {doc}\n")

    return "\n".join(aggregated)
```

---

### 策略 2：LLM 聚合（推荐）

```python
def aggregate_with_llm(
    original_query: str,
    results: dict
) -> str:
    """使用 LLM 聚合子查询结果"""
    # 构建上下文
    context_parts = []

    for key, value in results.items():
        query = value['query']
        docs = value['documents']

        context_parts.append(f"子问题：{query}")
        context_parts.append("相关信息：")
        for doc in docs:
            context_parts.append(f"- {doc}")
        context_parts.append("")

    context = "\n".join(context_parts)

    # 生成最终答案
    prompt = f"""基于以下子问题的检索结果，回答原始问题。

原始问题：{original_query}

子问题检索结果：
{context}

请综合以上信息，给出完整、结构化的答案："""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )

    return response.choices[0].message.content.strip()
```

---

### 策略 3：结构化聚合

```python
def aggregate_structured(
    original_query: str,
    results: dict
) -> dict:
    """结构化聚合子查询结果"""
    # 对于对比类查询，使用结构化聚合
    if "对比" in original_query or "vs" in original_query.lower():
        return {
            "entity_a": {
                "advantages": results.get("sub_query_0", {}).get("documents", []),
                "disadvantages": results.get("sub_query_1", {}).get("documents", [])
            },
            "entity_b": {
                "advantages": results.get("sub_query_2", {}).get("documents", []),
                "disadvantages": results.get("sub_query_3", {}).get("documents", [])
            },
            "differences": results.get("sub_query_4", {}).get("documents", [])
        }

    # 默认聚合
    return results
```

---

## 2025 年研究：Query Decomposition for RAG

### 核心发现

根据 arXiv 2025 的研究论文：

**性能提升：**
- 多跳问答任务：准确率提升 20-30%
- 复杂推理任务：准确率提升 25-45%
- 对比分析任务：准确率提升 30-50%

**关键挑战：**

1. **语义稀释问题**：过度分解导致子查询失去原始意图
2. **结果聚合困难**：如何有效合并子查询结果
3. **计算成本增加**：子查询数量与检索成本成正比

**引用来源：**
- [Query Decomposition for RAG](https://arxiv.org/abs/2510.18633) - arXiv 2025

---

### 最佳实践

#### 1. 控制分解粒度

```python
def adaptive_decompose(query: str) -> list[str]:
    """自适应查询分解"""
    # 评估查询复杂度
    complexity = estimate_complexity(query)

    if complexity < 3:
        # 简单查询，不分解
        return [query]
    elif complexity < 6:
        # 中等复杂度，分解为 2-3 个子查询
        return decompose_query(query, max_sub_queries=3)
    else:
        # 高复杂度，分解为 3-5 个子查询
        return decompose_query(query, max_sub_queries=5)
```

---

#### 2. 保持语义完整性

```python
SEMANTIC_PRESERVING_PROMPT = """分解查询时，必须保持原始查询的语义完整性。

规则：
1. 每个子查询必须与原始查询相关
2. 子查询的并集应该覆盖原始查询的所有方面
3. 不要添加原始查询中没有的意图

原始查询：{query}

子查询："""
```

---

#### 3. 优化聚合策略

```python
def smart_aggregate(
    original_query: str,
    results: dict,
    strategy: str = "auto"
) -> str:
    """智能聚合策略"""
    if strategy == "auto":
        # 根据查询类型自动选择策略
        if "对比" in original_query:
            strategy = "structured"
        elif len(results) > 5:
            strategy = "llm"
        else:
            strategy = "simple"

    if strategy == "simple":
        return aggregate_simple(results)
    elif strategy == "llm":
        return aggregate_with_llm(original_query, results)
    elif strategy == "structured":
        return aggregate_structured(original_query, results)
```

---

## 查询分解的完整流程

```python
from dataclasses import dataclass
from typing import List, Dict, Optional

@dataclass
class SubQuery:
    """子查询"""
    id: str
    query: str
    type: str
    dependency: Optional[str] = None

def query_decomposition_rag(query: str) -> str:
    """带查询分解的 RAG"""
    print(f"原始查询: {query}\n")

    # 1. 判断是否需要分解
    if not should_decompose(query):
        print("查询较简单，不需要分解")
        # 直接检索
        docs = collection.query(query_texts=[query], n_results=5)
        context = "\n\n".join(docs['documents'][0])
        return generate_answer(query, context)

    # 2. 分解查询
    sub_queries = decompose_query_structured(query)
    print(f"分解为 {len(sub_queries)} 个子查询:")
    for i, sq in enumerate(sub_queries, 1):
        print(f"  {i}. {sq['query']}")

    # 3. 执行子查询
    print("\n执行子查询...")
    results = execute_parallel(
        [sq['query'] for sq in sub_queries],
        k=3
    )

    # 4. 聚合结果
    print("\n聚合结果...")
    final_answer = aggregate_with_llm(query, results)

    return final_answer

def should_decompose(query: str) -> bool:
    """判断是否需要分解"""
    # 简单规则
    if len(query.split()) < 10:
        return False

    # 检查是否包含对比关键词
    comparison_keywords = ["对比", "比较", "区别", "优缺点", "vs"]
    if any(kw in query for kw in comparison_keywords):
        return True

    # 检查是否包含多个独立概念
    # （这里简化处理，实际可以用 NER）
    return False
```

---

## 评估查询分解的效果

### 评估指标

#### 1. 子查询质量

```python
def evaluate_sub_query_quality(
    original_query: str,
    sub_queries: list[str]
) -> dict:
    """评估子查询质量"""
    # 1. 覆盖率：子查询是否覆盖原始查询的所有方面
    coverage = calculate_coverage(original_query, sub_queries)

    # 2. 独立性：子查询之间的相似度
    independence = calculate_independence(sub_queries)

    # 3. 相关性：子查询与原始查询的相关性
    relevance = calculate_relevance(original_query, sub_queries)

    return {
        "coverage": coverage,
        "independence": independence,
        "relevance": relevance
    }
```

#### 2. 端到端效果

```python
def evaluate_end_to_end(
    test_queries: list[dict]
) -> dict:
    """评估端到端效果"""
    correct = 0
    total = len(test_queries)

    for test_case in test_queries:
        query = test_case["query"]
        expected_answer = test_case["answer"]

        # 使用查询分解
        answer = query_decomposition_rag(query)

        # 评估答案质量
        if is_correct(answer, expected_answer):
            correct += 1

    accuracy = correct / total

    return {
        "accuracy": accuracy,
        "correct": correct,
        "total": total
    }
```

---

## 常见问题与解决方案

### 问题 1：过度分解

**原因：** 分解粒度过细，子查询失去语义

**解决方案：**

```python
# 限制子查询数量
sub_queries = decompose_query(query, max_sub_queries=5)

# 合并相似的子查询
sub_queries = merge_similar_queries(sub_queries, threshold=0.8)
```

---

### 问题 2：聚合困难

**原因：** 子查询结果难以合并

**解决方案：**

```python
# 使用 LLM 聚合
final_answer = aggregate_with_llm(original_query, results)

# 或使用结构化聚合
structured_result = aggregate_structured(original_query, results)
```

---

### 问题 3：计算成本高

**原因：** 子查询数量多，检索成本高

**解决方案：**

```python
# 1. 减少子查询数量
sub_queries = decompose_query(query, max_sub_queries=3)

# 2. 减少每个子查询的返回数量
results = execute_parallel(sub_queries, k=2)

# 3. 使用缓存
@lru_cache(maxsize=1000)
def decompose_query_cached(query: str) -> list[str]:
    return decompose_query(query)
```

---

## 查询分解 vs 多查询生成

### 对比

| 特性 | 查询分解 | 多查询生成 |
|------|----------|------------|
| 目标 | 分解复杂查询 | 扩展单一查询 |
| 适用场景 | 复杂、多步骤查询 | 简单、单一意图查询 |
| 子查询关系 | 有逻辑关系 | 相互独立 |
| 执行策略 | 串行或混合 | 并行 |
| 聚合难度 | 高 | 低 |
| 计算成本 | 高 | 中 |
| 效果提升 | 25-45% | 20-40% |

### 选择建议

```python
def choose_strategy(query: str) -> str:
    """选择查询处理策略"""
    # 1. 简单查询：查询改写
    if len(query.split()) < 5:
        return "rewrite"

    # 2. 复杂查询：查询分解
    if any(kw in query for kw in ["对比", "步骤", "流程", "因果"]):
        return "decompose"

    # 3. 中等查询：多查询生成
    return "multi-query"
```

---

## 总结

### 关键要点

1. **适用场景**：复杂、多步骤、对比类查询
2. **核心原理**：分而治之、结构化检索、依赖关系处理
3. **执行策略**：并行、串行、混合
4. **聚合策略**：简单拼接、LLM 聚合、结构化聚合
5. **2025 研究**：准确率提升 25-45%，但需注意语义稀释
6. **最佳实践**：控制分解粒度、保持语义完整性、优化聚合策略

### 实战建议

1. **判断是否需要分解**：不是所有查询都需要分解
2. **控制子查询数量**：3-5 个最佳
3. **选择合适的执行策略**：根据依赖关系选择并行或串行
4. **使用 LLM 聚合**：提升聚合质量
5. **持续评估**：对比分解前后的效果

### 避免的误区

1. **不要过度分解**：简单查询不需要分解
2. **不要忽略语义完整性**：分解后的子查询应保持原意
3. **不要忽略聚合质量**：聚合策略很重要
4. **不要忽略成本**：查询分解增加计算成本

---

**记住：** 查询分解是处理复杂查询的有效方法，但要根据查询复杂度决定是否使用。简单查询用查询改写，中等查询用多查询生成，复杂查询用查询分解。

---

**引用来源汇总：**
- [Query Decomposition for RAG](https://arxiv.org/abs/2510.18633) - arXiv 2025
