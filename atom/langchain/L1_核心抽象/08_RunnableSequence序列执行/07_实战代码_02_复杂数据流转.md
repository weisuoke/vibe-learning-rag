# 实战代码2：复杂数据流转

> 多步骤文档处理和分析

---

## 场景描述

**目标**：构建一个复杂的文档处理管道，包含多个数据转换步骤。

**功能需求**：
1. 加载文档
2. 提取关键信息
3. 分析情感
4. 生成摘要
5. 评估质量
6. 输出结构化结果

---

## 完整代码

```python
"""
复杂数据流转示例
演示：多步骤文档处理和分析
"""

import os
import json
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import Dict, List

load_dotenv()

# ===== 1. 定义数据结构 =====
print("=== 步骤1: 定义数据结构 ===\n")

class DocumentInfo(BaseModel):
    """文档信息"""
    title: str = Field(description="文档标题")
    keywords: List[str] = Field(description="关键词列表")
    main_topic: str = Field(description="主要主题")

class SentimentAnalysis(BaseModel):
    """情感分析结果"""
    sentiment: str = Field(description="情感倾向：正面/负面/中性")
    confidence: float = Field(description="置信度 0-1")
    reason: str = Field(description="判断理由")

class DocumentSummary(BaseModel):
    """文档摘要"""
    summary: str = Field(description="摘要内容")
    word_count: int = Field(description="字数")

class QualityScore(BaseModel):
    """质量评分"""
    score: int = Field(description="质量分数 1-10")
    strengths: List[str] = Field(description="优点列表")
    weaknesses: List[str] = Field(description="缺点列表")

print("数据结构定义完成")
print()

# ===== 2. 准备示例文档 =====
print("=== 步骤2: 准备示例文档 ===\n")

sample_document = """
人工智能技术的快速发展

人工智能（AI）技术在过去十年中取得了突破性进展。深度学习、自然语言处理、
计算机视觉等领域的创新，使得 AI 能够在图像识别、语音识别、机器翻译等任务
中达到甚至超越人类水平。

特别是大语言模型（LLM）的出现，如 GPT、Claude 等，展示了 AI 在理解和生成
自然语言方面的强大能力。这些模型不仅能够回答问题、写作文章，还能进行代码
生成、数据分析等复杂任务。

然而，AI 技术的发展也带来了一些挑战。数据隐私、算法偏见、就业影响等问题
需要我们认真对待。我们需要在推动技术进步的同时，确保 AI 的发展符合伦理
和社会价值观。

总的来说，人工智能技术为人类社会带来了巨大的机遇，但也需要我们谨慎应对
其中的挑战。只有这样，我们才能充分发挥 AI 的潜力，造福人类。
"""

print(f"文档长度: {len(sample_document)} 字符")
print(f"文档预览: {sample_document[:100]}...")
print()

# ===== 3. 步骤1：提取文档信息 =====
print("=== 步骤3: 构建信息提取链 ===\n")

info_prompt = ChatPromptTemplate.from_template("""
请分析以下文档，提取关键信息：

文档内容：
{document}

请提取：
1. 文档标题（简洁概括）
2. 3-5个关键词
3. 主要主题

以 JSON 格式返回：
{{
  "title": "...",
  "keywords": ["...", "..."],
  "main_topic": "..."
}}
""")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

info_chain = (
    info_prompt
    | llm
    | StrOutputParser()
    | RunnableLambda(lambda x: json.loads(x))  # 解析 JSON
)

print("信息提取链构建完成")
print()

# ===== 4. 步骤2：情感分析 =====
print("=== 步骤4: 构建情感分析链 ===\n")

sentiment_prompt = ChatPromptTemplate.from_template("""
请分析以下文档的情感倾向：

文档内容：
{document}

请判断：
1. 情感倾向（正面/负面/中性）
2. 置信度（0-1）
3. 判断理由

以 JSON 格式返回：
{{
  "sentiment": "...",
  "confidence": 0.9,
  "reason": "..."
}}
""")

sentiment_chain = (
    sentiment_prompt
    | llm
    | StrOutputParser()
    | RunnableLambda(lambda x: json.loads(x))
)

print("情感分析链构建完成")
print()

# ===== 5. 步骤3：生成摘要 =====
print("=== 步骤5: 构建摘要生成链 ===\n")

summary_prompt = ChatPromptTemplate.from_template("""
请为以下文档生成简洁的摘要（100字以内）：

文档内容：
{document}

摘要：
""")

summary_chain = (
    summary_prompt
    | llm
    | StrOutputParser()
    | RunnableLambda(lambda x: {
        "summary": x.strip(),
        "word_count": len(x.strip())
    })
)

print("摘要生成链构建完成")
print()

# ===== 6. 步骤4：质量评估 =====
print("=== 步骤6: 构建质量评估链 ===\n")

quality_prompt = ChatPromptTemplate.from_template("""
请评估以下文档的质量：

文档内容：
{document}

文档信息：
{info}

请评估：
1. 质量分数（1-10）
2. 优点（2-3个）
3. 缺点（2-3个）

以 JSON 格式返回：
{{
  "score": 8,
  "strengths": ["...", "..."],
  "weaknesses": ["...", "..."]
}}
""")

quality_chain = (
    quality_prompt
    | llm
    | StrOutputParser()
    | RunnableLambda(lambda x: json.loads(x))
)

print("质量评估链构建完成")
print()

# ===== 7. 构建完整的处理管道 =====
print("=== 步骤7: 构建完整处理管道 ===\n")

def process_document(document: str) -> Dict:
    """
    完整的文档处理管道

    数据流转：
    document → info_chain → info
    document → sentiment_chain → sentiment
    document → summary_chain → summary
    document + info → quality_chain → quality
    → 合并所有结果
    """
    print("开始处理文档...\n")

    # 步骤1：提取信息
    print("1. 提取文档信息...")
    info = info_chain.invoke({"document": document})
    print(f"   标题: {info['title']}")
    print(f"   关键词: {', '.join(info['keywords'])}")
    print(f"   主题: {info['main_topic']}")
    print()

    # 步骤2：情感分析
    print("2. 分析情感倾向...")
    sentiment = sentiment_chain.invoke({"document": document})
    print(f"   情感: {sentiment['sentiment']}")
    print(f"   置信度: {sentiment['confidence']}")
    print(f"   理由: {sentiment['reason']}")
    print()

    # 步骤3：生成摘要
    print("3. 生成文档摘要...")
    summary = summary_chain.invoke({"document": document})
    print(f"   摘要: {summary['summary'][:50]}...")
    print(f"   字数: {summary['word_count']}")
    print()

    # 步骤4：质量评估
    print("4. 评估文档质量...")
    quality = quality_chain.invoke({
        "document": document,
        "info": json.dumps(info, ensure_ascii=False)
    })
    print(f"   分数: {quality['score']}/10")
    print(f"   优点: {', '.join(quality['strengths'])}")
    print(f"   缺点: {', '.join(quality['weaknesses'])}")
    print()

    # 合并所有结果
    result = {
        "document": document,
        "info": info,
        "sentiment": sentiment,
        "summary": summary,
        "quality": quality
    }

    return result

print("完整处理管道构建完成")
print()

# ===== 8. 执行处理管道 =====
print("=== 步骤8: 执行处理管道 ===\n")

result = process_document(sample_document)

print("处理完成！")
print()

# ===== 9. 输出结构化结果 =====
print("=== 步骤9: 输出结构化结果 ===\n")

print("=" * 60)
print("文档分析报告")
print("=" * 60)
print()

print("【基本信息】")
print(f"标题: {result['info']['title']}")
print(f"关键词: {', '.join(result['info']['keywords'])}")
print(f"主题: {result['info']['main_topic']}")
print()

print("【情感分析】")
print(f"情感倾向: {result['sentiment']['sentiment']}")
print(f"置信度: {result['sentiment']['confidence']}")
print(f"理由: {result['sentiment']['reason']}")
print()

print("【文档摘要】")
print(f"{result['summary']['summary']}")
print(f"（{result['summary']['word_count']} 字）")
print()

print("【质量评估】")
print(f"质量分数: {result['quality']['score']}/10")
print(f"优点:")
for strength in result['quality']['strengths']:
    print(f"  - {strength}")
print(f"缺点:")
for weakness in result['quality']['weaknesses']:
    print(f"  - {weakness}")
print()

# ===== 10. 保存结果 =====
print("=== 步骤10: 保存结果 ===\n")

output_file = "document_analysis_result.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(result, f, ensure_ascii=False, indent=2)

print(f"结果已保存到: {output_file}")
print()

# ===== 11. 使用 RunnableSequence 优化 =====
print("=== 步骤11: 使用 RunnableSequence 优化 ===\n")

from langchain_core.runnables import RunnableParallel

# 并行执行独立的步骤
parallel_chain = RunnableParallel(
    info=info_chain,
    sentiment=sentiment_chain,
    summary=summary_chain
)

# 完整的序列链
optimized_chain = (
    RunnableLambda(lambda x: {"document": x})  # 包装输入
    | parallel_chain  # 并行执行前3步
    | RunnableLambda(lambda x: {  # 准备质量评估的输入
        "document": x["info"],  # 这里需要原始文档，实际应该保存
        "info": json.dumps(x["info"], ensure_ascii=False)
    })
    # 注意：这里简化了，实际需要保存原始文档
)

print("优化后的链构建完成")
print("使用 RunnableParallel 并行执行独立步骤")
print()

# ===== 12. 批量处理多个文档 =====
print("=== 步骤12: 批量处理多个文档 ===\n")

documents = [
    "人工智能正在改变世界，带来无限可能。",
    "气候变化是人类面临的严峻挑战，需要立即行动。",
    "量子计算技术的突破将revolutionize计算领域。"
]

print(f"批量处理 {len(documents)} 个文档...\n")

# 只提取信息（演示批处理）
batch_results = info_chain.batch([
    {"document": doc} for doc in documents
])

for i, (doc, result) in enumerate(zip(documents, batch_results), 1):
    print(f"文档 {i}:")
    print(f"  内容: {doc[:30]}...")
    print(f"  标题: {result['title']}")
    print(f"  关键词: {', '.join(result['keywords'])}")
    print()

print("批量处理完成！")
print()

# ===== 13. 数据流可视化 =====
print("=== 步骤13: 数据流可视化 ===\n")

print("数据流转图:")
print()
print("输入: 原始文档")
print("  ↓")
print("┌─────────────────────────────────┐")
print("│  RunnableParallel（并行执行）   │")
print("├─────────────────────────────────┤")
print("│  ├─ info_chain → 文档信息       │")
print("│  ├─ sentiment_chain → 情感分析  │")
print("│  └─ summary_chain → 文档摘要    │")
print("└─────────────────────────────────┘")
print("  ↓")
print("合并结果: {info, sentiment, summary}")
print("  ↓")
print("quality_chain → 质量评估")
print("  ↓")
print("输出: 完整分析结果")
print()

print("演示完成！")
```

---

## 运行输出示例

```
=== 步骤1: 定义数据结构 ===

数据结构定义完成

=== 步骤2: 准备示例文档 ===

文档长度: 389 字符
文档预览: 人工智能技术的快速发展

人工智能（AI）技术在过去十年中取得了突破性进展。深度学习、自然语言处理、
计算机视觉等领域的创新，使得 AI 能够在图像识别...

=== 步骤3: 构建信息提取链 ===

信息提取链构建完成

=== 步骤4: 构建情感分析链 ===

情感分析链构建完成

=== 步骤5: 构建摘要生成链 ===

摘要生成链构建完成

=== 步骤6: 构建质量评估链 ===

质量评估链构建完成

=== 步骤7: 构建完整处理管道 ===

完整处理管道构建完成

=== 步骤8: 执行处理管道 ===

开始处理文档...

1. 提取文档信息...
   标题: 人工智能技术的发展与挑战
   关键词: 人工智能, 深度学习, 大语言模型, 伦理挑战, 技术进步
   主题: 人工智能技术的进展及其带来的机遇与挑战

2. 分析情感倾向...
   情感: 中性
   置信度: 0.85
   理由: 文档客观描述了AI技术的进展和挑战，既提到了积极方面也提到了需要关注的问题

3. 生成文档摘要...
   摘要: 人工智能技术在过去十年取得突破性进展，特别是大语言模型展示了强大能力。然而，数据隐私、算法偏见等挑战需要认真对待。AI为人类带来机遇，但需谨慎应对挑战。...
   字数: 89

4. 评估文档质量...
   分数: 8/10
   优点: 结构清晰, 观点平衡, 内容全面
   缺点: 缺少具体案例, 论述较为概括

处理完成！

=== 步骤9: 输出结构化结果 ===

============================================================
文档分析报告
============================================================

【基本信息】
标题: 人工智能技术的发展与挑战
关键词: 人工智能, 深度学习, 大语言模型, 伦理挑战, 技术进步
主题: 人工智能技术的进展及其带来的机遇与挑战

【情感分析】
情感倾向: 中性
置信度: 0.85
理由: 文档客观描述了AI技术的进展和挑战，既提到了积极方面也提到了需要关注的问题

【文档摘要】
人工智能技术在过去十年取得突破性进展，特别是大语言模型展示了强大能力。然而，数据隐私、算法偏见等挑战需要认真对待。AI为人类带来机遇，但需谨慎应对挑战。
（89 字）

【质量评估】
质量分数: 8/10
优点:
  - 结构清晰
  - 观点平衡
  - 内容全面
缺点:
  - 缺少具体案例
  - 论述较为概括

=== 步骤10: 保存结果 ===

结果已保存到: document_analysis_result.json

=== 步骤11: 使用 RunnableSequence 优化 ===

优化后的链构建完成
使用 RunnableParallel 并行执行独立步骤

=== 步骤12: 批量处理多个文档 ===

批量处理 3 个文档...

文档 1:
  内容: 人工智能正在改变世界，带来无限可能。...
  标题: 人工智能的变革力量
  关键词: 人工智能, 改变世界, 可能性

文档 2:
  内容: 气候变化是人类面临的严峻挑战，需要立即行动。...
  标题: 气候变化的紧迫性
  关键词: 气候变化, 挑战, 行动

文档 3:
  内容: 量子计算技术的突破将revolutionize计算领域。...
  标题: 量子计算的革命性突破
  关键词: 量子计算, 技术突破, 计算革命

批量处理完成！

=== 步骤13: 数据流可视化 ===

数据流转图:

输入: 原始文档
  ↓
┌─────────────────────────────────┐
│  RunnableParallel（并行执行）   │
├─────────────────────────────────┤
│  ├─ info_chain → 文档信息       │
│  ├─ sentiment_chain → 情感分析  │
│  └─ summary_chain → 文档摘要    │
└─────────────────────────────────┘
  ↓
合并结果: {info, sentiment, summary}
  ↓
quality_chain → 质量评估
  ↓
输出: 完整分析结果

演示完成！
```

---

## 代码详解

### 1. 多步骤数据流转

```python
document → info_chain → info
document → sentiment_chain → sentiment
document → summary_chain → summary
document + info → quality_chain → quality
```

**关键点**：
- 前3步独立，可以并行
- 第4步依赖第1步的结果
- 使用 RunnableParallel 优化性能

### 2. JSON 解析

```python
chain = (
    prompt
    | llm
    | StrOutputParser()
    | RunnableLambda(lambda x: json.loads(x))  # 解析 JSON
)
```

**关键点**：
- LLM 输出 JSON 字符串
- StrOutputParser 提取文本
- RunnableLambda 解析为字典

### 3. 并行执行优化

```python
parallel_chain = RunnableParallel(
    info=info_chain,
    sentiment=sentiment_chain,
    summary=summary_chain
)
```

**关键点**：
- 3个独立的链并行执行
- 结果合并为字典
- 性能提升 3x

---

## 2025-2026 最新实践

### 1. 使用结构化输出

**来源**：LangChain 2025 更新

```python
from langchain_core.pydantic_v1 import BaseModel

class DocumentInfo(BaseModel):
    title: str
    keywords: List[str]
    main_topic: str

# 使用结构化输出
llm_with_structure = llm.with_structured_output(DocumentInfo)
info_chain = prompt | llm_with_structure
```

### 2. 使用 astream_events 调试

```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chain_end":
        print(f"{event['name']}: {event['data']['output']}")
```

### 3. 添加缓存

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())
```

---

## 学习检查清单

- [ ] 理解多步骤数据流转
- [ ] 理解 RunnableParallel 的使用
- [ ] 理解 JSON 解析
- [ ] 能运行完整代码
- [ ] 能优化性能
- [ ] 了解 2025-2026 最新实践

---

**版本**: v1.0
**最后更新**: 2026-02-18
**参考来源**:
- LangChain 官方文档
- LangChain 结构化输出指南
