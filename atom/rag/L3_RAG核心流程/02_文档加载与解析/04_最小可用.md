# æœ€å°å¯ç”¨çŸ¥è¯†

> æŒæ¡20%æ ¸å¿ƒçŸ¥è¯†ï¼Œè§£å†³80%æ–‡æ¡£åŠ è½½é—®é¢˜

---

## æ ¸å¿ƒç†å¿µ

**æœ€å°å¯ç”¨ = å¿«é€Ÿä¸Šæ‰‹ + å®é™…å¯ç”¨ + ä¸ºè¿›é˜¶æ‰“åŸºç¡€**

æŒæ¡ä»¥ä¸‹5ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œå°±èƒ½æ„å»ºåŸºç¡€çš„RAGæ–‡æ¡£åŠ è½½ç®¡é“ã€‚

---

## 4.1 ä½¿ç”¨LangChainé¢„æ„å»ºLoader

**æ ¸å¿ƒæ¦‚å¿µï¼š** LangChainæä¾›100+é¢„æ„å»ºçš„æ–‡æ¡£åŠ è½½å™¨ï¼Œè¦†ç›–å¸¸è§æ ¼å¼

```python
"""
æœ€å°å¯ç”¨ç¤ºä¾‹ï¼šä½¿ç”¨LangChainåŠ è½½å¸¸è§æ–‡æ¡£
"""

from langchain.document_loaders import (
    PyPDFLoader,           # PDFæ–‡æ¡£
    TextLoader,            # çº¯æ–‡æœ¬
    Docx2txtLoader,        # Wordæ–‡æ¡£
    UnstructuredHTMLLoader # HTMLç½‘é¡µ
)

# ===== 1. åŠ è½½PDF =====
pdf_loader = PyPDFLoader("report.pdf")
pdf_docs = pdf_loader.load()

# ===== 2. åŠ è½½æ–‡æœ¬æ–‡ä»¶ =====
txt_loader = TextLoader("notes.txt", encoding="utf-8")
txt_docs = txt_loader.load()

# ===== 3. åŠ è½½Wordæ–‡æ¡£ =====
docx_loader = Docx2txtLoader("proposal.docx")
docx_docs = docx_loader.load()

# ===== 4. åŠ è½½HTML =====
html_loader = UnstructuredHTMLLoader("article.html")
html_docs = html_loader.load()

# æ‰€æœ‰Loaderè¿”å›ç»Ÿä¸€çš„Documentåˆ—è¡¨
print(f"PDFæ–‡æ¡£æ•°: {len(pdf_docs)}")
print(f"ç¬¬ä¸€é¡µå†…å®¹: {pdf_docs[0].page_content[:100]}...")
print(f"å…ƒæ•°æ®: {pdf_docs[0].metadata}")
```

**åœ¨RAGä¸­çš„åº”ç”¨ï¼š**
- å¿«é€Ÿæ„å»ºæ–‡æ¡£åŠ è½½ç®¡é“
- ä¸éœ€è¦æ‰‹åŠ¨å¤„ç†æ ¼å¼ç»†èŠ‚
- ç»Ÿä¸€çš„æ¥å£ä¾¿äºåç»­å¤„ç†

---

## 4.2 ç†è§£Documentå¯¹è±¡ç»“æ„

**æ ¸å¿ƒæ¦‚å¿µï¼š** Documentæ˜¯LangChainçš„æ ‡å‡†æ–‡æ¡£è¡¨ç¤ºï¼ŒåŒ…å«æ–‡æœ¬å’Œå…ƒæ•°æ®

```python
from langchain.schema import Document

# Documentå¯¹è±¡ç»“æ„
doc = Document(
    page_content="è¿™æ˜¯æ–‡æ¡£çš„æ–‡æœ¬å†…å®¹...",  # æ–‡æœ¬å†…å®¹
    metadata={                              # å…ƒæ•°æ®
        "source": "report.pdf",             # æ¥æºæ–‡ä»¶
        "page": 1,                          # é¡µç 
        "title": "æŠ€æœ¯æŠ¥å‘Š",                # æ ‡é¢˜
        "author": "å¼ ä¸‰",                   # ä½œè€…
        "created_date": "2025-01-15"        # åˆ›å»ºæ—¥æœŸ
    }
)

# è®¿é—®å†…å®¹
print(doc.page_content)  # æ–‡æœ¬å†…å®¹
print(doc.metadata)      # å…ƒæ•°æ®å­—å…¸

# åœ¨RAGä¸­ä½¿ç”¨
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(chunk_size=500)
chunks = splitter.split_documents([doc])  # æ¥æ”¶Documentåˆ—è¡¨

# æ¯ä¸ªchunkä¿ç•™åŸå§‹å…ƒæ•°æ®
for chunk in chunks:
    print(f"æ¥æº: {chunk.metadata['source']}")
    print(f"å†…å®¹: {chunk.page_content[:50]}...")
```

**åœ¨RAGä¸­çš„åº”ç”¨ï¼š**
- ç»Ÿä¸€çš„æ•°æ®ç»“æ„è´¯ç©¿æ•´ä¸ªRAGæµç¨‹
- å…ƒæ•°æ®ç”¨äºæ£€ç´¢è¿‡æ»¤å’Œç»“æœå±•ç¤º
- åˆ†å—åä¿ç•™åŸå§‹æ–‡æ¡£ä¿¡æ¯

---

## 4.3 æ‰¹é‡åŠ è½½ç›®å½•ä¸‹çš„æ–‡æ¡£

**æ ¸å¿ƒæ¦‚å¿µï¼š** ä½¿ç”¨DirectoryLoaderæ‰¹é‡åŠ è½½æ•´ä¸ªç›®å½•çš„æ–‡æ¡£

```python
from langchain.document_loaders import DirectoryLoader, TextLoader

# ===== 1. åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰txtæ–‡ä»¶ =====
loader = DirectoryLoader(
    "docs/",                    # ç›®å½•è·¯å¾„
    glob="**/*.txt",            # æ–‡ä»¶æ¨¡å¼ï¼ˆé€’å½’æŸ¥æ‰¾æ‰€æœ‰txtï¼‰
    loader_cls=TextLoader,      # ä½¿ç”¨çš„Loaderç±»
    loader_kwargs={"encoding": "utf-8"}  # Loaderå‚æ•°
)

docs = loader.load()
print(f"åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£")

# ===== 2. åŠ è½½å¤šç§æ ¼å¼ =====
from langchain.document_loaders import (
    DirectoryLoader,
    PyPDFLoader,
    Docx2txtLoader
)

# åŠ è½½PDF
pdf_loader = DirectoryLoader("docs/", glob="**/*.pdf", loader_cls=PyPDFLoader)
pdf_docs = pdf_loader.load()

# åŠ è½½DOCX
docx_loader = DirectoryLoader("docs/", glob="**/*.docx", loader_cls=Docx2txtLoader)
docx_docs = docx_loader.load()

# åˆå¹¶æ‰€æœ‰æ–‡æ¡£
all_docs = pdf_docs + docx_docs
print(f"æ€»å…±åŠ è½½ {len(all_docs)} ä¸ªæ–‡æ¡£")

# ===== 3. RAGåº”ç”¨ï¼šæ„å»ºçŸ¥è¯†åº“ =====
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# åŠ è½½æ‰€æœ‰æ–‡æ¡£
loader = DirectoryLoader("knowledge_base/", glob="**/*.pdf", loader_cls=PyPDFLoader)
documents = loader.load()

# åˆ†å—
from langchain.text_splitter import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)

# å‘é‡åŒ–å¹¶å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

print(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(chunks)} ä¸ªæ–‡æœ¬å—")
```

**åœ¨RAGä¸­çš„åº”ç”¨ï¼š**
- å¿«é€Ÿæ„å»ºçŸ¥è¯†åº“
- æ”¯æŒå¢é‡æ›´æ–°ï¼ˆæ–°å¢æ–‡æ¡£ï¼‰
- è‡ªåŠ¨å¤„ç†ç›®å½•ç»“æ„

---

## 4.4 æå–å’Œä¿ç•™å…ƒæ•°æ®

**æ ¸å¿ƒæ¦‚å¿µï¼š** å…ƒæ•°æ®å¯¹RAGæ£€ç´¢è´¨é‡è‡³å…³é‡è¦ï¼Œéœ€è¦æ­£ç¡®æå–å’Œä¿ç•™

```python
from langchain.document_loaders import PyPDFLoader
from datetime import datetime
import os

# ===== 1. è‡ªåŠ¨æå–å…ƒæ•°æ® =====
loader = PyPDFLoader("report.pdf")
docs = loader.load()

# æŸ¥çœ‹è‡ªåŠ¨æå–çš„å…ƒæ•°æ®
print(docs[0].metadata)
# {
#   'source': 'report.pdf',
#   'page': 0,
#   'total_pages': 10
# }

# ===== 2. æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ® =====
for doc in docs:
    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
    doc.metadata["file_size"] = os.path.getsize("report.pdf")
    doc.metadata["loaded_at"] = datetime.now().isoformat()

    # æ·»åŠ ä¸šåŠ¡å…ƒæ•°æ®
    doc.metadata["department"] = "æŠ€æœ¯éƒ¨"
    doc.metadata["category"] = "æŠ€æœ¯æŠ¥å‘Š"
    doc.metadata["year"] = 2025

# ===== 3. åœ¨æ£€ç´¢æ—¶ä½¿ç”¨å…ƒæ•°æ® =====
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# æ„å»ºå‘é‡åº“
vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())

# åŸºäºå…ƒæ•°æ®è¿‡æ»¤æ£€ç´¢
results = vectorstore.similarity_search(
    "RAGæŠ€æœ¯",
    k=5,
    filter={"year": 2025, "department": "æŠ€æœ¯éƒ¨"}  # åªæ£€ç´¢2025å¹´æŠ€æœ¯éƒ¨çš„æ–‡æ¡£
)

# ===== 4. åœ¨ç”Ÿæˆæ—¶å¼•ç”¨å…ƒæ•°æ® =====
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever(),
    return_source_documents=True  # è¿”å›æºæ–‡æ¡£
)

result = qa_chain({"query": "RAGçš„æ ¸å¿ƒæµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ"})

# æ˜¾ç¤ºç­”æ¡ˆå’Œæ¥æº
print(f"ç­”æ¡ˆ: {result['result']}")
print("\næ¥æº:")
for doc in result['source_documents']:
    print(f"- {doc.metadata['source']} (ç¬¬{doc.metadata['page']}é¡µ)")
```

**åœ¨RAGä¸­çš„åº”ç”¨ï¼š**
- æŒ‰æ—¶é—´ã€éƒ¨é—¨ã€ç±»åˆ«è¿‡æ»¤æ£€ç´¢ç»“æœ
- æä¾›ç­”æ¡ˆçš„å¯è¿½æº¯æ€§
- æ”¯æŒå¤šç§Ÿæˆ·åœºæ™¯ï¼ˆæŒ‰ç”¨æˆ·/ç»„ç»‡è¿‡æ»¤ï¼‰

---

## 4.5 å®ç°åŸºæœ¬çš„é”™è¯¯å¤„ç†

**æ ¸å¿ƒæ¦‚å¿µï¼š** æ–‡æ¡£åŠ è½½å¯èƒ½å¤±è´¥ï¼ˆæ–‡ä»¶æŸåã€ç¼–ç é”™è¯¯ï¼‰ï¼Œéœ€è¦å®¹é”™å¤„ç†

```python
from langchain.document_loaders import PyPDFLoader, TextLoader
from typing import List
from langchain.schema import Document

def load_documents_safely(file_paths: List[str]) -> List[Document]:
    """
    å®‰å…¨åŠ è½½å¤šä¸ªæ–‡æ¡£ï¼Œå¤±è´¥çš„æ–‡æ¡£ä¸å½±å“å…¶ä»–æ–‡æ¡£
    """
    all_docs = []
    failed_files = []

    for file_path in file_paths:
        try:
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©Loader
            if file_path.endswith('.pdf'):
                loader = PyPDFLoader(file_path)
            elif file_path.endswith('.txt'):
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                continue

            # åŠ è½½æ–‡æ¡£
            docs = loader.load()
            all_docs.extend(docs)
            print(f"âœ… æˆåŠŸåŠ è½½: {file_path} ({len(docs)} é¡µ)")

        except UnicodeDecodeError:
            # ç¼–ç é”™è¯¯ï¼Œå°è¯•å…¶ä»–ç¼–ç 
            try:
                loader = TextLoader(file_path, encoding='gbk')
                docs = loader.load()
                all_docs.extend(docs)
                print(f"âœ… ä½¿ç”¨GBKç¼–ç åŠ è½½: {file_path}")
            except Exception as e:
                failed_files.append((file_path, str(e)))
                print(f"âŒ åŠ è½½å¤±è´¥: {file_path} - {e}")

        except Exception as e:
            failed_files.append((file_path, str(e)))
            print(f"âŒ åŠ è½½å¤±è´¥: {file_path} - {e}")

    # æ±‡æ€»ç»“æœ
    print(f"\nğŸ“Š åŠ è½½å®Œæˆ:")
    print(f"  æˆåŠŸ: {len(all_docs)} ä¸ªæ–‡æ¡£")
    print(f"  å¤±è´¥: {len(failed_files)} ä¸ªæ–‡ä»¶")

    if failed_files:
        print("\nå¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
        for file_path, error in failed_files:
            print(f"  - {file_path}: {error}")

    return all_docs

# ===== ä½¿ç”¨ç¤ºä¾‹ =====
file_paths = [
    "docs/report1.pdf",
    "docs/report2.pdf",
    "docs/notes.txt",
    "docs/corrupted.pdf",  # å‡è®¾è¿™ä¸ªæ–‡ä»¶æŸå
]

documents = load_documents_safely(file_paths)

# ç»§ç»­RAGæµç¨‹
if documents:
    print(f"\nç»§ç»­å¤„ç† {len(documents)} ä¸ªæˆåŠŸåŠ è½½çš„æ–‡æ¡£...")
    # åˆ†å—ã€å‘é‡åŒ–ã€å­˜å‚¨...
else:
    print("\nâš ï¸  æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
```

**åœ¨RAGä¸­çš„åº”ç”¨ï¼š**
- æ‰¹é‡åŠ è½½æ—¶ä¸ä¼šå› ä¸ªåˆ«æ–‡ä»¶å¤±è´¥è€Œä¸­æ–­
- è®°å½•å¤±è´¥æ–‡ä»¶ä¾¿äºåç»­å¤„ç†
- æé«˜ç³»ç»Ÿçš„å¥å£®æ€§

---

## è¿™äº›çŸ¥è¯†è¶³ä»¥åšä»€ä¹ˆï¼Ÿ

æŒæ¡ä»¥ä¸Š5ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œä½ å¯ä»¥ï¼š

âœ… **æ„å»ºåŸºç¡€RAGæ–‡æ¡£åŠ è½½ç®¡é“**
- åŠ è½½PDFã€Wordã€æ–‡æœ¬ç­‰å¸¸è§æ ¼å¼
- æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ–‡æ¡£
- æå–å’Œä¿ç•™å…ƒæ•°æ®

âœ… **å¤„ç†å¸¸è§åœºæ™¯**
- çŸ¥è¯†åº“æ„å»ºï¼ˆåŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼‰
- å¢é‡æ›´æ–°ï¼ˆæ–°å¢æ–‡æ¡£ï¼‰
- å¤šæ ¼å¼æ··åˆå¤„ç†

âœ… **å®ç°åŸºæœ¬çš„è´¨é‡ä¿éšœ**
- é”™è¯¯å¤„ç†å’Œå®¹é”™
- å…ƒæ•°æ®ç®¡ç†
- ç»Ÿä¸€çš„Documentæ¥å£

âœ… **ä¸ºåç»­å­¦ä¹ æ‰“åŸºç¡€**
- ç†è§£Loaderæ¨¡å¼
- æŒæ¡Documentç»“æ„
- äº†è§£å…ƒæ•°æ®çš„é‡è¦æ€§

---

## ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

**å·²æŒæ¡æœ€å°å¯ç”¨çŸ¥è¯†åï¼Œå¯ä»¥ï¼š**

1. **æ·±å…¥å­¦ä¹ ç‰¹å®šæ ¼å¼**
   - PDFè§£ææŠ€æœ¯å¯¹æ¯”ï¼ˆpypdf vs pdfplumber vs Doclingï¼‰
   - Officeæ–‡æ¡£çš„å¤æ‚å¤„ç†ï¼ˆè¡¨æ ¼ã€å›¾ç‰‡ï¼‰
   - HTMLå†…å®¹æå–å’Œæ¸…æ´—

2. **å­¦ä¹ é«˜çº§ç‰¹æ€§**
   - æµå¼åŠ è½½å¤§æ–‡ä»¶
   - è‡ªå®šä¹‰Loader
   - å…ƒæ•°æ®æå–ç­–ç•¥

3. **ä¼˜åŒ–æ€§èƒ½**
   - å¹¶è¡ŒåŠ è½½
   - ç¼“å­˜æœºåˆ¶
   - å¢é‡æ›´æ–°

4. **å®æˆ˜é¡¹ç›®**
   - æ„å»ºä¼ä¸šçŸ¥è¯†åº“
   - å¤šç§Ÿæˆ·æ–‡æ¡£ç®¡ç†
   - æ–‡æ¡£ç‰ˆæœ¬æ§åˆ¶

---

## å¿«é€Ÿå‚è€ƒå¡

| ä»»åŠ¡ | ä»£ç  |
|------|------|
| åŠ è½½PDF | `PyPDFLoader("file.pdf").load()` |
| åŠ è½½æ–‡æœ¬ | `TextLoader("file.txt", encoding="utf-8").load()` |
| åŠ è½½Word | `Docx2txtLoader("file.docx").load()` |
| æ‰¹é‡åŠ è½½ | `DirectoryLoader("dir/", glob="**/*.pdf", loader_cls=PyPDFLoader).load()` |
| è®¿é—®å†…å®¹ | `doc.page_content` |
| è®¿é—®å…ƒæ•°æ® | `doc.metadata` |
| æ·»åŠ å…ƒæ•°æ® | `doc.metadata["key"] = "value"` |
| å…ƒæ•°æ®è¿‡æ»¤ | `vectorstore.similarity_search(query, filter={"year": 2025})` |

---

## å‚è€ƒæ¥æº

> **å‚è€ƒæ¥æºï¼š**
> - [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) - å®˜æ–¹æ–‡æ¡£ (2025)
> - [LangChain Document Schema](https://python.langchain.com/docs/modules/data_connection/document_loaders/#document) - Documentå¯¹è±¡è§„èŒƒ (2025)

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€åæ›´æ–°ï¼š** 2026-02-15
**ä¸‹ä¸€æ­¥ï¼š** é˜…è¯» [05_åŒé‡ç±»æ¯”.md](./05_åŒé‡ç±»æ¯”.md)
