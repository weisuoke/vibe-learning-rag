# 实战代码 - 场景3：数据处理管道

> 使用生成器构建高效的数据处理管道

---

## 场景描述

**问题：** 需要对大量数据进行多步处理（读取 → 清洗 → 转换 → 过滤 → 输出）

**挑战：**
- 数据量大，无法一次性加载到内存
- 多个处理步骤，需要链式调用
- 需要灵活组合不同的处理步骤

**解决方案：** 使用生成器构建数据处理管道

---

## 完整可运行代码

```python
"""
场景3：数据处理管道
演示：使用生成器构建灵活、高效的数据处理管道
"""

import json
from typing import Generator, Callable, Any
from collections import Counter

# ===== 1. 基础：简单的数据管道 =====
print("=== 1. 基础：简单的数据管道 ===\n")

def read_data() -> Generator[int, None, None]:
    """步骤1：读取数据"""
    for i in range(10):
        yield i

def filter_even(numbers: Generator[int, None, None]) -> Generator[int, None, None]:
    """步骤2：过滤偶数"""
    for num in numbers:
        if num % 2 == 0:
            yield num

def square(numbers: Generator[int, None, None]) -> Generator[int, None, None]:
    """步骤3：平方"""
    for num in numbers:
        yield num ** 2

# 构建管道
pipeline = square(filter_even(read_data()))

print("简单管道结果:")
for result in pipeline:
    print(f"  {result}")

print()

# ===== 2. 通用管道组件 =====
print("=== 2. 通用管道组件 ===\n")

def map_transform(data: Generator, func: Callable) -> Generator:
    """通用转换组件"""
    for item in data:
        yield func(item)

def filter_by(data: Generator, predicate: Callable) -> Generator:
    """通用过滤组件"""
    for item in data:
        if predicate(item):
            yield item

def take(data: Generator, n: int) -> Generator:
    """取前 N 个元素"""
    for i, item in enumerate(data):
        if i >= n:
            break
        yield item

# 使用通用组件
data = range(20)
pipeline = take(
    map_transform(
        filter_by(data, lambda x: x % 2 == 0),
        lambda x: x ** 2
    ),
    5
)

print("通用组件管道结果:")
for result in pipeline:
    print(f"  {result}")

print()

# ===== 3. 实际应用：CSV 数据处理 =====
print("=== 3. 实际应用：CSV 数据处理 ===\n")

# 创建测试 CSV 文件
test_csv = 'test_data.csv'
with open(test_csv, 'w') as f:
    f.write("id,name,age,city\n")
    f.write("1,Alice,25,Beijing\n")
    f.write("2,Bob,30,Shanghai\n")
    f.write("3,Charlie,35,Beijing\n")
    f.write("4,David,28,Guangzhou\n")
    f.write("5,Eve,32,Beijing\n")

def read_csv(filename: str) -> Generator[dict, None, None]:
    """读取 CSV 文件"""
    with open(filename, 'r') as f:
        headers = f.readline().strip().split(',')
        for line in f:
            values = line.strip().split(',')
            yield dict(zip(headers, values))

def filter_by_city(records: Generator[dict, None, None],
                   city: str) -> Generator[dict, None, None]:
    """按城市过滤"""
    for record in records:
        if record.get('city') == city:
            yield record

def add_age_group(records: Generator[dict, None, None]) -> Generator[dict, None, None]:
    """添加年龄组字段"""
    for record in records:
        age = int(record['age'])
        if age < 30:
            record['age_group'] = 'young'
        else:
            record['age_group'] = 'adult'
        yield record

# 构建 CSV 处理管道
csv_pipeline = add_age_group(
    filter_by_city(
        read_csv(test_csv),
        'Beijing'
    )
)

print("CSV 处理管道结果:")
for record in csv_pipeline:
    print(f"  {record}")

print()

# ===== 4. 管道类封装 =====
print("=== 4. 管道类封装 ===\n")

class Pipeline:
    """
    数据处理管道类

    支持链式调用，提供流畅的 API
    """

    def __init__(self, data: Generator):
        self.data = data

    def map(self, func: Callable) -> 'Pipeline':
        """转换数据"""
        def generator():
            for item in self.data:
                yield func(item)
        return Pipeline(generator())

    def filter(self, predicate: Callable) -> 'Pipeline':
        """过滤数据"""
        def generator():
            for item in self.data:
                if predicate(item):
                    yield item
        return Pipeline(generator())

    def take(self, n: int) -> 'Pipeline':
        """取前 N 个"""
        def generator():
            for i, item in enumerate(self.data):
                if i >= n:
                    break
                yield item
        return Pipeline(generator())

    def skip(self, n: int) -> 'Pipeline':
        """跳过前 N 个"""
        def generator():
            for i, item in enumerate(self.data):
                if i >= n:
                    yield item
        return Pipeline(generator())

    def collect(self) -> list:
        """收集结果"""
        return list(self.data)

    def __iter__(self):
        return iter(self.data)

# 使用管道类
result = (Pipeline(range(20))
    .filter(lambda x: x % 2 == 0)
    .map(lambda x: x ** 2)
    .take(5)
    .collect())

print("管道类结果:")
print(f"  {result}")

print()

# ===== 5. 复杂管道：日志分析 =====
print("=== 5. 复杂管道：日志分析 ===\n")

# 创建测试日志
test_log = 'test_app.log'
with open(test_log, 'w') as f:
    f.write('{"timestamp": "2026-02-11 10:00:00", "level": "INFO", "message": "Server started"}\n')
    f.write('{"timestamp": "2026-02-11 10:01:00", "level": "ERROR", "message": "Database connection failed"}\n')
    f.write('{"timestamp": "2026-02-11 10:02:00", "level": "INFO", "message": "Request processed"}\n')
    f.write('{"timestamp": "2026-02-11 10:03:00", "level": "ERROR", "message": "API timeout"}\n')
    f.write('{"timestamp": "2026-02-11 10:04:00", "level": "WARNING", "message": "High memory usage"}\n')

def read_json_lines(filename: str) -> Generator[dict, None, None]:
    """读取 JSON Lines 文件"""
    with open(filename, 'r') as f:
        for line in f:
            try:
                yield json.loads(line.strip())
            except json.JSONDecodeError:
                continue

def filter_by_level(logs: Generator[dict, None, None],
                    level: str) -> Generator[dict, None, None]:
    """按日志级别过滤"""
    for log in logs:
        if log.get('level') == level:
            yield log

def extract_field(logs: Generator[dict, None, None],
                  field: str) -> Generator[Any, None, None]:
    """提取特定字段"""
    for log in logs:
        if field in log:
            yield log[field]

def count_items(items: Generator) -> dict:
    """统计项目数量"""
    counter = Counter()
    for item in items:
        counter[item] += 1
    return dict(counter)

# 分析错误日志
error_messages = extract_field(
    filter_by_level(
        read_json_lines(test_log),
        'ERROR'
    ),
    'message'
)

print("错误日志分析:")
for message in error_messages:
    print(f"  {message}")

print()

# ===== 6. 并行管道：多数据源 =====
print("=== 6. 并行管道：多数据源 ===\n")

def merge_streams(*streams: Generator) -> Generator:
    """合并多个数据流"""
    for stream in streams:
        for item in stream:
            yield item

def source1() -> Generator[int, None, None]:
    """数据源1"""
    for i in range(5):
        yield i

def source2() -> Generator[int, None, None]:
    """数据源2"""
    for i in range(10, 15):
        yield i

# 合并多个数据源
merged = merge_streams(source1(), source2())

print("合并数据流结果:")
for item in merged:
    print(f"  {item}")

print()

# ===== 7. 批量处理管道 =====
print("=== 7. 批量处理管道 ===\n")

def batch(data: Generator, batch_size: int) -> Generator[list, None, None]:
    """将数据分批"""
    batch_data = []
    for item in data:
        batch_data.append(item)
        if len(batch_data) >= batch_size:
            yield batch_data
            batch_data = []
    if batch_data:
        yield batch_data

def process_batch(batches: Generator[list, None, None]) -> Generator[dict, None, None]:
    """批量处理"""
    for i, batch in enumerate(batches):
        yield {
            'batch_id': i,
            'size': len(batch),
            'sum': sum(batch)
        }

# 批量处理管道
batch_pipeline = process_batch(
    batch(range(25), batch_size=10)
)

print("批量处理结果:")
for result in batch_pipeline:
    print(f"  批次 {result['batch_id']}: {result['size']} 项, 总和 {result['sum']}")

print()

# ===== 8. 实战：用户数据ETL管道 =====
print("=== 8. 实战：用户数据ETL管道 ===\n")

# 模拟用户数据
users_data = [
    {"id": 1, "name": "Alice", "email": "alice@example.com", "age": 25, "status": "active"},
    {"id": 2, "name": "Bob", "email": "invalid-email", "age": 30, "status": "active"},
    {"id": 3, "name": "Charlie", "email": "charlie@example.com", "age": 17, "status": "inactive"},
    {"id": 4, "name": "David", "email": "david@example.com", "age": 35, "status": "active"},
    {"id": 5, "name": "Eve", "email": "eve@example.com", "age": 28, "status": "active"},
]

def extract_users(data: list) -> Generator[dict, None, None]:
    """E: Extract - 提取数据"""
    for user in data:
        yield user

def validate_email(email: str) -> bool:
    """验证邮箱格式"""
    return '@' in email and '.' in email.split('@')[1]

def transform_user(users: Generator[dict, None, None]) -> Generator[dict, None, None]:
    """T: Transform - 转换数据"""
    for user in users:
        # 验证邮箱
        if not validate_email(user.get('email', '')):
            continue

        # 过滤未成年人
        if user.get('age', 0) < 18:
            continue

        # 只保留活跃用户
        if user.get('status') != 'active':
            continue

        # 转换数据格式
        yield {
            'user_id': user['id'],
            'full_name': user['name'].upper(),
            'email': user['email'].lower(),
            'age_group': 'young' if user['age'] < 30 else 'adult'
        }

def load_users(users: Generator[dict, None, None]) -> None:
    """L: Load - 加载数据"""
    print("加载用户数据:")
    for user in users:
        print(f"  插入用户: {user}")

# ETL 管道
etl_pipeline = transform_user(extract_users(users_data))
load_users(etl_pipeline)

print()

# ===== 9. 性能监控管道 =====
print("=== 9. 性能监控管道 ===\n")

import time

def monitor_performance(data: Generator, name: str) -> Generator:
    """监控管道性能"""
    start = time.time()
    count = 0

    for item in data:
        count += 1
        yield item

    elapsed = time.time() - start
    print(f"  [{name}] 处理 {count} 项, 耗时 {elapsed:.4f}s")

# 带性能监控的管道
monitored_pipeline = (
    monitor_performance(
        monitor_performance(
            monitor_performance(
                range(1000),
                "读取数据"
            ),
            "过滤数据"
        ),
        "转换数据"
    )
)

print("性能监控管道:")
list(monitored_pipeline)

print()

# ===== 10. 实际应用：完整的数据处理系统 =====
print("=== 10. 实际应用：完整的数据处理系统 ===\n")

class DataProcessor:
    """
    数据处理系统

    功能：
    - 灵活的管道构建
    - 错误处理
    - 性能监控
    - 结果统计
    """

    def __init__(self):
        self.stats = {
            'total': 0,
            'processed': 0,
            'errors': 0
        }

    def process(self, data: Generator,
                steps: list[Callable]) -> Generator:
        """
        处理数据

        Args:
            data: 输入数据流
            steps: 处理步骤列表

        Yields:
            处理后的数据
        """
        current = data

        for step in steps:
            current = self._apply_step(current, step)

        return current

    def _apply_step(self, data: Generator,
                    step: Callable) -> Generator:
        """应用处理步骤"""
        for item in data:
            self.stats['total'] += 1
            try:
                result = step(item)
                if result is not None:
                    self.stats['processed'] += 1
                    yield result
            except Exception as e:
                self.stats['errors'] += 1
                print(f"  错误: {e}")

    def get_stats(self) -> dict:
        """获取统计信息"""
        return self.stats

# 定义处理步骤
def validate(item: dict) -> dict:
    """验证数据"""
    if 'id' not in item or 'value' not in item:
        raise ValueError("缺少必需字段")
    return item

def transform(item: dict) -> dict:
    """转换数据"""
    return {
        'id': item['id'],
        'value': item['value'] * 2
    }

def filter_positive(item: dict) -> dict:
    """过滤正值"""
    if item['value'] > 0:
        return item
    return None

# 测试数据
test_data = [
    {"id": 1, "value": 10},
    {"id": 2, "value": -5},
    {"id": 3},  # 缺少 value
    {"id": 4, "value": 20},
    {"id": 5, "value": 0},
]

# 使用数据处理系统
processor = DataProcessor()
pipeline = processor.process(
    iter(test_data),
    [validate, transform, filter_positive]
)

print("数据处理系统结果:")
for result in pipeline:
    print(f"  {result}")

print(f"\n统计信息: {processor.get_stats()}")

# 清理测试文件
import os
os.remove(test_csv)
os.remove(test_log)

print("\n=== 总结 ===")
print("生成器构建数据管道的优势:")
print("  1. 内存高效：按需处理，不存储中间结果")
print("  2. 灵活组合：可以自由组合不同的处理步骤")
print("  3. 代码清晰：每个步骤职责单一，易于理解")
print("  4. 易于测试：每个步骤可以独立测试")
print("\n适用场景:")
print("  - ETL 数据处理")
print("  - 日志分析")
print("  - 数据清洗")
print("  - 流式数据处理")
```

---

## 运行输出示例

```
=== 1. 基础：简单的数据管道 ===

简单管道结果:
  0
  4
  16
  36
  64

=== 2. 通用管道组件 ===

通用组件管道结果:
  0
  4
  16
  36
  64

=== 3. 实际应用：CSV 数据处理 ===

CSV 处理管道结果:
  {'id': '1', 'name': 'Alice', 'age': '25', 'city': 'Beijing', 'age_group': 'young'}
  {'id': '3', 'name': 'Charlie', 'age': '35', 'city': 'Beijing', 'age_group': 'adult'}
  {'id': '5', 'name': 'Eve', 'age': '32', 'city': 'Beijing', 'age_group': 'adult'}

=== 4. 管道类封装 ===

管道类结果:
  [0, 4, 16, 36, 64]

=== 5. 复杂管道：日志分析 ===

错误日志分析:
  Database connection failed
  API timeout

=== 6. 并行管道：多数据源 ===

合并数据流结果:
  0
  1
  2
  3
  4
  10
  11
  12
  13
  14

=== 7. 批量处理管道 ===

批量处理结果:
  批次 0: 10 项, 总和 45
  批次 1: 10 项, 总和 145
  批次 2: 5 项, 总和 110

=== 8. 实战：用户数据ETL管道 ===

加载用户数据:
  插入用户: {'user_id': 1, 'full_name': 'ALICE', 'email': 'alice@example.com', 'age_group': 'young'}
  插入用户: {'user_id': 4, 'full_name': 'DAVID', 'email': 'david@example.com', 'age_group': 'adult'}
  插入用户: {'user_id': 5, 'full_name': 'EVE', 'email': 'eve@example.com', 'age_group': 'young'}

=== 9. 性能监控管道 ===

性能监控管道:
  [读取数据] 处理 1000 项, 耗时 0.0001s
  [过滤数据] 处理 1000 项, 耗时 0.0001s
  [转换数据] 处理 1000 项, 耗时 0.0001s

=== 10. 实际应用：完整的数据处理系统 ===

数据处理系统结果:
  {'id': 1, 'value': 20}
  错误: 缺少必需字段
  {'id': 4, 'value': 40}

统计信息: {'total': 5, 'processed': 2, 'errors': 1}

=== 总结 ===
生成器构建数据管道的优势:
  1. 内存高效：按需处理，不存储中间结果
  2. 灵活组合：可以自由组合不同的处理步骤
  3. 代码清晰：每个步骤职责单一，易于理解
  4. 易于测试：每个步骤可以独立测试

适用场景:
  - ETL 数据处理
  - 日志分析
  - 数据清洗
  - 流式数据处理
```

---

## 关键技术点

### 1. 管道模式

```python
# 链式调用
result = step3(step2(step1(data)))

# 每个步骤都是生成器
for item in result:
    process(item)
```

### 2. 通用组件

```python
def map_transform(data, func):
    for item in data:
        yield func(item)

def filter_by(data, predicate):
    for item in data:
        if predicate(item):
            yield item
```

### 3. 流畅 API

```python
result = (Pipeline(data)
    .filter(lambda x: x > 0)
    .map(lambda x: x * 2)
    .take(10)
    .collect())
```

### 4. 错误处理

```python
def safe_process(data):
    for item in data:
        try:
            yield process(item)
        except Exception as e:
            logger.error(f"处理失败: {e}")
```

---

## 最佳实践

1. **每个步骤职责单一**
   ```python
   # ✅ 好：职责单一
   def filter_active(users):
       for user in users:
           if user['status'] == 'active':
               yield user

   # ❌ 不好：职责混杂
   def process_users(users):
       for user in users:
           if user['status'] == 'active':
               user['name'] = user['name'].upper()
               yield user
   ```

2. **使用类型注解**
   ```python
   def transform(data: Generator[dict, None, None]) -> Generator[dict, None, None]:
       for item in data:
           yield process(item)
   ```

3. **添加性能监控**
   ```python
   def monitor(data, name):
       start = time.time()
       count = 0
       for item in data:
           count += 1
           yield item
       print(f"{name}: {count} items in {time.time()-start:.2f}s")
   ```

4. **错误不中断管道**
   ```python
   def safe_transform(data):
       for item in data:
           try:
               yield transform(item)
           except Exception:
               continue  # 跳过错误项
   ```

---

**版本：** v1.0
**最后更新：** 2026-02-11
