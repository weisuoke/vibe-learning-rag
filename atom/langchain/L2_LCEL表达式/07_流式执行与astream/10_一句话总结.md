# 一句话总结

## 核心总结

**流式执行让 AI 应用像打字机一样逐步输出结果，通过 `astream()` 方法以异步迭代器形式实时返回 Agent 进度、LLM tokens 或自定义数据，显著提升用户体验且性能开销小于 10%。**

---

## 三个关键认知

### 1. 流式不是"快"，而是"感觉快"
```python
# 总耗时相同，但用户体验完全不同
invoke_result = chain.invoke(input)    # 等待 5 秒后一次性返回
async for chunk in chain.astream(input):  # 0.1 秒后开始输出，持续 5 秒
    print(chunk, end="")
```

**类比**：
- **前端**：懒加载 vs 全量加载 - 总数据量相同，但懒加载让用户更快看到内容
- **日常**：餐厅上菜 - 一道道上菜（流式）vs 等所有菜做好一起上（批量）

---

### 2. 三种流式模式解决不同问题

| 模式 | 用途 | 典型场景 |
|------|------|----------|
| **updates** | 追踪 Agent 执行步骤 | 多步推理、工具调用监控 |
| **messages** | 实时显示 LLM 输出 | ChatGPT 式对话、长文本生成 |
| **custom** | 发送自定义进度信号 | 数据处理进度、业务状态更新 |

```python
# 可以组合使用
for mode, data in agent.stream(
    input,
    stream_mode=["updates", "messages", "custom"]
):
    if mode == "updates":
        print(f"步骤: {data}")
    elif mode == "messages":
        print(f"Token: {data[0].text}", end="")
    elif mode == "custom":
        print(f"进度: {data}")
```

---

### 3. 2025-2026 流式已成生产级特性

**性能优化**：
- 2024 年：流式开销 ~20%
- 2025 年：流式开销 <10%
- 2026 年：支持 subgraph 流式、human-in-the-loop 流式

**生产环境可放心使用**：
```python
# 生产环境配置
agent = create_agent(
    model="gpt-4o-mini",
    tools=[...],
    checkpointer=checkpointer,  # 状态持久化
)

# 流式输出 + 中断处理
for mode, data in agent.stream(
    input,
    config={"configurable": {"thread_id": "user_123"}},
    stream_mode=["updates", "messages"],
):
    # 处理流式数据
    if mode == "__interrupt__":
        # 处理人工审核中断
        handle_interrupt(data)
```

---

## 最小记忆模型

### 核心 API
```python
# 同步流式
for chunk in chain.stream(input):
    process(chunk)

# 异步流式（推荐）
async for chunk in chain.astream(input):
    process(chunk)

# 多模式流式
for mode, data in agent.stream(input, stream_mode=["updates", "messages"]):
    handle(mode, data)
```

### 自定义流式数据
```python
from langgraph.config import get_stream_writer

def my_tool(query: str) -> str:
    writer = get_stream_writer()
    writer("步骤 1: 查询数据库...")
    writer("步骤 2: 处理结果...")
    return result
```

### 禁用流式
```python
# 方法 1: 模型初始化时禁用
model = ChatOpenAI(model="gpt-4", streaming=False)

# 方法 2: 基类参数（兼容所有模型）
model = ChatOpenAI(model="gpt-4", disable_streaming=True)
```

---

## 实战决策树

```
需要实时输出？
├─ 是 → 使用流式
│   ├─ 只需要 LLM 输出？
│   │   └─ stream_mode="messages"
│   ├─ 需要追踪 Agent 步骤？
│   │   └─ stream_mode="updates"
│   ├─ 需要自定义进度信号？
│   │   └─ stream_mode="custom" + get_stream_writer()
│   └─ 需要多种信息？
│       └─ stream_mode=["updates", "messages", "custom"]
└─ 否 → 使用 invoke()
    └─ 批处理、后台任务、不需要实时反馈
```

---

## 常见误区与真相

### ❌ 误区 1：流式会显著降低性能
**✅ 真相**：2025 年后流式开销 <10%，对用户体验提升远大于性能损失

### ❌ 误区 2：流式只能用于 LLM 输出
**✅ 真相**：三种模式支持 Agent 进度、LLM tokens、自定义数据

### ❌ 误区 3：流式不支持复杂场景
**✅ 真相**：支持 subgraph 流式、human-in-the-loop、多 Agent 协作

### ❌ 误区 4：流式和批处理互斥
**✅ 真相**：可以在流式中使用 `batch()`，也可以在批处理中对单个项目流式处理

---

## 与相关概念的关系

### vs invoke()
- **invoke()**：一次性返回完整结果，适合批处理
- **stream()**：逐步返回中间结果，适合实时交互

### vs batch()
- **batch()**：并行处理多个输入，返回多个完整结果
- **stream()**：串行处理单个输入，返回多个中间结果
- **可组合**：`[chunk async for chunk in chain.astream(input) for input in inputs]`

### vs astream_events()
- **astream()**：高层 API，返回结构化数据（updates/messages/custom）
- **astream_events()**：低层 API，返回所有执行事件（on_chain_start/on_llm_stream 等）
- **选择**：99% 场景用 `astream()`，调试或深度监控用 `astream_events()`

---

## 记忆口诀

**流式三模式，体验大不同**：
- **updates** 看步骤，Agent 进度清
- **messages** 看输出，逐字如打字
- **custom** 看业务，自定义随心

**性能无忧虑，生产可放心**：
- 开销小于 10%，体验提升大
- 支持多场景，中断也能流

---

## 快速参考

### 最小示例
```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")

# 流式输出
async for chunk in model.astream("讲个笑话"):
    print(chunk.content, end="", flush=True)
```

### 多模式示例
```python
from langchain.agents import create_agent

agent = create_agent(model="gpt-4o-mini", tools=[...])

for mode, data in agent.stream(
    {"messages": [{"role": "user", "content": "查询天气"}]},
    stream_mode=["updates", "messages"]
):
    if mode == "updates":
        print(f"\n[步骤] {list(data.keys())}")
    elif mode == "messages":
        token, metadata = data
        print(token.text, end="")
```

### 自定义流式
```python
from langgraph.config import get_stream_writer

def process_data(data: list) -> str:
    writer = get_stream_writer()
    for i, item in enumerate(data):
        writer(f"处理进度: {i+1}/{len(data)}")
        # 处理逻辑
    return "完成"

# 在 Agent 中使用
agent = create_agent(model="gpt-4o-mini", tools=[process_data])
for chunk in agent.stream(input, stream_mode="custom"):
    print(chunk)  # 输出: "处理进度: 1/10", "处理进度: 2/10", ...
```

---

## 进阶学习路径

1. **基础掌握**：理解三种流式模式的区别和使用场景
2. **实战应用**：在对话应用中实现 ChatGPT 式流式输出
3. **进阶优化**：使用 subgraph 流式处理多 Agent 协作
4. **生产部署**：结合 checkpointer 实现可恢复的流式执行

---

## 参考资源

- **官方文档**：https://docs.langchain.com/oss/python/langchain/streaming/overview
- **源码位置**：`langchain_core/runnables/base.py:132` (astream 方法)
- **相关知识点**：
  - 02_第一性原理 - 深入理解流式执行的设计原理
  - 03_核心概念 - 三种流式模式的详细讲解
  - 07_实战代码 - 完整的生产级代码示例

---

**版本**：LangChain 0.3.x (2025-2026)
**最后更新**：2026-02-21
**一句话**：流式执行让 AI 应用像打字机一样逐步输出，三种模式覆盖所有场景，性能开销小于 10%。
