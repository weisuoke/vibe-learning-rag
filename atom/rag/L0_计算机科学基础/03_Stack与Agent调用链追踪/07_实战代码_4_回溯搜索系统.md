# 实战代码4：回溯搜索系统

> 使用栈实现回溯搜索，应用于 AI Agent 决策树探索

---

## 学习目标

- 理解回溯搜索的核心原理
- 实现经典回溯算法
- 应用于 AI Agent 决策树
- 集成 2026 年最新技术（R-MCTS、EnCompass）

---

## 1. 回溯搜索基础

### 1.1 回溯算法模板

```python
from typing import List, Set, Optional

def backtrack_template(state, path, result):
    """回溯算法通用模板"""
    # 1. 终止条件
    if is_goal(state):
        result.append(path.copy())
        return

    # 2. 遍历所有选择
    for choice in get_choices(state):
        # 3. 做选择
        make_choice(state, choice)
        path.append(choice)

        # 4. 递归搜索
        backtrack_template(state, path, result)

        # 5. 撤销选择（回溯）
        path.pop()
        undo_choice(state, choice)

def is_goal(state) -> bool:
    """判断是否达到目标"""
    pass

def get_choices(state) -> List:
    """获取当前可选择的选项"""
    pass

def make_choice(state, choice):
    """做出选择"""
    pass

def undo_choice(state, choice):
    """撤销选择"""
    pass
```

### 1.2 使用显式栈实现回溯

```python
def backtrack_with_stack(initial_state):
    """使用显式栈的回溯搜索"""
    stack = [(initial_state, [])]  # (状态, 路径)
    results = []

    while stack:
        state, path = stack.pop()

        # 找到解
        if is_goal(state):
            results.append(path.copy())
            continue

        # 尝试所有选择
        for choice in get_choices(state):
            new_state = apply_choice(state, choice)
            new_path = path + [choice]
            stack.append((new_state, new_path))

    return results

def apply_choice(state, choice):
    """应用选择，返回新状态"""
    pass
```

---

## 2. 经典回溯问题

### 2.1 N 皇后问题

```python
from typing import List

class NQueens:
    """N 皇后问题"""

    def solve(self, n: int) -> List[List[str]]:
        """求解 N 皇后"""
        solutions = []
        self.backtrack(n, [], solutions)
        return solutions

    def backtrack(self, n: int, queens: List[int], solutions: List):
        """回溯搜索"""
        # 终止条件：放置了 n 个皇后
        if len(queens) == n:
            solutions.append(self.format_solution(queens, n))
            return

        # 当前行
        row = len(queens)

        # 尝试在当前行的每一列放置皇后
        for col in range(n):
            if self.is_safe(queens, row, col):
                # 做选择
                queens.append(col)

                # 递归
                self.backtrack(n, queens, solutions)

                # 撤销选择
                queens.pop()

    def is_safe(self, queens: List[int], row: int, col: int) -> bool:
        """检查位置是否安全"""
        for prev_row, prev_col in enumerate(queens):
            # 同列
            if prev_col == col:
                return False

            # 对角线
            if abs(prev_row - row) == abs(prev_col - col):
                return False

        return True

    def format_solution(self, queens: List[int], n: int) -> List[str]:
        """格式化解决方案"""
        board = []
        for col in queens:
            row = ['.' if i != col else 'Q' for i in range(n)]
            board.append(''.join(row))
        return board

# 测试
print("=== N 皇后问题 ===\n")

solver = NQueens()
solutions = solver.solve(4)

print(f"4 皇后问题有 {len(solutions)} 个解\n")
print("第一个解:")
for row in solutions[0]:
    print(row)
```

**输出：**
```
=== N 皇后问题 ===

4 皇后问题有 2 个解

第一个解:
.Q..
...Q
Q...
..Q.
```

### 2.2 组合总和问题

```python
class CombinationSum:
    """组合总和问题"""

    def find_combinations(self, candidates: List[int], target: int) -> List[List[int]]:
        """找到所有和为 target 的组合"""
        results = []
        candidates.sort()  # 排序以便剪枝
        self.backtrack(candidates, target, 0, [], results)
        return results

    def backtrack(self, candidates: List[int], target: int,
                  start: int, path: List[int], results: List):
        """回溯搜索"""
        # 终止条件
        if target == 0:
            results.append(path.copy())
            return

        if target < 0:
            return

        # 遍历选择
        for i in range(start, len(candidates)):
            # 剪枝：如果当前数字已经大于目标，后面的更大
            if candidates[i] > target:
                break

            # 做选择
            path.append(candidates[i])

            # 递归（可以重复使用当前数字）
            self.backtrack(candidates, target - candidates[i], i, path, results)

            # 撤销选择
            path.pop()

# 测试
print("\n=== 组合总和问题 ===\n")

solver = CombinationSum()
candidates = [2, 3, 6, 7]
target = 7

combinations = solver.find_combinations(candidates, target)
print(f"候选数字: {candidates}")
print(f"目标和: {target}")
print(f"找到 {len(combinations)} 个组合:")
for combo in combinations:
    print(f"  {combo}")
```

### 2.3 子集问题

```python
class Subsets:
    """子集问题"""

    def find_subsets(self, nums: List[int]) -> List[List[int]]:
        """找到所有子集"""
        results = []
        self.backtrack(nums, 0, [], results)
        return results

    def backtrack(self, nums: List[int], start: int,
                  path: List[int], results: List):
        """回溯搜索"""
        # 每个状态都是一个有效子集
        results.append(path.copy())

        # 遍历选择
        for i in range(start, len(nums)):
            # 做选择
            path.append(nums[i])

            # 递归
            self.backtrack(nums, i + 1, path, results)

            # 撤销选择
            path.pop()

# 测试
print("\n=== 子集问题 ===\n")

solver = Subsets()
nums = [1, 2, 3]

subsets = solver.find_subsets(nums)
print(f"数组: {nums}")
print(f"找到 {len(subsets)} 个子集:")
for subset in subsets:
    print(f"  {subset}")
```

---

## 3. AI Agent 决策树搜索

### 3.1 Agent 决策树回溯

```python
from dataclasses import dataclass
from typing import Dict, Any
import time

@dataclass
class AgentState:
    """Agent 状态"""
    task: str
    completed_actions: List[str]
    score: float
    depth: int

class AgentDecisionTree:
    """Agent 决策树搜索"""

    def __init__(self, max_depth: int = 5):
        self.max_depth = max_depth
        self.best_path = []
        self.best_score = float('-inf')
        self.explored_states = 0

    def search(self, initial_task: str) -> List[str]:
        """搜索最佳行动序列"""
        initial_state = AgentState(
            task=initial_task,
            completed_actions=[],
            score=0.0,
            depth=0
        )

        print(f"=== Agent 决策树搜索 ===")
        print(f"任务: {initial_task}\n")

        self.backtrack(initial_state, [])

        print(f"\n探索了 {self.explored_states} 个状态")
        print(f"最佳得分: {self.best_score:.2f}")
        print(f"最佳路径: {self.best_path}")

        return self.best_path

    def backtrack(self, state: AgentState, path: List[str]):
        """回溯搜索"""
        self.explored_states += 1

        indent = "  " * state.depth
        print(f"{indent}→ 深度 {state.depth}: {state.task}")

        # 终止条件1：达到最大深度
        if state.depth >= self.max_depth:
            self.update_best(state, path)
            print(f"{indent}  达到最大深度")
            return

        # 终止条件2：任务完成
        if self.is_goal(state):
            self.update_best(state, path)
            print(f"{indent}  ✓ 任务完成，得分: {state.score:.2f}")
            return

        # 获取可选行动
        actions = self.get_actions(state)

        if not actions:
            self.update_best(state, path)
            print(f"{indent}  无可用行动")
            return

        # 尝试每个行动
        for action in actions:
            print(f"{indent}  尝试行动: {action}")

            # 做选择
            new_state = self.apply_action(state, action)
            new_path = path + [action]

            # 递归搜索
            self.backtrack(new_state, new_path)

            # 回溯（状态已经是新对象，无需撤销）

    def is_goal(self, state: AgentState) -> bool:
        """判断是否达到目标"""
        return state.score >= 10.0 or "完成" in state.task

    def get_actions(self, state: AgentState) -> List[str]:
        """获取可选行动"""
        if "研究" in state.task:
            return ["搜索资料", "阅读文献", "总结要点"]
        elif "分析" in state.task:
            return ["数据收集", "数据处理", "得出结论"]
        elif "撰写" in state.task:
            return ["列大纲", "写正文", "校对"]
        else:
            return ["完成"]

    def apply_action(self, state: AgentState, action: str) -> AgentState:
        """应用行动"""
        time.sleep(0.01)  # 模拟执行时间

        # 计算新得分
        new_score = state.score + self.evaluate_action(action)

        return AgentState(
            task=f"{state.task} → {action}",
            completed_actions=state.completed_actions + [action],
            score=new_score,
            depth=state.depth + 1
        )

    def evaluate_action(self, action: str) -> float:
        """评估行动得分"""
        scores = {
            "搜索资料": 2.0,
            "阅读文献": 3.0,
            "总结要点": 2.5,
            "数据收集": 2.0,
            "数据处理": 3.5,
            "得出结论": 4.0,
            "列大纲": 2.0,
            "写正文": 4.0,
            "校对": 1.5,
            "完成": 5.0
        }
        return scores.get(action, 1.0)

    def update_best(self, state: AgentState, path: List[str]):
        """更新最佳路径"""
        if state.score > self.best_score:
            self.best_score = state.score
            self.best_path = path.copy()

# 测试
agent = AgentDecisionTree(max_depth=3)
best_path = agent.search("研究 AI Agent")
```

---

## 4. 2026 年技术：R-MCTS

### 4.1 Reflective Monte Carlo Tree Search

```python
import random
import math

class MCTSNode:
    """MCTS 节点"""

    def __init__(self, state: str, parent=None):
        self.state = state
        self.parent = parent
        self.children = []
        self.visits = 0
        self.value = 0.0
        self.untried_actions = self.get_actions()

    def get_actions(self) -> List[str]:
        """获取可选行动"""
        if "研究" in self.state:
            return ["搜索", "阅读", "总结"]
        elif "分析" in self.state:
            return ["收集", "处理", "结论"]
        else:
            return []

    def is_fully_expanded(self) -> bool:
        """是否完全展开"""
        return len(self.untried_actions) == 0

    def is_terminal(self) -> bool:
        """是否终止节点"""
        return len(self.get_actions()) == 0

    def best_child(self, c_param: float = 1.4) -> 'MCTSNode':
        """选择最佳子节点（UCB1）"""
        choices_weights = [
            (child.value / child.visits) +
            c_param * math.sqrt((2 * math.log(self.visits) / child.visits))
            for child in self.children
        ]
        return self.children[choices_weights.index(max(choices_weights))]

class ReflectiveMCTS:
    """反思式蒙特卡洛树搜索（2026）"""

    def __init__(self, max_iterations: int = 100):
        self.max_iterations = max_iterations
        self.reflection_memory = []  # 反思记忆

    def search(self, initial_state: str) -> List[str]:
        """MCTS 搜索"""
        root = MCTSNode(initial_state)

        print(f"=== Reflective MCTS ===")
        print(f"初始状态: {initial_state}\n")

        for i in range(self.max_iterations):
            # 1. 选择
            node = self.select(root)

            # 2. 扩展
            if not node.is_terminal():
                node = self.expand(node)

            # 3. 模拟
            reward = self.simulate(node)

            # 4. 反向传播
            self.backpropagate(node, reward)

            # 5. 反思（R-MCTS 特有）
            if i % 20 == 0:
                self.reflect(root)

        # 返回最佳路径
        path = self.get_best_path(root)
        print(f"\n最佳路径: {path}")
        return path

    def select(self, node: MCTSNode) -> MCTSNode:
        """选择节点"""
        while not node.is_terminal():
            if not node.is_fully_expanded():
                return node
            else:
                node = node.best_child()
        return node

    def expand(self, node: MCTSNode) -> MCTSNode:
        """扩展节点"""
        action = node.untried_actions.pop()
        new_state = f"{node.state} → {action}"
        child = MCTSNode(new_state, parent=node)
        node.children.append(child)
        return child

    def simulate(self, node: MCTSNode) -> float:
        """模拟"""
        state = node.state
        depth = 0
        max_depth = 5

        while depth < max_depth:
            actions = self.get_actions(state)
            if not actions:
                break

            action = random.choice(actions)
            state = f"{state} → {action}"
            depth += 1

        # 评估最终状态
        return self.evaluate_state(state)

    def backpropagate(self, node: MCTSNode, reward: float):
        """反向传播"""
        while node is not None:
            node.visits += 1
            node.value += reward
            node = node.parent

    def reflect(self, root: MCTSNode):
        """反思（R-MCTS 特有）"""
        # 分析当前最佳路径
        best_path = self.get_best_path(root)

        # 记录反思
        reflection = {
            'iteration': len(self.reflection_memory),
            'best_path': best_path,
            'avg_value': root.value / root.visits if root.visits > 0 else 0
        }
        self.reflection_memory.append(reflection)

        print(f"反思 {len(self.reflection_memory)}: 当前最佳路径 {best_path}")

    def get_actions(self, state: str) -> List[str]:
        """获取可选行动"""
        if "研究" in state:
            return ["搜索", "阅读", "总结"]
        elif "分析" in state:
            return ["收集", "处理", "结论"]
        else:
            return []

    def evaluate_state(self, state: str) -> float:
        """评估状态"""
        # 简单评估：根据深度和关键词
        score = state.count("→") * 2.0

        if "总结" in state:
            score += 5.0
        if "结论" in state:
            score += 5.0

        return score

    def get_best_path(self, root: MCTSNode) -> List[str]:
        """获取最佳路径"""
        path = []
        node = root

        while node.children:
            node = max(node.children, key=lambda c: c.visits)
            action = node.state.split(" → ")[-1]
            path.append(action)

        return path

# 测试
mcts = ReflectiveMCTS(max_iterations=50)
best_path = mcts.search("研究 AI Agent")
```

---

## 5. 2026 年技术：EnCompass 回溯

### 5.1 EnCompass 错误恢复

```python
from enum import Enum
from typing import Optional

class ExecutionStatus(Enum):
    """执行状态"""
    SUCCESS = "success"
    FAILURE = "failure"
    RETRY = "retry"

@dataclass
class ExecutionResult:
    """执行结果"""
    status: ExecutionStatus
    output: Any
    error: Optional[str] = None

class EnCompassBacktracker:
    """EnCompass 风格的回溯系统（2026）"""

    def __init__(self, max_retries: int = 3):
        self.max_retries = max_retries
        self.state_stack = []
        self.execution_log = []

    def execute_with_backtrack(self, task: str) -> ExecutionResult:
        """带回溯的执行"""
        print(f"=== EnCompass 回溯执行 ===")
        print(f"任务: {task}\n")

        # 保存初始状态
        self.save_state({"task": task, "attempts": 0})

        result = self.try_execute(task, depth=0)

        print(f"\n执行日志:")
        for i, log in enumerate(self.execution_log, 1):
            print(f"{i}. {log['action']}: {log['status']}")

        return result

    def try_execute(self, task: str, depth: int) -> ExecutionResult:
        """尝试执行（带重试）"""
        indent = "  " * depth
        print(f"{indent}→ 尝试执行: {task}")

        for attempt in range(self.max_retries):
            print(f"{indent}  尝试 {attempt + 1}/{self.max_retries}")

            # 执行任务
            result = self.execute_task(task)

            # 记录日志
            self.execution_log.append({
                'action': task,
                'attempt': attempt + 1,
                'status': result.status.value
            })

            # 成功
            if result.status == ExecutionStatus.SUCCESS:
                print(f"{indent}  ✓ 成功")
                return result

            # 失败，尝试回溯
            print(f"{indent}  ✗ 失败: {result.error}")

            if attempt < self.max_retries - 1:
                print(f"{indent}  回溯到上一个状态")
                self.restore_state()

                # 尝试替代方案
                alternative = self.find_alternative(task)
                if alternative:
                    print(f"{indent}  尝试替代方案: {alternative}")
                    result = self.try_execute(alternative, depth + 1)
                    if result.status == ExecutionStatus.SUCCESS:
                        return result

        # 所有尝试都失败
        print(f"{indent}  ✗ 所有尝试都失败")
        return ExecutionResult(
            status=ExecutionStatus.FAILURE,
            output=None,
            error="Max retries exceeded"
        )

    def execute_task(self, task: str) -> ExecutionResult:
        """执行任务（模拟）"""
        import time
        time.sleep(0.05)

        # 模拟随机失败
        if random.random() < 0.3:
            return ExecutionResult(
                status=ExecutionStatus.FAILURE,
                output=None,
                error="Random failure"
            )

        return ExecutionResult(
            status=ExecutionStatus.SUCCESS,
            output=f"完成: {task}"
        )

    def save_state(self, state: Dict):
        """保存状态"""
        self.state_stack.append(state.copy())

    def restore_state(self):
        """恢复状态"""
        if self.state_stack:
            return self.state_stack.pop()
        return None

    def find_alternative(self, task: str) -> Optional[str]:
        """找到替代方案"""
        alternatives = {
            "搜索网络": "查询数据库",
            "调用 API": "使用缓存",
            "生成内容": "使用模板"
        }
        return alternatives.get(task)

# 测试
backtracker = EnCompassBacktracker(max_retries=3)
result = backtracker.execute_with_backtrack("搜索网络")

print(f"\n最终结果: {result.status.value}")
if result.output:
    print(f"输出: {result.output}")
```

---

## 6. 完整示例：Agent 路径规划

### 6.1 综合回溯系统

```python
class AgentPathPlanner:
    """Agent 路径规划器（综合回溯）"""

    def __init__(self, max_depth: int = 5):
        self.max_depth = max_depth
        self.all_paths = []
        self.best_path = None
        self.best_score = float('-inf')

    def plan(self, start: str, goal: str) -> List[str]:
        """规划从起点到终点的路径"""
        print(f"=== Agent 路径规划 ===")
        print(f"起点: {start}")
        print(f"终点: {goal}\n")

        # 回溯搜索所有路径
        self.backtrack(start, goal, [], set(), 0)

        print(f"\n找到 {len(self.all_paths)} 条路径")
        print(f"最佳路径（得分 {self.best_score:.2f}）:")
        for i, step in enumerate(self.best_path, 1):
            print(f"  {i}. {step}")

        return self.best_path

    def backtrack(self, current: str, goal: str,
                  path: List[str], visited: Set[str], depth: int):
        """回溯搜索"""
        # 标记为已访问
        visited.add(current)
        path.append(current)

        indent = "  " * depth
        print(f"{indent}→ {current}")

        # 到达目标
        if current == goal:
            score = self.evaluate_path(path)
            print(f"{indent}  ✓ 到达目标，得分: {score:.2f}")

            self.all_paths.append(path.copy())

            if score > self.best_score:
                self.best_score = score
                self.best_path = path.copy()

        # 继续搜索
        elif depth < self.max_depth:
            neighbors = self.get_neighbors(current)

            for neighbor in neighbors:
                if neighbor not in visited:
                    self.backtrack(neighbor, goal, path, visited, depth + 1)

        # 回溯
        path.pop()
        visited.remove(current)

    def get_neighbors(self, node: str) -> List[str]:
        """获取邻居节点"""
        graph = {
            "开始": ["搜索", "分析"],
            "搜索": ["阅读", "总结"],
            "分析": ["处理", "结论"],
            "阅读": ["总结", "结论"],
            "处理": ["结论"],
            "总结": ["完成"],
            "结论": ["完成"],
            "完成": []
        }
        return graph.get(node, [])

    def evaluate_path(self, path: List[str]) -> float:
        """评估路径得分"""
        # 路径越短越好
        length_score = 10.0 / len(path)

        # 包含关键节点加分
        key_nodes = {"阅读", "处理", "总结", "结论"}
        key_score = sum(2.0 for node in path if node in key_nodes)

        return length_score + key_score

# 测试
planner = AgentPathPlanner(max_depth=6)
best_path = planner.plan("开始", "完成")
```

---

## 总结

**回溯搜索的核心要素：**

| 要素 | 说明 | 实现方式 |
|------|------|---------|
| **选择** | 尝试一个选项 | push 到栈 |
| **递归** | 深入搜索 | 递归调用 |
| **撤销** | 回退到上一步 | pop 出栈 |
| **剪枝** | 提前终止无效分支 | 条件判断 |

**经典应用：**
- N 皇后问题
- 组合总和
- 子集生成
- 路径搜索

**AI Agent 应用：**
- 决策树搜索
- 任务规划
- 错误恢复
- 路径优化

**2026 年最新技术：**
- **R-MCTS**: 反思式蒙特卡洛树搜索
- **EnCompass**: 错误恢复与回溯
- **LangGraph**: 状态机回溯
- **Agent 规划**: 多路径探索

**实现建议：**
- 使用显式栈避免递归深度限制
- 添加剪枝优化性能
- 记录所有路径用于分析
- 实现状态恢复机制

**记住：** 回溯搜索是 AI Agent 决策的核心技术，掌握回溯就掌握了智能搜索的精髓！
