# å®æˆ˜ä»£ç 3ï¼šæ€§èƒ½ä¼˜åŒ–ä¸ç”Ÿäº§å®è·µ

ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ RunnableBranch çš„æœ€ä½³å®è·µå’Œä¼˜åŒ–æŠ€å·§ã€‚

---

## ç¯å¢ƒå‡†å¤‡

```bash
uv sync
source .venv/bin/activate
```

---

## ç¤ºä¾‹1ï¼šæ¡ä»¶ç¼“å­˜ä¼˜åŒ–

```python
"""
ç¤ºä¾‹1ï¼šç¼“å­˜æ¡ä»¶è¯„ä¼°ç»“æœ
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
from functools import lru_cache
from dotenv import load_dotenv

load_dotenv()

# æ˜‚è´µçš„æ¡ä»¶è®¡ç®—
@lru_cache(maxsize=1000)
def expensive_check(text: str) -> bool:
    """æ¨¡æ‹Ÿæ˜‚è´µçš„æ¡ä»¶æ£€æŸ¥ï¼ˆå¦‚æ•°æ®åº“æŸ¥è¯¢ï¼‰"""
    import time
    time.sleep(0.1)  # æ¨¡æ‹Ÿå»¶è¿Ÿ
    return "premium" in text.lower()

# é¢„å¤„ç†ï¼šæå–å¯ç¼“å­˜çš„é”®
preprocessor = RunnableLambda(lambda x: {
    **x,
    "cache_key": x["text"][:50]  # ä½¿ç”¨å‰50ä¸ªå­—ç¬¦ä½œä¸ºç¼“å­˜é”®
})

# ä½¿ç”¨ç¼“å­˜çš„æ¡ä»¶
cached_router = RunnableBranch(
    (lambda x: expensive_check(x["cache_key"]),
     RunnableLambda(lambda x: f"Premium: {x['text']}")),
    RunnableLambda(lambda x: f"Standard: {x['text']}")
)

# å®Œæ•´é“¾
chain = preprocessor | cached_router

# æµ‹è¯•
print("=== ç¤ºä¾‹1ï¼šæ¡ä»¶ç¼“å­˜ ===")
import time

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ…¢ï¼‰
start = time.time()
result = chain.invoke({"text": "premium user request"})
print(f"First call: {time.time() - start:.3f}s - {result}")

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆå¿«ï¼Œä½¿ç”¨ç¼“å­˜ï¼‰
start = time.time()
result = chain.invoke({"text": "premium user request"})
print(f"Cached call: {time.time() - start:.3f}s - {result}")

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
print(f"Cache info: {expensive_check.cache_info()}")
```

---

## ç¤ºä¾‹2ï¼šå¹¶è¡Œæ¡ä»¶è¯„ä¼°

```python
"""
ç¤ºä¾‹2ï¼šå¹¶è¡Œè¯„ä¼°å¤šä¸ªç‹¬ç«‹æ¡ä»¶
"""
from langchain_core.runnables import RunnableBranch, RunnableParallel, RunnableLambda
import asyncio

# ç‹¬ç«‹çš„æ¡ä»¶æ£€æŸ¥
def check_sentiment(x):
    """æ£€æŸ¥æƒ…æ„Ÿ"""
    return "positive" if "good" in x.lower() else "negative"

def check_urgency(x):
    """æ£€æŸ¥ç´§æ€¥åº¦"""
    return "urgent" if "urgent" in x.lower() else "normal"

def check_category(x):
    """æ£€æŸ¥ç±»åˆ«"""
    if "bug" in x.lower():
        return "bug"
    elif "feature" in x.lower():
        return "feature"
    return "general"

# å¹¶è¡Œè¯„ä¼°æ‰€æœ‰æ¡ä»¶
condition_evaluator = RunnableParallel(
    sentiment=RunnableLambda(check_sentiment),
    urgency=RunnableLambda(check_urgency),
    category=RunnableLambda(check_category)
)

# åŸºäºè¯„ä¼°ç»“æœè·¯ç”±
def smart_router(x):
    """æ ¹æ®å¹¶è¡Œè¯„ä¼°ç»“æœè·¯ç”±"""
    if x["urgency"] == "urgent" and x["sentiment"] == "negative":
        return "ğŸš¨ High priority negative feedback"
    elif x["category"] == "bug":
        return "ğŸ› Bug report"
    elif x["category"] == "feature":
        return "âœ¨ Feature request"
    else:
        return "ğŸ“ General feedback"

router = RunnableLambda(smart_router)

# å®Œæ•´é“¾
chain = condition_evaluator | router

# æµ‹è¯•
print("\n=== ç¤ºä¾‹2ï¼šå¹¶è¡Œæ¡ä»¶è¯„ä¼° ===")
result = chain.invoke("This is an urgent bug! Not good!")
print(result)
```

---

## ç¤ºä¾‹3ï¼šæ‰¹å¤„ç†ä¼˜åŒ–

```python
"""
ç¤ºä¾‹3ï¼šæ‰¹é‡å¤„ç†ä¼˜åŒ–
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda

# åˆ›å»ºè·¯ç”±
router = RunnableBranch(
    (lambda x: x["priority"] == "high",
     RunnableLambda(lambda x: f"HIGH: {x['text']}")),
    (lambda x: x["priority"] == "medium",
     RunnableLambda(lambda x: f"MEDIUM: {x['text']}")),
    RunnableLambda(lambda x: f"LOW: {x['text']}")
)

# æ‰¹é‡è¾“å…¥
inputs = [
    {"priority": "high", "text": "Urgent issue"},
    {"priority": "medium", "text": "Normal request"},
    {"priority": "low", "text": "Info"},
    {"priority": "high", "text": "Critical bug"},
]

# æ‰¹å¤„ç†
print("\n=== ç¤ºä¾‹3ï¼šæ‰¹å¤„ç† ===")
results = router.batch(inputs)
for inp, res in zip(inputs, results):
    print(f"{inp['priority']:6} â†’ {res}")

# å¼‚æ­¥æ‰¹å¤„ç†
async def async_batch_example():
    results = await router.abatch(inputs)
    return results

# asyncio.run(async_batch_example())
```

---

## ç¤ºä¾‹4ï¼šæµå¼è¾“å‡ºä¼˜åŒ–

```python
"""
ç¤ºä¾‹4ï¼šæµå¼è¾“å‡ºä¼˜åŒ–
"""
from langchain_core.runnables import RunnableBranch
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# æµå¼è·¯ç”±
stream_router = RunnableBranch(
    (lambda x: len(x["text"]) > 100,
     ChatPromptTemplate.from_template("Summarize: {text}") | ChatOpenAI(model="gpt-4o-mini")),
    (lambda x: len(x["text"]) > 20,
     ChatPromptTemplate.from_template("Respond: {text}") | ChatOpenAI(model="gpt-4o-mini")),
    ChatPromptTemplate.from_template("Echo: {text}") | ChatOpenAI(model="gpt-4o-mini")
)

# æµå¼æ‰§è¡Œ
print("\n=== ç¤ºä¾‹4ï¼šæµå¼è¾“å‡º ===")
input_data = {"text": "Can you explain how RunnableBranch works in LangChain?"}

print("Streaming response: ", end="", flush=True)
for chunk in stream_router.stream(input_data):
    if hasattr(chunk, 'content'):
        print(chunk.content, end="", flush=True)
print()
```

---

## ç¤ºä¾‹5ï¼šç›‘æ§ä¸æ—¥å¿—

```python
"""
ç¤ºä¾‹5ï¼šç”Ÿäº§ç¯å¢ƒç›‘æ§
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
from langchain_core.callbacks import BaseCallbackHandler
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è‡ªå®šä¹‰å›è°ƒ
class BranchMonitor(BaseCallbackHandler):
    """ç›‘æ§ Branch æ‰§è¡Œ"""

    def __init__(self):
        self.metrics = {
            "total_calls": 0,
            "branch_counts": {},
            "execution_times": []
        }
        self.start_time = None

    def on_chain_start(self, serialized, inputs, **kwargs):
        self.start_time = time.time()
        self.metrics["total_calls"] += 1
        logger.info(f"Branch started with input: {inputs}")

    def on_chain_end(self, outputs, **kwargs):
        if self.start_time:
            duration = time.time() - self.start_time
            self.metrics["execution_times"].append(duration)
            logger.info(f"Branch completed in {duration:.3f}s")

    def get_stats(self):
        if self.metrics["execution_times"]:
            avg_time = sum(self.metrics["execution_times"]) / len(self.metrics["execution_times"])
            return {
                "total_calls": self.metrics["total_calls"],
                "avg_execution_time": f"{avg_time:.3f}s",
                "min_time": f"{min(self.metrics['execution_times']):.3f}s",
                "max_time": f"{max(self.metrics['execution_times']):.3f}s"
            }
        return self.metrics

# åˆ›å»ºè·¯ç”±
router = RunnableBranch(
    (lambda x: x > 100, RunnableLambda(lambda x: f"Large: {x}")),
    (lambda x: x > 50, RunnableLambda(lambda x: f"Medium: {x}")),
    RunnableLambda(lambda x: f"Small: {x}")
)

# ä½¿ç”¨ç›‘æ§
print("\n=== ç¤ºä¾‹5ï¼šç›‘æ§ä¸æ—¥å¿— ===")
monitor = BranchMonitor()

from langchain_core.runnables.config import RunnableConfig
config = RunnableConfig(callbacks=[monitor])

# æ‰§è¡Œå¤šæ¬¡
for val in [150, 75, 25, 200, 10]:
    result = router.invoke(val, config)
    print(f"{val} â†’ {result}")

# æŸ¥çœ‹ç»Ÿè®¡
print(f"\nStats: {monitor.get_stats()}")
```

---

## ç¤ºä¾‹6ï¼šé”™è¯¯è¿½è¸ª

```python
"""
ç¤ºä¾‹6ï¼šé”™è¯¯è¿½è¸ªä¸æ¢å¤
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
import logging

logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)

# å¯èƒ½å¤±è´¥çš„å¤„ç†å™¨
def risky_handler(x):
    if x < 0:
        raise ValueError(f"Negative value not allowed: {x}")
    return f"Processed: {x}"

# é”™è¯¯è¿½è¸ªåŒ…è£…
def with_error_tracking(handler, handler_name):
    """ä¸ºå¤„ç†å™¨æ·»åŠ é”™è¯¯è¿½è¸ª"""
    def wrapped(x):
        try:
            return handler(x)
        except Exception as e:
            logger.error(
                f"Handler '{handler_name}' failed",
                exc_info=True,
                extra={"input": x, "handler": handler_name}
            )
            return f"Error in {handler_name}: {str(e)}"
    return wrapped

# åˆ›å»ºå¸¦é”™è¯¯è¿½è¸ªçš„è·¯ç”±
router = RunnableBranch(
    (lambda x: x > 100,
     RunnableLambda(with_error_tracking(risky_handler, "large_handler"))),
    (lambda x: x > 0,
     RunnableLambda(with_error_tracking(risky_handler, "positive_handler"))),
    RunnableLambda(with_error_tracking(risky_handler, "default_handler"))
)

# æµ‹è¯•
print("\n=== ç¤ºä¾‹6ï¼šé”™è¯¯è¿½è¸ª ===")
test_values = [150, 50, -10]
for val in test_values:
    result = router.invoke(val)
    print(f"{val} â†’ {result}")
```

---

## ç¤ºä¾‹7ï¼šA/B æµ‹è¯•è·¯ç”±

```python
"""
ç¤ºä¾‹7ï¼šA/B æµ‹è¯•ä¸åŒçš„å¤„ç†ç­–ç•¥
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
import random

# A/B æµ‹è¯•é…ç½®
AB_TEST_CONFIG = {
    "variant_a_ratio": 0.5,  # 50% æµé‡åˆ° A
    "enabled": True
}

# å˜ä½“ Aï¼šç®€å•å¤„ç†
variant_a = RunnableLambda(lambda x: f"Variant A: {x}")

# å˜ä½“ Bï¼šå¤æ‚å¤„ç†
variant_b = RunnableLambda(lambda x: f"Variant B (enhanced): {x.upper()}")

# A/B æµ‹è¯•è·¯ç”±
def ab_test_condition(x):
    """å†³å®šä½¿ç”¨å“ªä¸ªå˜ä½“"""
    if not AB_TEST_CONFIG["enabled"]:
        return False  # ç¦ç”¨æ—¶ä½¿ç”¨é»˜è®¤
    return random.random() < AB_TEST_CONFIG["variant_a_ratio"]

ab_router = RunnableBranch(
    (ab_test_condition, variant_a),
    variant_b  # é»˜è®¤ä½¿ç”¨ B
)

# æµ‹è¯•
print("\n=== ç¤ºä¾‹7ï¼šA/B æµ‹è¯• ===")
results = {"A": 0, "B": 0}
for i in range(100):
    result = ab_router.invoke(f"test_{i}")
    if "Variant A" in result:
        results["A"] += 1
    else:
        results["B"] += 1

print(f"Distribution: A={results['A']}%, B={results['B']}%")
```

---

## ç¤ºä¾‹8ï¼šç”Ÿäº§éƒ¨ç½²é…ç½®

```python
"""
ç¤ºä¾‹8ï¼šç”Ÿäº§ç¯å¢ƒé…ç½®ç®¡ç†
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import os
from typing import Dict, Any

# ç”Ÿäº§é…ç½®
PRODUCTION_CONFIG = {
    "environment": os.getenv("ENVIRONMENT", "development"),
    "enable_caching": os.getenv("ENABLE_CACHING", "true").lower() == "true",
    "max_retries": int(os.getenv("MAX_RETRIES", "3")),
    "timeout": int(os.getenv("TIMEOUT", "30")),
    "log_level": os.getenv("LOG_LEVEL", "INFO"),
    "model_config": {
        "high_priority": os.getenv("HIGH_PRIORITY_MODEL", "gpt-4o"),
        "normal_priority": os.getenv("NORMAL_PRIORITY_MODEL", "gpt-4o-mini"),
    }
}

def create_production_router(config: Dict[str, Any]):
    """æ ¹æ®é…ç½®åˆ›å»ºç”Ÿäº§è·¯ç”±"""

    # é«˜ä¼˜å…ˆçº§å¤„ç†å™¨
    high_priority_handler = (
        ChatPromptTemplate.from_template("High priority: {text}")
        | ChatOpenAI(
            model=config["model_config"]["high_priority"],
            timeout=config["timeout"],
            max_retries=config["max_retries"]
        )
    )

    # æ™®é€šä¼˜å…ˆçº§å¤„ç†å™¨
    normal_priority_handler = (
        ChatPromptTemplate.from_template("Normal priority: {text}")
        | ChatOpenAI(
            model=config["model_config"]["normal_priority"],
            timeout=config["timeout"],
            max_retries=config["max_retries"]
        )
    )

    # åˆ›å»ºè·¯ç”±
    router = RunnableBranch(
        (lambda x: x.get("priority") == "high", high_priority_handler),
        normal_priority_handler
    )

    return router

# åˆ›å»ºç”Ÿäº§è·¯ç”±
print("\n=== ç¤ºä¾‹8ï¼šç”Ÿäº§é…ç½® ===")
print(f"Environment: {PRODUCTION_CONFIG['environment']}")
print(f"High priority model: {PRODUCTION_CONFIG['model_config']['high_priority']}")
print(f"Normal priority model: {PRODUCTION_CONFIG['model_config']['normal_priority']}")

router = create_production_router(PRODUCTION_CONFIG)

# æµ‹è¯•
# result = router.invoke({"priority": "high", "text": "Critical issue"})
# print(f"Result: {result.content[:50]}...")
```

---

## æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
import time

def benchmark_router(router, inputs, name):
    """åŸºå‡†æµ‹è¯•è·¯ç”±æ€§èƒ½"""
    start = time.time()
    results = []
    for inp in inputs:
        result = router.invoke(inp)
        results.append(result)
    duration = time.time() - start

    print(f"\n{name}:")
    print(f"  Total time: {duration:.3f}s")
    print(f"  Avg per call: {duration/len(inputs)*1000:.2f}ms")
    print(f"  Throughput: {len(inputs)/duration:.1f} calls/s")

    return results

# æµ‹è¯•æ•°æ®
test_inputs = [i for i in range(1000)]

# ç®€å•è·¯ç”±
simple_router = RunnableBranch(
    (lambda x: x > 500, RunnableLambda(lambda x: "large")),
    RunnableLambda(lambda x: "small")
)

# å¤æ‚è·¯ç”±
complex_router = RunnableBranch(
    (lambda x: x > 900, RunnableLambda(lambda x: "very_large")),
    (lambda x: x > 700, RunnableLambda(lambda x: "large")),
    (lambda x: x > 500, RunnableLambda(lambda x: "medium")),
    (lambda x: x > 300, RunnableLambda(lambda x: "small")),
    RunnableLambda(lambda x: "very_small")
)

print("\n=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===")
benchmark_router(simple_router, test_inputs, "Simple Router (2 branches)")
benchmark_router(complex_router, test_inputs, "Complex Router (5 branches)")
```

---

## ç”Ÿäº§æ£€æŸ¥æ¸…å•

```python
"""
ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•
"""

PRODUCTION_CHECKLIST = """
## RunnableBranch ç”Ÿäº§éƒ¨ç½²æ£€æŸ¥æ¸…å•

### 1. æ€§èƒ½ä¼˜åŒ–
- [ ] æ¡ä»¶å‡½æ•°ç®€å•å¿«é€Ÿï¼ˆé¿å…å¤æ‚è®¡ç®—ï¼‰
- [ ] ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–é‡å¤æ¡ä»¶è¯„ä¼°
- [ ] æ¡ä»¶é¡ºåºä¼˜åŒ–ï¼ˆå¸¸è§æƒ…å†µåœ¨å‰ï¼‰
- [ ] è€ƒè™‘ä½¿ç”¨ RunnableParallel å¹¶è¡Œè¯„ä¼°ç‹¬ç«‹æ¡ä»¶

### 2. é”™è¯¯å¤„ç†
- [ ] æ‰€æœ‰æ¡ä»¶å‡½æ•°æœ‰é”™è¯¯å¤„ç†
- [ ] æ‰€æœ‰åˆ†æ”¯å¤„ç†å™¨æœ‰é”™è¯¯å¤„ç†
- [ ] é»˜è®¤åˆ†æ”¯ä½œä¸ºå…œåº•
- [ ] é”™è¯¯æ—¥å¿—å®Œæ•´ï¼ˆåŒ…å«è¾“å…¥å’Œä¸Šä¸‹æ–‡ï¼‰

### 3. ç›‘æ§ä¸æ—¥å¿—
- [ ] æ·»åŠ  LangSmith è¿½è¸ª
- [ ] è®°å½•è·¯ç”±å†³ç­–ï¼ˆå“ªä¸ªåˆ†æ”¯è¢«é€‰æ‹©ï¼‰
- [ ] è®°å½•æ‰§è¡Œæ—¶é—´
- [ ] è®¾ç½®å‘Šè­¦é˜ˆå€¼

### 4. é…ç½®ç®¡ç†
- [ ] ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†é…ç½®
- [ ] æ”¯æŒåŠ¨æ€é…ç½®æ›´æ–°
- [ ] ä¸åŒç¯å¢ƒä½¿ç”¨ä¸åŒé…ç½®
- [ ] æ•æ„Ÿä¿¡æ¯ä¸ç¡¬ç¼–ç 

### 5. æµ‹è¯•
- [ ] å•å…ƒæµ‹è¯•æ‰€æœ‰æ¡ä»¶å‡½æ•°
- [ ] å•å…ƒæµ‹è¯•æ‰€æœ‰åˆ†æ”¯å¤„ç†å™¨
- [ ] é›†æˆæµ‹è¯•å®Œæ•´è·¯ç”±
- [ ] è´Ÿè½½æµ‹è¯•ï¼ˆæ‰¹å¤„ç†æ€§èƒ½ï¼‰
- [ ] è¾¹ç•Œæƒ…å†µæµ‹è¯•

### 6. å®‰å…¨
- [ ] è¾“å…¥éªŒè¯ï¼ˆé˜²æ­¢æ³¨å…¥æ”»å‡»ï¼‰
- [ ] æ•æ„Ÿæ•°æ®è„±æ•
- [ ] è®¿é—®æ§åˆ¶ï¼ˆåŸºäºè§’è‰²çš„è·¯ç”±ï¼‰
- [ ] å®¡è®¡æ—¥å¿—

### 7. å¯ç»´æŠ¤æ€§
- [ ] æ¡ä»¶å‡½æ•°æœ‰æ¸…æ™°çš„å‘½åå’Œæ–‡æ¡£
- [ ] è·¯ç”±é€»è¾‘æœ‰æ³¨é‡Šè¯´æ˜
- [ ] é¿å…è¿‡æ·±çš„åµŒå¥—ï¼ˆ< 3å±‚ï¼‰
- [ ] ä»£ç å®¡æŸ¥é€šè¿‡

### 8. æˆæœ¬ä¼˜åŒ–
- [ ] æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
- [ ] ä½¿ç”¨ç¼“å­˜å‡å°‘ LLM è°ƒç”¨
- [ ] æ‰¹å¤„ç†ä¼˜åŒ–
- [ ] ç›‘æ§ API ä½¿ç”¨é‡

### 9. å¯è§‚æµ‹æ€§
- [ ] é›†æˆ LangSmith
- [ ] è‡ªå®šä¹‰å›è°ƒè¿½è¸ª
- [ ] æŒ‡æ ‡æ”¶é›†ï¼ˆå»¶è¿Ÿã€æˆåŠŸç‡ã€é”™è¯¯ç‡ï¼‰
- [ ] å¯è§†åŒ–ä»ªè¡¨æ¿

### 10. æ–‡æ¡£
- [ ] API æ–‡æ¡£å®Œæ•´
- [ ] è·¯ç”±é€»è¾‘æ–‡æ¡£åŒ–
- [ ] è¿ç»´æ‰‹å†Œ
- [ ] æ•…éšœæ’æŸ¥æŒ‡å—
"""

print(PRODUCTION_CHECKLIST)
```

---

## è¿è¡Œè¾“å‡ºç¤ºä¾‹

```
=== ç¤ºä¾‹1ï¼šæ¡ä»¶ç¼“å­˜ ===
First call: 0.102s - Premium: premium user request
Cached call: 0.001s - Premium: premium user request
Cache info: CacheInfo(hits=1, misses=1, maxsize=1000, currsize=1)

=== ç¤ºä¾‹2ï¼šå¹¶è¡Œæ¡ä»¶è¯„ä¼° ===
ğŸš¨ High priority negative feedback

=== ç¤ºä¾‹3ï¼šæ‰¹å¤„ç† ===
high   â†’ HIGH: Urgent issue
medium â†’ MEDIUM: Normal request
low    â†’ LOW: Info
high   â†’ HIGH: Critical bug

=== ç¤ºä¾‹4ï¼šæµå¼è¾“å‡º ===
Streaming response: RunnableBranch is a component in LangChain that allows...

=== ç¤ºä¾‹5ï¼šç›‘æ§ä¸æ—¥å¿— ===
150 â†’ Large: 150
75 â†’ Medium: 75
25 â†’ Small: 25
200 â†’ Large: 200
10 â†’ Small: 10

Stats: {'total_calls': 5, 'avg_execution_time': '0.001s', 'min_time': '0.000s', 'max_time': '0.002s'}

=== ç¤ºä¾‹6ï¼šé”™è¯¯è¿½è¸ª ===
150 â†’ Processed: 150
50 â†’ Processed: 50
-10 â†’ Error in default_handler: Negative value not allowed: -10

=== ç¤ºä¾‹7ï¼šA/B æµ‹è¯• ===
Distribution: A=48%, B=52%

=== ç¤ºä¾‹8ï¼šç”Ÿäº§é…ç½® ===
Environment: development
High priority model: gpt-4o
Normal priority model: gpt-4o-mini

=== æ€§èƒ½åŸºå‡†æµ‹è¯• ===
Simple Router (2 branches):
  Total time: 0.045s
  Avg per call: 0.05ms
  Throughput: 22222.2 calls/s

Complex Router (5 branches):
  Total time: 0.089s
  Avg per call: 0.09ms
  Throughput: 11235.9 calls/s
```

---

## æœ€ä½³å®è·µæ€»ç»“

1. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - ç¼“å­˜æ¡ä»¶è¯„ä¼°ç»“æœ
   - å¹¶è¡Œè¯„ä¼°ç‹¬ç«‹æ¡ä»¶
   - ä¼˜åŒ–æ¡ä»¶é¡ºåº
   - ä½¿ç”¨æ‰¹å¤„ç†

2. **ç”Ÿäº§éƒ¨ç½²**ï¼š
   - å®Œå–„çš„é”™è¯¯å¤„ç†
   - è¯¦ç»†çš„ç›‘æ§æ—¥å¿—
   - é…ç½®ç®¡ç†
   - A/B æµ‹è¯•æ”¯æŒ

3. **å¯è§‚æµ‹æ€§**ï¼š
   - LangSmith é›†æˆ
   - è‡ªå®šä¹‰å›è°ƒ
   - æŒ‡æ ‡æ”¶é›†
   - æ€§èƒ½åŸºå‡†æµ‹è¯•

4. **æˆæœ¬æ§åˆ¶**ï¼š
   - æ ¹æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
   - ç¼“å­˜ç­–ç•¥
   - æ‰¹å¤„ç†ä¼˜åŒ–

5. **å®‰å…¨æ€§**ï¼š
   - è¾“å…¥éªŒè¯
   - é”™è¯¯è¿½è¸ª
   - è®¿é—®æ§åˆ¶
   - å®¡è®¡æ—¥å¿—
