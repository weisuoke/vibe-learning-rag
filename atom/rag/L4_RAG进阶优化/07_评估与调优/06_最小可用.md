# 最小可用

> 掌握哪 20% 就能解决 80% 问题

---

掌握以下内容，就能开始评估和优化你的 RAG 系统：

## 1. 构建评估数据集

评估 RAG 系统的第一步，是准备一份**标准化的评估数据集**。没有数据集，一切评估都无从谈起。

一个完整的评估样本需要 **4 个字段**：

| 字段 | 含义 | 来源 | 示例 |
|------|------|------|------|
| `question` | 用户提出的问题 | 人工编写或从真实日志中提取 | "什么是 RAG？" |
| `ground_truth` | 标准答案（人工标注） | 领域专家编写 | "RAG 是检索增强生成，结合检索与生成..." |
| `contexts` | 检索到的上下文列表 | RAG 系统检索阶段的输出 | ["RAG（Retrieval-Augmented Generation）是..."] |
| `answer` | RAG 系统生成的答案 | RAG 系统生成阶段的输出 | "RAG 是一种结合检索和生成的技术..." |

**为什么需要这 4 个字段？**

- `question` + `ground_truth`：评估**检索质量**（检索到的内容能否覆盖标准答案）
- `question` + `answer`：评估**答案相关性**（回答是否切题）
- `contexts` + `answer`：评估**忠实度**（答案是否忠于检索到的内容，有没有编造）
- `contexts` + `ground_truth`：评估**上下文召回**（检索到的内容是否足够全面）

```python
# 构建评估数据集
eval_dataset = [
    {
        "question": "什么是 RAG？",
        "ground_truth": "RAG（Retrieval-Augmented Generation）是检索增强生成技术，"
                        "通过先检索相关文档再结合 LLM 生成答案，解决大模型知识过时和幻觉问题。",
        "contexts": [
            "RAG（Retrieval-Augmented Generation）是一种结合信息检索与文本生成的技术架构，"
            "它先从知识库中检索相关文档，再将检索结果作为上下文输入给大语言模型生成答案。"
        ],
        "answer": "RAG 是一种结合检索和生成的技术，先从知识库检索相关信息，"
                  "再利用大语言模型基于检索结果生成答案。"
    },
    {
        "question": "RAG 系统中 Chunking 的作用是什么？",
        "ground_truth": "Chunking 是将长文档切分成小块的过程，"
                        "目的是让每个块在语义上相对完整，便于向量化和精准检索。",
        "contexts": [
            "文本分块（Chunking）是 RAG 流程中的关键步骤，将长文档按一定策略切分成较小的文本块，"
            "每个块独立进行 Embedding 编码并存入向量数据库。",
            "常见的分块策略包括固定大小分块、基于分隔符分块和语义分块。"
        ],
        "answer": "Chunking 的作用是将长文档切分成小块，让每个块语义完整，"
                  "便于 Embedding 编码和向量检索，提高检索的精准度。"
    },
    {
        "question": "什么是 ReRank？",
        "ground_truth": "ReRank 是对初步检索结果进行二次精排的技术，"
                        "使用 Cross-Encoder 等模型对 query-document 对进行精细打分，提升排序质量。",
        "contexts": [
            "ReRank（重排序）是在初步检索之后，使用更精细的模型对候选文档重新排序的技术。"
        ],
        "answer": "ReRank 是对检索结果进行二次排序的技术，能提升最终返回结果的相关性。"
    }
]

print(f"评估数据集大小: {len(eval_dataset)} 条")
print(f"第一条样本的字段: {list(eval_dataset[0].keys())}")
```

**实际应用建议：**

- **起步阶段**：手动编写 20-50 条高质量评估样本即可
- **数据来源**：从真实用户问题中挑选，覆盖常见场景和边界情况
- **标注原则**：`ground_truth` 由领域专家编写，确保准确性
- **持续积累**：每次发现 bad case 就加入评估集，逐步扩大覆盖面

---

## 2. 用 RAGAS 跑一次端到端评估

[RAGAS](https://github.com/explodinggradients/ragas) 是目前最流行的 RAG 评估框架，**5 行代码**就能完成端到端评估。

```python
"""
使用 RAGAS 进行 RAG 系统端到端评估
安装: pip install ragas datasets
"""
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

# ===== 1. 准备评估数据（从上面的 eval_dataset 转换格式） =====
eval_data = {
    "question": [
        "什么是 RAG？",
        "RAG 系统中 Chunking 的作用是什么？",
        "什么是 ReRank？",
    ],
    "answer": [
        "RAG 是一种结合检索和生成的技术，先从知识库检索相关信息，再利用大语言模型基于检索结果生成答案。",
        "Chunking 的作用是将长文档切分成小块，让每个块语义完整，便于 Embedding 编码和向量检索。",
        "ReRank 是对检索结果进行二次排序的技术，能提升最终返回结果的相关性。",
    ],
    "contexts": [
        ["RAG（Retrieval-Augmented Generation）是一种结合信息检索与文本生成的技术架构。"],
        ["文本分块（Chunking）是 RAG 流程中的关键步骤。", "常见的分块策略包括固定大小分块。"],
        ["ReRank（重排序）是在初步检索之后，使用更精细的模型对候选文档重新排序的技术。"],
    ],
    "ground_truth": [
        "RAG 是检索增强生成技术，通过先检索相关文档再结合 LLM 生成答案。",
        "Chunking 是将长文档切分成小块的过程，便于向量化和精准检索。",
        "ReRank 是对初步检索结果进行二次精排的技术，使用 Cross-Encoder 等模型提升排序质量。",
    ],
}

# ===== 2. 转换为 HuggingFace Dataset 格式 =====
dataset = Dataset.from_dict(eval_data)

# ===== 3. 一行代码运行评估 =====
result = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
)

# ===== 4. 查看结果 =====
print("=" * 50)
print("RAG 系统评估报告")
print("=" * 50)
print(result)

# 转换为 pandas DataFrame 查看每条样本的详细得分
df = result.to_pandas()
print("\n每条样本的详细得分：")
print(df[["question", "faithfulness", "answer_relevancy",
          "context_precision", "context_recall"]])
```

**如何解读输出：**

```
# 示例输出（分数范围 0-1，越高越好）
{
    'faithfulness': 0.8833,        # 答案忠实度
    'answer_relevancy': 0.9125,    # 答案相关性
    'context_precision': 0.7500,   # 上下文精确度
    'context_recall': 0.7222       # 上下文召回率
}
```

- **所有分数 > 0.8**：系统表现良好，可以上线
- **某个分数 < 0.7**：该维度存在明显问题，需要针对性优化
- **某个分数 < 0.5**：该维度严重不足，必须优先修复

---

## 3. 理解 4 个核心指标

RAGAS 的 4 个核心指标分别评估 RAG 系统的不同环节：

| 指标 | 衡量什么 | 直觉理解 | 好的分数 | 依赖字段 |
|------|----------|----------|----------|----------|
| **Faithfulness** | 答案是否忠于上下文 | "有没有编造内容" | > 0.8 | `answer` + `contexts` |
| **Answer Relevancy** | 答案是否回答了问题 | "有没有跑题" | > 0.8 | `question` + `answer` |
| **Context Precision** | 相关上下文排名是否靠前 | "好的排前面了吗" | > 0.7 | `question` + `contexts` + `ground_truth` |
| **Context Recall** | 上下文是否覆盖了答案要点 | "有没有漏掉关键信息" | > 0.7 | `contexts` + `ground_truth` |

**用一个例子理解这 4 个指标：**

假设用户问："Python 的 GIL 是什么？"

```
标准答案（ground_truth）：
  "GIL 是全局解释器锁，限制同一时刻只有一个线程执行 Python 字节码。"

检索到的上下文（contexts）：
  [1] "GIL（Global Interpreter Lock）是 CPython 中的互斥锁..."  ← 相关
  [2] "Python 支持多线程编程，使用 threading 模块..."           ← 部分相关
  [3] "Java 的并发模型基于 JVM..."                             ← 不相关

RAG 生成的答案（answer）：
  "GIL 是 Python 的全局解释器锁，它确保同一时刻只有一个线程执行字节码。
   这是 CPython 的实现特性，Jython 和 PyPy 没有 GIL。"
```

- **Faithfulness 高**：答案中的每句话都能在 contexts 中找到依据
- **Answer Relevancy 高**：答案确实在回答"GIL 是什么"
- **Context Precision 中等**：第 3 条上下文不相关，排在了前面
- **Context Recall 高**：上下文覆盖了标准答案的核心要点

---

## 4. 检索质量快速诊断

除了 RAGAS 的端到端评估，有时你需要**单独诊断检索环节**的质量。以下两个指标最常用：

```python
"""
检索质量快速诊断工具
Precision@K 和 Recall@K 是最基础的检索评估指标
"""
from typing import List


def precision_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:
    """
    Precision@K：前 K 个检索结果中，相关文档的比例
    直觉：检索结果有多"精准"，有没有混入无关内容

    参数:
        retrieved_ids: 检索返回的文档 ID 列表（按相关性排序）
        relevant_ids: 真正相关的文档 ID 列表
        k: 取前 K 个结果
    """
    retrieved_k = retrieved_ids[:k]
    relevant_count = len(set(retrieved_k) & set(relevant_ids))
    return relevant_count / k


def recall_at_k(retrieved_ids: List[str], relevant_ids: List[str], k: int) -> float:
    """
    Recall@K：前 K 个检索结果覆盖了多少相关文档
    直觉：有没有"漏掉"重要的文档

    参数:
        retrieved_ids: 检索返回的文档 ID 列表（按相关性排序）
        relevant_ids: 真正相关的文档 ID 列表
        k: 取前 K 个结果
    """
    if not relevant_ids:
        return 0.0
    retrieved_k = retrieved_ids[:k]
    relevant_count = len(set(retrieved_k) & set(relevant_ids))
    return relevant_count / len(relevant_ids)


def mrr(retrieved_ids: List[str], relevant_ids: List[str]) -> float:
    """
    MRR（Mean Reciprocal Rank）：第一个相关文档排在第几位
    直觉：用户能多快找到想要的结果

    参数:
        retrieved_ids: 检索返回的文档 ID 列表
        relevant_ids: 真正相关的文档 ID 列表
    """
    for i, doc_id in enumerate(retrieved_ids):
        if doc_id in relevant_ids:
            return 1.0 / (i + 1)
    return 0.0


# ===== 实际使用示例 =====
# 假设检索返回了 5 个文档，其中 doc_1, doc_3, doc_5 是真正相关的
retrieved = ["doc_1", "doc_4", "doc_3", "doc_2", "doc_5"]
relevant = ["doc_1", "doc_3", "doc_5"]

print("===== 检索质量诊断 =====")
for k in [1, 3, 5]:
    p = precision_at_k(retrieved, relevant, k)
    r = recall_at_k(retrieved, relevant, k)
    print(f"K={k}: Precision@{k}={p:.2f}, Recall@{k}={r:.2f}")

print(f"MRR={mrr(retrieved, relevant):.2f}")

# 预期输出：
# K=1: Precision@1=1.00, Recall@1=0.33
# K=3: Precision@3=0.67, Recall@3=0.67
# K=5: Precision@5=0.60, Recall@5=1.00
# MRR=1.00
```

**解读结果：**

- **Precision@K 高、Recall@K 低**：检索很精准但漏掉了很多相关文档 → 增大 K 值或优化召回策略
- **Precision@K 低、Recall@K 高**：检索覆盖面广但混入太多无关内容 → 添加 ReRank 或收紧检索条件
- **MRR 低**：最相关的文档排名靠后 → 优化排序算法或添加 ReRank

---

## 5. 基于评估结果的优化方向判断

评估的最终目的是**指导优化**。根据哪个指标低，可以快速判断优化方向：

```
评估结果分析决策树：
═══════════════════════════════════════════════════════════════

Context Recall 低（< 0.7）
  → 诊断：检索召回不足，相关文档没有被检索到
  → 优化方向：
    • 更换更好的 Embedding 模型（如 text-embedding-3-large）
    • 调整 Chunk 大小（过大导致语义模糊，过小导致信息碎片化）
    • 增加检索返回数量（Top-K 从 3 调到 5 或 10）
    • 引入混合检索（BM25 + 向量检索互补）

Context Precision 低（< 0.7）
  → 诊断：检索精度不足，无关文档混入结果
  → 优化方向：
    • 添加 ReRank 重排序（Cross-Encoder 精排）
    • 优化 Query 改写（让查询更精准）
    • 添加元数据过滤（按时间、类别等预筛选）

Faithfulness 低（< 0.8）
  → 诊断：生成阶段有幻觉，答案包含上下文中没有的内容
  → 优化方向：
    • 改进 Prompt（强调"只根据提供的上下文回答"）
    • 降低 temperature（减少随机性，如从 0.7 降到 0.1）
    • 添加引用约束（要求模型标注信息来源）
    • 减少无关上下文的注入（提高 Context Precision）

Answer Relevancy 低（< 0.8）
  → 诊断：答案跑题，没有正面回答用户问题
  → 优化方向：
    • 改进 Prompt（明确要求"直接回答用户的问题"）
    • 优化上下文注入方式（把最相关的内容放在最前面）
    • 检查是否上下文太多导致模型"迷失"（减少注入数量）

═══════════════════════════════════════════════════════════════
```

**实用口诀：**

```
召回低 → 检索不够广 → 扩大搜索范围
精度低 → 检索不够准 → 精细化排序
忠实低 → 生成在编造 → 约束 Prompt
相关低 → 回答在跑题 → 聚焦问题本身
```

---

## 这些知识足以：

- **对任何 RAG 系统进行基础评估**：构建数据集 + RAGAS 一键评估
- **快速定位系统瓶颈**：通过 4 个核心指标判断是检索问题还是生成问题
- **制定有针对性的优化方案**：根据决策树找到具体优化方向
- **用数据说话，而不是凭感觉调参**：每次优化前后都有量化对比
- **为后续深入学习打下基础**：理解评估框架后，可以进一步学习自定义指标和 A/B 测试
