# 实战代码 05：生产部署模式

> FastAPI、监控、Docker 的完整生产部署方案

---

## 概述

本文提供完整的生产部署代码示例，包括 FastAPI 集成、监控、Docker 容器化等。

---

## 1. FastAPI 集成

```python
"""
FastAPI 集成示例
main.py
"""
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import asyncio

load_dotenv()

app = FastAPI(title="LCEL API", version="1.0.0")

# 初始化链
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
prompt = ChatPromptTemplate.from_template("回答问题：{question}")
chain = prompt | llm | StrOutputParser()

class QueryRequest(BaseModel):
    question: str
    stream: bool = False

class QueryResponse(BaseModel):
    answer: str

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """查询端点"""
    try:
        if request.stream:
            async def generate():
                async for chunk in chain.astream({"question": request.question}):
                    yield f"data: {chunk}\n\n"
            return StreamingResponse(generate(), media_type="text/event-stream")
        else:
            answer = await chain.ainvoke({"question": request.question})
            return QueryResponse(answer=answer)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """健康检查"""
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 2. 监控集成

```python
"""
Prometheus 监控集成
monitoring.py
"""
from prometheus_client import Counter, Histogram, start_http_server
from langchain_core.callbacks import BaseCallbackHandler
import time

# 定义指标
request_counter = Counter('chain_requests_total', 'Total requests', ['status'])
latency_histogram = Histogram('chain_latency_seconds', 'Request latency')
token_counter = Counter('chain_tokens_total', 'Total tokens', ['type'])

class PrometheusHandler(BaseCallbackHandler):
    """Prometheus 监控回调"""

    def on_chain_start(self, serialized, inputs, **kwargs):
        self.start_time = time.time()

    def on_chain_end(self, outputs, **kwargs):
        latency = time.time() - self.start_time
        latency_histogram.observe(latency)
        request_counter.labels(status='success').inc()

    def on_chain_error(self, error, **kwargs):
        request_counter.labels(status='error').inc()

    def on_llm_end(self, response, **kwargs):
        if hasattr(response, 'llm_output') and response.llm_output:
            usage = response.llm_output.get('token_usage', {})
            token_counter.labels(type='input').inc(usage.get('prompt_tokens', 0))
            token_counter.labels(type='output').inc(usage.get('completion_tokens', 0))

# 启动 Prometheus 服务器
def start_monitoring(port=8001):
    start_http_server(port)
    print(f"Prometheus metrics available at http://localhost:{port}")

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from dotenv import load_dotenv

    load_dotenv()

    start_monitoring()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("回答：{question}")
    chain = prompt | llm | StrOutputParser()

    handler = PrometheusHandler()

    # 执行请求
    result = chain.invoke(
        {"question": "什么是 LCEL？"},
        config={"callbacks": [handler]}
    )

    print(f"结果: {result}")
    print("访问 http://localhost:8001 查看指标")
```

---

## 3. Docker 部署

```dockerfile
# Dockerfile
FROM python:3.13-slim

WORKDIR /app

# 安装依赖
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制代码
COPY . .

# 暴露端口
EXPOSE 8000

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# 启动服务
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana

volumes:
  grafana-storage:
```

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'lcel-app'
    static_configs:
      - targets: ['app:8001']
```

---

## 4. Kubernetes 部署

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lcel-app
  labels:
    app: lcel-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: lcel-app
  template:
    metadata:
      labels:
        app: lcel-app
    spec:
      containers:
      - name: lcel-app
        image: lcel-app:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-keys
              key: openai
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: lcel-app
spec:
  selector:
    app: lcel-app
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: lcel-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: lcel-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

## 5. 完整部署脚本

```bash
#!/bin/bash
# deploy.sh

set -e

echo "=== LCEL 应用部署脚本 ==="

# 1. 构建 Docker 镜像
echo "1. 构建 Docker 镜像..."
docker build -t lcel-app:latest .

# 2. 运行测试
echo "2. 运行测试..."
docker run --rm lcel-app:latest pytest

# 3. 推送到镜像仓库
echo "3. 推送到镜像仓库..."
docker tag lcel-app:latest registry.example.com/lcel-app:latest
docker push registry.example.com/lcel-app:latest

# 4. 部署到 Kubernetes
echo "4. 部署到 Kubernetes..."
kubectl apply -f k8s/

# 5. 等待部署完成
echo "5. 等待部署完成..."
kubectl rollout status deployment/lcel-app

# 6. 验证部署
echo "6. 验证部署..."
kubectl get pods -l app=lcel-app

echo "=== 部署完成！ ==="
```

---

## 6. 环境配置

```bash
# .env.example
OPENAI_API_KEY=your_key_here
LANGSMITH_API_KEY=your_key_here
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=production

# 可选配置
OPENAI_BASE_URL=https://api.openai.com/v1
LOG_LEVEL=INFO
MAX_CONCURRENCY=10
```

```python
# config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    openai_api_key: str
    langsmith_api_key: str = ""
    langsmith_tracing: bool = False
    langsmith_project: str = "default"
    log_level: str = "INFO"
    max_concurrency: int = 10

    class Config:
        env_file = ".env"

settings = Settings()
```

---

## 总结

### 部署模式总结

| 模式 | 适用场景 | 优点 | 缺点 |
|------|----------|------|------|
| FastAPI | API 服务 | 简单、快速 | 单机限制 |
| Docker | 容器化 | 可移植、隔离 | 需要编排 |
| Kubernetes | 大规模部署 | 自动扩缩容、高可用 | 复杂度高 |

### 关键要点

1. **FastAPI**：提供 REST API 和流式接口
2. **监控**：Prometheus + Grafana
3. **容器化**：Docker + docker-compose
4. **编排**：Kubernetes + HPA

---

**下一步**：阅读 `07_实战代码_06_迁移自动化工具.md` 学习自动化迁移
