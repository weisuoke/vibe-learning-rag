# 实战代码 2：语义缓存系统

## 场景描述

**AI Agent 经常收到相似但不完全相同的查询，传统缓存无法命中，导致重复调用 LLM 浪费资源。**

### 核心需求

1. **语义相似性匹配**：识别意思相同但表述不同的查询
2. **快速检索**：O(1) 或接近 O(1) 的查找速度
3. **成本节省**：减少 LLM API 调用次数
4. **缓存管理**：LRU 驱逐策略

---

## 完整实现

```python
from typing import Optional, Dict, Tuple
import hashlib
import time
from dataclasses import dataclass
from collections import OrderedDict


@dataclass
class CacheEntry:
    """缓存条目"""
    query: str
    response: str
    timestamp: float
    hit_count: int


class SemanticCache:
    """
    语义缓存系统
    使用哈希表 + 标准化实现快速语义匹配
    """

    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.cache: OrderedDict[int, CacheEntry] = OrderedDict()
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
        }

    def _normalize_query(self, query: str) -> str:
        """
        标准化查询
        - 转小写
        - 去除多余空格
        - 去除标点符号
        """
        import re

        # 转小写
        normalized = query.lower()

        # 去除标点符号
        normalized = re.sub(r'[^\w\s]', '', normalized)

        # 去除多余空格
        normalized = ' '.join(normalized.split())

        return normalized

    def _compute_hash(self, query: str) -> int:
        """计算查询的哈希值"""
        normalized = self._normalize_query(query)
        # 使用 MD5 生成稳定的哈希值
        hash_obj = hashlib.md5(normalized.encode())
        return int(hash_obj.hexdigest(), 16)

    def get(self, query: str) -> Optional[str]:
        """
        获取缓存的响应
        返回 None 如果未命中
        """
        query_hash = self._compute_hash(query)

        if query_hash in self.cache:
            # 缓存命中
            entry = self.cache[query_hash]
            entry.hit_count += 1

            # 移到末尾（LRU）
            self.cache.move_to_end(query_hash)

            self.stats["hits"] += 1
            return entry.response

        # 缓存未命中
        self.stats["misses"] += 1
        return None

    def set(self, query: str, response: str):
        """缓存查询和响应"""
        query_hash = self._compute_hash(query)

        # 检查是否需要驱逐
        if len(self.cache) >= self.max_size and query_hash not in self.cache:
            # 驱逐最旧的条目（LRU）
            self.cache.popitem(last=False)
            self.stats["evictions"] += 1

        # 添加或更新缓存
        self.cache[query_hash] = CacheEntry(
            query=query,
            response=response,
            timestamp=time.time(),
            hit_count=0,
        )

    def hit_rate(self) -> float:
        """计算缓存命中率"""
        total = self.stats["hits"] + self.stats["misses"]
        if total == 0:
            return 0.0
        return self.stats["hits"] / total

    def get_stats(self) -> Dict:
        """获取统计信息"""
        return {
            **self.stats,
            "cache_size": len(self.cache),
            "hit_rate": self.hit_rate(),
        }


# 使用示例
if __name__ == "__main__":
    cache = SemanticCache(max_size=100)

    # 第一次查询
    query1 = "如何学习 Python?"
    response1 = cache.get(query1)
    if response1 is None:
        response1 = "推荐从基础语法开始，然后做项目实践。"  # 模拟 LLM 调用
        cache.set(query1, response1)
        print(f"查询: {query1}")
        print(f"响应: {response1}")
        print("(LLM 调用)")

    # 第二次查询（相同问题，不同表述）
    query2 = "如何学习 PYTHON?"  # 大小写不同
    response2 = cache.get(query2)
    if response2:
        print(f"\n查询: {query2}")
        print(f"响应: {response2}")
        print("(缓存命中！)")

    # 第三次查询（相似问题）
    query3 = "怎样学习Python？"  # 标点符号不同
    response3 = cache.get(query3)
    if response3:
        print(f"\n查询: {query3}")
        print(f"响应: {response3}")
        print("(缓存命中！)")

    # 统计信息
    print(f"\n缓存统计: {cache.get_stats()}")
```

**预期输出：**
```
查询: 如何学习 Python?
响应: 推荐从基础语法开始，然后做项目实践。
(LLM 调用)

查询: 如何学习 PYTHON?
响应: 推荐从基础语法开始，然后做项目实践。
(缓存命中！)

查询: 怎样学习Python？
响应: 推荐从基础语法开始，然后做项目实践。
(缓存命中！)

缓存统计: {'hits': 2, 'misses': 1, 'evictions': 0, 'cache_size': 1, 'hit_rate': 0.6666666666666666}
```

---

## 进阶版本：LSH 语义缓存

### 使用 Locality-Sensitive Hashing

```python
import numpy as np
from typing import List, Tuple


class LSHSemanticCache:
    """
    使用 LSH 的语义缓存
    可以匹配语义相似但表述完全不同的查询
    """

    def __init__(
        self,
        max_size: int = 1000,
        num_hashes: int = 10,
        similarity_threshold: float = 0.7,
    ):
        self.max_size = max_size
        self.num_hashes = num_hashes
        self.similarity_threshold = similarity_threshold

        # 缓存：LSH 签名 → (embedding, query, response)
        self.cache: Dict[tuple, Tuple[np.ndarray, str, str]] = {}

        # 随机投影向量（用于 LSH）
        self.projections = [
            np.random.randn(384) for _ in range(num_hashes)  # 假设 384 维 embedding
        ]

        self.stats = {"hits": 0, "misses": 0, "evictions": 0}

    def _get_embedding(self, text: str) -> np.ndarray:
        """
        获取文本的 embedding
        实际应用中应该调用 embedding 模型
        这里使用简化版本
        """
        # 简化：使用字符的 ASCII 值生成伪 embedding
        chars = list(text.lower())[:384]
        embedding = np.zeros(384)
        for i, char in enumerate(chars):
            embedding[i] = ord(char) / 255.0

        # 归一化
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm

        return embedding

    def _lsh_hash(self, embedding: np.ndarray) -> tuple:
        """计算 LSH 签名"""
        signature = []
        for proj in self.projections:
            # 投影到随机向量，取符号
            bit = 1 if np.dot(embedding, proj) > 0 else 0
            signature.append(bit)
        return tuple(signature)

    def _cosine_similarity(self, v1: np.ndarray, v2: np.ndarray) -> float:
        """计算余弦相似度"""
        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

    def get(self, query: str) -> Optional[str]:
        """获取缓存的响应"""
        query_embedding = self._get_embedding(query)
        query_hash = self._lsh_hash(query_embedding)

        if query_hash in self.cache:
            cached_embedding, cached_query, cached_response = self.cache[query_hash]

            # 验证语义相似度
            similarity = self._cosine_similarity(query_embedding, cached_embedding)

            if similarity >= self.similarity_threshold:
                self.stats["hits"] += 1
                print(f"缓存命中！相似度: {similarity:.2f}")
                print(f"原查询: {cached_query}")
                print(f"新查询: {query}")
                return cached_response

        self.stats["misses"] += 1
        return None

    def set(self, query: str, response: str):
        """缓存查询和响应"""
        query_embedding = self._get_embedding(query)
        query_hash = self._lsh_hash(query_embedding)

        # 检查是否需要驱逐
        if len(self.cache) >= self.max_size and query_hash not in self.cache:
            # 简单策略：随机驱逐
            import random
            evict_key = random.choice(list(self.cache.keys()))
            del self.cache[evict_key]
            self.stats["evictions"] += 1

        self.cache[query_hash] = (query_embedding, query, response)

    def get_stats(self) -> Dict:
        """获取统计信息"""
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total if total > 0 else 0.0

        return {
            **self.stats,
            "cache_size": len(self.cache),
            "hit_rate": hit_rate,
        }


# 使用示例
if __name__ == "__main__":
    lsh_cache = LSHSemanticCache(max_size=100, similarity_threshold=0.7)

    # 第一次查询
    query1 = "Python 编程语言的特点"
    response1 = lsh_cache.get(query1)
    if response1 is None:
        response1 = "Python 是一门简洁、易学、功能强大的编程语言。"
        lsh_cache.set(query1, response1)
        print(f"查询: {query1}")
        print(f"响应: {response1}\n")

    # 第二次查询（语义相似）
    query2 = "Python 语言有什么特点"
    response2 = lsh_cache.get(query2)
    if response2:
        print(f"响应: {response2}\n")

    print(f"缓存统计: {lsh_cache.get_stats()}")
```

---

## 2026 实际应用

### 应用 1：GPTCache 集成

```python
class GPTCacheWrapper:
    """
    与 GPTCache 库集成的语义缓存
    """

    def __init__(self, openai_api_key: str):
        self.api_key = openai_api_key
        self.cache = SemanticCache(max_size=1000)

    def chat_completion(
        self,
        messages: List[Dict],
        model: str = "gpt-4",
        temperature: float = 0.7,
    ) -> str:
        """
        带缓存的 Chat Completion
        """
        # 将消息转换为缓存键
        cache_key = self._messages_to_key(messages)

        # 尝试从缓存获取
        cached_response = self.cache.get(cache_key)
        if cached_response:
            print("✓ 缓存命中，节省 API 调用")
            return cached_response

        # 缓存未命中，调用 API
        print("✗ 缓存未命中，调用 OpenAI API")
        response = self._call_openai_api(messages, model, temperature)

        # 缓存响应
        self.cache.set(cache_key, response)

        return response

    def _messages_to_key(self, messages: List[Dict]) -> str:
        """将消息列表转换为缓存键"""
        # 只使用最后一条用户消息作为键
        for msg in reversed(messages):
            if msg["role"] == "user":
                return msg["content"]
        return ""

    def _call_openai_api(
        self, messages: List[Dict], model: str, temperature: float
    ) -> str:
        """调用 OpenAI API（模拟）"""
        # 实际应用中应该调用真实 API
        import time
        time.sleep(0.1)  # 模拟网络延迟
        return f"这是来自 {model} 的响应"

    def get_cache_stats(self) -> Dict:
        """获取缓存统计"""
        stats = self.cache.get_stats()
        total_calls = stats["hits"] + stats["misses"]
        saved_calls = stats["hits"]
        cost_savings = saved_calls * 0.03  # 假设每次调用 $0.03

        return {
            **stats,
            "total_calls": total_calls,
            "saved_calls": saved_calls,
            "cost_savings_usd": cost_savings,
        }


# 使用示例
if __name__ == "__main__":
    import os

    gpt_cache = GPTCacheWrapper(openai_api_key="your-api-key")

    # 第一次调用
    messages1 = [{"role": "user", "content": "什么是机器学习？"}]
    response1 = gpt_cache.chat_completion(messages1)
    print(f"响应: {response1}\n")

    # 第二次调用（相似问题）
    messages2 = [{"role": "user", "content": "什么是机器学习"}]  # 去掉问号
    response2 = gpt_cache.chat_completion(messages2)
    print(f"响应: {response2}\n")

    # 统计
    print(f"缓存统计: {gpt_cache.get_cache_stats()}")
```

### 应用 2：RAG 系统查询缓存

```python
class RAGQueryCache:
    """
    RAG 系统的查询缓存
    缓存查询 → 检索结果 → 生成响应的完整流程
    """

    def __init__(self, max_size: int = 500):
        self.query_cache = SemanticCache(max_size=max_size)
        self.retrieval_cache = SemanticCache(max_size=max_size)

    def get_cached_retrieval(self, query: str) -> Optional[List[str]]:
        """获取缓存的检索结果"""
        import json

        cached = self.retrieval_cache.get(query)
        if cached:
            return json.loads(cached)
        return None

    def cache_retrieval(self, query: str, documents: List[str]):
        """缓存检索结果"""
        import json

        self.retrieval_cache.set(query, json.dumps(documents))

    def get_cached_response(self, query: str) -> Optional[str]:
        """获取缓存的最终响应"""
        return self.query_cache.get(query)

    def cache_response(self, query: str, response: str):
        """缓存最终响应"""
        self.query_cache.set(query, response)

    def rag_query(self, query: str) -> Tuple[str, bool]:
        """
        RAG 查询（带缓存）
        返回 (响应, 是否命中缓存)
        """
        # 1. 检查响应缓存
        cached_response = self.get_cached_response(query)
        if cached_response:
            return cached_response, True

        # 2. 检查检索缓存
        cached_docs = self.get_cached_retrieval(query)
        if cached_docs:
            print("✓ 检索结果命中缓存")
            documents = cached_docs
        else:
            print("✗ 执行文档检索")
            documents = self._retrieve_documents(query)
            self.cache_retrieval(query, documents)

        # 3. 生成响应
        print("✗ 调用 LLM 生成响应")
        response = self._generate_response(query, documents)

        # 4. 缓存响应
        self.cache_response(query, response)

        return response, False

    def _retrieve_documents(self, query: str) -> List[str]:
        """检索文档（模拟）"""
        return [
            "文档1：关于查询的相关内容",
            "文档2：更多相关信息",
        ]

    def _generate_response(self, query: str, documents: List[str]) -> str:
        """生成响应（模拟）"""
        return f"基于 {len(documents)} 个文档的回答"

    def get_stats(self) -> Dict:
        """获取统计信息"""
        return {
            "query_cache": self.query_cache.get_stats(),
            "retrieval_cache": self.retrieval_cache.get_stats(),
        }


# 使用示例
if __name__ == "__main__":
    rag_cache = RAGQueryCache(max_size=100)

    # 第一次查询
    query1 = "什么是向量数据库？"
    response1, hit1 = rag_cache.rag_query(query1)
    print(f"查询: {query1}")
    print(f"响应: {response1}")
    print(f"缓存命中: {hit1}\n")

    # 第二次查询（相同问题）
    query2 = "什么是向量数据库"  # 去掉问号
    response2, hit2 = rag_cache.rag_query(query2)
    print(f"查询: {query2}")
    print(f"响应: {response2}")
    print(f"缓存命中: {hit2}\n")

    # 统计
    print(f"缓存统计: {rag_cache.get_stats()}")
```

---

## 性能测试

```python
import time


def benchmark_semantic_cache():
    """性能测试"""
    cache = SemanticCache(max_size=1000)

    # 准备测试数据
    queries = [
        ("如何学习 Python?", "推荐从基础开始"),
        ("如何学习 PYTHON", "推荐从基础开始"),  # 应该命中
        ("怎样学习Python", "推荐从基础开始"),  # 应该命中
        ("Python 学习方法", "推荐从基础开始"),  # 应该命中
    ]

    # 测试缓存性能
    print("=== 缓存性能测试 ===\n")

    # 第一次查询（未命中）
    start = time.time()
    result = cache.get(queries[0][0])
    if result is None:
        cache.set(queries[0][0], queries[0][1])
        time.sleep(0.1)  # 模拟 LLM 调用延迟
    miss_time = time.time() - start
    print(f"缓存未命中时间: {miss_time:.4f}s")

    # 后续查询（命中）
    hit_times = []
    for query, _ in queries[1:]:
        start = time.time()
        result = cache.get(query)
        hit_time = time.time() - start
        hit_times.append(hit_time)
        print(f"缓存命中时间: {hit_time:.6f}s")

    avg_hit_time = sum(hit_times) / len(hit_times)
    speedup = miss_time / avg_hit_time

    print(f"\n平均命中时间: {avg_hit_time:.6f}s")
    print(f"加速比: {speedup:.1f}x")
    print(f"缓存统计: {cache.get_stats()}")


if __name__ == "__main__":
    benchmark_semantic_cache()
```

**预期输出：**
```
=== 缓存性能测试 ===

缓存未命中时间: 0.1023s
缓存命中时间: 0.000012s
缓存命中时间: 0.000011s
缓存命中时间: 0.000013s

平均命中时间: 0.000012s
加速比: 8525.0x
缓存统计: {'hits': 3, 'misses': 1, 'evictions': 0, 'cache_size': 1, 'hit_rate': 0.75}
```

---

## 核心要点

### Hash Table 的作用

1. **O(1) 查找**：快速检索缓存条目
2. **标准化键**：通过标准化实现语义匹配
3. **LRU 管理**：OrderedDict 实现高效驱逐

### 实际应用场景

- **LLM API 缓存**：减少重复调用，节省成本
- **RAG 查询缓存**：加速检索和生成流程
- **对话系统**：缓存常见问题的回答

### 2026 最佳实践

- 使用 LSH 实现真正的语义匹配
- 分层缓存：查询缓存 + 检索缓存
- 监控命中率，动态调整缓存大小
- 结合 embedding 模型提升匹配精度

**记住：语义缓存可以将 LLM 调用成本降低 50-80%，Hash Table 是实现快速查找的关键。**
