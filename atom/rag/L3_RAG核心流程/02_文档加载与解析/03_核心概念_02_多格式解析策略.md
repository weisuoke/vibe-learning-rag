# 核心概念2：多格式解析策略

> 理解如何统一处理PDF、Office、HTML等多种文档格式

---

## 什么是多格式解析策略？

**多格式解析策略**是指在RAG系统中，通过格式检测、解析器选择和统一文档模型，实现对不同格式文档的标准化处理。

**核心目标：**
```
多种格式 → 自动检测 → 选择解析器 → 统一输出
(PDF/DOCX/HTML/MD) → (Format Detection) → (Parser Selection) → (Document)
```

---

## 1. 格式检测 (Format Detection)

### 1.1 为什么需要格式检测？

**问题：** 用户可能上传各种格式的文档，系统需要自动识别并选择合适的解析器

```python
# ❌ 手动判断格式：容易出错
if file_path.endswith('.pdf'):
    loader = PyPDFLoader(file_path)
elif file_path.endswith('.docx'):
    loader = Docx2txtLoader(file_path)
# 问题：文件扩展名可能被修改或不准确

# ✅ 自动检测格式：更可靠
import magic  # python-magic库

def detect_file_type(file_path: str) -> str:
    """通过文件内容检测真实格式"""
    mime = magic.from_file(file_path, mime=True)
    return mime
```

### 1.2 格式检测方法

**方法1：基于文件扩展名（快速但不可靠）**

```python
import os

def detect_by_extension(file_path: str) -> str:
    """基于文件扩展名检测"""
    ext = os.path.splitext(file_path)[1].lower()

    format_map = {
        '.pdf': 'pdf',
        '.docx': 'docx',
        '.doc': 'doc',
        '.txt': 'text',
        '.html': 'html',
        '.htm': 'html',
        '.md': 'markdown',
        '.csv': 'csv',
        '.json': 'json',
        '.xml': 'xml'
    }

    return format_map.get(ext, 'unknown')
```

**方法2：基于文件头（Magic Number）（可靠）**

```python
def detect_by_magic_number(file_path: str) -> str:
    """通过文件头（Magic Number）检测格式"""
    with open(file_path, 'rb') as f:
        header = f.read(8)

    # PDF: %PDF
    if header.startswith(b'%PDF'):
        return 'pdf'

    # ZIP-based formats (DOCX, XLSX, PPTX)
    if header.startswith(b'PK\x03\x04'):
        # 进一步检查ZIP内容
        import zipfile
        with zipfile.ZipFile(file_path) as zf:
            if 'word/' in zf.namelist()[0]:
                return 'docx'
            elif 'xl/' in zf.namelist()[0]:
                return 'xlsx'
            elif 'ppt/' in zf.namelist()[0]:
                return 'pptx'

    # HTML: <!DOCTYPE or <html
    if b'<!DOCTYPE' in header or b'<html' in header:
        return 'html'

    return 'unknown'
```

**方法3：使用python-magic库（推荐）**

```python
import magic

def detect_by_magic_lib(file_path: str) -> str:
    """使用python-magic库检测格式"""
    mime = magic.from_file(file_path, mime=True)

    mime_to_format = {
        'application/pdf': 'pdf',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'xlsx',
        'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'pptx',
        'text/html': 'html',
        'text/plain': 'text',
        'text/markdown': 'markdown',
        'application/json': 'json',
        'text/csv': 'csv'
    }

    return mime_to_format.get(mime, 'unknown')
```

### 1.3 综合格式检测策略

```python
from typing import Optional

class FormatDetector:
    """综合格式检测器"""

    def detect(self, file_path: str) -> str:
        """
        综合多种方法检测文件格式
        优先级：Magic Number > MIME Type > Extension
        """
        # 1. 尝试Magic Number检测
        format_by_magic = self._detect_by_magic_number(file_path)
        if format_by_magic != 'unknown':
            return format_by_magic

        # 2. 尝试MIME Type检测
        try:
            import magic
            format_by_mime = self._detect_by_mime(file_path)
            if format_by_mime != 'unknown':
                return format_by_mime
        except ImportError:
            pass

        # 3. 回退到扩展名检测
        return self._detect_by_extension(file_path)

    def _detect_by_magic_number(self, file_path: str) -> str:
        """通过文件头检测"""
        # 实现见上文
        pass

    def _detect_by_mime(self, file_path: str) -> str:
        """通过MIME类型检测"""
        # 实现见上文
        pass

    def _detect_by_extension(self, file_path: str) -> str:
        """通过扩展名检测"""
        # 实现见上文
        pass

# 使用示例
detector = FormatDetector()
file_format = detector.detect("document.pdf")
print(f"检测到格式: {file_format}")
```

---

## 2. 解析器选择逻辑 (Parser Selection)

### 2.1 解析器注册表模式

```python
from typing import Dict, Type, Callable
from langchain.document_loaders.base import BaseLoader

class ParserRegistry:
    """解析器注册表"""

    def __init__(self):
        self._parsers: Dict[str, Type[BaseLoader]] = {}
        self._custom_parsers: Dict[str, Callable] = {}

    def register(self, format: str, parser_class: Type[BaseLoader]):
        """注册标准解析器"""
        self._parsers[format] = parser_class

    def register_custom(self, format: str, parser_func: Callable):
        """注册自定义解析器"""
        self._custom_parsers[format] = parser_func

    def get_parser(self, format: str) -> Optional[Type[BaseLoader]]:
        """获取解析器"""
        # 优先返回自定义解析器
        if format in self._custom_parsers:
            return self._custom_parsers[format]

        # 返回标准解析器
        return self._parsers.get(format)

    def list_supported_formats(self) -> list:
        """列出支持的格式"""
        return list(set(self._parsers.keys()) | set(self._custom_parsers.keys()))

# 创建全局注册表
parser_registry = ParserRegistry()

# 注册标准解析器
from langchain.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    UnstructuredHTMLLoader,
    CSVLoader
)

parser_registry.register('pdf', PyPDFLoader)
parser_registry.register('docx', Docx2txtLoader)
parser_registry.register('text', TextLoader)
parser_registry.register('html', UnstructuredHTMLLoader)
parser_registry.register('csv', CSVLoader)

# 注册自定义解析器
def custom_markdown_parser(file_path: str):
    # 自定义Markdown解析逻辑
    pass

parser_registry.register_custom('markdown', custom_markdown_parser)
```

### 2.2 智能解析器选择

```python
from langchain.schema import Document
from typing import List

class SmartDocumentLoader:
    """智能文档加载器：自动检测格式并选择解析器"""

    def __init__(self):
        self.detector = FormatDetector()
        self.registry = parser_registry

    def load(self, file_path: str) -> List[Document]:
        """
        智能加载文档
        1. 检测格式
        2. 选择解析器
        3. 加载文档
        """
        # 1. 检测格式
        file_format = self.detector.detect(file_path)
        print(f"检测到格式: {file_format}")

        # 2. 选择解析器
        parser_class = self.registry.get_parser(file_format)
        if not parser_class:
            raise ValueError(f"不支持的格式: {file_format}")

        # 3. 加载文档
        loader = parser_class(file_path)
        documents = loader.load()

        # 4. 添加格式信息到元数据
        for doc in documents:
            doc.metadata['detected_format'] = file_format

        return documents

    def load_batch(self, file_paths: List[str]) -> List[Document]:
        """批量加载多个文档"""
        all_docs = []
        for file_path in file_paths:
            try:
                docs = self.load(file_path)
                all_docs.extend(docs)
                print(f"✅ 成功加载: {file_path}")
            except Exception as e:
                print(f"❌ 加载失败: {file_path} - {e}")

        return all_docs

# 使用示例
loader = SmartDocumentLoader()

# 自动检测并加载单个文档
docs = loader.load("report.pdf")  # 自动检测为PDF并使用PyPDFLoader

# 批量加载多种格式
files = [
    "report.pdf",
    "proposal.docx",
    "notes.txt",
    "article.html"
]
all_docs = loader.load_batch(files)
print(f"总共加载 {len(all_docs)} 个文档")
```

---

## 3. 统一文档模型 (Unified Document Model)

### 3.1 标准Document结构

```python
from langchain.schema import Document
from typing import Dict, Any, Optional
from datetime import datetime

class UnifiedDocument:
    """统一的文档模型"""

    def __init__(
        self,
        content: str,
        metadata: Optional[Dict[str, Any]] = None
    ):
        self.content = content
        self.metadata = metadata or {}

        # 确保必需的元数据字段
        self._ensure_required_metadata()

    def _ensure_required_metadata(self):
        """确保必需的元数据字段存在"""
        required_fields = {
            'source': 'unknown',
            'format': 'unknown',
            'loaded_at': datetime.now().isoformat()
        }

        for field, default_value in required_fields.items():
            if field not in self.metadata:
                self.metadata[field] = default_value

    def to_langchain_document(self) -> Document:
        """转换为LangChain Document对象"""
        return Document(
            page_content=self.content,
            metadata=self.metadata
        )

    @classmethod
    def from_langchain_document(cls, doc: Document) -> 'UnifiedDocument':
        """从LangChain Document创建"""
        return cls(
            content=doc.page_content,
            metadata=doc.metadata
        )
```

### 3.2 格式标准化处理

```python
class DocumentNormalizer:
    """文档标准化处理器"""

    def normalize(self, documents: List[Document]) -> List[Document]:
        """
        标准化文档列表
        - 统一换行符
        - 去除多余空白
        - 标准化元数据
        """
        normalized_docs = []

        for doc in documents:
            # 1. 标准化内容
            content = self._normalize_content(doc.page_content)

            # 2. 标准化元数据
            metadata = self._normalize_metadata(doc.metadata)

            # 3. 创建新文档
            normalized_doc = Document(
                page_content=content,
                metadata=metadata
            )
            normalized_docs.append(normalized_doc)

        return normalized_docs

    def _normalize_content(self, content: str) -> str:
        """标准化文本内容"""
        # 统一换行符
        content = content.replace('\r\n', '\n').replace('\r', '\n')

        # 去除多余空行（保留最多一个空行）
        import re
        content = re.sub(r'\n{3,}', '\n\n', content)

        # 去除行首行尾空白
        lines = [line.strip() for line in content.split('\n')]
        content = '\n'.join(lines)

        return content

    def _normalize_metadata(self, metadata: Dict) -> Dict:
        """标准化元数据"""
        normalized = metadata.copy()

        # 确保source字段存在
        if 'source' not in normalized:
            normalized['source'] = 'unknown'

        # 标准化日期格式
        if 'created_date' in normalized:
            normalized['created_date'] = self._normalize_date(
                normalized['created_date']
            )

        # 添加处理时间戳
        normalized['normalized_at'] = datetime.now().isoformat()

        return normalized

    def _normalize_date(self, date_value: Any) -> str:
        """标准化日期格式为ISO 8601"""
        if isinstance(date_value, datetime):
            return date_value.isoformat()
        elif isinstance(date_value, str):
            # 尝试解析并转换
            try:
                from dateutil import parser
                dt = parser.parse(date_value)
                return dt.isoformat()
            except:
                return date_value
        return str(date_value)
```

---

## 4. 内容提取管道 (Content Extraction Pipeline)

### 4.1 管道模式设计

```python
from abc import ABC, abstractmethod
from typing import List

class ProcessingStage(ABC):
    """处理阶段的抽象基类"""

    @abstractmethod
    def process(self, documents: List[Document]) -> List[Document]:
        """处理文档"""
        pass

class LoadStage(ProcessingStage):
    """加载阶段"""

    def __init__(self, loader: SmartDocumentLoader):
        self.loader = loader

    def process(self, file_paths: List[str]) -> List[Document]:
        """加载文档"""
        return self.loader.load_batch(file_paths)

class NormalizeStage(ProcessingStage):
    """标准化阶段"""

    def __init__(self):
        self.normalizer = DocumentNormalizer()

    def process(self, documents: List[Document]) -> List[Document]:
        """标准化文档"""
        return self.normalizer.normalize(documents)

class CleanStage(ProcessingStage):
    """清洗阶段"""

    def process(self, documents: List[Document]) -> List[Document]:
        """清洗文档内容"""
        cleaned_docs = []

        for doc in documents:
            # 去除过短的文档
            if len(doc.page_content.strip()) < 50:
                continue

            # 去除特殊字符过多的文档（可能是乱码）
            if self._is_garbled(doc.page_content):
                continue

            cleaned_docs.append(doc)

        return cleaned_docs

    def _is_garbled(self, text: str) -> bool:
        """检查是否为乱码"""
        special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
        return special_chars / len(text) > 0.5

class EnrichStage(ProcessingStage):
    """元数据增强阶段"""

    def process(self, documents: List[Document]) -> List[Document]:
        """增强元数据"""
        for doc in documents:
            # 添加内容统计
            doc.metadata['content_length'] = len(doc.page_content)
            doc.metadata['word_count'] = len(doc.page_content.split())

            # 添加语言检测
            doc.metadata['language'] = self._detect_language(doc.page_content)

        return documents

    def _detect_language(self, text: str) -> str:
        """简单的语言检测"""
        # 检查是否包含中文字符
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        if chinese_chars / len(text) > 0.1:
            return 'zh'
        return 'en'

class DocumentPipeline:
    """文档处理管道"""

    def __init__(self):
        self.stages: List[ProcessingStage] = []

    def add_stage(self, stage: ProcessingStage):
        """添加处理阶段"""
        self.stages.append(stage)
        return self

    def process(self, input_data: Any) -> List[Document]:
        """执行管道处理"""
        data = input_data

        for i, stage in enumerate(self.stages, 1):
            print(f"执行阶段 {i}/{len(self.stages)}: {stage.__class__.__name__}")
            data = stage.process(data)
            print(f"  输出: {len(data)} 个文档")

        return data

# 使用示例
pipeline = DocumentPipeline()
pipeline.add_stage(LoadStage(SmartDocumentLoader()))
pipeline.add_stage(NormalizeStage())
pipeline.add_stage(CleanStage())
pipeline.add_stage(EnrichStage())

# 处理文档
file_paths = ["doc1.pdf", "doc2.docx", "doc3.html"]
final_documents = pipeline.process(file_paths)

print(f"\n最终得到 {len(final_documents)} 个高质量文档")
```

---

## 5. 质量vs速度权衡

### 5.1 解析器性能对比

| 解析器 | 速度 | 质量 | 表格支持 | 布局保留 | 适用场景 |
|--------|------|------|---------|---------|---------|
| **PyPDFLoader** | 极快 | 中等 | 差 | 差 | 简单PDF、快速原型 |
| **pdfplumber** | 中等 | 好 | 好 | 好 | 复杂PDF、表格提取 |
| **Docling** | 慢 | 优秀 | 优秀 | 优秀 | 高质量要求、复杂文档 |
| **TextLoader** | 极快 | N/A | N/A | N/A | 纯文本文件 |
| **Unstructured** | 慢 | 好 | 好 | 好 | 多格式统一处理 |

### 5.2 自适应解析策略

```python
class AdaptiveParser:
    """自适应解析器：根据文档复杂度选择解析器"""

    def __init__(self, quality_threshold: float = 0.8):
        self.quality_threshold = quality_threshold

    def parse(self, file_path: str) -> List[Document]:
        """
        自适应解析策略：
        1. 先用快速解析器
        2. 评估质量
        3. 如果质量不够，切换到高质量解析器
        """
        # 1. 快速解析
        print("尝试快速解析...")
        quick_docs = self._quick_parse(file_path)

        # 2. 评估质量
        quality_score = self._assess_quality(quick_docs)
        print(f"质量评分: {quality_score:.2f}")

        # 3. 决策
        if quality_score >= self.quality_threshold:
            print("✅ 快速解析质量足够")
            return quick_docs
        else:
            print("⚠️  切换到高质量解析器")
            return self._high_quality_parse(file_path)

    def _quick_parse(self, file_path: str) -> List[Document]:
        """快速解析"""
        from langchain.document_loaders import PyPDFLoader
        return PyPDFLoader(file_path).load()

    def _high_quality_parse(self, file_path: str) -> List[Document]:
        """高质量解析"""
        import pdfplumber
        from langchain.schema import Document

        docs = []
        with pdfplumber.open(file_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                text = page.extract_text()
                doc = Document(
                    page_content=text,
                    metadata={
                        "source": file_path,
                        "page": page_num,
                        "parser": "pdfplumber"
                    }
                )
                docs.append(doc)
        return docs

    def _assess_quality(self, documents: List[Document]) -> float:
        """评估解析质量"""
        if not documents:
            return 0.0

        scores = []
        for doc in documents:
            text = doc.page_content

            # 指标1：文本长度（太短可能解析失败）
            length_score = min(len(text) / 500, 1.0)

            # 指标2：特殊字符比例（太高可能是乱码）
            special_ratio = sum(1 for c in text if not c.isalnum() and not c.isspace()) / len(text)
            special_score = 1.0 - min(special_ratio / 0.3, 1.0)

            # 指标3：空白比例（太高可能提取不完整）
            whitespace_ratio = sum(1 for c in text if c.isspace()) / len(text)
            whitespace_score = 1.0 - abs(whitespace_ratio - 0.15) / 0.15

            # 综合评分
            doc_score = (length_score + special_score + whitespace_score) / 3
            scores.append(doc_score)

        return sum(scores) / len(scores)

# 使用示例
parser = AdaptiveParser(quality_threshold=0.8)
docs = parser.parse("complex_document.pdf")
```

---

## 6. 在RAG中的应用

### 6.1 构建多格式知识库

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# 1. 智能加载多格式文档
loader = SmartDocumentLoader()
documents = loader.load_batch([
    "reports/2025_q1.pdf",
    "proposals/project_a.docx",
    "notes/meeting_notes.txt",
    "articles/tech_blog.html"
])

# 2. 标准化处理
normalizer = DocumentNormalizer()
normalized_docs = normalizer.normalize(documents)

# 3. 分块
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = splitter.split_documents(normalized_docs)

# 4. 向量化并存储
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    chunks,
    embeddings,
    persist_directory="./multi_format_kb"
)

print(f"知识库构建完成，包含 {len(chunks)} 个文本块")

# 5. 检索时可以按格式过滤
results = vectorstore.similarity_search(
    "项目进展",
    k=5,
    filter={"detected_format": "docx"}  # 只检索Word文档
)
```

---

## 总结

**多格式解析策略的核心价值：**

1. **自动化**：自动检测格式，无需手动指定
2. **统一化**：不同格式输出统一的Document对象
3. **智能化**：根据文档复杂度自适应选择解析器
4. **可扩展**：易于添加新格式支持
5. **质量保障**：通过管道模式确保输出质量

**在RAG中的作用：**
- 支持用户上传任意格式文档
- 自动选择最优解析策略
- 确保解析质量满足检索需求
- 为后续分块和向量化提供高质量输入

---

## 参考来源

> **参考来源：**
> - [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) (2025)
> - [python-magic Documentation](https://github.com/ahupp/python-magic) - 文件类型检测 (2025)
> - [File Format Detection Best Practices](https://www.garykessler.net/library/file_sigs.html) - Magic Number参考

---

**版本：** v1.0
**最后更新：** 2026-02-15
**下一步：** 阅读 [03_核心概念_03_元数据提取与保留.md](./03_核心概念_03_元数据提取与保留.md)
