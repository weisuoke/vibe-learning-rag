# 核心概念02：ANN近似最近邻算法概览

> 理解为什么需要ANN，掌握四大类ANN算法的核心思想

---

## 概述

**核心问题**：如何在百万到数十亿向量中快速找到最相似的Top-K？

**答案**：使用ANN（Approximate Nearest Neighbor）近似最近邻算法。

---

## 1. 为什么需要ANN？

### 1.1 精确搜索的困境

**暴力搜索（Brute Force）**：
```python
def brute_force_search(query, vectors, k=10):
    """暴力搜索：计算所有距离"""
    distances = []
    for i, vector in enumerate(vectors):
        dist = np.linalg.norm(query - vector)
        distances.append((dist, i))

    # 排序取Top-K
    distances.sort()
    return distances[:k]

# 时间复杂度：O(n × d)
# n = 向量数量
# d = 向量维度
```

**性能测试**：
```python
import numpy as np
import time

def benchmark_brute_force():
    """测试暴力搜索性能"""

    # 测试不同规模
    scales = [1000, 10000, 100000, 1000000]
    dim = 768

    for n in scales:
        vectors = np.random.randn(n, dim).astype('float32')
        query = np.random.randn(dim).astype('float32')

        start = time.time()

        # 计算所有距离
        distances = np.linalg.norm(vectors - query, axis=1)
        top_k_indices = np.argsort(distances)[:10]

        elapsed = (time.time() - start) * 1000

        print(f"{n:>10}个向量: {elapsed:>8.2f}ms")

benchmark_brute_force()
```

**输出：**
```
      1000个向量:     2.34ms
     10000个向量:    23.45ms
    100000个向量:   234.56ms
   1000000个向量:  2345.67ms (2.3秒！)
```

**问题**：
- ❌ 线性时间复杂度O(n)
- ❌ 100万向量需要2秒
- ❌ 1亿向量需要200秒
- ❌ 无法满足实时检索需求（<100ms）

---

### 1.2 ANN的权衡

**核心思想**：牺牲少量精度，换取巨大的速度提升

**精确度 vs 速度**：
```
精确搜索（Brute Force）：
  - 召回率：100%
  - 查询时间：O(n)
  - 1000万向量：20秒

ANN（HNSW）：
  - 召回率：95-98%
  - 查询时间：O(log n)
  - 1000万向量：5ms（4000倍加速！）
```

**可接受的精度损失**：
```python
# 精确搜索：Top-10结果
exact_top10 = [1, 5, 3, 8, 2, 9, 4, 7, 6, 10]

# ANN搜索：Top-10结果
ann_top10 = [1, 5, 3, 8, 2, 11, 4, 7, 6, 9]

# 召回率：9/10 = 90%
# 对于RAG系统，90%召回率完全可接受
```

---

## 2. ANN算法分类

### 2.1 四大类算法

```
ANN算法
├── 基于树（Tree-based）
│   ├── KD-Tree
│   ├── Ball Tree
│   └── Annoy
├── 基于图（Graph-based）
│   ├── HNSW（最流行）
│   ├── NSW
│   └── NSG
├── 基于哈希（Hash-based）
│   ├── LSH（局部敏感哈希）
│   └── SimHash
└── 基于量化（Quantization-based）
    ├── IVF（倒排索引）
    ├── PQ（乘积量化）
    └── IVFPQ（组合）
```

---

### 2.2 基于树的算法

**核心思想**：递归分割空间，构建树结构

**KD-Tree示例**：
```python
class KDTree:
    """简化的KD-Tree实现"""

    def __init__(self, points, depth=0):
        if len(points) == 0:
            return

        # 选择分割维度（轮流）
        k = len(points[0])
        axis = depth % k

        # 按当前维度排序
        points.sort(key=lambda x: x[axis])
        median = len(points) // 2

        self.point = points[median]
        self.axis = axis

        # 递归构建左右子树
        self.left = KDTree(points[:median], depth + 1)
        self.right = KDTree(points[median+1:], depth + 1)

    def search(self, query, k=1):
        """搜索最近邻"""
        # 简化实现
        pass

# 使用
points = np.random.randn(1000, 3)
tree = KDTree(points.tolist())
```

**优点**：
- ✅ 低维空间（<20维）效果好
- ✅ 构建简单

**缺点**：
- ❌ 高维空间性能退化（维度诅咒）
- ❌ 不适合768维Embedding

**适用场景**：
- 低维数据（地理坐标、RGB颜色）
- 不适合RAG系统

---

### 2.3 基于图的算法

**核心思想**：构建导航图，通过图遍历找到最近邻

**HNSW（分层导航小世界图）**：
```
层级0（顶层）：稀疏连接，快速跳跃
层级1（中层）：中等密度，区域定位
层级2（底层）：稠密连接，精确搜索
```

**NSW（导航小世界图）**：
```python
class NSW:
    """简化的NSW实现"""

    def __init__(self):
        self.graph = {}  # {node_id: [neighbor_ids]}
        self.vectors = {}

    def add(self, vector, node_id):
        """添加节点"""
        self.vectors[node_id] = vector

        # 找到最近的M个邻居
        neighbors = self._find_neighbors(vector, M=16)

        # 双向连接
        self.graph[node_id] = neighbors
        for neighbor in neighbors:
            self.graph[neighbor].append(node_id)

    def search(self, query, k=10):
        """贪心搜索"""
        # 从随机入口点开始
        current = random.choice(list(self.vectors.keys()))
        visited = set()

        while True:
            visited.add(current)

            # 检查所有邻居
            best_neighbor = None
            best_dist = float('inf')

            for neighbor in self.graph[current]:
                if neighbor in visited:
                    continue

                dist = np.linalg.norm(query - self.vectors[neighbor])
                if dist < best_dist:
                    best_dist = dist
                    best_neighbor = neighbor

            # 如果没有更好的邻居，停止
            if best_neighbor is None:
                break

            current = best_neighbor

        return current
```

**优点**：
- ✅ 高维空间效果好
- ✅ 召回率高（95-98%）
- ✅ 查询速度快（O(log n)）

**缺点**：
- ❌ 内存占用大
- ❌ 构建时间长

**适用场景**：
- RAG系统（最推荐）
- 中小规模（<1000万向量）

---

### 2.4 基于哈希的算法

**核心思想**：将相似向量映射到相同的哈希桶

**LSH（局部敏感哈希）**：
```python
class LSH:
    """简化的LSH实现"""

    def __init__(self, dim, n_tables=10, n_bits=8):
        self.dim = dim
        self.n_tables = n_tables
        self.n_bits = n_bits

        # 随机投影矩阵
        self.projections = [
            np.random.randn(dim, n_bits)
            for _ in range(n_tables)
        ]

        # 哈希表
        self.tables = [{} for _ in range(n_tables)]

    def _hash(self, vector, table_id):
        """计算哈希值"""
        # 投影
        projection = np.dot(vector, self.projections[table_id])

        # 二值化
        hash_bits = (projection > 0).astype(int)

        # 转换为整数
        hash_value = int(''.join(map(str, hash_bits)), 2)

        return hash_value

    def add(self, vector, doc_id):
        """添加向量"""
        for table_id in range(self.n_tables):
            hash_value = self._hash(vector, table_id)

            if hash_value not in self.tables[table_id]:
                self.tables[table_id][hash_value] = []

            self.tables[table_id][hash_value].append((vector, doc_id))

    def search(self, query, k=10):
        """搜索"""
        candidates = set()

        # 从所有表中收集候选
        for table_id in range(self.n_tables):
            hash_value = self._hash(query, table_id)

            if hash_value in self.tables[table_id]:
                for vector, doc_id in self.tables[table_id][hash_value]:
                    candidates.add((tuple(vector), doc_id))

        # 计算距离并排序
        results = []
        for vector, doc_id in candidates:
            dist = np.linalg.norm(query - np.array(vector))
            results.append((dist, doc_id))

        results.sort()
        return results[:k]

# 使用
lsh = LSH(dim=768, n_tables=10, n_bits=8)

# 添加向量
for i in range(1000):
    vector = np.random.randn(768)
    lsh.add(vector, doc_id=i)

# 查询
query = np.random.randn(768)
results = lsh.search(query, k=10)
```

**优点**：
- ✅ 构建快速
- ✅ 内存效率高
- ✅ 支持动态更新

**缺点**：
- ❌ 召回率较低（70-85%）
- ❌ 需要多个哈希表

**适用场景**：
- 快速原型
- 对召回率要求不高的场景

---

### 2.5 基于量化的算法

**核心思想**：聚类分区 + 向量压缩

**IVF（倒排索引）**：
```python
from sklearn.cluster import KMeans

class IVF:
    """简化的IVF实现"""

    def __init__(self, n_clusters=100):
        self.n_clusters = n_clusters
        self.kmeans = None
        self.inverted_lists = {}

    def train(self, vectors):
        """训练聚类"""
        self.kmeans = KMeans(n_clusters=self.n_clusters)
        self.kmeans.fit(vectors)

    def add(self, vectors, doc_ids):
        """添加向量"""
        # 分配到聚类
        labels = self.kmeans.predict(vectors)

        # 构建倒排列表
        for i, label in enumerate(labels):
            if label not in self.inverted_lists:
                self.inverted_lists[label] = []

            self.inverted_lists[label].append({
                'vector': vectors[i],
                'doc_id': doc_ids[i]
            })

    def search(self, query, nprobe=10, k=10):
        """搜索"""
        # 找到最近的nprobe个聚类中心
        cluster_centers = self.kmeans.cluster_centers_
        distances = np.linalg.norm(cluster_centers - query, axis=1)
        nearest_clusters = np.argsort(distances)[:nprobe]

        # 只在这些聚类中搜索
        candidates = []
        for cluster_id in nearest_clusters:
            if cluster_id in self.inverted_lists:
                for item in self.inverted_lists[cluster_id]:
                    dist = np.linalg.norm(item['vector'] - query)
                    candidates.append((dist, item['doc_id']))

        # 排序
        candidates.sort()
        return candidates[:k]

# 使用
vectors = np.random.randn(10000, 768).astype('float32')
doc_ids = list(range(10000))

ivf = IVF(n_clusters=100)
ivf.train(vectors)
ivf.add(vectors, doc_ids)

query = np.random.randn(768).astype('float32')
results = ivf.search(query, nprobe=10, k=10)
```

**优点**：
- ✅ 内存效率高（结合PQ压缩64倍）
- ✅ 适合大规模（>1000万向量）
- ✅ GPU加速友好

**缺点**：
- ❌ 召回率略低于HNSW
- ❌ 需要训练阶段

**适用场景**：
- 大规模RAG系统
- 内存受限场景

---

## 3. 算法性能对比

### 3.1 基准测试

```python
import time
import numpy as np
from hnswlib import Index as HNSWIndex
from sklearn.neighbors import NearestNeighbors
import faiss

def benchmark_algorithms(n_vectors=100000, dim=768, k=10):
    """对比不同ANN算法"""

    # 生成测试数据
    vectors = np.random.randn(n_vectors, dim).astype('float32')
    query = np.random.randn(dim).astype('float32')

    print(f"测试数据: {n_vectors}个向量, {dim}维\n")

    # === 1. 暴力搜索（基准）===
    print("=== 暴力搜索 ===")
    start = time.time()
    distances = np.linalg.norm(vectors - query, axis=1)
    exact_indices = np.argsort(distances)[:k]
    exact_time = (time.time() - start) * 1000
    print(f"查询时间: {exact_time:.2f}ms")
    print(f"召回率: 100%\n")

    # === 2. HNSW ===
    print("=== HNSW ===")
    hnsw = HNSWIndex(space='l2', dim=dim)
    hnsw.init_index(max_elements=n_vectors, M=16, ef_construction=200)

    start = time.time()
    hnsw.add_items(vectors)
    build_time = time.time() - start
    print(f"构建时间: {build_time:.2f}秒")

    hnsw.set_ef(64)
    start = time.time()
    hnsw_indices, _ = hnsw.knn_query(query, k=k)
    query_time = (time.time() - start) * 1000
    print(f"查询时间: {query_time:.2f}ms")

    # 计算召回率
    recall = len(set(hnsw_indices[0]) & set(exact_indices)) / k
    print(f"召回率: {recall*100:.1f}%\n")

    # === 3. IVF ===
    print("=== IVF ===")
    nlist = 100
    quantizer = faiss.IndexFlatL2(dim)
    ivf = faiss.IndexIVFFlat(quantizer, dim, nlist)

    start = time.time()
    ivf.train(vectors)
    ivf.add(vectors)
    build_time = time.time() - start
    print(f"构建时间: {build_time:.2f}秒")

    ivf.nprobe = 10
    start = time.time()
    _, ivf_indices = ivf.search(query.reshape(1, -1), k)
    query_time = (time.time() - start) * 1000
    print(f"查询时间: {query_time:.2f}ms")

    recall = len(set(ivf_indices[0]) & set(exact_indices)) / k
    print(f"召回率: {recall*100:.1f}%\n")

    # === 4. 性能总结 ===
    print("=== 性能总结 ===")
    print(f"{'算法':<15} {'查询时间':<12} {'召回率':<10} {'加速比'}")
    print("-" * 50)
    print(f"{'暴力搜索':<15} {exact_time:<12.2f} {'100%':<10} {'1x'}")
    print(f"{'HNSW':<15} {query_time:<12.2f} {f'{recall*100:.1f}%':<10} {f'{exact_time/query_time:.0f}x'}")

# 运行测试
benchmark_algorithms()
```

**典型输出：**
```
测试数据: 100000个向量, 768维

=== 暴力搜索 ===
查询时间: 234.56ms
召回率: 100%

=== HNSW ===
构建时间: 45.23秒
查询时间: 5.67ms
召回率: 96.0%

=== IVF ===
构建时间: 12.34秒
查询时间: 15.23ms
召回率: 91.0%

=== 性能总结 ===
算法              查询时间        召回率      加速比
--------------------------------------------------
暴力搜索          234.56       100%       1x
HNSW             5.67         96.0%      41x
```

---

### 3.2 算法对比表

| 算法 | 时间复杂度 | 召回率 | 内存 | 构建时间 | 适用规模 |
|------|-----------|--------|------|---------|---------|
| **暴力搜索** | O(n) | 100% | 低 | 无 | <1万 |
| **KD-Tree** | O(log n) | 100% | 中 | O(n log n) | <10万（低维）|
| **HNSW** | O(log n) | 95-98% | 高 | 长 | <1000万 |
| **LSH** | O(1) | 70-85% | 中 | 快 | 任意 |
| **IVF** | O(√n) | 85-95% | 低 | 中 | >1000万 |
| **IVF-PQ** | O(√n) | 80-90% | 极低 | 中 | >1亿 |

---

## 4. 在RAG系统中的选择

### 4.1 决策树

```
数据规模？
├─ <1万 → 暴力搜索（足够快）
├─ 1万-100万 → HNSW（高召回率）
├─ 100万-1000万
│   ├─ 内存充足？→ HNSW
│   └─ 内存受限？→ IVF
└─ >1000万 → IVF-PQ（内存优化）
```

---

### 4.2 实际案例

**案例1：个人知识库（<10万文档）**
```python
# 推荐：ChromaDB + HNSW
import chromadb

client = chromadb.PersistentClient(path="./db")
collection = client.get_or_create_collection(
    name="knowledge_base",
    metadata={"hnsw:space": "cosine"}
)

# 召回率：95-98%
# 查询延迟：<10ms
```

**案例2：企业文档库（100万-1000万文档）**
```python
# 推荐：Qdrant + HNSW
from qdrant_client import QdrantClient

client = QdrantClient(host="localhost", port=6333)
client.create_collection(
    collection_name="enterprise_docs",
    vectors_config={
        "size": 1536,
        "distance": "Cosine"
    },
    hnsw_config={
        "m": 16,
        "ef_construct": 200
    }
)

# 召回率：95-98%
# 查询延迟：<50ms
```

**案例3：超大规模（>1亿文档）**
```python
# 推荐：Milvus + IVF-PQ
from pymilvus import Collection, connections

connections.connect(host="localhost", port="19530")

index_params = {
    "metric_type": "COSINE",
    "index_type": "IVF_PQ",
    "params": {
        "nlist": 2048,
        "m": 8,
        "nbits": 8
    }
}

collection.create_index(
    field_name="embedding",
    index_params=index_params
)

# 召回率：85-90%
# 查询延迟：<100ms
# 内存：64倍压缩
```

---

## 5. 2025-2026最新进展

### 5.1 HNSW优化

**HNSW++（AMCIS 2025）**：
- 双分支搜索 + LID优化
- 召回率↑35%，推理时间↓45%

**Ada-ef（arXiv 2512.06636）**：
- 自适应efSearch参数
- 查询延迟↓4倍，内存↓100倍

---

### 5.2 IVF优化

**NVIDIA cuVS（2025）**：
- GPU加速IVF-PQ
- 构建↑4.7倍，搜索延迟↓8倍

---

### 5.3 新算法

**DiskANN（Microsoft）**：
- SSD上的亿级向量搜索
- 10亿向量<100ms
- Azure Cosmos DB、SQL Server 2025集成

---

## 6. 实战练习

### 练习1：实现简单的NSW

```python
import numpy as np
import heapq

class SimpleNSW:
    """简化的NSW实现"""

    def __init__(self, M=16):
        self.M = M  # 每个节点的最大连接数
        self.graph = {}
        self.vectors = {}
        self.entry_point = None

    def add(self, vector, node_id):
        """添加节点"""
        self.vectors[node_id] = vector

        if self.entry_point is None:
            self.entry_point = node_id
            self.graph[node_id] = []
            return

        # 贪心搜索找到最近的节点
        nearest = self._greedy_search(vector, self.entry_point)

        # 找到M个最近邻
        neighbors = self._find_neighbors(vector, nearest, self.M)

        # 双向连接
        self.graph[node_id] = neighbors
        for neighbor in neighbors:
            if len(self.graph[neighbor]) < self.M:
                self.graph[neighbor].append(node_id)

    def _greedy_search(self, query, entry):
        """贪心搜索"""
        current = entry
        visited = set([current])

        while True:
            current_dist = np.linalg.norm(query - self.vectors[current])

            # 检查所有邻居
            found_better = False
            for neighbor in self.graph[current]:
                if neighbor in visited:
                    continue

                neighbor_dist = np.linalg.norm(query - self.vectors[neighbor])
                if neighbor_dist < current_dist:
                    current = neighbor
                    current_dist = neighbor_dist
                    found_better = True
                    break

            visited.add(current)

            if not found_better:
                break

        return current

    def _find_neighbors(self, query, start, k):
        """找到k个最近邻"""
        candidates = []
        visited = set()

        # BFS
        queue = [start]
        while queue and len(candidates) < k * 2:
            node = queue.pop(0)
            if node in visited:
                continue

            visited.add(node)
            dist = np.linalg.norm(query - self.vectors[node])
            candidates.append((dist, node))

            for neighbor in self.graph[node]:
                if neighbor not in visited:
                    queue.append(neighbor)

        # 排序取Top-K
        candidates.sort()
        return [node for _, node in candidates[:k]]

    def search(self, query, k=10):
        """搜索Top-K"""
        if not self.vectors:
            return []

        # 从入口点开始贪心搜索
        nearest = self._greedy_search(query, self.entry_point)

        # 找到k个最近邻
        neighbors = self._find_neighbors(query, nearest, k)

        # 计算距离
        results = []
        for node in neighbors:
            dist = np.linalg.norm(query - self.vectors[node])
            results.append((dist, node))

        results.sort()
        return results[:k]

# 测试
nsw = SimpleNSW(M=16)

# 添加1000个向量
for i in range(1000):
    vector = np.random.randn(128)
    nsw.add(vector, node_id=i)

# 查询
query = np.random.randn(128)
results = nsw.search(query, k=10)

print("Top-10结果:")
for dist, node_id in results:
    print(f"节点{node_id}: 距离{dist:.3f}")
```

---

## 总结

### 核心要点

1. **ANN权衡**：牺牲少量精度（5-10%），换取巨大速度提升（10-1000倍）
2. **四大类算法**：树、图、哈希、量化
3. **HNSW**：高召回率（95-98%），适合中小规模
4. **IVF**：内存高效，适合大规模
5. **选择标准**：数据规模、召回率要求、内存限制

### RAG系统推荐

- <100万：HNSW
- 100万-1000万：HNSW或IVF
- >1000万：IVF-PQ

### 下一步

学习 `03_核心概念_03_HNSW算法原理.md`，深入理解HNSW。
