# 核心概念1：稠密检索 (Dense Retrieval)

> 基于语义向量的检索方式，RAG系统理解用户意图的核心技术

---

## 一句话定义

**稠密检索是将文本转换为稠密向量（Dense Vector），通过计算向量相似度进行语义检索的方法，能够理解查询意图和文档语义，而不仅仅是关键词匹配。**

---

## 核心原理

### 什么是"稠密"？

**稠密（Dense）vs 稀疏（Sparse）**：

```python
# 稀疏向量（Sparse Vector）- 大部分元素为0
sparse_vector = [0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 2, 0, ...]  # 维度可能很高（10K-100K）
# 只有少数位置有值，表示词频

# 稠密向量（Dense Vector）- 大部分元素非0
dense_vector = [0.23, -0.45, 0.67, 0.12, -0.89, 0.34, ...]  # 维度较低（768-3072）
# 每个维度都有意义，表示语义特征
```

**为什么叫"稠密"？**
- 向量的每个维度都有值（非零）
- 维度相对较低（768-3072维）
- 每个维度代表某种语义特征

**参考**: [Embeddings in Practice 2026](https://medium.com/@adnanmasood/embeddings-in-practice-a-research-implementation-guide-9dbf20961590)

---

## 工作流程

### 1. 文档向量化（Indexing阶段）

```python
"""
文档向量化流程
"""
from sentence_transformers import SentenceTransformer
import chromadb

# 1. 加载Embedding模型
model = SentenceTransformer('all-MiniLM-L6-v2')  # 384维

# 2. 文档列表
documents = [
    "Python是一种高级编程语言",
    "JavaScript用于Web开发",
    "机器学习是人工智能的子领域",
    "深度学习使用神经网络"
]

# 3. 生成文档向量
doc_embeddings = model.encode(documents)

print(f"文档数量: {len(documents)}")
print(f"向量维度: {doc_embeddings.shape}")
# 输出: 文档数量: 4, 向量维度: (4, 384)

# 4. 存储到向量数据库
client = chromadb.Client()
collection = client.create_collection("my_docs")

collection.add(
    documents=documents,
    embeddings=doc_embeddings.tolist(),
    ids=[f"doc{i}" for i in range(len(documents))]
)

print("✅ 文档向量化完成")
```

**关键步骤**：
1. 选择Embedding模型
2. 将文档转换为向量
3. 存储向量到数据库

### 2. 查询向量化（Query阶段）

```python
"""
查询向量化
"""
# 用户查询
query = "如何学习编程"

# 查询向量化
query_embedding = model.encode([query])[0]

print(f"查询: {query}")
print(f"查询向量维度: {query_embedding.shape}")
# 输出: 查询向量维度: (384,)
```

### 3. 相似度计算与检索

```python
"""
向量相似度检索
"""
import numpy as np

def cosine_similarity(a, b):
    """计算余弦相似度"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# 计算查询与每个文档的相似度
similarities = []
for i, doc_emb in enumerate(doc_embeddings):
    sim = cosine_similarity(query_embedding, doc_emb)
    similarities.append((i, documents[i], sim))

# 按相似度排序
similarities.sort(key=lambda x: x[2], reverse=True)

# 返回Top-3
print("\n=== 检索结果 ===")
for rank, (idx, doc, sim) in enumerate(similarities[:3], 1):
    print(f"{rank}. [{sim:.3f}] {doc}")

# 输出示例:
# 1. [0.652] Python是一种高级编程语言
# 2. [0.487] JavaScript用于Web开发
# 3. [0.234] 机器学习是人工智能的子领域
```

**参考**: [Dense–Sparse Hybrid Retrieval - Emergent Mind 2025](https://www.emergentmind.com/topics/dense-sparse-hybrid-retrieval)

---

## 核心优势

### 优势1：语义理解

**能理解同义词和相关概念**：

```python
"""
语义理解示例
"""
# 查询和文档用词完全不同，但语义相关
query = "便宜的手机"
documents = [
    "性价比高的智能机推荐",  # ✅ 能匹配（语义相关）
    "iPhone 15 Pro Max",      # ❌ 不匹配（价格不符）
    "经济实惠的通讯设备"      # ✅ 能匹配（同义表达）
]

# Dense检索能找到语义相关的文档
# 即使没有"便宜"、"手机"这些关键词
```

### 优势2：跨语言检索

**支持多语言语义匹配**：

```python
"""
跨语言检索示例
"""
from sentence_transformers import SentenceTransformer

# 使用多语言模型
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 中文查询
query_zh = "如何学习编程"

# 英文文档
docs_en = [
    "How to learn programming",
    "Python tutorial for beginners",
    "JavaScript basics"
]

# 生成向量
query_emb = model.encode([query_zh])[0]
doc_embs = model.encode(docs_en)

# 计算相似度
for doc, doc_emb in zip(docs_en, doc_embs):
    sim = cosine_similarity(query_emb, doc_emb)
    print(f"[{sim:.3f}] {doc}")

# 输出:
# [0.782] How to learn programming  ✅ 高相似度
# [0.654] Python tutorial for beginners
# [0.423] JavaScript basics
```

**参考**: [Embeddings in Practice 2026](https://medium.com/@adnanmasood/embeddings-in-practice-a-research-implementation-guide-9dbf20961590)

### 优势3：处理长尾查询

**对罕见词汇和新词有更好的泛化能力**：

```python
"""
长尾查询示例
"""
# 罕见词汇查询
query = "如何使用asyncio进行并发编程"

# 即使文档中没有"asyncio"这个词，也能找到相关内容
relevant_docs = [
    "Python异步编程指南",        # ✅ 语义相关
    "协程和事件循环详解",        # ✅ 相关概念
    "concurrent.futures使用"    # ✅ 并发相关
]
```

---

## 核心劣势

### 劣势1：可能漏掉关键术语

**对专有名词、产品型号等精准匹配不足**：

```python
"""
精准匹配问题示例
"""
# 查询: "iPhone 15 Pro Max 价格"
# Dense检索可能返回:
results_dense = [
    "iPhone 15 系列对比",        # ❌ 不是Pro Max
    "高端智能手机价格分析",      # ❌ 太泛化
    "iPhone 14 Pro Max 评测"    # ❌ 版本不对
]

# 用户真正想要的:
expected = "iPhone 15 Pro Max 官方售价"  # 精准匹配型号
```

### 劣势2：计算成本高

**向量生成和相似度计算都需要大量计算**：

```python
"""
计算成本对比
"""
import time

# Dense检索成本
start = time.time()
query_emb = model.encode([query])  # Embedding生成
similarities = [cosine_similarity(query_emb[0], doc_emb) for doc_emb in doc_embeddings]
dense_time = time.time() - start

print(f"Dense检索耗时: {dense_time:.3f}s")
# 输出: Dense检索耗时: 0.125s

# Sparse检索（BM25）成本
from rank_bm25 import BM25Okapi
start = time.time()
bm25 = BM25Okapi([doc.split() for doc in documents])
scores = bm25.get_scores(query.split())
sparse_time = time.time() - start

print(f"Sparse检索耗时: {sparse_time:.3f}s")
# 输出: Sparse检索耗时: 0.003s

print(f"Dense是Sparse的 {dense_time/sparse_time:.1f} 倍")
# 输出: Dense是Sparse的 41.7 倍
```

### 劣势3：需要高质量Embedding模型

**模型质量直接影响检索效果**：

```python
"""
模型质量影响示例
"""
# 低质量模型
model_small = SentenceTransformer('all-MiniLM-L6-v2')  # 384维

# 高质量模型
model_large = SentenceTransformer('all-mpnet-base-v2')  # 768维

# 同一查询，不同模型效果差异
query = "深度学习中的注意力机制"

# 小模型可能无法很好理解"注意力机制"的语义
# 大模型能更准确捕捉技术概念的语义
```

---

## Embedding模型选择

### 2025-2026推荐模型

| 模型 | 维度 | 适用场景 | 性能 |
|------|------|----------|------|
| **OpenAI text-embedding-3-small** | 1536 | 通用RAG（推荐） | 性价比最优 |
| **OpenAI text-embedding-3-large** | 3072 | 高精度需求 | 精度提升5-10% |
| **sentence-transformers/all-mpnet-base-v2** | 768 | 本地部署 | 开源最优 |
| **sentence-transformers/paraphrase-multilingual** | 384 | 多语言 | 跨语言效果好 |
| **Cohere embed-v3** | 1024 | 企业级 | 商业支持 |

**参考**: [Embeddings in Practice 2026](https://medium.com/@adnanmasood/embeddings-in-practice-a-research-implementation-guide-9dbf20961590)

### 模型选择代码

```python
"""
不同Embedding模型对比
"""
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import time

# 1. OpenAI API（推荐）
client = OpenAI()

def get_openai_embedding(text, model="text-embedding-3-small"):
    start = time.time()
    response = client.embeddings.create(
        model=model,
        input=text
    )
    elapsed = time.time() - start
    return response.data[0].embedding, elapsed

# 2. Sentence Transformers（本地）
model_local = SentenceTransformer('all-mpnet-base-v2')

def get_local_embedding(text):
    start = time.time()
    embedding = model_local.encode([text])[0]
    elapsed = time.time() - start
    return embedding, elapsed

# 对比测试
test_text = "什么是RAG系统？"

# OpenAI
openai_emb, openai_time = get_openai_embedding(test_text)
print(f"OpenAI: 维度={len(openai_emb)}, 耗时={openai_time:.3f}s")

# Local
local_emb, local_time = get_local_embedding(test_text)
print(f"Local: 维度={len(local_emb)}, 耗时={local_time:.3f}s")

# 输出示例:
# OpenAI: 维度=1536, 耗时=0.234s
# Local: 维度=768, 耗时=0.089s
```

---

## 在RAG中的应用

### 应用1：文档检索

```python
"""
RAG文档检索完整流程
"""
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. 文档分块
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = text_splitter.split_text(long_document)

# 2. 创建向量存储
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_texts(
    texts=chunks,
    embedding=embeddings,
    collection_name="my_docs"
)

# 3. Dense检索
query = "如何优化RAG系统性能？"
results = vectorstore.similarity_search(
    query=query,
    k=5  # Top-5
)

# 4. 返回结果
for i, doc in enumerate(results, 1):
    print(f"\n{i}. {doc.page_content[:100]}...")
```

### 应用2：语义去重

```python
"""
使用Dense检索进行文档去重
"""
import numpy as np

def semantic_deduplication(documents, threshold=0.9):
    """
    基于语义相似度的去重

    Args:
        documents: 文档列表
        threshold: 相似度阈值（>threshold视为重复）
    """
    # 生成向量
    embeddings = model.encode(documents)

    # 去重
    unique_docs = []
    unique_embs = []

    for doc, emb in zip(documents, embeddings):
        # 检查是否与已有文档重复
        is_duplicate = False
        for unique_emb in unique_embs:
            sim = cosine_similarity(emb, unique_emb)
            if sim > threshold:
                is_duplicate = True
                break

        if not is_duplicate:
            unique_docs.append(doc)
            unique_embs.append(emb)

    print(f"原始文档数: {len(documents)}")
    print(f"去重后文档数: {len(unique_docs)}")
    print(f"去重率: {(1 - len(unique_docs)/len(documents))*100:.1f}%")

    return unique_docs

# 测试
docs = [
    "Python是一种编程语言",
    "Python是一门编程语言",  # 语义重复
    "JavaScript用于Web开发",
    "JS是Web开发语言"        # 语义重复
]

unique = semantic_deduplication(docs, threshold=0.85)
# 输出: 原始文档数: 4, 去重后文档数: 2, 去重率: 50.0%
```

### 应用3：相关文档推荐

```python
"""
基于当前文档推荐相关文档
"""
def recommend_similar_docs(current_doc, all_docs, top_k=3):
    """
    推荐与当前文档相关的其他文档
    """
    # 当前文档向量化
    current_emb = model.encode([current_doc])[0]

    # 所有文档向量化
    all_embs = model.encode(all_docs)

    # 计算相似度
    similarities = []
    for i, (doc, emb) in enumerate(zip(all_docs, all_embs)):
        if doc == current_doc:
            continue  # 跳过自己
        sim = cosine_similarity(current_emb, emb)
        similarities.append((doc, sim))

    # 排序并返回Top-K
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]

# 使用
current = "Python异步编程教程"
all_documents = [
    "Python异步编程教程",
    "asyncio库详解",           # 相关
    "Python多线程编程",        # 相关
    "JavaScript Promise",      # 不太相关
    "协程和事件循环"           # 相关
]

recommendations = recommend_similar_docs(current, all_documents, top_k=3)

print("=== 相关文档推荐 ===")
for doc, sim in recommendations:
    print(f"[{sim:.3f}] {doc}")

# 输出:
# [0.876] asyncio库详解
# [0.823] 协程和事件循环
# [0.654] Python多线程编程
```

---

## 参数配置建议

### 1. Top-K选择

```python
"""
Top-K参数选择
"""
# 不同场景的Top-K推荐值
scenarios = {
    "精准问答": {
        "k": 3,
        "说明": "只需要最相关的少数文档"
    },
    "通用检索": {
        "k": 5,
        "说明": "平衡相关性和覆盖面"
    },
    "探索性搜索": {
        "k": 10,
        "说明": "提供更多选择"
    },
    "ReRank前": {
        "k": 20,
        "说明": "先召回更多候选，再精排"
    }
}

# 实际使用
def adaptive_top_k(query_type):
    """根据查询类型动态调整Top-K"""
    if "是什么" in query_type or "定义" in query_type:
        return 3  # 精准问答
    elif "如何" in query_type or "怎样" in query_type:
        return 5  # 通用检索
    else:
        return 10  # 探索性搜索

query = "什么是RAG系统？"
k = adaptive_top_k(query)
results = vectorstore.similarity_search(query, k=k)
```

### 2. 相似度阈值

```python
"""
相似度阈值设置
"""
def filter_by_threshold(results, threshold=0.7):
    """
    过滤低相似度结果

    Args:
        results: 检索结果（包含相似度分数）
        threshold: 相似度阈值
    """
    filtered = [
        (doc, score) for doc, score in results
        if score >= threshold
    ]

    print(f"原始结果数: {len(results)}")
    print(f"过滤后结果数: {len(filtered)}")

    return filtered

# 推荐阈值
thresholds = {
    "高精度场景": 0.8,   # 只返回高度相关的
    "通用场景": 0.7,     # 平衡
    "召回优先": 0.6      # 宁可多返回
}
```

---

## 2025-2026最佳实践

### 1. 使用OpenAI text-embedding-3-small

**性价比最优，满足大部分需求**：

```python
"""
OpenAI Embedding最佳实践
"""
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()

def get_embedding_batch(texts, model="text-embedding-3-small"):
    """
    批量生成Embedding（提升效率）
    """
    # 批量处理（最多2048个文本）
    batch_size = 2048
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        response = client.embeddings.create(
            model=model,
            input=batch
        )
        embeddings = [item.embedding for item in response.data]
        all_embeddings.extend(embeddings)

    return all_embeddings

# 使用
documents = [...]  # 大量文档
embeddings = get_embedding_batch(documents)
```

### 2. 缓存Embedding结果

**避免重复计算，降低成本**：

```python
"""
Embedding缓存策略
"""
import hashlib
import json
import os

class EmbeddingCache:
    def __init__(self, cache_dir="./embedding_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def _get_cache_key(self, text, model):
        """生成缓存键"""
        content = f"{model}:{text}"
        return hashlib.md5(content.encode()).hexdigest()

    def get(self, text, model):
        """获取缓存"""
        key = self._get_cache_key(text, model)
        cache_file = os.path.join(self.cache_dir, f"{key}.json")

        if os.path.exists(cache_file):
            with open(cache_file, 'r') as f:
                return json.load(f)
        return None

    def set(self, text, model, embedding):
        """设置缓存"""
        key = self._get_cache_key(text, model)
        cache_file = os.path.join(self.cache_dir, f"{key}.json")

        with open(cache_file, 'w') as f:
            json.dump(embedding, f)

    def get_or_compute(self, text, model, compute_fn):
        """获取缓存或计算"""
        cached = self.get(text, model)
        if cached is not None:
            return cached

        # 计算并缓存
        embedding = compute_fn(text, model)
        self.set(text, model, embedding)
        return embedding

# 使用
cache = EmbeddingCache()

def compute_embedding(text, model):
    response = client.embeddings.create(model=model, input=text)
    return response.data[0].embedding

# 第一次：计算
emb1 = cache.get_or_compute("测试文本", "text-embedding-3-small", compute_embedding)

# 第二次：从缓存读取（快速）
emb2 = cache.get_or_compute("测试文本", "text-embedding-3-small", compute_embedding)
```

### 3. 监控Embedding质量

```python
"""
Embedding质量监控
"""
def monitor_embedding_quality(embeddings):
    """
    监控Embedding质量指标
    """
    import numpy as np

    # 1. 向量范数分布
    norms = [np.linalg.norm(emb) for emb in embeddings]
    print(f"向量范数: 均值={np.mean(norms):.3f}, 标准差={np.std(norms):.3f}")

    # 2. 向量相似度分布
    n = len(embeddings)
    similarities = []
    for i in range(min(100, n)):  # 采样100对
        for j in range(i+1, min(100, n)):
            sim = cosine_similarity(embeddings[i], embeddings[j])
            similarities.append(sim)

    print(f"相似度分布: 均值={np.mean(similarities):.3f}, 标准差={np.std(similarities):.3f}")

    # 3. 异常检测
    if np.std(norms) > 0.5:
        print("⚠️ 警告: 向量范数方差过大，可能存在质量问题")

    if np.mean(similarities) > 0.8:
        print("⚠️ 警告: 向量相似度过高，可能缺乏区分度")

# 使用
embeddings = [model.encode(doc) for doc in documents]
monitor_embedding_quality(embeddings)
```

---

## 常见问题

### Q1: Dense检索一定比Sparse好吗？

**A**: 不一定。Dense检索在语义理解上更强，但在精准匹配（如产品型号、专有名词）上不如Sparse。2025-2026标准是混合检索（Hybrid），结合两者优势。

### Q2: 如何选择Embedding模型？

**A**:
- 通用场景：OpenAI text-embedding-3-small（性价比最优）
- 高精度需求：OpenAI text-embedding-3-large
- 本地部署：sentence-transformers/all-mpnet-base-v2
- 多语言：paraphrase-multilingual-MiniLM-L12-v2

### Q3: Embedding维度越高越好吗？

**A**: 不是。维度提升带来的精度提升有限（5-10%），但成本和计算量显著增加。大部分场景1536维足够。

### Q4: 如何降低Embedding成本？

**A**:
1. 使用缓存避免重复计算
2. 批量处理提升效率
3. 选择性价比高的模型（text-embedding-3-small）
4. 本地部署开源模型（适合大规模）

---

## 总结

**Dense检索的核心价值**：
1. ✅ 语义理解：能理解同义词和相关概念
2. ✅ 跨语言：支持多语言语义匹配
3. ✅ 泛化能力：对长尾查询和新词有更好的处理

**核心局限**：
1. ❌ 精准匹配不足：可能漏掉关键术语
2. ❌ 计算成本高：需要GPU加速
3. ❌ 依赖模型质量：模型选择很重要

**2025-2026标准实践**：
- 不要只用Dense检索，要结合Sparse检索（混合检索）
- 使用OpenAI text-embedding-3-small（性价比最优）
- 实施缓存策略降低成本
- 监控Embedding质量

**下一步**：学习【核心概念2：稀疏检索】，理解BM25算法和词法匹配原理。
