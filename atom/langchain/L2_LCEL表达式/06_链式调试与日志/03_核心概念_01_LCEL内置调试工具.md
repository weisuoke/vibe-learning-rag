# 核心概念1：LCEL内置调试工具

## 概述

LCEL提供了强大的内置调试工具，让你能够实时查看链的执行过程。这些工具就像给链装上了"X光机"，让你看清每个步骤的输入输出。

**核心工具**：
1. **astream_events() v2** - 推荐使用（2025-2026标准）
2. **astream_log()** - 遗留方法
3. **stream_log()** - 同步方法

---

## astream_events() v2 详解

### 为什么是v2？

LangChain在2024年推出了 `astream_events()` v2版本，相比v1有重大改进：

| 特性 | v1 | v2 |
|------|----|----|
| **事件结构** | 不一致 | 统一标准 |
| **性能** | 较慢 | 优化20% |
| **过滤** | 有限 | 强大 |
| **兼容性** | 部分组件 | 所有组件 |
| **推荐度** | ❌ 已弃用 | ✅ 推荐 |

**重要**：从LangChain v0.3+开始，必须显式指定 `version="v2"`。

---

### 基础使用

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import asyncio

load_dotenv()

# 构建链
prompt = ChatPromptTemplate.from_template("回答问题: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model | StrOutputParser()

# 使用astream_events调试
async def debug_chain():
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2"  # 必须指定v2
    ):
        kind = event["event"]
        name = event["name"]
        print(f"[{kind}] {name}")

asyncio.run(debug_chain())
```

**输出示例**：
```
[on_chain_start] RunnableSequence
[on_prompt_start] ChatPromptTemplate
[on_prompt_end] ChatPromptTemplate
[on_chat_model_start] ChatOpenAI
[on_chat_model_stream] ChatOpenAI
[on_chat_model_stream] ChatOpenAI
[on_chat_model_stream] ChatOpenAI
...
[on_chat_model_end] ChatOpenAI
[on_parser_start] StrOutputParser
[on_parser_stream] StrOutputParser
[on_parser_end] StrOutputParser
[on_chain_end] RunnableSequence
```

---

### 事件结构

每个事件都是一个字典，包含以下字段：

```python
{
    "event": "on_chat_model_stream",  # 事件类型
    "name": "ChatOpenAI",              # 组件名称
    "run_id": "uuid-string",           # 运行ID
    "tags": ["tag1", "tag2"],          # 标签
    "metadata": {},                    # 元数据
    "data": {                          # 事件数据
        "chunk": AIMessageChunk(content="LCEL")
    }
}
```

**字段说明**：

| 字段 | 类型 | 说明 |
|------|------|------|
| `event` | str | 事件类型（如 `on_chain_start`） |
| `name` | str | 组件名称（如 `ChatOpenAI`） |
| `run_id` | str | 唯一运行ID |
| `tags` | list | 标签列表 |
| `metadata` | dict | 元数据字典 |
| `data` | dict | 事件相关数据 |

---

### 事件类型完整列表

#### 1. 链事件 (Chain Events)

| 事件 | 触发时机 | data内容 |
|------|----------|----------|
| `on_chain_start` | 链开始执行 | `{"input": {...}}` |
| `on_chain_stream` | 链流式输出 | `{"chunk": ...}` |
| `on_chain_end` | 链执行完成 | `{"output": {...}}` |

**示例**：
```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chain_start":
        print(f"Chain started with: {event['data']['input']}")
    elif event["event"] == "on_chain_end":
        print(f"Chain completed with: {event['data']['output']}")
```

---

#### 2. LLM事件 (LLM Events)

| 事件 | 触发时机 | data内容 |
|------|----------|----------|
| `on_llm_start` | LLM开始调用 | `{"input": {"prompts": [...]}}` |
| `on_llm_stream` | LLM流式输出 | `{"chunk": GenerationChunk(...)}` |
| `on_llm_end` | LLM调用完成 | `{"output": LLMResult(...)}` |

**注意**：`on_llm_*` 用于传统LLM，`on_chat_model_*` 用于聊天模型。

---

#### 3. 聊天模型事件 (Chat Model Events)

| 事件 | 触发时机 | data内容 |
|------|----------|----------|
| `on_chat_model_start` | 聊天模型开始 | `{"input": {"messages": [...]}}` |
| `on_chat_model_stream` | 聊天模型流式输出 | `{"chunk": AIMessageChunk(...)}` |
| `on_chat_model_end` | 聊天模型完成 | `{"output": AIMessage(...)}` |

**示例**：
```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_start":
        messages = event["data"]["input"]["messages"]
        print(f"Sending to LLM: {messages}")
    elif event["event"] == "on_chat_model_stream":
        chunk = event["data"]["chunk"]
        print(chunk.content, end="", flush=True)
```

---

#### 4. 工具事件 (Tool Events)

| 事件 | 触发时机 | data内容 |
|------|----------|----------|
| `on_tool_start` | 工具开始执行 | `{"input": ...}` |
| `on_tool_end` | 工具执行完成 | `{"output": ...}` |

**示例**：
```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_tool_start":
        print(f"Tool called: {event['name']}")
        print(f"Tool input: {event['data']['input']}")
    elif event["event"] == "on_tool_end":
        print(f"Tool output: {event['data']['output']}")
```

---

#### 5. 检索器事件 (Retriever Events)

| 事件 | 触发时机 | data内容 |
|------|----------|----------|
| `on_retriever_start` | 检索器开始 | `{"input": {"query": "..."}}` |
| `on_retriever_end` | 检索器完成 | `{"output": [Document(...)]}` |

**示例**：
```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_retriever_end":
        docs = event["data"]["output"]
        print(f"Retrieved {len(docs)} documents:")
        for doc in docs:
            print(f"- {doc.page_content[:100]}")
```

---

#### 6. Prompt事件 (Prompt Events)

| 事件 | 触发时机 | data内容 |
|------|----------|----------|
| `on_prompt_start` | Prompt开始 | `{"input": {...}}` |
| `on_prompt_end` | Prompt完成 | `{"output": ChatPromptValue(...)}` |

**示例**：
```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_prompt_end":
        prompt_value = event["data"]["output"]
        messages = prompt_value.to_messages()
        print(f"Final prompt: {messages}")
```

---

#### 7. 解析器事件 (Parser Events)

| 事件 | 触发时机 | data内容 |
|------|----------|----------|
| `on_parser_start` | 解析器开始 | `{"input": ...}` |
| `on_parser_stream` | 解析器流式输出 | `{"chunk": ...}` |
| `on_parser_end` | 解析器完成 | `{"output": ...}` |

---

### 事件过滤

#### 按类型过滤 (include_types)

只查看特定类型的事件：

```python
async def filter_by_type():
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2",
        include_types=["chat_model"]  # 只看聊天模型事件
    ):
        print(f"[{event['event']}] {event['name']}")

asyncio.run(filter_by_type())
```

**输出**：
```
[on_chat_model_start] ChatOpenAI
[on_chat_model_stream] ChatOpenAI
[on_chat_model_stream] ChatOpenAI
...
[on_chat_model_end] ChatOpenAI
```

**常用类型**：
- `"chat_model"` - 聊天模型
- `"llm"` - 传统LLM
- `"tool"` - 工具
- `"retriever"` - 检索器
- `"prompt"` - Prompt
- `"parser"` - 解析器

---

#### 排除类型 (exclude_types)

排除不关心的事件：

```python
async def exclude_types():
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2",
        exclude_types=["parser"]  # 排除解析器事件
    ):
        print(f"[{event['event']}] {event['name']}")

asyncio.run(exclude_types())
```

---

#### 按名称过滤 (include_names)

只查看特定名称的组件：

```python
async def filter_by_name():
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2",
        include_names=["ChatOpenAI"]  # 只看ChatOpenAI
    ):
        print(f"[{event['event']}] {event['name']}")

asyncio.run(filter_by_name())
```

---

#### 按标签过滤 (include_tags)

为链添加标签，然后按标签过滤：

```python
# 添加标签
chain_with_tags = chain.with_config(tags=["my_chain"])

async def filter_by_tag():
    async for event in chain_with_tags.astream_events(
        {"question": "什么是LCEL?"},
        version="v2",
        include_tags=["my_chain"]
    ):
        print(f"[{event['event']}] {event['name']}")

asyncio.run(filter_by_tag())
```

---

### 实时查看LLM输出

最常见的用例：实时查看LLM生成的内容

```python
async def stream_llm_output():
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2"
    ):
        kind = event["event"]

        # 只处理聊天模型的流式输出
        if kind == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            print(chunk.content, end="", flush=True)

    print()  # 换行

asyncio.run(stream_llm_output())
```

**输出**：
```
LCEL（LangChain Expression Language）是一种声明式的链构建语言...
```

---

### 调试复杂链

对于复杂的多步骤链，可以查看每个步骤的详细信息：

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.runnables import RunnablePassthrough

# 构建RAG链
vectorstore = Chroma.from_texts(
    ["LCEL是LangChain的表达式语言", "LCEL支持流式处理"],
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

async def debug_rag_chain():
    async for event in rag_chain.astream_events(
        "什么是LCEL?",
        version="v2"
    ):
        kind = event["event"]
        name = event["name"]

        # 查看检索结果
        if kind == "on_retriever_end":
            docs = event["data"]["output"]
            print(f"\n=== Retrieved {len(docs)} documents ===")
            for i, doc in enumerate(docs):
                print(f"{i+1}. {doc.page_content}")

        # 查看发送给LLM的prompt
        if kind == "on_chat_model_start":
            messages = event["data"]["input"]["messages"]
            print(f"\n=== Prompt to LLM ===")
            for msg in messages:
                print(f"{msg.type}: {msg.content}")

        # 查看LLM输出
        if kind == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            print(chunk.content, end="", flush=True)

    print()

asyncio.run(debug_rag_chain())
```

**输出示例**：
```
=== Retrieved 2 documents ===
1. LCEL是LangChain的表达式语言
2. LCEL支持流式处理

=== Prompt to LLM ===
system: 你是一个有帮助的助手
human: 基于以下上下文回答问题:
LCEL是LangChain的表达式语言
LCEL支持流式处理

问题: 什么是LCEL?

LCEL是LangChain Expression Language的缩写...
```

---

### 性能分析

使用事件时间戳分析性能瓶颈：

```python
import time

async def analyze_performance():
    start_times = {}
    durations = {}

    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2"
    ):
        kind = event["event"]
        name = event["name"]
        key = f"{name}_{kind}"

        if kind.endswith("_start"):
            start_times[name] = time.time()
        elif kind.endswith("_end"):
            if name in start_times:
                duration = time.time() - start_times[name]
                durations[name] = duration

    # 打印性能报告
    print("\n=== Performance Report ===")
    for name, duration in sorted(durations.items(), key=lambda x: x[1], reverse=True):
        print(f"{name}: {duration:.3f}s")

asyncio.run(analyze_performance())
```

**输出示例**：
```
=== Performance Report ===
ChatOpenAI: 1.234s
ChatPromptTemplate: 0.002s
StrOutputParser: 0.001s
```

---

## astream_log() 方法（遗留）

`astream_log()` 是v1时代的方法，现在不推荐使用。

### 基础使用

```python
async def use_astream_log():
    async for chunk in chain.astream_log({"question": "什么是LCEL?"}):
        print(chunk)

asyncio.run(use_astream_log())
```

### 为什么不推荐？

1. **事件结构不一致**：不同组件返回的结构不同
2. **性能较差**：比v2慢约20%
3. **过滤功能有限**：不支持 `include_types` 等高级过滤
4. **维护状态**：已进入维护模式，不再添加新功能

**建议**：使用 `astream_events(version="v2")` 替代。

---

## stream_log() 同步方法

如果你的代码不支持异步，可以使用同步版本：

```python
def debug_sync():
    for chunk in chain.stream_log({"question": "什么是LCEL?"}):
        print(chunk)

debug_sync()
```

**注意**：同步方法性能较差，推荐使用异步版本。

---

## 性能影响分析

### 开销测试

```python
import time

async def benchmark():
    # 测试1: 不使用调试工具
    start = time.time()
    result = await chain.ainvoke({"question": "什么是LCEL?"})
    baseline = time.time() - start
    print(f"Baseline: {baseline:.3f}s")

    # 测试2: 使用astream_events
    start = time.time()
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2"
    ):
        pass
    with_events = time.time() - start
    print(f"With events: {with_events:.3f}s")

    # 计算开销
    overhead = (with_events - baseline) / baseline * 100
    print(f"Overhead: {overhead:.1f}%")

asyncio.run(benchmark())
```

**典型结果**：
```
Baseline: 1.234s
With events: 1.456s
Overhead: 18.0%
```

### 开销来源

1. **事件序列化**：每个事件都需要序列化成字典（~40%）
2. **内存分配**：事件对象占用额外内存（~30%）
3. **异步开销**：异步迭代器的开销（~20%）
4. **数据复制**：事件数据需要复制（~10%）

### 优化建议

**开发环境**：
- ✅ 放心使用，性能不是首要考虑
- ✅ 使用过滤减少事件数量

**生产环境**：
- ✅ 移除调试代码
- ✅ 或使用采样（1-5%）
- ✅ 或只在错误时启用

```python
import random
import os

# 采样策略
if random.random() < 0.01:  # 1%采样
    os.environ["DEBUG_MODE"] = "true"

# 使用
if os.getenv("DEBUG_MODE") == "true":
    async for event in chain.astream_events(input, version="v2"):
        logger.debug(f"{event['event']}: {event['name']}")
else:
    result = await chain.ainvoke(input)
```

---

## 最佳实践

### 1. 总是指定version="v2"

```python
# ✅ 正确
async for event in chain.astream_events(input, version="v2"):
    pass

# ❌ 错误（会使用v1）
async for event in chain.astream_events(input):
    pass
```

---

### 2. 使用过滤减少噪音

```python
# ✅ 正确：只看关心的事件
async for event in chain.astream_events(
    input,
    version="v2",
    include_types=["chat_model"]
):
    pass

# ❌ 错误：处理所有事件
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_stream":
        pass  # 大量无用事件被遍历
```

---

### 3. 检查事件类型

```python
# ✅ 正确：检查事件类型
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_stream":
        chunk = event["data"]["chunk"]
        print(chunk.content, end="")

# ❌ 错误：假设所有事件都有chunk
async for event in chain.astream_events(input, version="v2"):
    print(event["data"]["chunk"].content)  # 可能KeyError
```

---

### 4. 生产环境移除或采样

```python
# ✅ 正确：生产环境采样
if os.getenv("ENV") == "production":
    if random.random() < 0.01:  # 1%采样
        async for event in chain.astream_events(input, version="v2"):
            logger.debug(event)
else:
    # 开发环境详细调试
    async for event in chain.astream_events(input, version="v2"):
        print(event)
```

---

## 常见问题

### Q1: 为什么必须指定version="v2"？

**A**: LangChain v0.3+默认使用v1（为了向后兼容），但v1已弃用。显式指定v2可以使用最新功能。

---

### Q2: 如何查看完整的prompt？

**A**: 监听 `on_chat_model_start` 事件：

```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_start":
        messages = event["data"]["input"]["messages"]
        for msg in messages:
            print(f"{msg.type}: {msg.content}")
```

---

### Q3: 如何查看Token使用？

**A**: 监听 `on_chat_model_end` 事件：

```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_end":
        output = event["data"]["output"]
        if hasattr(output, "response_metadata"):
            tokens = output.response_metadata.get("token_usage", {})
            print(f"Tokens: {tokens}")
```

---

### Q4: 性能开销太大怎么办？

**A**: 使用过滤或采样：

```python
# 方案1: 过滤
async for event in chain.astream_events(
    input,
    version="v2",
    include_types=["chat_model"]  # 只看LLM事件
):
    pass

# 方案2: 采样
if random.random() < 0.01:  # 1%采样
    async for event in chain.astream_events(input, version="v2"):
        pass
```

---

## 参考资源

### 官方文档
1. **LangChain Event Streaming**: https://python.langchain.com/docs/expression_language/streaming
2. **Streaming Events API**: https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_events

### 源码参考
1. **Event Stream Implementation**: `langchain/libs/core/langchain_core/tracers/event_stream.py`
2. **Base Runnable**: `langchain/libs/core/langchain_core/runnables/base.py`

### 相关文章
1. **LangChain v0.3 Release Notes** (2025): 介绍astream_events v2的改进
2. **Debugging LCEL Chains** (2025): 官方调试指南

---

## 总结

**核心要点**：

1. **使用v2**：总是指定 `version="v2"`
2. **事件过滤**：使用 `include_types` 减少噪音
3. **性能意识**：开发用，生产移除或采样
4. **实时查看**：监听 `on_chat_model_stream` 查看LLM输出
5. **调试RAG**：监听 `on_retriever_end` 查看检索结果

**记住**：`astream_events()` 是LCEL最强大的调试工具，掌握它可以让你快速定位问题。

---

**版本信息**
- LangChain: v0.3+ (2025-2026)
- Python: 3.13+
- 最后更新: 2026-02-20
