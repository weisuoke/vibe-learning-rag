# 最小可用

> 掌握以下内容，就能开始使用 LCEL 构建 AI 应用

---

## 核心理念

**20%的核心知识解决80%的问题**

LCEL 虽然功能强大，但日常开发中只需要掌握以下5个核心知识点，就能构建大部分 AI 应用。

---

## 4.1 管道操作符 `|` - 连接组件

**一句话**：用 `|` 把多个组件连接成链，数据从左到右流动。

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# 定义组件
prompt = ChatPromptTemplate.from_template("讲一个关于{topic}的笑话")
llm = ChatOpenAI()
parser = StrOutputParser()

# 用 | 连接成链
chain = prompt | llm | parser

# 调用链
result = chain.invoke({"topic": "程序员"})
print(result)
```

**输出示例**：
```
为什么程序员总是分不清万圣节和圣诞节？
因为 Oct 31 == Dec 25
（八进制的31等于十进制的25）
```

**在 AI Agent 后端中的应用**：
- 简单问答：`prompt | llm | parser`
- RAG 检索：`retriever | prompt | llm | parser`
- 多步推理：`step1 | step2 | step3`

**类比**：
- **前端类比**：就像 Promise 链 `.then().then()`
- **日常类比**：就像流水线（原料 → 加工 → 包装 → 出厂）

---

## 4.2 三大核心组件 - 构建基础链

**一句话**：PromptTemplate（格式化输入）+ ChatModel（调用 LLM）+ OutputParser（解析输出）= 完整的 AI 流程。

### PromptTemplate - 格式化输入

```python
from langchain_core.prompts import ChatPromptTemplate

# 创建 Prompt 模板
prompt = ChatPromptTemplate.from_template(
    "你是一个{role}，请回答：{question}"
)

# 格式化输入
formatted = prompt.invoke({
    "role": "Python 专家",
    "question": "什么是装饰器？"
})
print(formatted)
```

**输出**：
```
[HumanMessage(content='你是一个Python 专家，请回答：什么是装饰器？')]
```

### ChatModel - 调用 LLM

```python
from langchain_openai import ChatOpenAI

# 创建 LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# 调用 LLM
response = llm.invoke([
    {"role": "user", "content": "什么是 LCEL？"}
])
print(response.content)
```

### OutputParser - 解析输出

```python
from langchain_core.output_parsers import StrOutputParser

# 创建解析器
parser = StrOutputParser()

# 解析 LLM 输出
result = parser.invoke(response)
print(result)  # 纯字符串
```

**组合使用**：
```python
# 完整的链
chain = prompt | llm | parser

# 一次调用完成所有步骤
result = chain.invoke({
    "role": "Python 专家",
    "question": "什么是装饰器？"
})
```

**在 AI Agent 后端中的应用**：
```python
# FastAPI 端点
@app.post("/ask")
async def ask(question: str):
    chain = prompt | llm | parser
    answer = await chain.ainvoke({"question": question})
    return {"answer": answer}
```

---

## 4.3 invoke() 和 ainvoke() - 同步和异步调用

**一句话**：`invoke()` 用于同步调用，`ainvoke()` 用于异步调用（FastAPI 推荐）。

### 同步调用 - invoke()

```python
# 同步调用（阻塞）
result = chain.invoke({"topic": "Python"})
print(result)
```

**适用场景**：
- 脚本、命令行工具
- 不需要并发的场景

### 异步调用 - ainvoke()

```python
import asyncio

# 异步调用（非阻塞）
async def main():
    result = await chain.ainvoke({"topic": "Python"})
    print(result)

asyncio.run(main())
```

**适用场景**：
- FastAPI 端点（推荐）
- 需要并发处理多个请求
- 需要同时调用多个 API

**在 FastAPI 中的应用**：
```python
@app.post("/chat")
async def chat(message: str):
    # 使用 ainvoke() 而不是 invoke()
    response = await chain.ainvoke({"message": message})
    return {"response": response}
```

**为什么 FastAPI 要用 ainvoke()？**
- FastAPI 是异步框架，用 `ainvoke()` 不会阻塞其他请求
- 可以同时处理多个用户的请求
- 性能更好

---

## 4.4 流式输出 - astream() 实时返回

**一句话**：用 `astream()` 逐块返回结果，像 ChatGPT 一样逐字显示。

```python
# 流式调用
async def stream_example():
    async for chunk in chain.astream({"topic": "Python"}):
        print(chunk, end="", flush=True)

asyncio.run(stream_example())
```

**输出效果**：
```
为什么Python程序员喜欢用缩进？
因为他们觉得Tab键比花括号更优雅！
```
（逐字显示，而不是等待完整结果）

**在 FastAPI 中的应用**：
```python
from fastapi.responses import StreamingResponse

@app.post("/chat/stream")
async def chat_stream(message: str):
    async def generate():
        async for chunk in chain.astream({"message": message}):
            yield f"data: {chunk}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

**前端接收**：
```javascript
// 前端使用 EventSource 接收流式数据
const eventSource = new EventSource('/chat/stream');
eventSource.onmessage = (event) => {
    console.log(event.data);  // 逐块显示
};
```

**为什么需要流式输出？**
- 用户体验更好（实时反馈）
- 减少等待时间（不用等完整结果）
- 适合长文本生成

---

## 4.5 RunnablePassthrough - 传递原始输入

**一句话**：用 `RunnablePassthrough()` 把输入原样传递到下一步，常用于需要保留原始数据的场景。

```python
from langchain_core.runnables import RunnablePassthrough

# 场景：需要同时使用原始问题和检索结果
chain = {
    "context": retriever,           # 检索相关文档
    "question": RunnablePassthrough()  # 保留原始问题
} | prompt | llm | parser

# 调用
result = chain.invoke("什么是 LCEL？")
```

**等价于**：
```python
# 手动实现
def manual_chain(question):
    context = retriever.invoke(question)
    formatted = prompt.invoke({
        "context": context,
        "question": question  # 保留原始问题
    })
    response = llm.invoke(formatted)
    return parser.invoke(response)
```

**在 RAG 中的应用**：
```python
# RAG 链：需要同时传递检索结果和原始问题
rag_chain = {
    "context": vectorstore.as_retriever() | format_docs,
    "question": RunnablePassthrough()
} | ChatPromptTemplate.from_template(
    "基于以下上下文回答问题：\n\n{context}\n\n问题：{question}"
) | llm | StrOutputParser()

# 调用
answer = rag_chain.invoke("什么是向量检索？")
```

**为什么需要 RunnablePassthrough？**
- 保留原始输入，避免数据丢失
- 在多步骤链中传递上下文
- 构建复杂的数据流

---

## 最小可用知识总结

掌握以上5个知识点，你就能：

### ✅ 能做什么

1. **构建简单问答链**
   ```python
   chain = prompt | llm | parser
   ```

2. **构建 RAG 检索链**
   ```python
   chain = {
       "context": retriever,
       "question": RunnablePassthrough()
   } | prompt | llm | parser
   ```

3. **在 FastAPI 中使用**
   ```python
   @app.post("/chat")
   async def chat(message: str):
       return await chain.ainvoke({"message": message})
   ```

4. **实现流式输出**
   ```python
   async for chunk in chain.astream(input):
       print(chunk, end="")
   ```

5. **保留原始输入**
   ```python
   chain = {
       "original": RunnablePassthrough(),
       "processed": some_processor
   } | next_step
   ```

### ✅ 为后续学习打基础

- **并行执行**：RunnableParallel（基于 `{}` 字典语法）
- **条件分支**：RunnableBranch（基于管道操作符）
- **错误处理**：with_retry()、with_fallbacks()
- **自定义组件**：继承 Runnable 接口

---

## 快速上手示例

### 示例1：最简单的问答链

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. 定义组件
prompt = ChatPromptTemplate.from_template("回答：{question}")
llm = ChatOpenAI()
parser = StrOutputParser()

# 2. 组合成链
chain = prompt | llm | parser

# 3. 调用
answer = chain.invoke({"question": "什么是 LCEL？"})
print(answer)
```

### 示例2：FastAPI 集成

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

# 定义链
chain = prompt | llm | parser

class Question(BaseModel):
    text: str

@app.post("/ask")
async def ask(question: Question):
    answer = await chain.ainvoke({"question": question.text})
    return {"answer": answer}
```

### 示例3：流式输出

```python
from fastapi.responses import StreamingResponse

@app.post("/ask/stream")
async def ask_stream(question: Question):
    async def generate():
        async for chunk in chain.astream({"question": question.text}):
            yield f"data: {chunk}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

## 常见问题

### Q1: 什么时候用 invoke()，什么时候用 ainvoke()？

**A**:
- **FastAPI 端点**：用 `ainvoke()`（异步）
- **脚本/命令行**：用 `invoke()`（同步）
- **规则**：如果函数定义是 `async def`，就用 `ainvoke()`

### Q2: 流式输出和普通输出有什么区别？

**A**:
- **普通输出**：等待完整结果，一次性返回
- **流式输出**：逐块返回，实时显示
- **选择**：用户体验要求高 → 流式；简单场景 → 普通

### Q3: RunnablePassthrough 什么时候用？

**A**:
- 需要在链中保留原始输入时
- 构建 RAG 链时（需要同时传递问题和检索结果）
- 多步骤链中需要传递上下文时

### Q4: 为什么要用 LCEL 而不是直接调用 API？

**A**:
- **可组合**：组件可以复用
- **可流式**：内置流式输出支持
- **可测试**：每个组件可以单独测试
- **可维护**：代码更清晰

---

## 学习检查清单

掌握最小可用知识后，你应该能够：

- [ ] 使用 `|` 连接多个组件
- [ ] 创建 PromptTemplate、ChatModel、OutputParser
- [ ] 使用 `invoke()` 同步调用链
- [ ] 使用 `ainvoke()` 异步调用链（FastAPI）
- [ ] 使用 `astream()` 实现流式输出
- [ ] 使用 `RunnablePassthrough()` 保留原始输入
- [ ] 构建简单的问答链
- [ ] 将 LCEL 链集成到 FastAPI 端点
- [ ] 实现流式响应的 FastAPI 端点

---

## 下一步学习

掌握最小可用知识后，可以继续学习：

1. **核心概念** - 深入理解 Runnable 接口、并行执行、条件分支
2. **实战代码** - 学习更多实际场景的实现
3. **高级特性** - 错误处理、重试、降级策略

---

**版本**: v1.0
**最后更新**: 2026-02-12
