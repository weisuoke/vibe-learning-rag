# å®æˆ˜ä»£ç  07ï¼šè‡ªå®šä¹‰ Provider å®ç°

> **å®æˆ˜ç›®æ ‡**ï¼šå®ç°å®Œæ•´çš„è‡ªå®šä¹‰ Provider é…ç½®ï¼Œæ”¯æŒ Ollamaã€ä¼ä¸šä»£ç†å’Œ OpenRouter ç­‰åœºæ™¯

---

## ä¸€ã€Ollama æœ¬åœ°æ¨¡å‹é…ç½®

### 1.1 å®‰è£…å’Œå¯åŠ¨ Ollama

```bash
#!/bin/bash
# setup-ollama.sh - å®‰è£…å’Œé…ç½® Ollama

# macOS å®‰è£…
if [[ "$OSTYPE" == "darwin"* ]]; then
    echo "ğŸ“¦ å®‰è£… Ollama (macOS)..."
    brew install ollama
fi

# Linux å®‰è£…
if [[ "$OSTYPE" == "linux-gnu"* ]]; then
    echo "ğŸ“¦ å®‰è£… Ollama (Linux)..."
    curl -fsSL https://ollama.com/install.sh | sh
fi

# å¯åŠ¨ Ollama æœåŠ¡
echo "ğŸš€ å¯åŠ¨ Ollama æœåŠ¡..."
ollama serve &

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 3

# æ‹‰å–æ¨¡å‹
echo "ğŸ“¥ æ‹‰å–æ¨¡å‹..."
ollama pull llama3.1:8b
ollama pull qwen2.5-coder:7b
ollama pull deepseek-r1:7b

# éªŒè¯å®‰è£…
echo "âœ… éªŒè¯å®‰è£…..."
ollama list

echo "ğŸ‰ Ollama é…ç½®å®Œæˆ"
```

### 1.2 é…ç½® models.json

```bash
#!/bin/bash
# configure-ollama.sh - é…ç½® Ollama Provider

mkdir -p ~/.pi/agent

cat > ~/.pi/agent/models.json << 'EOF'
{
  "providers": {
    "ollama": {
      "baseUrl": "http://localhost:11434/v1",
      "api": "openai-completions",
      "apiKey": "ollama",
      "models": [
        {
          "id": "llama3.1:8b",
          "name": "Llama 3.1 8B (Local)",
          "contextWindow": 128000,
          "maxTokens": 32000,
          "cost": {
            "input": 0,
            "output": 0,
            "cacheRead": 0,
            "cacheWrite": 0
          }
        },
        {
          "id": "qwen2.5-coder:7b",
          "name": "Qwen 2.5 Coder 7B",
          "contextWindow": 32768,
          "maxTokens": 8192,
          "cost": {
            "input": 0,
            "output": 0,
            "cacheRead": 0,
            "cacheWrite": 0
          }
        },
        {
          "id": "deepseek-r1:7b",
          "name": "DeepSeek R1 7B",
          "reasoning": true,
          "contextWindow": 64000,
          "maxTokens": 8192,
          "cost": {
            "input": 0,
            "output": 0,
            "cacheRead": 0,
            "cacheWrite": 0
          }
        }
      ]
    }
  }
}
EOF

echo "âœ… Ollama Provider é…ç½®å®Œæˆ"
```

### 1.3 æµ‹è¯• Ollama

```bash
#!/bin/bash
# test-ollama.sh - æµ‹è¯• Ollama é…ç½®

echo "ğŸ” æµ‹è¯• Ollama..."

# æµ‹è¯• API
response=$(curl -s http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.1:8b",
    "messages": [{"role": "user", "content": "Hi"}],
    "max_tokens": 10
  }')

if echo "$response" | grep -q "choices"; then
    echo "âœ… Ollama API æ­£å¸¸"
    echo "å“åº”: $(echo $response | jq -r '.choices[0].message.content')"
else
    echo "âŒ Ollama API å¼‚å¸¸"
    echo "é”™è¯¯: $response"
fi

# æµ‹è¯• Pi
echo ""
echo "ğŸ–ï¸ æµ‹è¯• Pi with Ollama..."
pi --provider ollama --model llama3.1:8b --print "ç”Ÿæˆä¸€ä¸ª Hello World å‡½æ•°"
```

---

## äºŒã€ä¼ä¸šä»£ç†é…ç½®

### 2.1 ä¼ä¸šä»£ç† models.json

```json
// ~/.pi/agent/models.json - ä¼ä¸šä»£ç†é…ç½®
{
  "providers": {
    "corp-proxy": {
      "baseUrl": "https://llm-proxy.corp.example.com/v1",
      "api": "anthropic-messages",
      "apiKey": "!aws secretsmanager get-secret-value --secret-id anthropic-key --query SecretString --output text",
      "headers": {
        "x-corp-auth": "CORP_AUTH_TOKEN",
        "x-department": "engineering",
        "x-cost-center": "12345"
      },
      "models": [
        {
          "id": "claude-opus-4",
          "name": "Claude Opus 4 (Corp Proxy)",
          "contextWindow": 200000,
          "maxTokens": 16384,
          "cost": {
            "input": 15,
            "output": 75,
            "cacheRead": 1.5,
            "cacheWrite": 18.75
          }
        },
        {
          "id": "claude-sonnet-4",
          "name": "Claude Sonnet 4 (Corp Proxy)",
          "contextWindow": 200000,
          "maxTokens": 16384,
          "cost": {
            "input": 3,
            "output": 15,
            "cacheRead": 0.3,
            "cacheWrite": 3.75
          }
        }
      ]
    }
  }
}
```

### 2.2 ä¼ä¸šä»£ç†é…ç½®è„šæœ¬

```bash
#!/bin/bash
# setup-corp-proxy.sh - é…ç½®ä¼ä¸šä»£ç†

# 1. é…ç½®ç¯å¢ƒå˜é‡
export CORP_AUTH_TOKEN=$(aws secretsmanager get-secret-value \
  --secret-id corp-auth-token \
  --query SecretString \
  --output text)

# 2. åˆ›å»º models.json
mkdir -p ~/.pi/agent

cat > ~/.pi/agent/models.json << EOF
{
  "providers": {
    "corp-proxy": {
      "baseUrl": "https://llm-proxy.corp.example.com/v1",
      "api": "anthropic-messages",
      "apiKey": "!aws secretsmanager get-secret-value --secret-id anthropic-key --query SecretString --output text",
      "headers": {
        "x-corp-auth": "$CORP_AUTH_TOKEN",
        "x-department": "engineering"
      },
      "models": [
        {
          "id": "claude-opus-4",
          "name": "Claude Opus 4 (Corp)"
        }
      ]
    }
  }
}
EOF

echo "âœ… ä¼ä¸šä»£ç†é…ç½®å®Œæˆ"
```

### 2.3 æµ‹è¯•ä¼ä¸šä»£ç†

```bash
#!/bin/bash
# test-corp-proxy.sh - æµ‹è¯•ä¼ä¸šä»£ç†

echo "ğŸ” æµ‹è¯•ä¼ä¸šä»£ç†..."

# æµ‹è¯•è¿æ¥
if curl -s -o /dev/null -w "%{http_code}" https://llm-proxy.corp.example.com/health | grep -q "200"; then
    echo "âœ… ä»£ç†æœåŠ¡å¯è®¿é—®"
else
    echo "âŒ ä»£ç†æœåŠ¡ä¸å¯è®¿é—®"
    exit 1
fi

# æµ‹è¯• Pi
pi --provider corp-proxy --model claude-opus-4 --print "æµ‹è¯•ä¼ä¸šä»£ç†"
```

---

## ä¸‰ã€OpenRouter é…ç½®

### 3.1 OpenRouter models.json

```json
// ~/.pi/agent/models.json - OpenRouter é…ç½®
{
  "providers": {
    "openrouter": {
      "baseUrl": "https://openrouter.ai/api/v1",
      "api": "openai-completions",
      "apiKey": "OPENROUTER_API_KEY",
      "models": [
        {
          "id": "anthropic/claude-sonnet-4",
          "name": "Claude Sonnet 4 (OpenRouter)",
          "contextWindow": 200000,
          "maxTokens": 16384,
          "cost": {
            "input": 3,
            "output": 15,
            "cacheRead": 0.3,
            "cacheWrite": 3.75
          },
          "compat": {
            "openRouterRouting": {
              "order": ["anthropic"],
              "fallbacks": ["openai"]
            }
          }
        },
        {
          "id": "openai/gpt-4o",
          "name": "GPT-4o (OpenRouter)",
          "contextWindow": 128000,
          "maxTokens": 16384,
          "cost": {
            "input": 2.5,
            "output": 10,
            "cacheRead": 0,
            "cacheWrite": 0
          }
        },
        {
          "id": "google/gemini-2.0-flash-exp",
          "name": "Gemini 2.0 Flash (OpenRouter)",
          "contextWindow": 1000000,
          "maxTokens": 8192,
          "cost": {
            "input": 0,
            "output": 0,
            "cacheRead": 0,
            "cacheWrite": 0
          }
        }
      ]
    }
  }
}
```

### 3.2 OpenRouter é…ç½®è„šæœ¬

```bash
#!/bin/bash
# setup-openrouter.sh - é…ç½® OpenRouter

# 1. è·å– API Key
cat << 'EOF'
ğŸ“ è·å– OpenRouter API Key

æ­¥éª¤:
1. è®¿é—® https://openrouter.ai/keys
2. åˆ›å»º API Key
3. å¤åˆ¶ API Key (sk-or-...)
EOF

read -p "è¾“å…¥ OpenRouter API Key: " api_key

# 2. é…ç½®ç¯å¢ƒå˜é‡
export OPENROUTER_API_KEY=$api_key
echo "export OPENROUTER_API_KEY=$api_key" >> ~/.bashrc

# 3. åˆ›å»º models.json
mkdir -p ~/.pi/agent

cat > ~/.pi/agent/models.json << 'EOF'
{
  "providers": {
    "openrouter": {
      "baseUrl": "https://openrouter.ai/api/v1",
      "api": "openai-completions",
      "apiKey": "OPENROUTER_API_KEY",
      "models": [
        {
          "id": "anthropic/claude-sonnet-4",
          "name": "Claude Sonnet 4 (OpenRouter)"
        },
        {
          "id": "openai/gpt-4o",
          "name": "GPT-4o (OpenRouter)"
        }
      ]
    }
  }
}
EOF

echo "âœ… OpenRouter é…ç½®å®Œæˆ"
```

---

## å››ã€Vercel AI Gateway é…ç½®

### 4.1 Vercel AI Gateway models.json

```json
// ~/.pi/agent/models.json - Vercel AI Gateway é…ç½®
{
  "providers": {
    "vercel-gateway": {
      "baseUrl": "https://ai-gateway.vercel.sh/v1",
      "api": "openai-completions",
      "apiKey": "AI_GATEWAY_API_KEY",
      "models": [
        {
          "id": "moonshotai/kimi-k2.5",
          "name": "Kimi K2.5 (Fireworks)",
          "reasoning": true,
          "input": ["text", "image"],
          "contextWindow": 262144,
          "maxTokens": 262144,
          "cost": {
            "input": 0.6,
            "output": 3,
            "cacheRead": 0,
            "cacheWrite": 0
          },
          "compat": {
            "vercelGatewayRouting": {
              "only": ["fireworks", "novita"],
              "order": ["fireworks", "novita"]
            }
          }
        }
      ]
    }
  }
}
```

---

## äº”ã€å®Œæ•´é…ç½®ç¤ºä¾‹

### 5.1 å¤š Provider å®Œæ•´é…ç½®

```json
// ~/.pi/agent/models.json - å®Œæ•´é…ç½®
{
  "providers": {
    "ollama": {
      "baseUrl": "http://localhost:11434/v1",
      "api": "openai-completions",
      "apiKey": "ollama",
      "models": [
        {
          "id": "llama3.1:8b",
          "name": "Llama 3.1 8B (Local)",
          "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 }
        }
      ]
    },
    "corp-proxy": {
      "baseUrl": "https://llm-proxy.corp.example.com/v1",
      "api": "anthropic-messages",
      "apiKey": "!aws secretsmanager get-secret-value --secret-id anthropic-key --query SecretString --output text",
      "headers": {
        "x-corp-auth": "CORP_AUTH_TOKEN"
      },
      "models": [
        {
          "id": "claude-opus-4",
          "name": "Claude Opus 4 (Corp)"
        }
      ]
    },
    "openrouter": {
      "baseUrl": "https://openrouter.ai/api/v1",
      "api": "openai-completions",
      "apiKey": "OPENROUTER_API_KEY",
      "models": [
        {
          "id": "anthropic/claude-sonnet-4",
          "name": "Claude Sonnet 4 (OpenRouter)"
        }
      ]
    }
  }
}
```

### 5.2 é…ç½®ç®¡ç†è„šæœ¬

```typescript
// manage-providers.ts - Provider é…ç½®ç®¡ç†

import * as fs from 'fs';
import * as path from 'path';

interface ProviderConfig {
  baseUrl: string;
  api: string;
  apiKey: string;
  headers?: Record<string, string>;
  models: ModelConfig[];
}

interface ModelConfig {
  id: string;
  name: string;
  contextWindow?: number;
  maxTokens?: number;
  cost?: {
    input: number;
    output: number;
    cacheRead: number;
    cacheWrite: number;
  };
}

interface ModelsConfig {
  providers: Record<string, ProviderConfig>;
}

class ProviderManager {
  private configPath: string;
  private config: ModelsConfig;

  constructor() {
    this.configPath = path.join(
      process.env.HOME!,
      '.pi/agent/models.json'
    );
    this.config = this.loadConfig();
  }

  private loadConfig(): ModelsConfig {
    if (fs.existsSync(this.configPath)) {
      return JSON.parse(fs.readFileSync(this.configPath, 'utf-8'));
    }
    return { providers: {} };
  }

  private saveConfig(): void {
    fs.writeFileSync(
      this.configPath,
      JSON.stringify(this.config, null, 2)
    );
  }

  addProvider(name: string, config: ProviderConfig): void {
    this.config.providers[name] = config;
    this.saveConfig();
    console.log(`âœ… å·²æ·»åŠ  Provider: ${name}`);
  }

  removeProvider(name: string): void {
    if (this.config.providers[name]) {
      delete this.config.providers[name];
      this.saveConfig();
      console.log(`âœ… å·²åˆ é™¤ Provider: ${name}`);
    } else {
      console.log(`âš ï¸  Provider ä¸å­˜åœ¨: ${name}`);
    }
  }

  listProviders(): void {
    console.log('ğŸ“‹ å·²é…ç½®çš„ Provider:');
    for (const [name, config] of Object.entries(this.config.providers)) {
      console.log(`\n${name}:`);
      console.log(`  Base URL: ${config.baseUrl}`);
      console.log(`  API: ${config.api}`);
      console.log(`  Models: ${config.models.length}`);
      config.models.forEach(model => {
        console.log(`    - ${model.name} (${model.id})`);
      });
    }
  }

  addModel(provider: string, model: ModelConfig): void {
    if (!this.config.providers[provider]) {
      console.log(`âŒ Provider ä¸å­˜åœ¨: ${provider}`);
      return;
    }

    this.config.providers[provider].models.push(model);
    this.saveConfig();
    console.log(`âœ… å·²æ·»åŠ æ¨¡å‹: ${model.name} åˆ° ${provider}`);
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const manager = new ProviderManager();

// æ·»åŠ  Ollama Provider
manager.addProvider('ollama', {
  baseUrl: 'http://localhost:11434/v1',
  api: 'openai-completions',
  apiKey: 'ollama',
  models: [
    {
      id: 'llama3.1:8b',
      name: 'Llama 3.1 8B (Local)',
      cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 }
    }
  ]
});

// åˆ—å‡ºæ‰€æœ‰ Provider
manager.listProviders();
```

---

## å…­ã€éªŒè¯å’Œæµ‹è¯•

### 6.1 Provider éªŒè¯è„šæœ¬

```bash
#!/bin/bash
# validate-providers.sh - éªŒè¯ Provider é…ç½®

echo "ğŸ” éªŒè¯ Provider é…ç½®..."

# 1. æ£€æŸ¥ models.json
if [ ! -f ~/.pi/agent/models.json ]; then
    echo "âŒ models.json ä¸å­˜åœ¨"
    exit 1
fi

echo "âœ… models.json å­˜åœ¨"

# 2. éªŒè¯ JSON æ ¼å¼
if jq empty ~/.pi/agent/models.json 2>/dev/null; then
    echo "âœ… JSON æ ¼å¼æ­£ç¡®"
else
    echo "âŒ JSON æ ¼å¼é”™è¯¯"
    exit 1
fi

# 3. åˆ—å‡ºæ‰€æœ‰ Provider
echo ""
echo "ğŸ“‹ å·²é…ç½®çš„ Provider:"
jq -r '.providers | keys[]' ~/.pi/agent/models.json

# 4. éªŒè¯æ¯ä¸ª Provider
jq -r '.providers | keys[]' ~/.pi/agent/models.json | while read provider; do
    echo ""
    echo "ğŸ” éªŒè¯ $provider..."

    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    base_url=$(jq -r ".providers.$provider.baseUrl" ~/.pi/agent/models.json)
    api=$(jq -r ".providers.$provider.api" ~/.pi/agent/models.json)
    model_count=$(jq -r ".providers.$provider.models | length" ~/.pi/agent/models.json)

    echo "  Base URL: $base_url"
    echo "  API: $api"
    echo "  Models: $model_count"

    # æµ‹è¯•è¿æ¥
    if curl -s -o /dev/null -w "%{http_code}" "$base_url" | grep -q "200\|404"; then
        echo "  âœ… å¯ä»¥è®¿é—®"
    else
        echo "  âš ï¸  æ— æ³•è®¿é—®"
    fi
done

echo ""
echo "âœ¨ éªŒè¯å®Œæˆ"
```

### 6.2 æ¨¡å‹æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash
# test-all-models.sh - æµ‹è¯•æ‰€æœ‰æ¨¡å‹

echo "ğŸ§ª æµ‹è¯•æ‰€æœ‰æ¨¡å‹..."

# è·å–æ‰€æœ‰ Provider å’Œæ¨¡å‹
jq -r '.providers | to_entries[] | "\(.key):\(.value.models[].id)"' ~/.pi/agent/models.json | while IFS=: read provider model; do
    echo ""
    echo "ğŸ” æµ‹è¯• $provider - $model..."

    # ä½¿ç”¨ Pi æµ‹è¯•
    if pi --provider "$provider" --model "$model" --print "Hi" 2>/dev/null; then
        echo "âœ… $provider - $model æ­£å¸¸"
    else
        echo "âŒ $provider - $model å¤±è´¥"
    fi
done

echo ""
echo "âœ¨ æµ‹è¯•å®Œæˆ"
```

---

## ä¸ƒã€æ•…éšœæ’æŸ¥

### 7.1 Provider è¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# diagnose-provider.sh - Provider æ•…éšœè¯Šæ–­

provider=$1

if [ -z "$provider" ]; then
    echo "ç”¨æ³•: ./diagnose-provider.sh <provider>"
    exit 1
fi

echo "ğŸ”§ è¯Šæ–­ Provider: $provider"
echo ""

# 1. æ£€æŸ¥é…ç½®
echo "1ï¸âƒ£ æ£€æŸ¥é…ç½®:"
if jq -e ".providers.$provider" ~/.pi/agent/models.json > /dev/null 2>&1; then
    echo "âœ… Provider é…ç½®å­˜åœ¨"

    # æ˜¾ç¤ºé…ç½®
    echo ""
    echo "é…ç½®è¯¦æƒ…:"
    jq ".providers.$provider" ~/.pi/agent/models.json
else
    echo "âŒ Provider é…ç½®ä¸å­˜åœ¨"
    exit 1
fi

echo ""

# 2. æ£€æŸ¥ Base URL
echo "2ï¸âƒ£ æ£€æŸ¥ Base URL:"
base_url=$(jq -r ".providers.$provider.baseUrl" ~/.pi/agent/models.json)
echo "Base URL: $base_url"

if curl -s -o /dev/null -w "%{http_code}" "$base_url" | grep -q "200\|404"; then
    echo "âœ… å¯ä»¥è®¿é—®"
else
    echo "âŒ æ— æ³•è®¿é—®"
fi

echo ""

# 3. æ£€æŸ¥ API Key
echo "3ï¸âƒ£ æ£€æŸ¥ API Key:"
api_key_config=$(jq -r ".providers.$provider.apiKey" ~/.pi/agent/models.json)

if [[ $api_key_config == !* ]]; then
    echo "API Key ç±»å‹: Shell å‘½ä»¤"
    echo "å‘½ä»¤: ${api_key_config:1}"
elif [[ $api_key_config =~ ^[A-Z_]+$ ]]; then
    echo "API Key ç±»å‹: ç¯å¢ƒå˜é‡"
    echo "å˜é‡: $api_key_config"
    if [ -n "${!api_key_config}" ]; then
        echo "âœ… ç¯å¢ƒå˜é‡å·²è®¾ç½®"
    else
        echo "âŒ ç¯å¢ƒå˜é‡æœªè®¾ç½®"
    fi
else
    echo "API Key ç±»å‹: å­—é¢å€¼"
    echo "âœ… API Key å·²é…ç½®"
fi

echo ""

# 4. æ£€æŸ¥æ¨¡å‹
echo "4ï¸âƒ£ æ£€æŸ¥æ¨¡å‹:"
model_count=$(jq -r ".providers.$provider.models | length" ~/.pi/agent/models.json)
echo "æ¨¡å‹æ•°é‡: $model_count"

jq -r ".providers.$provider.models[].name" ~/.pi/agent/models.json | while read model_name; do
    echo "  - $model_name"
done

echo ""
echo "âœ¨ è¯Šæ–­å®Œæˆ"
```

---

## å…«ã€æ€»ç»“

### 8.1 é…ç½®æ£€æŸ¥æ¸…å•

- [ ] models.json å·²åˆ›å»º
- [ ] JSON æ ¼å¼æ­£ç¡®
- [ ] æ‰€æœ‰ Provider å·²é…ç½®
- [ ] Base URL å¯è®¿é—®
- [ ] API Key å·²è®¾ç½®
- [ ] æ¨¡å‹åˆ—è¡¨å®Œæ•´
- [ ] é…ç½®å·²æµ‹è¯•éªŒè¯

### 8.2 å¿«é€Ÿå‚è€ƒ

```bash
# åˆ›å»º models.json
mkdir -p ~/.pi/agent
cat > ~/.pi/agent/models.json << 'EOF'
{"providers":{"ollama":{"baseUrl":"http://localhost:11434/v1","api":"openai-completions","apiKey":"ollama","models":[{"id":"llama3.1:8b","name":"Llama 3.1 8B"}]}}}
EOF

# éªŒè¯é…ç½®
jq empty ~/.pi/agent/models.json

# åˆ—å‡º Provider
jq -r '.providers | keys[]' ~/.pi/agent/models.json

# æµ‹è¯• Provider
pi --provider ollama --model llama3.1:8b --print "Hi"
```

---

**å‚è€ƒèµ„æ–™:**
- [Pi Models Documentation](https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/docs/models.md)
- [Pi Custom Provider Documentation](https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/docs/custom-provider.md)

**æ–‡æ¡£ç‰ˆæœ¬:** v1.0 (2026-02-18)
