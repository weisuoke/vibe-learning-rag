# æœ€å°å¯ç”¨çŸ¥è¯†

> **20% æ ¸å¿ƒçŸ¥è¯†è§£å†³ 80% é—®é¢˜**

---

## æ ¸å¿ƒçŸ¥è¯†åœ°å›¾

### 1. RunnableConfig çš„ 3 ä¸ªå¿…çŸ¥å­—æ®µ

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    callbacks=[...],      # â† ç›‘æ§å’Œæ—¥å¿—
    tags=["prod"],        # â† è¿‡æ»¤å›è°ƒ
    metadata={"user": "123"}  # â† è¿½è¸ªä¿¡æ¯
)
```

**è®°ä½è¿™ä¸ª**ï¼š80% çš„åœºæ™¯åªéœ€è¦è¿™ 3 ä¸ªå­—æ®µã€‚

---

### 2. è‡ªå®šä¹‰å›è°ƒçš„ 3 ä¸ªæ ¸å¿ƒæ–¹æ³•

```python
from langchain_core.callbacks import BaseCallbackHandler

class MyCallback(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLM å¼€å§‹è°ƒç”¨æ—¶è§¦å‘"""
        print(f"å¼€å§‹è°ƒç”¨: {prompts}")

    def on_llm_end(self, response, **kwargs):
        """LLM è°ƒç”¨ç»“æŸæ—¶è§¦å‘"""
        tokens = response.llm_output["token_usage"]["total_tokens"]
        print(f"æ¶ˆè€— {tokens} tokens")

    def on_llm_error(self, error, **kwargs):
        """LLM è°ƒç”¨å¤±è´¥æ—¶è§¦å‘"""
        print(f"é”™è¯¯: {error}")
```

**è®°ä½è¿™ä¸ª**ï¼š90% çš„ç›‘æ§éœ€æ±‚åªéœ€è¦è¿™ 3 ä¸ªæ–¹æ³•ã€‚

---

### 3. é…ç½®ä¼ é€’çš„ 2 ç§æ–¹å¼

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

# æ–¹å¼ 1ï¼šæ„é€ æ—¶ä¼ å…¥ï¼ˆé™æ€é…ç½®ï¼‰
llm_with_config = llm.with_config(tags=["prod"])

# æ–¹å¼ 2ï¼šè°ƒç”¨æ—¶ä¼ å…¥ï¼ˆåŠ¨æ€é…ç½®ï¼‰
llm.invoke("ä½ å¥½", config={"tags": ["prod"], "metadata": {"user": "123"}})
```

**è®°ä½è¿™ä¸ª**ï¼š
- é™æ€é…ç½®ç”¨ `with_config()`
- åŠ¨æ€é…ç½®ç”¨ `invoke(..., config=...)`
- åŠ¨æ€é…ç½®ä¼˜å…ˆçº§æ›´é«˜

---

## æœ€å°å¯ç”¨å®æˆ˜

### åœºæ™¯ 1ï¼šæˆæœ¬è¿½è¸ªï¼ˆæœ€å¸¸ç”¨ï¼‰

**éœ€æ±‚**ï¼šè¿½è¸ªæ¯æ¬¡ LLM è°ƒç”¨çš„ token æ¶ˆè€—å’Œæˆæœ¬ã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import BaseCallbackHandler

class CostTracker(BaseCallbackHandler):
    def __init__(self):
        self.total_cost = 0

    def on_llm_end(self, response, **kwargs):
        # è·å– token ä½¿ç”¨é‡
        usage = response.llm_output.get("token_usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        # è®¡ç®—æˆæœ¬ï¼ˆGPT-4 ä»·æ ¼ï¼‰
        cost = (prompt_tokens * 0.00003 + completion_tokens * 0.00006)
        self.total_cost += cost

        print(f"æœ¬æ¬¡æˆæœ¬: ${cost:.4f}")
        print(f"ç´¯è®¡æˆæœ¬: ${self.total_cost:.4f}")

# ä½¿ç”¨
tracker = CostTracker()
llm = ChatOpenAI(model="gpt-4")

response = llm.invoke(
    "è§£é‡Šé‡å­çº ç¼ ",
    config={"callbacks": [tracker]}
)

print(f"\næ€»æˆæœ¬: ${tracker.total_cost:.4f}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æœ¬æ¬¡æˆæœ¬: $0.0049
ç´¯è®¡æˆæœ¬: $0.0049

æ€»æˆæœ¬: $0.0049
```

---

### åœºæ™¯ 2ï¼šç»“æ„åŒ–æ—¥å¿—ï¼ˆç”Ÿäº§å¿…å¤‡ï¼‰

**éœ€æ±‚**ï¼šè®°å½•æ¯æ¬¡è°ƒç”¨çš„è¾“å…¥ã€è¾“å‡ºã€è€—æ—¶ï¼Œä¾¿äºé—®é¢˜å®šä½ã€‚

```python
import time
import json
from langchain_core.callbacks import BaseCallbackHandler

class StructuredLogger(BaseCallbackHandler):
    def __init__(self):
        self.start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()
        metadata = kwargs.get("metadata", {})

        log = {
            "event": "llm_start",
            "timestamp": time.time(),
            "prompts": prompts,
            "metadata": metadata
        }
        print(json.dumps(log, ensure_ascii=False))

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time

        log = {
            "event": "llm_end",
            "timestamp": time.time(),
            "duration_ms": int(duration * 1000),
            "output": response.generations[0][0].text[:100],  # æˆªå–å‰100å­—ç¬¦
            "tokens": response.llm_output.get("token_usage", {})
        }
        print(json.dumps(log, ensure_ascii=False))

# ä½¿ç”¨
logger = StructuredLogger()
llm = ChatOpenAI()

llm.invoke(
    "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    config={
        "callbacks": [logger],
        "metadata": {"user_id": "user_123", "session": "abc"}
    }
)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```json
{"event": "llm_start", "timestamp": 1708329600.123, "prompts": ["ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"], "metadata": {"user_id": "user_123", "session": "abc"}}
{"event": "llm_end", "timestamp": 1708329602.456, "duration_ms": 2333, "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯...", "tokens": {"prompt_tokens": 15, "completion_tokens": 120, "total_tokens": 135}}
```

---

### åœºæ™¯ 3ï¼šLangSmith è‡ªåŠ¨è¿½è¸ªï¼ˆé›¶ä»£ç ï¼‰

**éœ€æ±‚**ï¼šåœ¨ LangSmith å¹³å°æŸ¥çœ‹æ‰€æœ‰è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯ã€‚

```bash
# 1. è®¾ç½®ç¯å¢ƒå˜é‡
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=your_api_key
export LANGCHAIN_PROJECT=my_project
```

```python
from langchain_openai import ChatOpenAI

# 2. æ­£å¸¸ä½¿ç”¨ï¼Œè‡ªåŠ¨è¿½è¸ª
llm = ChatOpenAI()

response = llm.invoke(
    "è§£é‡Šæ·±åº¦å­¦ä¹ ",
    config={
        "tags": ["production", "deep-learning"],
        "metadata": {"user_id": "user_123"}
    }
)

# 3. è®¿é—® https://smith.langchain.com æŸ¥çœ‹è¿½è¸ª
```

**LangSmith ä¼šè‡ªåŠ¨è®°å½•**ï¼š
- è¾“å…¥è¾“å‡º
- Token æ¶ˆè€—
- å»¶è¿Ÿ
- æˆæœ¬
- é”™è¯¯å †æ ˆ

**å¼•ç”¨**ï¼š
> "LangSmith provides automatic tracing for all LangChain applications with zero code changes."
> â€” [LangSmith Observability Platform](https://www.langchain.com/langsmith/observability)

---

### åœºæ™¯ 4ï¼šæŒ‰ç”¨æˆ·å½’å› æˆæœ¬

**éœ€æ±‚**ï¼šç»Ÿè®¡æ¯ä¸ªç”¨æˆ·çš„ token æ¶ˆè€—ï¼Œç”¨äºè®¡è´¹æˆ–é…é¢ç®¡ç†ã€‚

```python
from collections import defaultdict
from langchain_core.callbacks import BaseCallbackHandler

class UserCostTracker(BaseCallbackHandler):
    def __init__(self):
        self.user_costs = defaultdict(float)

    def on_llm_end(self, response, **kwargs):
        # ä» metadata è·å– user_id
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        # è®¡ç®—æˆæœ¬
        usage = response.llm_output.get("token_usage", {})
        tokens = usage.get("total_tokens", 0)
        cost = tokens * 0.00002  # GPT-3.5 ä»·æ ¼

        # å½’å› åˆ°ç”¨æˆ·
        self.user_costs[user_id] += cost

    def get_report(self):
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        for user_id, cost in sorted(self.user_costs.items()):
            print(f"ç”¨æˆ· {user_id}: ${cost:.4f}")

# ä½¿ç”¨
tracker = UserCostTracker()
llm = ChatOpenAI()

# æ¨¡æ‹Ÿå¤šä¸ªç”¨æˆ·çš„è°ƒç”¨
for user_id in ["user_001", "user_002", "user_001"]:
    llm.invoke(
        "ä½ å¥½",
        config={
            "callbacks": [tracker],
            "metadata": {"user_id": user_id}
        }
    )

tracker.get_report()
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
ç”¨æˆ· user_001: $0.0008
ç”¨æˆ· user_002: $0.0004
```

---

### åœºæ™¯ 5ï¼šé”™è¯¯å‘Šè­¦

**éœ€æ±‚**ï¼šLLM è°ƒç”¨å¤±è´¥æ—¶å‘é€å‘Šè­¦é€šçŸ¥ã€‚

```python
from langchain_core.callbacks import BaseCallbackHandler
import requests

class ErrorAlerter(BaseCallbackHandler):
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url

    def on_llm_error(self, error, **kwargs):
        metadata = kwargs.get("metadata", {})

        # å‘é€å‘Šè­¦åˆ° Slack/é’‰é’‰/ä¼ä¸šå¾®ä¿¡
        alert = {
            "text": f"ğŸš¨ LLM è°ƒç”¨å¤±è´¥",
            "error": str(error),
            "metadata": metadata
        }

        try:
            requests.post(self.webhook_url, json=alert)
        except Exception as e:
            print(f"å‘Šè­¦å‘é€å¤±è´¥: {e}")

# ä½¿ç”¨
alerter = ErrorAlerter(webhook_url="https://hooks.slack.com/...")
llm = ChatOpenAI()

try:
    llm.invoke(
        "ä½ å¥½",
        config={
            "callbacks": [alerter],
            "metadata": {"user_id": "user_123", "env": "production"}
        }
    )
except Exception as e:
    print(f"è°ƒç”¨å¤±è´¥: {e}")
```

---

## å¿«é€Ÿå‚è€ƒå¡ç‰‡

### RunnableConfig å­—æ®µé€ŸæŸ¥

| å­—æ®µ | ç±»å‹ | ç”¨é€” | ç¤ºä¾‹ |
|------|------|------|------|
| `callbacks` | `List[BaseCallbackHandler]` | ç›‘æ§ã€æ—¥å¿— | `[CostTracker()]` |
| `tags` | `List[str]` | è¿‡æ»¤å›è°ƒ | `["prod", "critical"]` |
| `metadata` | `Dict[str, Any]` | è¿½è¸ªä¿¡æ¯ | `{"user_id": "123"}` |
| `run_name` | `str` | å¯è¯»æ ‡è¯† | `"user_query_123"` |
| `max_concurrency` | `int` | å¹¶å‘æ§åˆ¶ | `10` |
| `recursion_limit` | `int` | é€’å½’é™åˆ¶ | `25` |

---

### BaseCallbackHandler æ–¹æ³•é€ŸæŸ¥

| æ–¹æ³• | è§¦å‘æ—¶æœº | å¸¸ç”¨åœºæ™¯ |
|------|----------|----------|
| `on_llm_start` | LLM å¼€å§‹è°ƒç”¨ | è®°å½•è¾“å…¥ |
| `on_llm_end` | LLM è°ƒç”¨ç»“æŸ | æˆæœ¬è¿½è¸ª |
| `on_llm_error` | LLM è°ƒç”¨å¤±è´¥ | é”™è¯¯å‘Šè­¦ |
| `on_chain_start` | Chain å¼€å§‹æ‰§è¡Œ | é“¾è·¯è¿½è¸ª |
| `on_chain_end` | Chain æ‰§è¡Œç»“æŸ | æ€§èƒ½ç›‘æ§ |
| `on_retriever_end` | æ£€ç´¢å®Œæˆ | æ£€ç´¢è´¨é‡ç›‘æ§ |

**å®Œæ•´æ–¹æ³•åˆ—è¡¨**ï¼š[BaseCallbackHandler API](https://reference.langchain.com/v0.3/python/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html)

---

### é…ç½®ä¼ é€’ä¼˜å…ˆçº§

```python
# ä¼˜å…ˆçº§ï¼šè°ƒç”¨æ—¶ > with_config > æ„é€ æ—¶
llm = ChatOpenAI(tags=["default"])           # ä¼˜å…ˆçº§ 3
llm_with_config = llm.with_config(tags=["prod"])  # ä¼˜å…ˆçº§ 2
llm_with_config.invoke("ä½ å¥½", config={"tags": ["critical"]})  # ä¼˜å…ˆçº§ 1ï¼ˆæœ€é«˜ï¼‰
```

---

## å¸¸è§é”™è¯¯ä¸è§£å†³

### é”™è¯¯ 1ï¼šå›è°ƒä¸­è®¿é—®ä¸åˆ° metadata

```python
# âŒ é”™è¯¯ï¼šç›´æ¥è®¿é—® kwargs
def on_llm_end(self, response, **kwargs):
    user_id = kwargs["metadata"]["user_id"]  # KeyError!

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ get() æ–¹æ³•
def on_llm_end(self, response, **kwargs):
    metadata = kwargs.get("metadata", {})
    user_id = metadata.get("user_id", "unknown")
```

---

### é”™è¯¯ 2ï¼šmetadata ä¸å¯åºåˆ—åŒ–

```python
# âŒ é”™è¯¯ï¼šä¼ å…¥å¯¹è±¡å®ä¾‹
config = {
    "metadata": {"user": User(id=123)}  # æ— æ³•åºåˆ—åŒ–ï¼
}

# âœ… æ­£ç¡®ï¼šåªä¼ å…¥åŸºæœ¬ç±»å‹
config = {
    "metadata": {"user_id": 123, "user_name": "Alice"}
}
```

---

### é”™è¯¯ 3ï¼šå›è°ƒä¸­æŠ›å‡ºå¼‚å¸¸å¯¼è‡´ä¸»æµç¨‹å¤±è´¥

```python
# âŒ é”™è¯¯ï¼šå›è°ƒå¼‚å¸¸æœªæ•è·
class MyCallback(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        result = response.llm_output["nonexistent_key"]  # KeyError!

# âœ… æ­£ç¡®ï¼šæ•è·å¼‚å¸¸
class MyCallback(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        try:
            result = response.llm_output.get("key", "default")
        except Exception as e:
            print(f"å›è°ƒé”™è¯¯: {e}")
```

---

## è¿›é˜¶æ–¹å‘æŒ‡å¼•

æŒæ¡ä»¥ä¸Šå†…å®¹åï¼Œå¯ä»¥æ¢ç´¢ï¼š

1. **å¤šå›è°ƒç»„åˆ**ï¼šåŒæ—¶ä½¿ç”¨æˆæœ¬è¿½è¸ªã€æ—¥å¿—è®°å½•ã€é”™è¯¯å‘Šè­¦
2. **å¼‚æ­¥å›è°ƒ**ï¼šä½¿ç”¨ `AsyncCallbackHandler` å¤„ç†é«˜å¹¶å‘åœºæ™¯
3. **å›è°ƒè¿‡æ»¤**ï¼šä½¿ç”¨ `tags` å’Œ `ignore_*` å‚æ•°ç²¾ç¡®æ§åˆ¶å›è°ƒæ‰§è¡Œ
4. **å¯é…ç½®å­—æ®µ**ï¼šä½¿ç”¨ `configurable_fields()` å®ç°åŠ¨æ€å‚æ•°è°ƒæ•´
5. **LangSmith é«˜çº§åŠŸèƒ½**ï¼šå®æ—¶å‘Šè­¦ã€å¤šè½®è¯„ä¼°ã€æˆæœ¬ä¼˜åŒ–å»ºè®®

---

## ä¸€å¥è¯æ€»ç»“

**æŒæ¡ RunnableConfig çš„ 3 ä¸ªå­—æ®µï¼ˆcallbacksã€tagsã€metadataï¼‰å’Œ BaseCallbackHandler çš„ 3 ä¸ªæ–¹æ³•ï¼ˆon_llm_start/end/errorï¼‰ï¼Œå°±èƒ½è§£å†³ 80% çš„ç”Ÿäº§ç›‘æ§éœ€æ±‚ã€‚**

---

## å‚è€ƒèµ„æ–™

- [RunnableConfig API Reference](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.config.RunnableConfig.html)
- [BaseCallbackHandler API Reference](https://reference.langchain.com/v0.3/python/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html)
- [Custom Callbacks Guide](https://python.langchain.com/docs/how_to/custom_callbacks/)
- [LangSmith Observability Platform](https://www.langchain.com/langsmith/observability)
