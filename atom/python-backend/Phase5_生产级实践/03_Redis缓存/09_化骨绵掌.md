# 化骨绵掌：10个2分钟知识卡片

## 卡片1：Redis缓存的直觉理解

**一句话：** Redis缓存就像办公桌上的便签纸，记录常用信息避免重复查找

**举例：**
```python
# 没有缓存：每次都查字典
def get_word_meaning(word: str):
    return dictionary.lookup(word)  # 每次都翻字典，慢

# 有缓存：查过的记在便签上
cache = {}
def get_word_meaning_cached(word: str):
    if word in cache:
        return cache[word]  # 直接看便签，快
    meaning = dictionary.lookup(word)
    cache[word] = meaning
    return meaning
```

**应用：** 在AI Agent中，缓存LLM响应就像记录常见问题的答案，避免重复调用API

---

## 卡片2：Redis的核心数据结构

**一句话：** Redis支持5种数据结构，每种适合不同场景

**5种数据结构：**

| 类型 | 特点 | 使用场景 | 时间复杂度 |
|------|------|---------|-----------|
| String | 简单键值对 | 缓存LLM响应 | O(1) |
| Hash | 对象的多个字段 | 缓存用户信息 | O(1) |
| List | 有序列表 | 消息队列 | O(1) |
| Set | 无序不重复集合 | 标签系统 | O(1) |
| Sorted Set | 有序集合 | 排行榜 | O(log N) |

**举例：**
```python
# String：缓存LLM响应
redis_client.setex("llm:abc123", 3600, "Paris")

# Hash：缓存用户对象
redis_client.hset("user:1001", mapping={
    "name": "John",
    "email": "john@example.com"
})

# Sorted Set：排行榜
redis_client.zadd("leaderboard", {"user1": 100, "user2": 95})
```

**应用：** 根据数据特性选择合适的数据结构，提升性能和内存效率

---

## 卡片3：TTL过期机制

**一句话：** TTL（Time To Live）让缓存自动过期，平衡内存和新鲜度

**三种过期策略：**

1. **定时删除**：设置定时器，到期立即删除（CPU消耗高）
2. **惰性删除**：访问时检查是否过期（内存占用高）
3. **定期删除**：定期随机检查部分key（Redis默认）

**举例：**
```python
# 设置TTL
redis_client.setex("key", 3600, "value")  # 1小时后过期

# 查看剩余时间
ttl = redis_client.ttl("key")
print(f"剩余{ttl}秒")

# 延长TTL
redis_client.expire("key", 7200)  # 延长到2小时
```

**应用：** 根据数据更新频率设置合理的TTL，避免缓存过期和内存浪费

---

## 卡片4：连接池原理

**一句话：** 连接池复用Redis连接，避免频繁建立/销毁连接

**原理：**
```python
# 没有连接池：每次都建立新连接
def get_data(key):
    client = redis.Redis()  # 建立连接（慢）
    data = client.get(key)
    client.close()          # 关闭连接
    return data

# 有连接池：复用连接
pool = redis.ConnectionPool(max_connections=10)

def get_data_pooled(key):
    client = redis.Redis(connection_pool=pool)  # 从池中获取（快）
    data = client.get(key)
    # 连接自动归还给池
    return data
```

**性能对比：**
- 建立连接：10ms
- 从连接池获取：0.1ms
- 性能提升：100倍

**应用：** FastAPI应用中使用连接池，提升并发性能

---

## 卡片5：精确缓存实现

**一句话：** 使用prompt的hash作为key，缓存LLM的完整响应

**实现：**
```python
import hashlib
import json

def cache_llm_response(prompt: str, response: str, ttl: int = 3600):
    """缓存LLM响应"""
    # 生成缓存key
    cache_key = f"llm:{hashlib.md5(prompt.encode()).hexdigest()}"
    redis_client.setex(cache_key, ttl, response)

def get_cached_response(prompt: str) -> Optional[str]:
    """获取缓存的LLM响应"""
    cache_key = f"llm:{hashlib.md5(prompt.encode()).hexdigest()}"
    return redis_client.get(cache_key)

# 使用
prompt = "What is Python?"
cached = get_cached_response(prompt)
if not cached:
    cached = call_llm_api(prompt)
    cache_llm_response(prompt, cached)
```

**优点：**
- 实现简单
- 查询快速（O(1)）
- 100%准确

**缺点：**
- 命中率低（表达方式稍有不同就无法命中）

**应用：** 适合FAQ场景，问题固定

---

## 卡片6：语义缓存实现

**一句话：** 使用Embedding计算相似度，匹配语义相同的问题

**实现：**
```python
import numpy as np

def get_embedding(text: str) -> list[float]:
    """获取文本的Embedding"""
    # 先查Embedding缓存
    cache_key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)

    # 调用Embedding API
    embedding = openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    ).data[0].embedding

    # 缓存Embedding
    redis_client.setex(cache_key, 86400, json.dumps(embedding))
    return embedding

def cosine_similarity(a: list[float], b: list[float]) -> float:
    """计算余弦相似度"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def semantic_cache_lookup(prompt: str, threshold: float = 0.9):
    """语义缓存查询"""
    query_embedding = get_embedding(prompt)

    # 获取所有缓存
    cached_items = redis_client.hgetall("semantic_cache")

    # 找最相似的
    best_match = None
    best_score = 0.0

    for cache_key, cache_data in cached_items.items():
        data = json.loads(cache_data)
        similarity = cosine_similarity(query_embedding, data["embedding"])
        if similarity > best_score:
            best_score = similarity
            best_match = data

    # 判断是否命中
    if best_score >= threshold:
        return best_match["response"]

    return None
```

**优点：**
- 命中率高（可以匹配同义问题）
- 用户体验好

**缺点：**
- 实现复杂
- 查询较慢（需要计算相似度）

**应用：** 适合开放式问答，用户表达方式多样

---

## 卡片7：缓存穿透防护

**一句话：** 使用布隆过滤器和空值缓存，防止查询不存在的数据

**问题场景：**
```python
# 恶意用户不断查询不存在的用户ID
for i in range(10000):
    get_user(random.randint(100000, 999999))  # 每次都查数据库
```

**防护方案：**
```python
class BloomFilter:
    """简单的布隆过滤器"""
    def __init__(self):
        self.bits = set()

    def add(self, item: str):
        for i in range(3):  # 3个hash函数
            hash_val = hash(item + str(i)) % 10000
            self.bits.add(hash_val)

    def might_exist(self, item: str) -> bool:
        for i in range(3):
            hash_val = hash(item + str(i)) % 10000
            if hash_val not in self.bits:
                return False
        return True

bloom = BloomFilter()

def get_user_protected(user_id: int):
    # 1. 布隆过滤器快速判断
    if not bloom.might_exist(f"user:{user_id}"):
        return None  # 不存在，直接返回

    # 2. 查询缓存
    cached = redis_client.get(f"user:{user_id}")
    if cached == "NULL":
        return None  # 空值缓存
    if cached:
        return json.loads(cached)

    # 3. 查询数据库
    user = db.query(User).filter_by(id=user_id).first()
    if user:
        redis_client.setex(f"user:{user_id}", 3600, json.dumps(user))
    else:
        redis_client.setex(f"user:{user_id}", 60, "NULL")  # 空值缓存

    return user
```

**应用：** 保护数据库免受恶意查询攻击

---

## 卡片8：缓存击穿防护

**一句话：** 使用互斥锁，防止热点数据过期时大量请求同时打到数据库

**问题场景：**
```python
# 热门问题的缓存过期，1000个请求同时查询数据库
for _ in range(1000):
    get_popular_question()  # 缓存过期，都查数据库
```

**防护方案：**
```python
import asyncio

locks = {}  # 全局锁字典

async def get_with_mutex(key: str, fetch_func):
    """带互斥锁的缓存获取"""
    # 1. 查询缓存
    cached = redis_client.get(key)
    if cached:
        return cached

    # 2. 获取互斥锁
    if key not in locks:
        locks[key] = asyncio.Lock()

    async with locks[key]:
        # 3. 双重检查（可能其他请求已加载）
        cached = redis_client.get(key)
        if cached:
            return cached

        # 4. 查询数据库（只有一个请求执行）
        data = await fetch_func()
        redis_client.setex(key, 3600, data)
        return data

# 使用
async def get_popular_question():
    return await get_with_mutex(
        "question:popular",
        lambda: db.query(Question).first()
    )
```

**效果：** 1000个并发请求，只有1个查询数据库

**应用：** 保护数据库免受突发流量冲击

---

## 卡片9：缓存雪崩防护

**一句话：** 使用随机TTL，防止大量缓存同时过期

**问题场景：**
```python
# 凌晨2点批量导入数据，所有缓存TTL=1小时
for item in items:
    redis_client.setex(f"item:{item.id}", 3600, json.dumps(item))

# 凌晨3点，所有缓存同时过期，数据库压力激增
```

**防护方案：**
```python
import random

def set_cache_with_random_ttl(key: str, value: str, base_ttl: int):
    """设置随机TTL"""
    # 在base_ttl基础上±10%随机
    random_ttl = base_ttl + random.randint(-base_ttl//10, base_ttl//10)
    redis_client.setex(key, random_ttl, value)

# 使用
for item in items:
    set_cache_with_random_ttl(
        f"item:{item.id}",
        json.dumps(item),
        3600  # 基础TTL：1小时
    )
    # 实际TTL：3240-3960秒（54-66分钟）
```

**效果：** 缓存过期时间分散，避免同时失效

**应用：** 批量导入数据时使用随机TTL

---

## 卡片10：生产级缓存管理器

**一句话：** 封装缓存逻辑，提供统一的缓存接口

**完整实现：**
```python
from typing import Optional, Callable
import asyncio
import hashlib
import json
import random

class CacheManager:
    """生产级缓存管理器"""

    def __init__(self, redis_client):
        self.redis = redis_client
        self.locks = {}
        self.bloom_filter = BloomFilter()

    async def get_or_set(
        self,
        key: str,
        fetch_func: Callable,
        ttl: int = 3600,
        use_mutex: bool = True,
        use_random_ttl: bool = True
    ) -> Optional[str]:
        """
        获取或设置缓存

        参数：
        - key: 缓存key
        - fetch_func: 数据获取函数
        - ttl: 过期时间（秒）
        - use_mutex: 是否使用互斥锁（防击穿）
        - use_random_ttl: 是否使用随机TTL（防雪崩）
        """
        # 1. 查询缓存
        cached = self.redis.get(key)
        if cached:
            if cached == "NULL":
                return None
            return cached

        # 2. 使用互斥锁
        if use_mutex:
            if key not in self.locks:
                self.locks[key] = asyncio.Lock()

            async with self.locks[key]:
                # 双重检查
                cached = self.redis.get(key)
                if cached:
                    return cached if cached != "NULL" else None

                # 获取数据
                data = await fetch_func()

                # 设置缓存
                if data:
                    final_ttl = self._calculate_ttl(ttl, use_random_ttl)
                    self.redis.setex(key, final_ttl, data)
                else:
                    self.redis.setex(key, 60, "NULL")  # 空值缓存

                return data
        else:
            # 不使用互斥锁
            data = await fetch_func()
            if data:
                final_ttl = self._calculate_ttl(ttl, use_random_ttl)
                self.redis.setex(key, final_ttl, data)
            else:
                self.redis.setex(key, 60, "NULL")
            return data

    def _calculate_ttl(self, base_ttl: int, use_random: bool) -> int:
        """计算最终TTL"""
        if use_random:
            return base_ttl + random.randint(-base_ttl//10, base_ttl//10)
        return base_ttl

    async def cache_llm_response(
        self,
        prompt: str,
        llm_func: Callable,
        ttl: int = 3600
    ) -> str:
        """缓存LLM响应"""
        cache_key = f"llm:{hashlib.md5(prompt.encode()).hexdigest()}"
        return await self.get_or_set(cache_key, llm_func, ttl)

    async def cache_embedding(
        self,
        text: str,
        embedding_func: Callable,
        ttl: int = 86400
    ) -> list[float]:
        """缓存Embedding"""
        cache_key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"
        cached = await self.get_or_set(cache_key, embedding_func, ttl)
        return json.loads(cached) if cached else None

# 使用示例
cache_manager = CacheManager(redis_client)

# 缓存LLM响应
async def call_llm():
    return await openai_client.chat.completions.create(...)

response = await cache_manager.cache_llm_response(
    "What is Python?",
    call_llm
)

# 缓存Embedding
async def get_embedding():
    return await openai_client.embeddings.create(...)

embedding = await cache_manager.cache_embedding(
    "Python is a programming language",
    get_embedding
)
```

**应用：** 在FastAPI项目中使用统一的缓存管理器，简化缓存逻辑

---

## 学习检查清单

完成这10个卡片后，你应该能够：

- [ ] 理解Redis缓存的核心原理（内存存储 + TTL过期）
- [ ] 选择合适的数据结构（String、Hash、Set、Sorted Set、List）
- [ ] 设置合理的TTL策略（根据数据更新频率）
- [ ] 使用连接池管理Redis连接
- [ ] 实现精确缓存（prompt hash）
- [ ] 实现语义缓存（Embedding + 相似度）
- [ ] 防护缓存穿透（布隆过滤器 + 空值缓存）
- [ ] 防护缓存击穿（互斥锁）
- [ ] 防护缓存雪崩（随机TTL）
- [ ] 封装生产级缓存管理器

---

## 下一步学习建议

1. **实践项目**：在AI Agent项目中实现三层缓存（精确 + 语义 + Embedding）
2. **性能测试**：对比有缓存和无缓存的性能差异
3. **监控告警**：实现缓存命中率监控和告警
4. **进阶学习**：
   - Redis集群和分片
   - Redis持久化（RDB、AOF）
   - Redis Sentinel高可用
   - Redis Stream消息队列

---

**记住：** 这10个卡片涵盖了Redis缓存在AI Agent开发中的核心知识，每个卡片都是独立完整的，可以随时回顾！
