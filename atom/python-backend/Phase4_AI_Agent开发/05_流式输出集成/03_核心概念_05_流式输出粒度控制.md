# 核心概念05: 流式输出粒度控制

> 理解 Token 流式、Chunk 流式、完整响应流式的区别和选择

---

## 概述

**流式输出粒度** 指的是每次发送数据的大小单位。不同的粒度适合不同的场景,需要根据实际需求选择。

**本节目标:**
- 理解三种流式输出粒度
- 掌握粒度选择的决策标准
- 学习如何实现不同粒度的流式输出
- 优化流式输出的性能

---

## 1. 三种流式输出粒度

### 1.1 Token 流式 (最细粒度)

**定义:** 逐 Token 发送,每生成一个 Token 立即发送

```python
"""
Token 流式输出
"""

from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

async for chunk in llm.astream("讲个笑话"):
    print(chunk.content, end="")  # 为, 什, 么, 程, 序, 员, ...
```

**特点:**
- ✅ 实时性最好 (立即看到第一个字)
- ✅ 用户体验最好 (打字机效果)
- ❌ 网络开销大 (每个 Token 一次传输)
- ❌ 前端渲染压力大 (频繁更新 DOM)

**适用场景:**
- 聊天机器人
- 对话式 AI 助手
- 需要打字机效果的场景

### 1.2 Chunk 流式 (中等粒度)

**定义:** 逐 Chunk 发送,每生成一个 Chunk (句子/段落) 发送一次

```python
"""
Chunk 流式输出
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("详细解释{topic}")
llm = ChatOpenAI()
chain = prompt | llm

async for chunk in chain.astream({"topic": "AI"}):
    print(chunk)  # AI 是人工智能\n它可以...\n应用包括...
```

**特点:**
- ✅ 实时性好 (几秒内看到第一段)
- ✅ 网络开销适中
- ✅ 前端渲染压力小
- ❌ 实时性不如 Token 流式

**适用场景:**
- RAG 问答 (先显示检索结果,再显示生成内容)
- 长文本生成 (逐段显示)
- 需要显示中间步骤的场景

### 1.3 完整响应流式 (最粗粒度)

**定义:** 分段发送完整响应,每完成一个阶段发送一次

```python
"""
完整响应流式输出
"""

async def generate():
    # 阶段1: 检索
    yield f"data: {json.dumps({'stage': 'retrieval', 'status': 'started'})}\n\n"
    docs = await retriever.ainvoke(question)
    yield f"data: {json.dumps({'stage': 'retrieval', 'docs': docs})}\n\n"

    # 阶段2: 生成
    yield f"data: {json.dumps({'stage': 'generation', 'status': 'started'})}\n\n"
    response = await llm.ainvoke(question)
    yield f"data: {json.dumps({'stage': 'generation', 'response': response})}\n\n"
```

**特点:**
- ✅ 网络开销最小
- ✅ 前端渲染压力最小
- ✅ 逻辑清晰 (按阶段划分)
- ❌ 实时性最差

**适用场景:**
- 多阶段处理 (检索 → 生成 → 后处理)
- 需要显示进度的场景
- 网络带宽有限的场景

---

## 2. 粒度对比

### 2.1 性能对比

| 维度 | Token 流式 | Chunk 流式 | 完整响应流式 |
|------|-----------|-----------|-------------|
| **实时性** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ |
| **网络开销** | ⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **前端渲染** | ⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **用户体验** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ |
| **实现复杂度** | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ |

### 2.2 数据量对比

假设生成 1000 字的文章:

**Token 流式:**
- Token 数量: ~1500 个
- 网络请求: 1500 次
- 总数据量: ~150KB (包含 SSE 格式开销)

**Chunk 流式:**
- Chunk 数量: ~10 个 (每段 100 字)
- 网络请求: 10 次
- 总数据量: ~15KB

**完整响应流式:**
- 阶段数量: 3 个 (检索 → 生成 → 完成)
- 网络请求: 3 次
- 总数据量: ~5KB

### 2.3 用户感知时间对比

假设生成 1000 字需要 20 秒:

**Token 流式:**
- 首字节时间: 0.5 秒
- 用户感知等待: 0.5 秒 ⭐⭐⭐⭐⭐

**Chunk 流式:**
- 首字节时间: 2 秒
- 用户感知等待: 2 秒 ⭐⭐⭐⭐

**完整响应流式:**
- 首字节时间: 10 秒
- 用户感知等待: 10 秒 ⭐⭐

---

## 3. 粒度选择决策树

```
生成内容 < 50 字?
    ├─ 是 → 不用流式输出 (直接返回)
    └─ 否 → 继续判断
        ↓
需要打字机效果?
    ├─ 是 → Token 流式 ⭐⭐⭐⭐⭐
    └─ 否 → 继续判断
        ↓
需要显示中间步骤?
    ├─ 是 → Chunk 流式 或 完整响应流式 ⭐⭐⭐⭐
    └─ 否 → 继续判断
        ↓
网络带宽有限?
    ├─ 是 → 完整响应流式 ⭐⭐⭐
    └─ 否 → Token 流式 或 Chunk 流式 ⭐⭐⭐⭐
```

---

## 4. 实现不同粒度的流式输出

### 4.1 Token 流式实现

```python
"""
Token 流式输出完整实现
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain_openai import ChatOpenAI
import json

app = FastAPI()
llm = ChatOpenAI(model="gpt-3.5-turbo")

@app.post("/token-stream")
async def token_stream(message: str):
    """Token 级流式输出"""
    async def generate():
        try:
            # 发送开始事件
            yield f"event: start\ndata: {json.dumps({'status': 'started'})}\n\n"

            # Token 流式生成
            token_count = 0
            async for chunk in llm.astream(message):
                if chunk.content:
                    token_count += 1
                    # 发送 Token
                    yield f"data: {chunk.content}\n\n"

            # 发送完成事件
            yield f"event: done\ndata: {json.dumps({'token_count': token_count})}\n\n"

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

### 4.2 Chunk 流式实现

```python
"""
Chunk 流式输出完整实现
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import json

app = FastAPI()

@app.post("/chunk-stream")
async def chunk_stream(topic: str):
    """Chunk 级流式输出"""
    async def generate():
        try:
            # 构建链
            prompt = ChatPromptTemplate.from_template(
                "详细解释{topic},分3段讲解:定义、原理、应用"
            )
            llm = ChatOpenAI(model="gpt-3.5-turbo")
            chain = prompt | llm

            # 发送开始事件
            yield f"event: start\ndata: {json.dumps({'topic': topic})}\n\n"

            # Chunk 流式生成
            chunk_count = 0
            buffer = ""

            async for chunk in chain.astream({"topic": topic}):
                buffer += chunk.content

                # 检测到段落结束 (双换行)
                if "\n\n" in buffer:
                    parts = buffer.split("\n\n")
                    # 发送完整段落
                    for part in parts[:-1]:
                        chunk_count += 1
                        yield f"event: chunk\ndata: {json.dumps({'chunk_id': chunk_count, 'content': part})}\n\n"
                    # 保留未完成的部分
                    buffer = parts[-1]

            # 发送剩余内容
            if buffer:
                chunk_count += 1
                yield f"event: chunk\ndata: {json.dumps({'chunk_id': chunk_count, 'content': buffer})}\n\n"

            # 发送完成事件
            yield f"event: done\ndata: {json.dumps({'chunk_count': chunk_count})}\n\n"

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

### 4.3 完整响应流式实现

```python
"""
完整响应流式输出实现
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import json

app = FastAPI()

# 初始化组件
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)
retriever = vectorstore.as_retriever()
llm = ChatOpenAI(model="gpt-3.5-turbo")

@app.post("/stage-stream")
async def stage_stream(question: str):
    """阶段式流式输出"""
    async def generate():
        try:
            # 阶段1: 检索
            yield f"event: stage\ndata: {json.dumps({'stage': 'retrieval', 'status': 'started'})}\n\n"

            docs = await retriever.ainvoke(question)
            docs_data = [{"content": doc.page_content} for doc in docs]

            yield f"event: stage\ndata: {json.dumps({'stage': 'retrieval', 'status': 'completed', 'docs': docs_data})}\n\n"

            # 阶段2: 生成
            yield f"event: stage\ndata: {json.dumps({'stage': 'generation', 'status': 'started'})}\n\n"

            context = "\n\n".join([doc.page_content for doc in docs])
            prompt = f"根据以下文档回答问题:\n\n{context}\n\n问题: {question}"

            response = await llm.ainvoke(prompt)

            yield f"event: stage\ndata: {json.dumps({'stage': 'generation', 'status': 'completed', 'response': response.content})}\n\n"

            # 阶段3: 完成
            yield f"event: done\ndata: {json.dumps({'status': 'completed'})}\n\n"

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## 5. 混合粒度策略

### 5.1 什么是混合粒度?

**混合粒度** = 在不同阶段使用不同的粒度

```python
"""
混合粒度流式输出
"""

@app.post("/hybrid-stream")
async def hybrid_stream(question: str):
    """混合粒度流式输出"""
    async def generate():
        try:
            # 阶段1: 检索 (完整响应流式)
            yield f"event: retrieval_start\ndata: Retrieving...\n\n"
            docs = await retriever.ainvoke(question)
            yield f"event: retrieval_done\ndata: {json.dumps({'docs': docs})}\n\n"

            # 阶段2: 生成 (Token 流式)
            yield f"event: generation_start\ndata: Generating...\n\n"
            async for chunk in llm.astream(question):
                if chunk.content:
                    yield f"data: {chunk.content}\n\n"

            yield f"event: done\ndata: Completed\n\n"

        except Exception as e:
            yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

**优势:**
- 检索阶段用完整响应流式 (快速显示结果)
- 生成阶段用 Token 流式 (打字机效果)
- 兼顾性能和用户体验

### 5.2 混合粒度的应用场景

**场景1: RAG 问答**
- 检索: 完整响应流式
- 生成: Token 流式

**场景2: Agent 执行**
- 思考: 完整响应流式
- Tool 调用: 完整响应流式
- 最终答案: Token 流式

**场景3: 长文本生成**
- 大纲: 完整响应流式
- 内容: Chunk 流式

---

## 6. 性能优化

### 6.1 缓冲区优化

```python
"""
使用缓冲区减少网络开销
"""

@app.post("/buffered-stream")
async def buffered_stream(message: str):
    """带缓冲区的流式输出"""
    async def generate():
        buffer = []
        buffer_size = 5  # 每5个 Token 发送一次

        async for chunk in llm.astream(message):
            if chunk.content:
                buffer.append(chunk.content)

                # 缓冲区满了,批量发送
                if len(buffer) >= buffer_size:
                    yield f"data: {''.join(buffer)}\n\n"
                    buffer.clear()

        # 发送剩余数据
        if buffer:
            yield f"data: {''.join(buffer)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

**效果:**
- Token 流式: 1500 次网络请求
- 缓冲区优化: 300 次网络请求 (减少 80%)

### 6.2 自适应粒度

```python
"""
根据网络状况自适应调整粒度
"""

@app.post("/adaptive-stream")
async def adaptive_stream(message: str, network_speed: str = "fast"):
    """自适应粒度流式输出"""
    async def generate():
        # 根据网络速度选择缓冲区大小
        buffer_size = {
            "fast": 1,    # 快速网络: Token 流式
            "medium": 5,  # 中速网络: 小缓冲区
            "slow": 20    # 慢速网络: 大缓冲区
        }.get(network_speed, 5)

        buffer = []

        async for chunk in llm.astream(message):
            if chunk.content:
                buffer.append(chunk.content)

                if len(buffer) >= buffer_size:
                    yield f"data: {''.join(buffer)}\n\n"
                    buffer.clear()

        if buffer:
            yield f"data: {''.join(buffer)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## 7. 前端接收不同粒度

### 7.1 Token 流式接收

```javascript
/**
 * Token 流式接收
 */

const eventSource = new EventSource('/token-stream?message=讲个笑话');
let currentMessage = '';

eventSource.onmessage = (event) => {
    // 逐 Token 累加
    currentMessage += event.data;
    // 实时更新 UI
    document.getElementById('message').textContent = currentMessage;
};

eventSource.addEventListener('done', () => {
    console.log('完成');
    eventSource.close();
});
```

### 7.2 Chunk 流式接收

```javascript
/**
 * Chunk 流式接收
 */

const eventSource = new EventSource('/chunk-stream?topic=AI');
const chunks = [];

eventSource.addEventListener('chunk', (event) => {
    const data = JSON.parse(event.data);
    chunks.push(data.content);

    // 显示新段落
    const paragraph = document.createElement('p');
    paragraph.textContent = data.content;
    document.getElementById('content').appendChild(paragraph);
});

eventSource.addEventListener('done', () => {
    console.log(`共 ${chunks.length} 个段落`);
    eventSource.close();
});
```

### 7.3 完整响应流式接收

```javascript
/**
 * 完整响应流式接收
 */

const eventSource = new EventSource('/stage-stream?question=什么是AI');

eventSource.addEventListener('stage', (event) => {
    const data = JSON.parse(event.data);

    if (data.stage === 'retrieval' && data.status === 'completed') {
        // 显示检索结果
        console.log('检索到文档:', data.docs);
        displayDocs(data.docs);
    }

    if (data.stage === 'generation' && data.status === 'completed') {
        // 显示生成内容
        console.log('生成答案:', data.response);
        displayResponse(data.response);
    }
});

eventSource.addEventListener('done', () => {
    console.log('完成');
    eventSource.close();
});
```

---

## 8. 实战建议

### 8.1 选择标准

**Token 流式:**
- ✅ 聊天机器人
- ✅ 对话式 AI 助手
- ✅ 需要打字机效果
- ❌ 网络带宽有限
- ❌ 前端性能有限

**Chunk 流式:**
- ✅ RAG 问答
- ✅ 长文本生成
- ✅ 需要显示中间步骤
- ✅ 网络带宽适中
- ✅ 前端性能适中

**完整响应流式:**
- ✅ 多阶段处理
- ✅ 需要显示进度
- ✅ 网络带宽有限
- ❌ 需要实时反馈

### 8.2 优化建议

1. **使用缓冲区** - 减少网络开销
2. **自适应粒度** - 根据网络状况调整
3. **混合粒度** - 不同阶段用不同粒度
4. **监控性能** - 记录网络请求次数和延迟
5. **A/B 测试** - 测试不同粒度的用户体验

---

## 总结

**流式输出粒度的核心要点:**

1. **三种粒度**: Token、Chunk、完整响应
2. **选择标准**: 根据场景、网络、性能选择
3. **混合策略**: 不同阶段用不同粒度
4. **性能优化**: 缓冲区、自适应粒度

**决策建议:**
- 聊天机器人: Token 流式
- RAG 问答: Chunk 流式 或 混合粒度
- 多阶段处理: 完整响应流式
- 网络有限: 完整响应流式 或 大缓冲区

**下一步:**

理解了流式输出粒度后,可以学习:
- 错误处理与重连
- 生产环境优化
- 实战代码示例

---

**记住:** 流式输出粒度没有绝对的好坏,只有适合不适合。根据实际场景选择合适的粒度,才能达到最佳效果。
