# 核心概念_03_批量优化策略

> 性能优化和成本优化的高级策略

---

## 优化策略概述

批量优化包括两个维度：
1. **性能优化**：提升吞吐量，降低延迟
2. **成本优化**：降低 API 调用成本

---

## 性能优化策略

### 1. 批处理分片

**问题**：大批量任务（10000+）一次性处理容易失败。

**解决方案**：
```python
def chunked_batch(chain, inputs, chunk_size=100, max_concurrency=10):
    """分片批处理"""
    all_results = []
    total_chunks = (len(inputs) + chunk_size - 1) // chunk_size

    for chunk_idx in range(total_chunks):
        start = chunk_idx * chunk_size
        end = min(start + chunk_size, len(inputs))
        chunk = inputs[start:end]

        print(f"处理分片 {chunk_idx + 1}/{total_chunks}")

        chunk_results = chain.batch(
            chunk,
            config={"max_concurrency": max_concurrency}
        )
        all_results.extend(chunk_results)

        # 分片间休息，避免限流
        if chunk_idx < total_chunks - 1:
            time.sleep(1)

    return all_results
```

**优势**：
- 降低失败风险
- 提供进度反馈
- 避免 API 限流

---

### 2. 缓存策略

**问题**：批处理中有重复的输入，浪费 API 调用。

**解决方案**：
```python
import hashlib
import json

class CachedBatchProcessor:
    def __init__(self, chain, cache_size=1000):
        self.chain = chain
        self.cache = {}
        self.cache_size = cache_size

    def _hash_input(self, inp):
        """生成输入的哈希值"""
        inp_str = json.dumps(inp, sort_keys=True)
        return hashlib.md5(inp_str.encode()).hexdigest()

    def batch(self, inputs):
        """带缓存的批处理"""
        results = []
        uncached_inputs = []
        uncached_indices = []

        # 检查缓存
        for i, inp in enumerate(inputs):
            cache_key = self._hash_input(inp)
            if cache_key in self.cache:
                results.append(self.cache[cache_key])
            else:
                results.append(None)
                uncached_inputs.append(inp)
                uncached_indices.append(i)

        # 批处理未缓存的输入
        if uncached_inputs:
            uncached_results = self.chain.batch(uncached_inputs)

            # 更新结果和缓存
            for idx, result in zip(uncached_indices, uncached_results):
                results[idx] = result
                cache_key = self._hash_input(inputs[idx])
                self.cache[cache_key] = result

                # 限制缓存大小
                if len(self.cache) > self.cache_size:
                    self.cache.pop(next(iter(self.cache)))

        return results
```

**性能提升**：
- 缓存命中率 50%：性能提升 2 倍
- 缓存命中率 80%：性能提升 5 倍

**参考来源**：[LangChain 最佳实践](https://www.swarnendu.de/blog/langchain-best-practices/)

---

### 3. 预热与连接池

**问题**：首次请求延迟高（冷启动）。

**解决方案**：
```python
def warmup_chain(chain, sample_input):
    """预热链，建立连接池"""
    try:
        chain.invoke(sample_input)
        print("预热完成")
    except:
        pass

# 使用示例
warmup_chain(chain, {"query": "test"})

# 后续批处理会更快
results = chain.batch(inputs)
```

---

### 4. 异步批处理优化

**问题**：高并发场景下，同步批处理性能不足。

**解决方案**：
```python
import asyncio

async def optimized_abatch(chain, inputs, max_concurrency=100):
    """优化的异步批处理"""
    semaphore = asyncio.Semaphore(max_concurrency)

    async def process_one(inp):
        async with semaphore:
            return await chain.ainvoke(inp)

    # 并发执行
    tasks = [process_one(inp) for inp in inputs]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # 处理异常
    final_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"任务 {i} 失败: {result}")
            final_results.append(None)
        else:
            final_results.append(result)

    return final_results
```

**性能对比**：
- 同步 batch()：50 个任务 = 10 秒
- 异步 abatch()：50 个任务 = 5 秒（快 2 倍）
- 高并发（1000+）：abatch() 快 5-10 倍

**参考来源**：[LangChain 并发编程指南](https://medium.com/@oscar.f.agreda/unlocking-pythons-potential-concurrency-with-langchain-and-beyond-876149aaf475)

---

## 成本优化策略

### 1. 使用批处理 API（50% 折扣）

**OpenAI/Anthropic Batch API**：

```python
# 传统方式（全价）
results = chain.batch(inputs)
# 成本：100 次 × $0.01 = $1.00

# 批处理 API（50% 折扣）
# 使用 langasync（2026）
from langasync import wrap_chain

async_chain = wrap_chain(chain)
job = async_chain.submit_batch(inputs)
results = job.wait()  # 24小时内完成
# 成本：$0.50（50% 折扣）
```

**权衡**：
- 优势：成本降低 50%
- 劣势：24 小时内完成（非实时）
- 适用：评估、标注、离线分析

**参考来源**：
- [langasync GitHub](https://github.com/langasync/langasync)
- [langasync 官网](https://langasync.com/)

---

### 2. 模型选择策略

**问题**：所有任务都使用昂贵的模型。

**解决方案**：
```python
from langchain_openai import ChatOpenAI

# 定义不同价格的模型
cheap_llm = ChatOpenAI(model="gpt-4o-mini")  # $0.15/1M tokens
expensive_llm = ChatOpenAI(model="gpt-4")    # $30/1M tokens

def smart_batch(inputs):
    """根据任务复杂度选择模型"""
    results = []

    for inp in inputs:
        # 简单任务使用便宜的模型
        if is_simple_task(inp):
            result = cheap_llm.invoke(inp)
        else:
            result = expensive_llm.invoke(inp)

        results.append(result)

    return results

def is_simple_task(inp):
    """判断任务复杂度"""
    # 简单规则：输入长度 < 100 字符
    return len(str(inp)) < 100
```

**成本节省**：
- 假设 80% 任务是简单任务
- 成本：0.8 × $0.15 + 0.2 × $30 = $6.12
- 节省：(30 - 6.12) / 30 = 79.6%

---

### 3. Token 优化

**问题**：Prompt 过长，浪费 token。

**解决方案**：
```python
def optimize_prompt(query, context):
    """优化 prompt，减少 token"""
    # 1. 移除不必要的空白
    context = " ".join(context.split())

    # 2. 截断过长的上下文
    max_context_length = 1000
    if len(context) > max_context_length:
        context = context[:max_context_length] + "..."

    # 3. 使用更短的指令
    prompt = f"Q: {query}\nContext: {context}\nA:"

    return prompt
```

**Token 节省**：
- 原始 prompt：2000 tokens
- 优化后：1000 tokens
- 节省：50%

---

### 4. 缓存复用

**问题**：相同的输入重复调用 API。

**解决方案**：
```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# 启用缓存
set_llm_cache(InMemoryCache())

# 后续相同的输入会使用缓存
result1 = chain.invoke({"query": "什么是 AI？"})
result2 = chain.invoke({"query": "什么是 AI？"})  # 使用缓存，成本 $0
```

**成本节省**：
- 缓存命中率 50%：成本降低 50%
- 缓存命中率 80%：成本降低 80%

---

## 混合优化策略

### 1. 智能路由

**问题**：不同任务需要不同的优化策略。

**解决方案**：
```python
class SmartRouter:
    def __init__(self):
        self.cheap_llm = ChatOpenAI(model="gpt-4o-mini")
        self.expensive_llm = ChatOpenAI(model="gpt-4")
        self.cache = {}

    def route(self, inp):
        """智能路由"""
        # 1. 检查缓存
        cache_key = self._hash(inp)
        if cache_key in self.cache:
            return self.cache[cache_key]

        # 2. 根据复杂度选择模型
        if self._is_simple(inp):
            result = self.cheap_llm.invoke(inp)
        else:
            result = self.expensive_llm.invoke(inp)

        # 3. 更新缓存
        self.cache[cache_key] = result

        return result

    def batch(self, inputs):
        """批处理"""
        return [self.route(inp) for inp in inputs]
```

---

### 2. 分层批处理

**问题**：不同优先级的任务混在一起。

**解决方案**：
```python
def tiered_batch(inputs, priorities):
    """分层批处理"""
    # 1. 按优先级分组
    high_priority = [inp for inp, p in zip(inputs, priorities) if p == "high"]
    low_priority = [inp for inp, p in zip(inputs, priorities) if p == "low"]

    # 2. 高优先级：实时处理
    high_results = chain.batch(high_priority, config={"max_concurrency": 20})

    # 3. 低优先级：批处理 API（成本优化）
    from langasync import wrap_chain
    async_chain = wrap_chain(chain)
    job = async_chain.submit_batch(low_priority)
    low_results = job.wait()

    # 4. 合并结果
    results = []
    high_idx = 0
    low_idx = 0
    for p in priorities:
        if p == "high":
            results.append(high_results[high_idx])
            high_idx += 1
        else:
            results.append(low_results[low_idx])
            low_idx += 1

    return results
```

---

## 监控和调优

### 1. 性能监控

```python
import time

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "total_requests": 0,
            "total_time": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }

    def batch(self, chain, inputs):
        """带监控的批处理"""
        start = time.time()

        results = chain.batch(inputs)

        duration = time.time() - start
        self.metrics["total_requests"] += len(inputs)
        self.metrics["total_time"] += duration

        # 打印统计
        self.print_stats()

        return results

    def print_stats(self):
        """打印统计信息"""
        avg_latency = self.metrics["total_time"] / self.metrics["total_requests"]
        throughput = self.metrics["total_requests"] / self.metrics["total_time"]

        print(f"总请求数: {self.metrics['total_requests']}")
        print(f"平均延迟: {avg_latency:.3f}秒")
        print(f"吞吐量: {throughput:.2f} 请求/秒")
```

---

### 2. 成本监控

```python
class CostMonitor:
    def __init__(self):
        self.total_cost = 0.0
        self.request_count = 0

    def estimate_cost(self, input_text, output_text):
        """估算成本"""
        input_tokens = len(input_text.split()) * 1.3
        output_tokens = len(output_text.split()) * 1.3

        # GPT-4o-mini 价格
        input_cost = (input_tokens / 1000000) * 0.15
        output_cost = (output_tokens / 1000000) * 0.60

        return input_cost + output_cost

    def batch(self, chain, inputs):
        """带成本监控的批处理"""
        results = chain.batch(inputs)

        # 估算成本
        for inp, result in zip(inputs, results):
            cost = self.estimate_cost(str(inp), result.content)
            self.total_cost += cost
            self.request_count += 1

        print(f"总成本: ${self.total_cost:.4f}")
        print(f"平均成本: ${self.total_cost / self.request_count:.4f}/请求")

        return results
```

**参考来源**：[LangChain 成本优化](https://python.plainenglish.io/how-to-reduce-chatgpt-api-costs-in-python-projects-02b7ebdce6f6)

---

## 2025-2026 最新优化技巧

### 1. langasync 集成（2026）

```python
class HybridBatchProcessor:
    def __init__(self, chain):
        self.chain = chain
        try:
            from langasync import wrap_chain
            self.async_chain = wrap_chain(chain)
            self.langasync_available = True
        except ImportError:
            self.langasync_available = False

    def batch(self, inputs, use_batch_api=False):
        """混合批处理"""
        if use_batch_api and self.langasync_available:
            # 使用批处理 API（成本降低 50%）
            job = self.async_chain.submit_batch(inputs)
            return job.wait()
        else:
            # 使用普通批处理（实时）
            return self.chain.batch(inputs)
```

---

### 2. 自适应批处理

```python
class AdaptiveBatchProcessor:
    def __init__(self, chain):
        self.chain = chain
        self.concurrency = 10
        self.success_rate = []

    def batch(self, inputs):
        """自适应批处理"""
        try:
            results = self.chain.batch(
                inputs,
                config={"max_concurrency": self.concurrency}
            )
            self.success_rate.append(1)
            self.adjust_concurrency(success=True)
            return results
        except Exception as e:
            self.success_rate.append(0)
            self.adjust_concurrency(success=False)
            raise

    def adjust_concurrency(self, success):
        """动态调整并发数"""
        if success and len(self.success_rate) > 10:
            recent_success = sum(self.success_rate[-10:]) / 10
            if recent_success > 0.95:
                self.concurrency = min(self.concurrency + 2, 50)
        elif not success:
            self.concurrency = max(self.concurrency - 2, 5)
```

---

## 实战案例

### 案例1：批量评估优化

```python
# 原始方案
results = chain.batch(eval_data)  # 100 个任务，10 秒，$1.00

# 优化方案
cached_processor = CachedBatchProcessor(chain)
results = cached_processor.batch(eval_data)
# 缓存命中率 50%：5 秒，$0.50
```

### 案例2：大规模数据处理

```python
# 原始方案
results = chain.batch(large_dataset)  # 10000 个任务，容易失败

# 优化方案
results = chunked_batch(chain, large_dataset, chunk_size=100)
# 分 100 片处理，稳定可靠
```

### 案例3：成本敏感场景

```python
# 原始方案
results = chain.batch(inputs)  # $10.00

# 优化方案
from langasync import wrap_chain
async_chain = wrap_chain(chain)
job = async_chain.submit_batch(inputs)
results = job.wait()  # $5.00（50% 折扣）
```

---

## 总结

批量优化策略的核心要点：

**性能优化**：
1. 批处理分片 - 降低失败风险
2. 缓存策略 - 避免重复调用
3. 异步批处理 - 高并发场景
4. 预热与连接池 - 降低冷启动延迟

**成本优化**：
1. 批处理 API - 50% 折扣
2. 模型选择 - 根据复杂度选择
3. Token 优化 - 减少不必要的 token
4. 缓存复用 - 避免重复调用

**混合策略**：
1. 智能路由 - 根据任务特点选择策略
2. 分层批处理 - 区分优先级
3. 自适应调整 - 动态优化参数

**2025-2026 新特性**：
1. langasync 工具 - 零代码改动实现成本优化
2. 自适应批处理 - 动态调整并发数
3. 混合批处理 - 灵活选择实时或离线

---

## 参考来源

1. [LangChain 最佳实践](https://www.swarnendu.de/blog/langchain-best-practices/) - 性能优化策略
2. [LangChain 成本优化](https://python.plainenglish.io/how-to-reduce-chatgpt-api-costs-in-python-projects-02b7ebdce6f6) - 成本控制技巧
3. [langasync GitHub](https://github.com/langasync/langasync) - 批处理 API 工具
4. [LangChain 并发编程指南](https://medium.com/@oscar.f.agreda/unlocking-pythons-potential-concurrency-with-langchain-and-beyond-876149aaf475) - 异步优化
5. [LangChain 成本报告](https://www.langchain.com/state-of-agent-engineering) - 2026 年成本优化趋势

---

**下一步**：阅读实战代码文件，通过完整示例掌握批处理与并发控制的实际应用
