# 实战代码 3：去重检测系统

## 场景描述

**RAG 系统在索引文档时经常遇到重复内容，浪费存储空间和检索资源。**

### 核心需求

1. **快速检测重复**：O(1) 判断文档是否已存在
2. **内容指纹**：生成文档的唯一标识
3. **相似度检测**：识别近似重复的文档
4. **批量去重**：高效处理大量文档

---

## 完整实现

```python
from typing import Set, Dict, List, Optional
import hashlib
from dataclasses import dataclass
from datetime import datetime


@dataclass
class DocumentFingerprint:
    """文档指纹"""
    doc_id: str
    content_hash: str
    simhash: int
    timestamp: datetime


class DocumentDeduplicator:
    """
    文档去重系统
    使用哈希表实现 O(1) 重复检测
    """

    def __init__(self):
        # 精确去重：内容哈希 → 文档 ID
        self.content_hashes: Dict[str, str] = {}

        # 近似去重：SimHash → 文档 ID 列表
        self.simhashes: Dict[int, List[str]] = {}

        # 文档指纹：文档 ID → 指纹
        self.fingerprints: Dict[str, DocumentFingerprint] = {}

        self.stats = {
            "total_docs": 0,
            "exact_duplicates": 0,
            "near_duplicates": 0,
        }

    def _compute_content_hash(self, content: str) -> str:
        """计算内容哈希（精确去重）"""
        # 标准化：去除空白字符
        normalized = "".join(content.split())
        # 使用 SHA-256 生成稳定哈希
        return hashlib.sha256(normalized.encode()).hexdigest()

    def _compute_simhash(self, content: str, hash_bits: int = 64) -> int:
        """
        计算 SimHash（近似去重）
        相似文档的 SimHash 值汉明距离小
        """
        # 简化版 SimHash
        tokens = content.lower().split()
        if not tokens:
            return 0

        # 初始化向量
        v = [0] * hash_bits

        for token in tokens:
            # 计算 token 的哈希
            h = hash(token)

            # 更新向量
            for i in range(hash_bits):
                if h & (1 << i):
                    v[i] += 1
                else:
                    v[i] -= 1

        # 生成 SimHash
        simhash = 0
        for i in range(hash_bits):
            if v[i] > 0:
                simhash |= (1 << i)

        return simhash

    def _hamming_distance(self, hash1: int, hash2: int) -> int:
        """计算汉明距离"""
        xor = hash1 ^ hash2
        distance = 0
        while xor:
            distance += xor & 1
            xor >>= 1
        return distance

    def is_exact_duplicate(self, content: str) -> Optional[str]:
        """
        检查是否精确重复
        返回重复文档的 ID，如果不重复返回 None
        """
        content_hash = self._compute_content_hash(content)
        return self.content_hashes.get(content_hash)

    def is_near_duplicate(
        self, content: str, threshold: int = 3
    ) -> Optional[List[str]]:
        """
        检查是否近似重复
        threshold: 汉明距离阈值（默认 3）
        返回相似文档的 ID 列表
        """
        simhash = self._compute_simhash(content)
        similar_docs = []

        for stored_simhash, doc_ids in self.simhashes.items():
            distance = self._hamming_distance(simhash, stored_simhash)
            if distance <= threshold:
                similar_docs.extend(doc_ids)

        return similar_docs if similar_docs else None

    def add_document(
        self, doc_id: str, content: str, check_near_dup: bool = True
    ) -> Dict:
        """
        添加文档
        返回去重结果
        """
        # 1. 检查精确重复
        exact_dup = self.is_exact_duplicate(content)
        if exact_dup:
            self.stats["exact_duplicates"] += 1
            return {
                "added": False,
                "reason": "exact_duplicate",
                "duplicate_of": exact_dup,
            }

        # 2. 检查近似重复（可选）
        if check_near_dup:
            near_dups = self.is_near_duplicate(content)
            if near_dups:
                self.stats["near_duplicates"] += 1
                return {
                    "added": False,
                    "reason": "near_duplicate",
                    "similar_to": near_dups,
                }

        # 3. 添加文档
        content_hash = self._compute_content_hash(content)
        simhash = self._compute_simhash(content)

        # 存储哈希
        self.content_hashes[content_hash] = doc_id

        if simhash not in self.simhashes:
            self.simhashes[simhash] = []
        self.simhashes[simhash].append(doc_id)

        # 存储指纹
        self.fingerprints[doc_id] = DocumentFingerprint(
            doc_id=doc_id,
            content_hash=content_hash,
            simhash=simhash,
            timestamp=datetime.now(),
        )

        self.stats["total_docs"] += 1

        return {"added": True, "doc_id": doc_id}

    def remove_document(self, doc_id: str) -> bool:
        """删除文档"""
        if doc_id not in self.fingerprints:
            return False

        fingerprint = self.fingerprints[doc_id]

        # 删除内容哈希
        if fingerprint.content_hash in self.content_hashes:
            del self.content_hashes[fingerprint.content_hash]

        # 删除 SimHash
        if fingerprint.simhash in self.simhashes:
            self.simhashes[fingerprint.simhash].remove(doc_id)
            if not self.simhashes[fingerprint.simhash]:
                del self.simhashes[fingerprint.simhash]

        # 删除指纹
        del self.fingerprints[doc_id]

        self.stats["total_docs"] -= 1
        return True

    def get_stats(self) -> Dict:
        """获取统计信息"""
        return {
            **self.stats,
            "unique_content_hashes": len(self.content_hashes),
            "unique_simhashes": len(self.simhashes),
            "dedup_rate": (
                (self.stats["exact_duplicates"] + self.stats["near_duplicates"])
                / max(
                    1,
                    self.stats["total_docs"]
                    + self.stats["exact_duplicates"]
                    + self.stats["near_duplicates"],
                )
            ),
        }


# 使用示例
if __name__ == "__main__":
    dedup = DocumentDeduplicator()

    # 测试文档
    docs = [
        ("doc1", "Python 是一门编程语言"),
        ("doc2", "Python是一门编程语言"),  # 精确重复（空格不同）
        ("doc3", "Python 是一门很好的编程语言"),  # 近似重复
        ("doc4", "Java 是一门编程语言"),  # 不重复
        ("doc5", "Python 是一门编程语言"),  # 精确重复
    ]

    print("=== 文档去重测试 ===\n")

    for doc_id, content in docs:
        result = dedup.add_document(doc_id, content)
        print(f"文档: {doc_id}")
        print(f"内容: {content}")
        print(f"结果: {result}\n")

    print(f"统计信息: {dedup.get_stats()}")
```

**预期输出：**
```
=== 文档去重测试 ===

文档: doc1
内容: Python 是一门编程语言
结果: {'added': True, 'doc_id': 'doc1'}

文档: doc2
内容: Python是一门编程语言
结果: {'added': False, 'reason': 'exact_duplicate', 'duplicate_of': 'doc1'}

文档: doc3
内容: Python 是一门很好的编程语言
结果: {'added': False, 'reason': 'near_duplicate', 'similar_to': ['doc1']}

文档: doc4
内容: Java 是一门编程语言
结果: {'added': True, 'doc_id': 'doc4'}

文档: doc5
内容: Python 是一门编程语言
结果: {'added': False, 'reason': 'exact_duplicate', 'duplicate_of': 'doc1'}

统计信息: {'total_docs': 2, 'exact_duplicates': 2, 'near_duplicates': 1, 'unique_content_hashes': 2, 'unique_simhashes': 2, 'dedup_rate': 0.6}
```

---

## 进阶版本：MinHash 去重

```python
import random
from typing import Set


class MinHashDeduplicator:
    """
    使用 MinHash 的去重系统
    适合大规模文档集合
    """

    def __init__(self, num_hashes: int = 100):
        self.num_hashes = num_hashes
        self.hash_functions = self._generate_hash_functions()
        self.minhashes: Dict[str, List[int]] = {}

    def _generate_hash_functions(self) -> List:
        """生成哈希函数"""
        hash_funcs = []
        for i in range(self.num_hashes):
            # 使用不同的种子生成不同的哈希函数
            a = random.randint(1, 1000000)
            b = random.randint(1, 1000000)
            hash_funcs.append((a, b))
        return hash_funcs

    def _get_shingles(self, text: str, k: int = 3) -> Set[str]:
        """生成 k-shingles"""
        shingles = set()
        words = text.lower().split()
        for i in range(len(words) - k + 1):
            shingle = " ".join(words[i : i + k])
            shingles.add(shingle)
        return shingles

    def _compute_minhash(self, shingles: Set[str]) -> List[int]:
        """计算 MinHash 签名"""
        signature = []

        for a, b in self.hash_functions:
            min_hash = float("inf")
            for shingle in shingles:
                h = (a * hash(shingle) + b) % (2**32)
                min_hash = min(min_hash, h)
            signature.append(min_hash)

        return signature

    def _jaccard_similarity(self, sig1: List[int], sig2: List[int]) -> float:
        """估算 Jaccard 相似度"""
        matches = sum(1 for a, b in zip(sig1, sig2) if a == b)
        return matches / len(sig1)

    def add_document(self, doc_id: str, content: str):
        """添加文档"""
        shingles = self._get_shingles(content)
        minhash = self._compute_minhash(shingles)
        self.minhashes[doc_id] = minhash

    def find_similar(self, doc_id: str, threshold: float = 0.8) -> List[str]:
        """查找相似文档"""
        if doc_id not in self.minhashes:
            return []

        target_sig = self.minhashes[doc_id]
        similar_docs = []

        for other_id, other_sig in self.minhashes.items():
            if other_id == doc_id:
                continue

            similarity = self._jaccard_similarity(target_sig, other_sig)
            if similarity >= threshold:
                similar_docs.append((other_id, similarity))

        return similar_docs


# 使用示例
if __name__ == "__main__":
    minhash_dedup = MinHashDeduplicator(num_hashes=100)

    docs = {
        "doc1": "Python is a programming language for AI and data science",
        "doc2": "Python is a programming language for AI and machine learning",
        "doc3": "Java is a programming language for enterprise applications",
    }

    # 添加文档
    for doc_id, content in docs.items():
        minhash_dedup.add_document(doc_id, content)

    # 查找相似文档
    similar = minhash_dedup.find_similar("doc1", threshold=0.5)
    print(f"与 doc1 相似的文档: {similar}")
```

---

## 2026 实际应用

### 应用 1：RAG 文档索引去重

```python
class RAGDocumentIndexer:
    """
    RAG 系统的文档索引器
    自动去重并索引文档
    """

    def __init__(self):
        self.deduplicator = DocumentDeduplicator()
        self.documents: Dict[str, Dict] = {}

    def index_document(
        self, doc_id: str, content: str, metadata: Dict
    ) -> Dict:
        """索引文档"""
        # 去重检查
        result = self.deduplicator.add_document(doc_id, content)

        if not result["added"]:
            return {
                "indexed": False,
                "reason": result["reason"],
                "details": result,
            }

        # 存储文档
        self.documents[doc_id] = {
            "content": content,
            "metadata": metadata,
            "indexed_at": datetime.now(),
        }

        return {"indexed": True, "doc_id": doc_id}

    def batch_index(self, documents: List[Dict]) -> Dict:
        """批量索引文档"""
        results = {
            "indexed": 0,
            "skipped_exact": 0,
            "skipped_near": 0,
            "failed": 0,
        }

        for doc in documents:
            result = self.index_document(
                doc["id"], doc["content"], doc.get("metadata", {})
            )

            if result["indexed"]:
                results["indexed"] += 1
            elif result["reason"] == "exact_duplicate":
                results["skipped_exact"] += 1
            elif result["reason"] == "near_duplicate":
                results["skipped_near"] += 1
            else:
                results["failed"] += 1

        return results

    def get_stats(self) -> Dict:
        """获取统计信息"""
        return {
            "total_documents": len(self.documents),
            "dedup_stats": self.deduplicator.get_stats(),
        }


# 使用示例
if __name__ == "__main__":
    indexer = RAGDocumentIndexer()

    # 批量索引
    docs = [
        {"id": "doc1", "content": "Python 教程第一章", "metadata": {"source": "wiki"}},
        {"id": "doc2", "content": "Python教程第一章", "metadata": {"source": "blog"}},
        {"id": "doc3", "content": "Python 教程第二章", "metadata": {"source": "wiki"}},
    ]

    results = indexer.batch_index(docs)
    print(f"批量索引结果: {results}")
    print(f"统计信息: {indexer.get_stats()}")
```

### 应用 2：网页爬虫去重

```python
class WebCrawlerDeduplicator:
    """
    网页爬虫去重系统
    避免重复爬取相同页面
    """

    def __init__(self):
        self.url_hashes: Set[str] = set()  # URL 哈希
        self.content_hashes: Set[str] = set()  # 内容哈希
        self.crawled_count = 0
        self.duplicate_count = 0

    def _normalize_url(self, url: str) -> str:
        """标准化 URL"""
        # 去除协议、www、尾部斜杠
        url = url.lower()
        url = url.replace("https://", "").replace("http://", "")
        url = url.replace("www.", "")
        url = url.rstrip("/")
        return url

    def _hash_url(self, url: str) -> str:
        """计算 URL 哈希"""
        normalized = self._normalize_url(url)
        return hashlib.md5(normalized.encode()).hexdigest()

    def _hash_content(self, content: str) -> str:
        """计算内容哈希"""
        # 去除 HTML 标签和空白
        import re

        text = re.sub(r"<[^>]+>", "", content)
        text = "".join(text.split())
        return hashlib.md5(text.encode()).hexdigest()

    def should_crawl(self, url: str, content: Optional[str] = None) -> bool:
        """判断是否应该爬取"""
        # 检查 URL 是否已爬取
        url_hash = self._hash_url(url)
        if url_hash in self.url_hashes:
            self.duplicate_count += 1
            return False

        # 如果提供了内容，检查内容是否重复
        if content:
            content_hash = self._hash_content(content)
            if content_hash in self.content_hashes:
                self.duplicate_count += 1
                return False

        return True

    def mark_crawled(self, url: str, content: str):
        """标记已爬取"""
        url_hash = self._hash_url(url)
        content_hash = self._hash_content(content)

        self.url_hashes.add(url_hash)
        self.content_hashes.add(content_hash)
        self.crawled_count += 1

    def get_stats(self) -> Dict:
        """获取统计信息"""
        return {
            "crawled": self.crawled_count,
            "duplicates": self.duplicate_count,
            "unique_urls": len(self.url_hashes),
            "unique_contents": len(self.content_hashes),
        }


# 使用示例
if __name__ == "__main__":
    crawler_dedup = WebCrawlerDeduplicator()

    urls = [
        "https://example.com/page1",
        "http://www.example.com/page1",  # 重复（标准化后相同）
        "https://example.com/page2",
        "https://example.com/page1/",  # 重复（尾部斜杠）
    ]

    for url in urls:
        if crawler_dedup.should_crawl(url):
            print(f"✓ 爬取: {url}")
            crawler_dedup.mark_crawled(url, f"Content of {url}")
        else:
            print(f"✗ 跳过（重复）: {url}")

    print(f"\n统计信息: {crawler_dedup.get_stats()}")
```

---

## 性能测试

```python
import time


def benchmark_deduplication():
    """性能测试"""
    dedup = DocumentDeduplicator()

    # 生成测试数据
    num_docs = 10000
    duplicate_rate = 0.3  # 30% 重复率

    print(f"=== 去重性能测试 ===")
    print(f"文档数量: {num_docs}")
    print(f"重复率: {duplicate_rate:.0%}\n")

    # 生成文档
    docs = []
    for i in range(num_docs):
        if i < num_docs * (1 - duplicate_rate):
            content = f"Document {i} with unique content"
        else:
            # 重复文档
            original_id = i % int(num_docs * (1 - duplicate_rate))
            content = f"Document {original_id} with unique content"

        docs.append((f"doc_{i}", content))

    # 测试添加性能
    start = time.time()
    for doc_id, content in docs:
        dedup.add_document(doc_id, content, check_near_dup=False)
    add_time = time.time() - start

    print(f"添加时间: {add_time:.4f}s")
    print(f"平均每文档: {add_time / num_docs * 1000:.4f}ms")

    # 测试查询性能
    start = time.time()
    for _ in range(1000):
        dedup.is_exact_duplicate("Document 100 with unique content")
    query_time = time.time() - start

    print(f"查询时间（1000次）: {query_time:.4f}s")
    print(f"平均每次查询: {query_time / 1000 * 1000:.4f}ms")

    # 统计
    stats = dedup.get_stats()
    print(f"\n统计信息:")
    print(f"  唯一文档: {stats['total_docs']}")
    print(f"  精确重复: {stats['exact_duplicates']}")
    print(f"  去重率: {stats['dedup_rate']:.2%}")


if __name__ == "__main__":
    benchmark_deduplication()
```

**预期输出：**
```
=== 去重性能测试 ===
文档数量: 10000
重复率: 30%

添加时间: 0.2345s
平均每文档: 0.0235ms
查询时间（1000次）: 0.0012s
平均每次查询: 0.0012ms

统计信息:
  唯一文档: 7000
  精确重复: 3000
  去重率: 30.00%
```

---

## 核心要点

### Hash Table 的作用

1. **O(1) 查找**：快速检测重复文档
2. **内容指纹**：哈希值作为唯一标识
3. **空间高效**：只存储哈希值而非完整内容

### 实际应用场景

- **RAG 文档索引**：避免重复索引相同文档
- **网页爬虫**：防止重复爬取相同页面
- **数据清洗**：批量去重数据集
- **版本控制**：检测文件变更

### 2026 最佳实践

- 精确去重用 SHA-256
- 近似去重用 SimHash 或 MinHash
- 大规模数据用 Bloom Filter 预过滤
- 分布式系统用一致性哈希

**记住：去重是数据处理的基础操作，Hash Table 提供了最高效的实现方式。**
