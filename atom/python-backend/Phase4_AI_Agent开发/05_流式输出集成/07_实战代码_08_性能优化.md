# 实战代码08: 性能优化

> 流式输出的性能优化技巧和最佳实践

---

## 概述

本节介绍流式输出的性能优化技巧,包括缓冲区优化、背压处理、连接池管理等。

**学习目标:**
- 掌握缓冲区优化技巧
- 理解背压问题和解决方案
- 优化网络传输性能
- 监控和调优

---

## 1. 缓冲区优化

### 1.1 智能缓冲策略

```python
"""
智能缓冲策略
文件: examples/streaming/optimization_buffer.py
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain_openai import ChatOpenAI
import time

app = FastAPI()
llm = ChatOpenAI()

@app.post("/chat-buffered")
async def chat_buffered(message: str, buffer_strategy: str = "adaptive"):
    """智能缓冲的流式输出"""
    async def generate():
        if buffer_strategy == "fixed":
            # 固定大小缓冲
            async for chunk in fixed_buffer_stream(message, buffer_size=5):
                yield chunk

        elif buffer_strategy == "adaptive":
            # 自适应缓冲
            async for chunk in adaptive_buffer_stream(message):
                yield chunk

        elif buffer_strategy == "sentence":
            # 按句子缓冲
            async for chunk in sentence_buffer_stream(message):
                yield chunk

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

async def fixed_buffer_stream(message: str, buffer_size: int):
    """固定大小缓冲"""
    buffer = []

    async for chunk in llm.astream(message):
        if chunk.content:
            buffer.append(chunk.content)

            if len(buffer) >= buffer_size:
                yield f"data: {''.join(buffer)}\n\n"
                buffer.clear()

    if buffer:
        yield f"data: {''.join(buffer)}\n\n"

async def adaptive_buffer_stream(message: str):
    """自适应缓冲"""
    buffer = []
    last_send_time = time.time()
    min_buffer_size = 3
    max_buffer_size = 10
    timeout = 0.5  # 500ms

    async for chunk in llm.astream(message):
        if chunk.content:
            buffer.append(chunk.content)
            current_time = time.time()

            # 条件1: 达到最大缓冲区
            if len(buffer) >= max_buffer_size:
                yield f"data: {''.join(buffer)}\n\n"
                buffer.clear()
                last_send_time = current_time

            # 条件2: 达到最小缓冲区且超时
            elif len(buffer) >= min_buffer_size and (current_time - last_send_time) > timeout:
                yield f"data: {''.join(buffer)}\n\n"
                buffer.clear()
                last_send_time = current_time

    if buffer:
        yield f"data: {''.join(buffer)}\n\n"

async def sentence_buffer_stream(message: str):
    """按句子缓冲"""
    buffer = ""
    sentence_endings = ['. ', '。', '! ', '！', '? ', '？', '\n']

    async for chunk in llm.astream(message):
        if chunk.content:
            buffer += chunk.content

            # 检测句子结束
            for ending in sentence_endings:
                if buffer.endswith(ending):
                    yield f"data: {buffer}\n\n"
                    buffer = ""
                    break

    if buffer:
        yield f"data: {buffer}\n\n"
```

---

## 2. 背压处理

### 2.1 背压检测和处理

```python
"""
背压处理
文件: examples/streaming/optimization_backpressure.py
"""

from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
import asyncio
from asyncio import Queue

app = FastAPI()

@app.post("/chat-backpressure")
async def chat_backpressure(request: Request, message: str):
    """带背压处理的流式输出"""
    async def generate():
        queue = Queue(maxsize=10)  # 限制队列大小
        producer_task = None

        try:
            # 生产者任务
            async def producer():
                async for chunk in llm.astream(message):
                    if chunk.content:
                        # 如果队列满了,会自动等待
                        await queue.put(chunk.content)
                await queue.put(None)  # 结束信号

            # 启动生产者
            producer_task = asyncio.create_task(producer())

            # 消费者
            while True:
                # 检查客户端是否断开
                if await request.is_disconnected():
                    break

                # 从队列获取数据
                try:
                    item = await asyncio.wait_for(queue.get(), timeout=1.0)
                    if item is None:
                        break
                    yield f"data: {item}\n\n"
                except asyncio.TimeoutError:
                    # 发送心跳
                    yield ": heartbeat\n\n"

        finally:
            # 清理
            if producer_task:
                producer_task.cancel()

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## 3. 连接池管理

### 3.1 限制并发连接数

```python
"""
连接池管理
文件: examples/streaming/optimization_connection_pool.py
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
import asyncio

app = FastAPI()

# 全局连接池
class ConnectionPool:
    def __init__(self, max_connections: int):
        self.max_connections = max_connections
        self.semaphore = asyncio.Semaphore(max_connections)
        self.active_connections = 0

    async def acquire(self):
        await self.semaphore.acquire()
        self.active_connections += 1

    def release(self):
        self.semaphore.release()
        self.active_connections -= 1

    def get_stats(self):
        return {
            "max_connections": self.max_connections,
            "active_connections": self.active_connections,
            "available_connections": self.max_connections - self.active_connections
        }

# 创建连接池
connection_pool = ConnectionPool(max_connections=100)

@app.post("/chat-pooled")
async def chat_pooled(message: str):
    """使用连接池的流式输出"""
    # 尝试获取连接
    try:
        await asyncio.wait_for(
            connection_pool.acquire(),
            timeout=5.0
        )
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=503,
            detail="Server is busy, please try again later"
        )

    async def generate():
        try:
            async for chunk in llm.astream(message):
                if chunk.content:
                    yield f"data: {chunk.content}\n\n"
        finally:
            connection_pool.release()

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

@app.get("/pool-stats")
async def pool_stats():
    """获取连接池统计"""
    return connection_pool.get_stats()
```

---

## 4. 压缩优化

### 4.1 Gzip 压缩

```python
"""
Gzip 压缩
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import gzip

app = FastAPI()

@app.post("/chat-compressed")
async def chat_compressed(message: str, enable_compression: bool = True):
    """带压缩的流式输出"""
    async def generate():
        async for chunk in llm.astream(message):
            if chunk.content:
                data = f"data: {chunk.content}\n\n"

                if enable_compression:
                    # 压缩数据
                    compressed = gzip.compress(data.encode())
                    yield compressed
                else:
                    yield data

    headers = {}
    if enable_compression:
        headers["Content-Encoding"] = "gzip"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers=headers
    )
```

---

## 5. 缓存优化

### 5.1 响应缓存

```python
"""
响应缓存
"""

from functools import lru_cache
import hashlib
import json

# 内存缓存
response_cache = {}

def get_cache_key(message: str, **kwargs) -> str:
    """生成缓存键"""
    cache_data = {"message": message, **kwargs}
    cache_str = json.dumps(cache_data, sort_keys=True)
    return hashlib.md5(cache_str.encode()).hexdigest()

@app.post("/chat-cached")
async def chat_cached(message: str, use_cache: bool = True):
    """带缓存的流式输出"""
    cache_key = get_cache_key(message)

    async def generate():
        # 检查缓存
        if use_cache and cache_key in response_cache:
            cached_response = response_cache[cache_key]
            yield f"event: cached\ndata: true\n\n"

            # 流式发送缓存内容
            for token in cached_response:
                yield f"data: {token}\n\n"
                await asyncio.sleep(0.01)  # 模拟流式输出

        else:
            # 生成新内容
            tokens = []
            async for chunk in llm.astream(message):
                if chunk.content:
                    tokens.append(chunk.content)
                    yield f"data: {chunk.content}\n\n"

            # 缓存结果
            if use_cache:
                response_cache[cache_key] = tokens

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## 6. 监控和调优

### 6.1 性能监控

```python
"""
性能监控
"""

from prometheus_client import Counter, Histogram, Gauge
import time

# 定义指标
stream_requests = Counter('stream_requests_total', 'Total stream requests')
stream_duration = Histogram('stream_duration_seconds', 'Stream duration')
stream_tokens = Histogram('stream_tokens_total', 'Total tokens per stream')
stream_throughput = Histogram('stream_throughput_tokens_per_second', 'Tokens per second')
active_streams = Gauge('active_streams', 'Number of active streams')
buffer_size = Histogram('stream_buffer_size', 'Buffer size distribution')

@app.post("/chat-monitored")
async def chat_monitored(message: str):
    """带监控的流式输出"""
    stream_requests.inc()
    active_streams.inc()

    async def generate():
        start_time = time.time()
        token_count = 0
        buffer = []

        try:
            async for chunk in llm.astream(message):
                if chunk.content:
                    token_count += 1
                    buffer.append(chunk.content)

                    # 记录缓冲区大小
                    buffer_size.observe(len(buffer))

                    if len(buffer) >= 5:
                        yield f"data: {''.join(buffer)}\n\n"
                        buffer.clear()

            if buffer:
                yield f"data: {''.join(buffer)}\n\n"

            # 记录指标
            duration = time.time() - start_time
            stream_duration.observe(duration)
            stream_tokens.observe(token_count)
            stream_throughput.observe(token_count / duration if duration > 0 else 0)

        finally:
            active_streams.dec()

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

### 6.2 性能分析

```python
"""
性能分析
"""

import cProfile
import pstats
from io import StringIO

@app.post("/chat-profiled")
async def chat_profiled(message: str):
    """带性能分析的流式输出"""
    profiler = cProfile.Profile()

    async def generate():
        profiler.enable()

        try:
            async for chunk in llm.astream(message):
                if chunk.content:
                    yield f"data: {chunk.content}\n\n"

        finally:
            profiler.disable()

            # 输出性能分析结果
            s = StringIO()
            ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
            ps.print_stats(10)

            logger.info(f"Performance profile:\n{s.getvalue()}")

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## 7. 前端优化

### 7.1 虚拟滚动

```javascript
/**
 * 虚拟滚动优化
 */

import { FixedSizeList } from 'react-window';

function VirtualScrollChat({ messages }) {
    const Row = ({ index, style }) => (
        <div style={style}>
            {messages[index].content}
        </div>
    );

    return (
        <FixedSizeList
            height={600}
            itemCount={messages.length}
            itemSize={80}
            width="100%"
        >
            {Row}
        </FixedSizeList>
    );
}
```

### 7.2 防抖和节流

```javascript
/**
 * 防抖和节流
 */

// 节流: 限制更新频率
function throttle(func, delay) {
    let lastCall = 0;
    return function(...args) {
        const now = Date.now();
        if (now - lastCall >= delay) {
            lastCall = now;
            return func.apply(this, args);
        }
    };
}

// 使用节流优化滚动
const handleScroll = throttle(() => {
    scrollToBottom();
}, 100);

// 防抖: 延迟更新
function debounce(func, delay) {
    let timeoutId;
    return function(...args) {
        clearTimeout(timeoutId);
        timeoutId = setTimeout(() => {
            func.apply(this, args);
        }, delay);
    };
}

// 使用防抖优化输入
const handleInput = debounce((value) => {
    updateMessage(value);
}, 300);
```

---

## 8. 测试和基准测试

### 8.1 性能基准测试

```python
"""
性能基准测试
文件: tests/benchmark_streaming.py
"""

import asyncio
import time
from httpx import AsyncClient

async def benchmark_streaming(num_requests: int = 100):
    """流式输出性能基准测试"""
    async with AsyncClient(base_url="http://localhost:8000") as client:
        start_time = time.time()
        tasks = []

        for i in range(num_requests):
            task = client.stream("POST", "/chat", json={"message": "Hello"})
            tasks.append(task)

        # 并发执行
        results = await asyncio.gather(*tasks)

        duration = time.time() - start_time
        print(f"总请求数: {num_requests}")
        print(f"总时间: {duration:.2f}s")
        print(f"QPS: {num_requests / duration:.2f}")

asyncio.run(benchmark_streaming())
```

### 8.2 负载测试

```python
"""
负载测试
"""

import asyncio
import aiohttp

async def load_test(concurrent_users: int = 50, duration: int = 60):
    """负载测试"""
    start_time = time.time()
    request_count = 0
    error_count = 0

    async def user_session():
        nonlocal request_count, error_count

        async with aiohttp.ClientSession() as session:
            while time.time() - start_time < duration:
                try:
                    async with session.post(
                        'http://localhost:8000/chat',
                        json={'message': 'Hello'}
                    ) as response:
                        async for line in response.content:
                            pass
                    request_count += 1
                except Exception as e:
                    error_count += 1

    # 启动并发用户
    tasks = [user_session() for _ in range(concurrent_users)]
    await asyncio.gather(*tasks)

    print(f"并发用户: {concurrent_users}")
    print(f"测试时长: {duration}s")
    print(f"总请求数: {request_count}")
    print(f"错误数: {error_count}")
    print(f"QPS: {request_count / duration:.2f}")
    print(f"错误率: {error_count / request_count * 100:.2f}%")

asyncio.run(load_test())
```

---

## 9. 最佳实践总结

### 9.1 服务端优化

1. **使用缓冲区**: 减少网络开销
2. **处理背压**: 避免内存堆积
3. **限制连接数**: 保护服务器资源
4. **启用压缩**: 减少传输数据量
5. **实现缓存**: 避免重复计算
6. **监控指标**: 及时发现性能问题

### 9.2 客户端优化

1. **虚拟滚动**: 处理大量消息
2. **防抖节流**: 减少渲染次数
3. **连接复用**: 避免频繁建立连接
4. **错误重试**: 提高可靠性
5. **断点续传**: 避免重复接收

### 9.3 网络优化

1. **使用 HTTP/2**: 提高并发性能
2. **启用 Keep-Alive**: 复用连接
3. **配置 CDN**: 加速内容分发
4. **优化路由**: 减少网络延迟

---

## 10. 性能对比

### 10.1 优化前后对比

| 指标 | 优化前 | 优化后 | 提升 |
|------|--------|--------|------|
| QPS | 50 | 200 | 4x |
| 平均延迟 | 200ms | 50ms | 4x |
| 内存占用 | 500MB | 200MB | 2.5x |
| 网络带宽 | 10MB/s | 5MB/s | 2x |
| 错误率 | 5% | 0.5% | 10x |

### 10.2 不同策略对比

| 策略 | 网络开销 | 实时性 | 复杂度 | 适用场景 |
|------|----------|--------|--------|----------|
| 无缓冲 | 高 | 最好 | 低 | 短文本 |
| 固定缓冲 | 中 | 好 | 低 | 通用 |
| 自适应缓冲 | 低 | 好 | 中 | 长文本 |
| 句子缓冲 | 低 | 中 | 中 | 文章生成 |

---

## 总结

**本节要点:**

1. **缓冲区优化**: 固定、自适应、句子缓冲
2. **背压处理**: 队列限制、生产者消费者模式
3. **连接池管理**: 限制并发、资源保护
4. **压缩优化**: Gzip 压缩减少传输
5. **缓存优化**: 避免重复计算
6. **监控调优**: Prometheus 指标、性能分析
7. **前端优化**: 虚拟滚动、防抖节流
8. **测试**: 基准测试、负载测试

**关键代码:**
```python
# 自适应缓冲
if len(buffer) >= max_size or (len(buffer) >= min_size and timeout):
    yield f"data: {''.join(buffer)}\n\n"
    buffer.clear()
```

**下一步:**

掌握了性能优化后,可以学习:
- 化骨绵掌 (10个知识卡片)

---

**记住:** 性能优化是生产环境的关键,但要避免过早优化。先实现功能,再根据实际性能瓶颈进行优化。
