# 核心概念3：AI 流式输出集成

> 掌握 OpenAI/Anthropic 流式 API 集成、Server-Sent Events (SSE) 协议和生产级实现

---

## 1. OpenAI 流式 API 集成

### 1.1 基础流式调用

```python
from openai import AsyncOpenAI
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()
client = AsyncOpenAI()

async def stream_openai_chat(prompt: str):
    """流式调用 OpenAI API"""
    # 创建流式请求
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        stream=True  # 开启流式模式
    )

    # 逐 token 返回
    async for chunk in stream:
        # 提取增量内容
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

@app.post("/chat")
async def chat_endpoint(prompt: str):
    return StreamingResponse(
        stream_openai_chat(prompt),
        media_type="text/plain"
    )
```

**关键点：**
- `stream=True` 开启流式模式
- `async for chunk in stream` 遍历流式响应
- `chunk.choices[0].delta.content` 提取增量内容

### 1.2 完整的流式响应结构

```python
async def stream_openai_detailed(prompt: str):
    """详细的流式响应，包含所有信息"""
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )

    async for chunk in stream:
        # chunk 的结构
        print(f"Chunk ID: {chunk.id}")
        print(f"Model: {chunk.model}")
        print(f"Created: {chunk.created}")

        # choices 数组
        for choice in chunk.choices:
            print(f"Index: {choice.index}")
            print(f"Finish Reason: {choice.finish_reason}")

            # delta 对象（增量内容）
            delta = choice.delta
            if delta.content:
                print(f"Content: {delta.content}")
                yield delta.content
            if delta.role:
                print(f"Role: {delta.role}")
            if delta.function_call:
                print(f"Function Call: {delta.function_call}")
```

**流式响应的生命周期：**
```
第1个 chunk: role="assistant", content=None
第2个 chunk: content="Hello"
第3个 chunk: content=" world"
第4个 chunk: content="!"
...
最后一个 chunk: content=None, finish_reason="stop"
```

### 1.3 处理流式响应的特殊情况

```python
async def stream_with_special_cases(prompt: str):
    """处理流式响应的特殊情况"""
    stream = await client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )

    full_content = ""  # 累积完整内容（用于日志）

    async for chunk in stream:
        choice = chunk.choices[0]

        # 情况1：第一个 chunk（包含 role）
        if choice.delta.role:
            print(f"开始生成，角色: {choice.delta.role}")

        # 情况2：内容 chunk
        if choice.delta.content:
            content = choice.delta.content
            full_content += content
            yield content

        # 情况3：函数调用（如果使用 function calling）
        if choice.delta.function_call:
            print(f"函数调用: {choice.delta.function_call}")

        # 情况4：最后一个 chunk（finish_reason）
        if choice.finish_reason:
            print(f"生成结束，原因: {choice.finish_reason}")
            print(f"完整内容: {full_content}")
            # 可以在这里记录日志、保存到数据库等
```

---

## 2. Anthropic Claude 流式 API 集成

### 2.1 基础流式调用

```python
from anthropic import AsyncAnthropic

client = AsyncAnthropic()

async def stream_claude_chat(prompt: str):
    """流式调用 Claude API"""
    # 创建流式请求
    async with client.messages.stream(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    ) as stream:
        # 逐 token 返回
        async for text in stream.text_stream:
            yield text

@app.post("/claude")
async def claude_endpoint(prompt: str):
    return StreamingResponse(
        stream_claude_chat(prompt),
        media_type="text/plain"
    )
```

**关键点：**
- 使用 `client.messages.stream()` 创建流式请求
- `async with` 确保资源正确释放
- `stream.text_stream` 直接获取文本流

### 2.2 Claude 流式响应的详细处理

```python
async def stream_claude_detailed(prompt: str):
    """详细的 Claude 流式响应处理"""
    async with client.messages.stream(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    ) as stream:
        # 方式1：使用 text_stream（推荐）
        async for text in stream.text_stream:
            yield text

        # 方式2：使用原始事件流
        # async for event in stream:
        #     if event.type == "content_block_delta":
        #         if event.delta.type == "text_delta":
        #             yield event.delta.text

        # 获取最终消息
        final_message = await stream.get_final_message()
        print(f"完整响应: {final_message.content}")
        print(f"使用的 tokens: {final_message.usage}")
```

### 2.3 OpenAI vs Claude 流式 API 对比

```python
# OpenAI 风格
async def openai_style():
    stream = await openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Hello"}],
        stream=True
    )
    async for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# Claude 风格
async def claude_style():
    async with claude_client.messages.stream(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    ) as stream:
        async for text in stream.text_stream:
            yield text
```

**主要区别：**
| 特性 | OpenAI | Claude |
|------|--------|--------|
| 流式开启 | `stream=True` | `messages.stream()` |
| 上下文管理 | 不需要 | 需要 `async with` |
| 内容提取 | `chunk.choices[0].delta.content` | `stream.text_stream` |
| 最终消息 | 需要手动累积 | `stream.get_final_message()` |

---

## 3. Server-Sent Events (SSE) 协议

### 3.1 SSE 协议基础

**SSE 是什么？**
- Server-Sent Events（服务器发送事件）
- 单向通信协议（服务器 → 客户端）
- 基于 HTTP，使用 `text/event-stream` MIME 类型
- 浏览器原生支持（EventSource API）

**SSE 格式规范：**
```
data: 消息内容\n\n

event: 事件类型\n
data: 消息内容\n\n

id: 消息ID\n
data: 消息内容\n\n

retry: 重连时间（毫秒）\n\n
```

### 3.2 基础 SSE 实现

```python
import asyncio
import json

async def sse_stream():
    """基础 SSE 流"""
    for i in range(10):
        # SSE 格式：data: 内容\n\n
        yield f"data: {i}\n\n"
        await asyncio.sleep(0.5)

@app.get("/sse")
async def sse_endpoint():
    return StreamingResponse(
        sse_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # 禁用 Nginx 缓冲
        }
    )
```

**前端接收（JavaScript）：**
```javascript
const eventSource = new EventSource('/sse');

eventSource.onmessage = (event) => {
    console.log('收到消息:', event.data);
};

eventSource.onerror = (error) => {
    console.error('连接错误:', error);
    eventSource.close();
};
```

### 3.3 高级 SSE：自定义事件类型

```python
async def sse_with_events():
    """带事件类型的 SSE"""
    # 发送 "start" 事件
    yield "event: start\n"
    yield "data: 开始生成\n\n"

    # 发送 "progress" 事件
    for i in range(5):
        yield "event: progress\n"
        yield f"data: {json.dumps({'progress': i * 20})}\n\n"
        await asyncio.sleep(0.5)

    # 发送 "complete" 事件
    yield "event: complete\n"
    yield "data: 生成完成\n\n"

@app.get("/sse-events")
async def sse_events_endpoint():
    return StreamingResponse(
        sse_with_events(),
        media_type="text/event-stream"
    )
```

**前端接收（JavaScript）：**
```javascript
const eventSource = new EventSource('/sse-events');

eventSource.addEventListener('start', (event) => {
    console.log('开始:', event.data);
});

eventSource.addEventListener('progress', (event) => {
    const data = JSON.parse(event.data);
    console.log('进度:', data.progress + '%');
});

eventSource.addEventListener('complete', (event) => {
    console.log('完成:', event.data);
    eventSource.close();
});
```

### 3.4 SSE 的消息 ID 和重连

```python
async def sse_with_id():
    """带消息 ID 的 SSE（支持断线重连）"""
    message_id = 0

    for i in range(100):
        message_id += 1

        # 发送消息 ID
        yield f"id: {message_id}\n"
        yield f"data: 消息 {i}\n\n"

        await asyncio.sleep(0.1)

@app.get("/sse-reconnect")
async def sse_reconnect_endpoint(last_event_id: str = None):
    """支持断线重连的 SSE 端点"""
    # 如果客户端提供了 last_event_id，从该位置继续
    start_id = int(last_event_id) if last_event_id else 0

    async def generate():
        # 设置重连时间（3秒）
        yield "retry: 3000\n\n"

        for i in range(start_id, 100):
            yield f"id: {i}\n"
            yield f"data: 消息 {i}\n\n"
            await asyncio.sleep(0.1)

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

**前端自动重连：**
```javascript
const eventSource = new EventSource('/sse-reconnect');

// EventSource 会自动重连，并发送 Last-Event-ID 头
eventSource.onmessage = (event) => {
    console.log('消息 ID:', event.lastEventId);
    console.log('消息内容:', event.data);
};
```

---

## 4. AI 流式输出的 SSE 封装

### 4.1 OpenAI + SSE

```python
async def openai_sse_stream(prompt: str):
    """OpenAI 流式输出 + SSE 格式"""
    try:
        # 发送开始事件
        yield "event: start\n"
        yield f"data: {json.dumps({'message': '开始生成'})}\n\n"

        # 流式调用 OpenAI
        stream = await client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )

        message_id = 0
        full_content = ""

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_content += content
                message_id += 1

                # 发送内容事件
                yield f"id: {message_id}\n"
                yield "event: content\n"
                yield f"data: {json.dumps({'content': content})}\n\n"

        # 发送完成事件
        yield "event: complete\n"
        yield f"data: {json.dumps({'full_content': full_content})}\n\n"

    except Exception as e:
        # 发送错误事件
        yield "event: error\n"
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.post("/ai-sse")
async def ai_sse_endpoint(prompt: str):
    return StreamingResponse(
        openai_sse_stream(prompt),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )
```

**前端接收：**
```javascript
const eventSource = new EventSource('/ai-sse?prompt=写一首诗');

eventSource.addEventListener('start', (event) => {
    console.log('开始生成');
});

eventSource.addEventListener('content', (event) => {
    const data = JSON.parse(event.data);
    document.getElementById('output').innerText += data.content;
});

eventSource.addEventListener('complete', (event) => {
    const data = JSON.parse(event.data);
    console.log('生成完成，完整内容:', data.full_content);
    eventSource.close();
});

eventSource.addEventListener('error', (event) => {
    const data = JSON.parse(event.data);
    console.error('错误:', data.error);
    eventSource.close();
});
```

### 4.2 Claude + SSE

```python
async def claude_sse_stream(prompt: str):
    """Claude 流式输出 + SSE 格式"""
    try:
        yield "event: start\n"
        yield "data: {\"message\": \"开始生成\"}\n\n"

        async with claude_client.messages.stream(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        ) as stream:
            message_id = 0

            async for text in stream.text_stream:
                message_id += 1

                yield f"id: {message_id}\n"
                yield "event: content\n"
                yield f"data: {json.dumps({'content': text})}\n\n"

            # 获取最终消息
            final_message = await stream.get_final_message()

            yield "event: complete\n"
            yield f"data: {json.dumps({
                'full_content': final_message.content[0].text,
                'usage': {
                    'input_tokens': final_message.usage.input_tokens,
                    'output_tokens': final_message.usage.output_tokens
                }
            })}\n\n"

    except Exception as e:
        yield "event: error\n"
        yield f"data: {json.dumps({'error': str(e)})}\n\n"
```

---

## 5. 生产级实现：错误处理和重试

### 5.1 完整的错误处理

```python
from typing import AsyncGenerator
import logging

logger = logging.getLogger(__name__)

async def robust_ai_stream(prompt: str) -> AsyncGenerator[str, None]:
    """生产级 AI 流式输出（带错误处理）"""
    max_retries = 3
    retry_count = 0

    while retry_count < max_retries:
        try:
            # 发送开始事件
            if retry_count == 0:
                yield "event: start\n"
                yield "data: {\"message\": \"开始生成\"}\n\n"
            else:
                yield "event: retry\n"
                yield f"data: {{\"attempt\": {retry_count + 1}}}\n\n"

            # 流式调用 API
            stream = await client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                timeout=30.0  # 设置超时
            )

            message_id = 0
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    message_id += 1

                    yield f"id: {message_id}\n"
                    yield "event: content\n"
                    yield f"data: {json.dumps({'content': content})}\n\n"

            # 成功完成
            yield "event: complete\n"
            yield "data: {\"message\": \"生成完成\"}\n\n"
            return

        except asyncio.TimeoutError:
            logger.error(f"超时，重试 {retry_count + 1}/{max_retries}")
            retry_count += 1
            if retry_count >= max_retries:
                yield "event: error\n"
                yield "data: {\"error\": \"请求超时，已达最大重试次数\"}\n\n"
                return
            await asyncio.sleep(1)  # 等待1秒后重试

        except Exception as e:
            logger.error(f"错误: {e}")
            yield "event: error\n"
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
            return
```

### 5.2 客户端断开连接的处理

```python
async def stream_with_disconnect_handling(prompt: str):
    """处理客户端断开连接"""
    try:
        stream = await client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    except GeneratorExit:
        # 客户端断开连接
        logger.info("客户端断开连接，停止生成")
        # 可以在这里记录日志、清理资源等

    except Exception as e:
        logger.error(f"流式生成错误: {e}")
        yield f"\n\n[错误: {e}]"

    finally:
        # 确保资源清理
        logger.info("流式生成结束，资源已清理")
```

### 5.3 速率限制和背压处理

```python
import asyncio
from collections import deque

async def stream_with_backpressure(prompt: str):
    """带背压处理的流式输出"""
    buffer = deque(maxlen=100)  # 缓冲区，最多100个 token

    async def producer():
        """生产者：从 API 获取数据"""
        stream = await client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content

                # 如果缓冲区满了，等待
                while len(buffer) >= buffer.maxlen:
                    await asyncio.sleep(0.01)

                buffer.append(content)

        buffer.append(None)  # 结束标记

    async def consumer():
        """消费者：发送数据到客户端"""
        while True:
            if buffer:
                item = buffer.popleft()
                if item is None:
                    break
                yield item
            else:
                await asyncio.sleep(0.01)

    # 启动生产者
    asyncio.create_task(producer())

    # 消费数据
    async for item in consumer():
        yield item
```

---

## 6. 实战示例：完整的 AI 对话系统

```python
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
import json
import logging

app = FastAPI()
logger = logging.getLogger(__name__)

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    model: str = "gpt-4"
    temperature: float = 0.7
    max_tokens: Optional[int] = None

async def stream_chat_completion(request: ChatRequest):
    """完整的流式对话系统"""
    try:
        # 1. 发送开始事件
        yield "event: start\n"
        yield f"data: {json.dumps({'model': request.model})}\n\n"

        # 2. 调用 OpenAI API
        stream = await client.chat.completions.create(
            model=request.model,
            messages=[msg.dict() for msg in request.messages],
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            stream=True
        )

        # 3. 流式返回内容
        message_id = 0
        full_content = ""
        finish_reason = None

        async for chunk in stream:
            choice = chunk.choices[0]

            # 内容块
            if choice.delta.content:
                content = choice.delta.content
                full_content += content
                message_id += 1

                yield f"id: {message_id}\n"
                yield "event: content\n"
                yield f"data: {json.dumps({'content': content})}\n\n"

            # 结束原因
            if choice.finish_reason:
                finish_reason = choice.finish_reason

        # 4. 发送完成事件
        yield "event: complete\n"
        yield f"data: {json.dumps({
            'full_content': full_content,
            'finish_reason': finish_reason,
            'message_count': message_id
        })}\n\n"

    except Exception as e:
        logger.error(f"流式对话错误: {e}")
        yield "event: error\n"
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """OpenAI 兼容的流式对话端点"""
    return StreamingResponse(
        stream_chat_completion(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

# 测试
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**前端完整示例（React）：**
```javascript
import { useState } from 'react';

function ChatComponent() {
    const [messages, setMessages] = useState([]);
    const [input, setInput] = useState('');
    const [streaming, setStreaming] = useState(false);

    const sendMessage = async () => {
        if (!input.trim()) return;

        const userMessage = { role: 'user', content: input };
        setMessages(prev => [...prev, userMessage]);
        setInput('');
        setStreaming(true);

        try {
            const response = await fetch('/v1/chat/completions', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    messages: [...messages, userMessage],
                    model: 'gpt-4'
                })
            });

            const reader = response.body.getReader();
            const decoder = new TextDecoder();
            let assistantMessage = { role: 'assistant', content: '' };
            setMessages(prev => [...prev, assistantMessage]);

            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                const chunk = decoder.decode(value);
                const lines = chunk.split('\n');

                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        const data = JSON.parse(line.slice(6));
                        if (data.content) {
                            assistantMessage.content += data.content;
                            setMessages(prev => [...prev.slice(0, -1), { ...assistantMessage }]);
                        }
                    }
                }
            }
        } catch (error) {
            console.error('Error:', error);
        } finally {
            setStreaming(false);
        }
    };

    return (
        <div>
            <div className="messages">
                {messages.map((msg, i) => (
                    <div key={i} className={msg.role}>
                        {msg.content}
                    </div>
                ))}
            </div>
            <input
                value={input}
                onChange={(e) => setInput(e.target.value)}
                onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
                disabled={streaming}
            />
            <button onClick={sendMessage} disabled={streaming}>
                {streaming ? '生成中...' : '发送'}
            </button>
        </div>
    );
}
```

---

## 总结

**AI 流式输出集成的核心要点：**

1. **OpenAI 集成**：`stream=True` + `async for chunk`
2. **Claude 集成**：`messages.stream()` + `async with`
3. **SSE 协议**：`text/event-stream` + 事件类型
4. **错误处理**：重试机制 + 超时处理
5. **生产实践**：背压处理 + 资源清理

**记住：** AI 流式输出是流式响应的最重要应用场景，掌握它就能构建现代化的 AI 对话系统！
