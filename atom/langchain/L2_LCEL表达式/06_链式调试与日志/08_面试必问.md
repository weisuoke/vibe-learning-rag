# 面试必问：链式调试与日志

## 概述

本文档整理了链式调试与日志相关的常见面试问题，包括基础、进阶和高级问题。

**难度标识**：
- ⭐ 基础问题
- ⭐⭐ 进阶问题
- ⭐⭐⭐ 高级问题

---

## 问题1：如何调试LCEL链的执行过程？⭐

### 标准答案

使用 `astream_events()` v2 方法查看链的执行过程：

```python
async for event in chain.astream_events(input, version="v2"):
    print(f"{event['event']}: {event['name']}")
```

### 优秀答案

**分层回答**：

1. **开发阶段**：使用 `astream_events()` v2
   - 查看所有事件
   - 过滤特定类型：`include_types=["chat_model"]`
   - 实时查看LLM输出

2. **集成阶段**：使用 `BaseCallbackHandler`
   - 自定义日志格式
   - 记录到文件或数据库
   - 追踪性能指标

3. **生产阶段**：使用 LangSmith 或 OpenTelemetry
   - 全链路追踪
   - 可视化仪表板
   - 告警和通知

**代码示例**：
```python
# 开发：查看LLM输出
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_stream":
        print(event["data"]["chunk"].content, end="")

# 生产：LangSmith
os.environ["LANGCHAIN_TRACING_V2"] = "true"
result = chain.invoke(input)
```

### 追问

**Q**: `astream_events()` 有性能开销吗？

**A**: 有10-20%的性能开销，主要来自事件序列化和内存分配。生产环境建议移除或使用采样。

---

## 问题2：如何实现自定义日志记录？⭐⭐

### 标准答案

继承 `BaseCallbackHandler` 并实现回调方法：

```python
from langchain_core.callbacks import BaseCallbackHandler

class MyLogHandler(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        logger.info(f"Chain started: {inputs}")

chain.invoke(input, config={"callbacks": [MyLogHandler()]})
```

### 优秀答案

**完整实现**：

1. **文件日志**：
```python
class FileLogHandler(BaseCallbackHandler):
    def __init__(self, log_file):
        self.log_file = log_file

    def _write_log(self, event_type, data):
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "event": event_type,
            "data": data
        }
        with open(self.log_file, "a") as f:
            f.write(json.dumps(log_entry) + "\n")

    def on_chain_start(self, serialized, inputs, **kwargs):
        self._write_log("chain_start", {"inputs": inputs})
```

2. **异步日志**（避免阻塞）：
```python
from langchain_core.callbacks import AsyncCallbackHandler

class AsyncLogHandler(AsyncCallbackHandler):
    async def on_chain_start(self, serialized, inputs, **kwargs):
        await self._write_to_database(inputs)
```

3. **性能追踪**：
```python
class PerformanceHandler(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        self.start_time = time.time()

    def on_chain_end(self, outputs, **kwargs):
        duration = time.time() - self.start_time
        logger.info(f"Duration: {duration:.2f}s")
```

### 追问

**Q**: 同步回调和异步回调有什么区别？

**A**: 
- 同步回调会阻塞链执行
- 异步回调不会阻塞，适合慢速操作（IO、网络）
- 使用异步回调需要用 `ainvoke()` 而不是 `invoke()`

---

## 问题3：LangSmith和OpenTelemetry有什么区别？⭐⭐

### 标准答案

| 特性 | LangSmith | OpenTelemetry |
|------|-----------|---------------|
| **集成难度** | 简单（环境变量） | 中等（需要配置） |
| **成本** | 付费 | 免费（开源） |
| **数据控制** | 数据在LangSmith | 数据在自己的系统 |

### 优秀答案

**详细对比**：

1. **LangSmith**：
   - **优势**：零代码集成，可视化仪表板，成本追踪
   - **劣势**：付费，数据在第三方
   - **适用**：快速上手，小团队

2. **OpenTelemetry**：
   - **优势**：开源免费，数据自主，灵活可定制
   - **劣势**：需要配置，需要自己搭建监控系统
   - **适用**：数据隐私，已有监控系统，大团队

**选择建议**：
- 快速上手 → LangSmith
- 数据隐私 → OpenTelemetry
- 已有监控系统 → OpenTelemetry
- 多语言/多框架 → OpenTelemetry

**代码对比**：
```python
# LangSmith（最简单）
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your_key"
result = chain.invoke(input)

# OpenTelemetry（灵活）
from opentelemetry import trace
tracer = trace.get_tracer(__name__)
with tracer.start_as_current_span("chain"):
    result = chain.invoke(input)
```

### 追问

**Q**: 可以同时使用LangSmith和OpenTelemetry吗？

**A**: 可以。LangSmith通过环境变量自动追踪，OpenTelemetry通过代码手动追踪，两者不冲突。

---

## 问题4：如何在生产环境中减少监控成本？⭐⭐⭐

### 标准答案

使用采样策略，只追踪部分请求：

```python
import random
if random.random() < 0.1:  # 10%采样
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
```

### 优秀答案

**完整采样策略**：

1. **条件采样**：
```python
def should_trace(error=None, duration=None, sample_rate=0.1):
    # 总是追踪错误
    if error:
        return True
    
    # 总是追踪慢请求（>5s）
    if duration and duration > 5.0:
        return True
    
    # 随机采样
    return random.random() < sample_rate
```

2. **分层采样**：
```python
# 开发：100%
if env == "development":
    sample_rate = 1.0

# 预发布：10%
elif env == "staging":
    sample_rate = 0.1

# 生产：1%
elif env == "production":
    sample_rate = 0.01
```

3. **数据分级**：
```python
# 级别1：关键指标（100%）
- 延迟、错误率、吞吐量

# 级别2：重要信息（10-50%）
- Token使用、成本、用户ID

# 级别3：详细数据（1-10%）
- 完整输入输出、中间结果
```

4. **成本优化**：
- 限制日志大小：`str(output)[:200]`
- 使用异步回调：避免阻塞
- 定期清理旧日志

### 追问

**Q**: 如何决定采样率？

**A**: 
- 考虑因素：流量大小、预算、问题频率
- 建议：正常请求1-10%，错误和慢请求100%
- 动态调整：根据实际情况调整采样率

---

## 问题5：如何追踪RAG链的检索结果？⭐⭐

### 标准答案

监听 `on_retriever_end` 事件：

```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_retriever_end":
        docs = event["data"]["output"]
        print(f"Retrieved {len(docs)} documents")
```

### 优秀答案

**完整追踪方案**：

1. **查看检索查询**：
```python
if event["event"] == "on_retriever_start":
    query = event["data"]["input"]
    print(f"Query: {query}")
```

2. **查看检索结果**：
```python
if event["event"] == "on_retriever_end":
    docs = event["data"]["output"]
    print(f"Retrieved {len(docs)} documents:")
    for i, doc in enumerate(docs):
        print(f"{i+1}. {doc.page_content[:100]}")
        print(f"   Score: {doc.metadata.get('score', 'N/A')}")
```

3. **查看发送给LLM的prompt**：
```python
if event["event"] == "on_chat_model_start":
    messages = event["data"]["input"]["messages"]
    print(f"Prompt to LLM:")
    for msg in messages:
        print(f"{msg.type}: {msg.content}")
```

4. **完整RAG调试**：
```python
async def debug_rag_chain():
    async for event in rag_chain.astream_events(query, version="v2"):
        kind = event["event"]
        
        # 检索
        if kind == "on_retriever_end":
            docs = event["data"]["output"]
            print(f"\n=== Retrieved {len(docs)} docs ===")
            for doc in docs:
                print(f"- {doc.page_content[:80]}")
        
        # Prompt
        if kind == "on_chat_model_start":
            messages = event["data"]["input"]["messages"]
            print(f"\n=== Prompt ===")
            print(messages[0].content[:200])
        
        # LLM输出
        if kind == "on_chat_model_stream":
            print(event["data"]["chunk"].content, end="")
```

### 追问

**Q**: 如何评估检索质量？

**A**: 
- 查看检索分数（score）
- 检查文档相关性
- 统计检索召回率
- 使用LangSmith的评估功能

---

## 问题6：如何处理监控中的敏感信息？⭐⭐⭐

### 标准答案

在日志中过滤敏感信息：

```python
def sanitize(data):
    # 移除敏感字段
    if "password" in data:
        data["password"] = "***"
    return data
```

### 优秀答案

**完整安全策略**：

1. **数据脱敏**：
```python
import re

def sanitize_data(data):
    """脱敏敏感信息"""
    if isinstance(data, str):
        # 脱敏邮箱
        data = re.sub(r'\b[\w.-]+@[\w.-]+\.\w+\b', '***@***.***', data)
        # 脱敏手机号
        data = re.sub(r'\b\d{11}\b', '***********', data)
        # 脱敏身份证
        data = re.sub(r'\b\d{18}\b', '******************', data)
    elif isinstance(data, dict):
        # 脱敏字典中的敏感字段
        sensitive_keys = ['password', 'token', 'api_key', 'secret']
        for key in sensitive_keys:
            if key in data:
                data[key] = '***'
    return data
```

2. **回调中脱敏**：
```python
class SecureLogHandler(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        # 脱敏后记录
        sanitized_inputs = sanitize_data(inputs)
        logger.info(f"Inputs: {sanitized_inputs}")
```

3. **LangSmith脱敏**：
```python
# 使用metadata而不是直接记录敏感数据
config = {
    "metadata": {
        "user_id": "user_123",  # 使用ID而不是真实信息
        "session_id": "session_abc"
    }
}
result = chain.invoke(input, config=config)
```

4. **OpenTelemetry脱敏**：
```python
with tracer.start_as_current_span("chain") as span:
    # 只记录非敏感属性
    span.set_attribute("user_id", user_id)
    span.set_attribute("request_type", "query")
    # 不记录完整输入
```

5. **环境隔离**：
```python
# 开发环境：详细日志
if env == "development":
    log_full_data()

# 生产环境：脱敏日志
elif env == "production":
    log_sanitized_data()
```

### 追问

**Q**: 如何平衡安全性和可调试性？

**A**: 
- 开发环境：完整日志
- 生产环境：脱敏日志 + 采样
- 使用加密存储敏感日志
- 设置日志访问权限

---

## 问题7：如何设计生产环境的告警规则？⭐⭐⭐

### 标准答案

设置关键指标的阈值：

```python
if latency > 5.0:
    send_alert("High latency")
if error_rate > 0.01:
    send_alert("High error rate")
```

### 优秀答案

**完整告警策略**：

1. **告警规则设计**：
```python
@dataclass
class AlertRule:
    name: str
    condition: Callable
    message: str
    severity: str  # "info", "warning", "error", "critical"
    cooldown: int  # 冷却时间（秒）

# 规则1：高延迟
AlertRule(
    name="high_latency",
    condition=lambda m: m["p95_latency"] > 5.0,
    message="P95 latency > 5s",
    severity="warning",
    cooldown=300  # 5分钟内不重复告警
)

# 规则2：错误率
AlertRule(
    name="high_error_rate",
    condition=lambda m: m["error_rate"] > 0.01,
    message="Error rate > 1%",
    severity="error",
    cooldown=600
)

# 规则3：成本
AlertRule(
    name="high_cost",
    condition=lambda m: m["hourly_cost"] > 10.0,
    message="Hourly cost > $10",
    severity="warning",
    cooldown=3600
)
```

2. **告警分级**：
```python
# Critical：立即处理
- 错误率 > 5%
- P99延迟 > 30s
- 服务不可用

# Error：尽快处理
- 错误率 > 1%
- P95延迟 > 10s
- 成本超预算50%

# Warning：关注
- P95延迟 > 5s
- 成本超预算20%
- Token使用异常

# Info：记录
- 新功能上线
- 配置变更
```

3. **告警通知**：
```python
class AlertManager:
    def send_alert(self, alert):
        if alert["severity"] == "critical":
            # 电话 + 短信 + Slack
            self.call_oncall()
            self.send_sms()
            self.send_slack()
        elif alert["severity"] == "error":
            # 短信 + Slack
            self.send_sms()
            self.send_slack()
        elif alert["severity"] == "warning":
            # Slack
            self.send_slack()
        else:
            # 邮件
            self.send_email()
```

4. **告警抑制**：
```python
class AlertSuppressor:
    def __init__(self):
        self.last_alert_time = {}

    def should_alert(self, rule_name, cooldown):
        now = time.time()
        last_time = self.last_alert_time.get(rule_name, 0)
        
        if now - last_time > cooldown:
            self.last_alert_time[rule_name] = now
            return True
        return False
```

### 追问

**Q**: 如何避免告警疲劳？

**A**: 
- 设置合理的阈值
- 使用冷却时间
- 告警分级
- 定期审查和调整规则

---

## 快速复习卡片

### 卡片1：调试工具选择

| 阶段 | 工具 | 用途 |
|------|------|------|
| 开发 | astream_events | 理解流程 |
| 集成 | BaseCallbackHandler | 自定义日志 |
| 生产 | LangSmith/OpenTelemetry | 全链路追踪 |

---

### 卡片2：性能开销

- astream_events: 10-20%
- BaseCallbackHandler: 5-10%
- LangSmith/OpenTelemetry: <5%

---

### 卡片3：采样策略

- 正常请求: 1-10%
- 错误请求: 100%
- 慢请求: 100%

---

### 卡片4：关键指标

- 延迟（P50, P95, P99）
- 错误率
- 吞吐量
- Token使用
- 成本

---

### 卡片5：告警阈值

- 延迟: P95 > 5s
- 错误率: > 1%
- 成本: 超预算20%

---

## 面试技巧

### 回答结构

1. **直接回答**：先给出核心答案
2. **分层展开**：从简单到复杂
3. **代码示例**：提供实际代码
4. **对比分析**：对比不同方案
5. **最佳实践**：给出建议

### 加分项

- 提到性能开销
- 提到采样策略
- 提到安全性
- 提到成本优化
- 提到实际经验

### 避免

- 只说理论，不给代码
- 只说一种方案
- 忽略生产环境考虑
- 忽略成本和性能

---

## 总结

**核心知识点**：

1. 三层监控架构
2. 事件流和回调处理器
3. LangSmith vs OpenTelemetry
4. 采样策略
5. 告警规则
6. 安全性

**面试准备**：

- 理解原理
- 熟悉代码
- 了解最佳实践
- 准备实际案例

---

**版本信息**
- LangChain: v0.3+ (2025-2026)
- Python: 3.13+
- 最后更新: 2026-02-20
