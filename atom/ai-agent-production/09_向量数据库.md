# 向量数据库深度

**知识点数**: 5个 | **学习时长**: 0.5周 | **优先级**: P2

---

## 概览

**适合人群**: RAG方向、检索优化

**核心内容**: Pinecone → Weaviate → Qdrant → 索引策略 → 混合检索

---

## 核心知识点

### 01_Pinecone高级特性

**核心概念**: 企业级向量检索。

**快速开始**:
```python
import pinecone

# 初始化
pinecone.init(api_key="your-api-key", environment="us-west1-gcp")

# 创建索引
pinecone.create_index(
    name="agent-memory",
    dimension=1536,
    metric="cosine"
)

# 连接索引
index = pinecone.Index("agent-memory")

# 插入向量
index.upsert(vectors=[
    ("id1", [0.1] * 1536, {"text": "content", "user_id": "user1"}),
    ("id2", [0.2] * 1536, {"text": "content2", "user_id": "user1"})
])

# 查询
results = index.query(
    vector=[0.15] * 1536,
    top_k=5,
    filter={"user_id": "user1"}
)
```

**命名空间管理**:
```python
# 使用命名空间隔离数据
index.upsert(
    vectors=[("id1", [0.1] * 1536, {"text": "content"})],
    namespace="user1"
)

# 查询特定命名空间
results = index.query(
    vector=[0.15] * 1536,
    top_k=5,
    namespace="user1"
)
```

---

### 02_Weaviate集成

**核心概念**: 知识图谱 + 向量检索。

**快速开始**:
```python
import weaviate

# 连接
client = weaviate.Client("http://localhost:8080")

# 创建Schema
schema = {
    "class": "Document",
    "vectorizer": "text2vec-openai",
    "properties": [
        {"name": "content", "dataType": ["text"]},
        {"name": "author", "dataType": ["string"]},
        {"name": "category", "dataType": ["string"]}
    ]
}
client.schema.create_class(schema)

# 插入数据
client.data_object.create(
    data_object={
        "content": "AI is transforming the world",
        "author": "John Doe",
        "category": "AI"
    },
    class_name="Document"
)

# 向量检索
result = client.query.get("Document", ["content", "author"])\
    .with_near_text({"concepts": ["artificial intelligence"]})\
    .with_limit(5)\
    .do()
```

**GraphQL查询**:
```python
# 复杂查询
result = client.query.get("Document", ["content", "author"])\
    .with_near_text({"concepts": ["AI"]})\
    .with_where({
        "path": ["category"],
        "operator": "Equal",
        "valueString": "AI"
    })\
    .with_limit(5)\
    .do()
```

---

### 03_Qdrant性能优化

**核心概念**: 高性能向量检索。

**快速开始**:
```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# 连接
client = QdrantClient("localhost", port=6333)

# 创建集合
client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
)

# 插入数据
client.upsert(
    collection_name="documents",
    points=[
        PointStruct(
            id=1,
            vector=[0.1] * 1536,
            payload={"text": "content", "category": "AI"}
        )
    ]
)

# 搜索
results = client.search(
    collection_name="documents",
    query_vector=[0.15] * 1536,
    limit=5,
    query_filter={
        "must": [
            {"key": "category", "match": {"value": "AI"}}
        ]
    }
)
```

**量化优化**:
```python
from qdrant_client.models import ScalarQuantization, ScalarQuantizationConfig

# 创建带量化的集合
client.create_collection(
    collection_name="documents",
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
    quantization_config=ScalarQuantizationConfig(
        scalar=ScalarQuantization(
            type="int8",
            quantile=0.99,
            always_ram=True
        )
    )
)
```

---

### 04_向量索引策略

**核心概念**: 优化检索性能。

**HNSW算法**:
```python
# Qdrant HNSW配置
from qdrant_client.models import HnswConfigDiff

client.update_collection(
    collection_name="documents",
    hnsw_config=HnswConfigDiff(
        m=16,  # 每层的连接数
        ef_construct=100,  # 构建时的搜索深度
        full_scan_threshold=10000  # 全扫描阈值
    )
)

# 搜索时配置
results = client.search(
    collection_name="documents",
    query_vector=[0.15] * 1536,
    limit=5,
    search_params={"hnsw_ef": 128}  # 搜索时的深度
)
```

**IVF索引**:
```python
# Faiss IVF索引
import faiss
import numpy as np

# 训练数据
train_data = np.random.random((10000, 1536)).astype('float32')

# 创建IVF索引
nlist = 100  # 聚类中心数
quantizer = faiss.IndexFlatL2(1536)
index = faiss.IndexIVFFlat(quantizer, 1536, nlist)

# 训练
index.train(train_data)

# 添加向量
index.add(train_data)

# 搜索
index.nprobe = 10  # 搜索的聚类数
D, I = index.search(np.random.random((1, 1536)).astype('float32'), 5)
```

---

### 05_混合检索实现

**核心概念**: 向量检索 + 关键词检索。

**实战代码**:
```python
from typing import List, Dict
import numpy as np

class HybridRetriever:
    def __init__(self, vector_store, keyword_index):
        self.vector_store = vector_store
        self.keyword_index = keyword_index

    def retrieve(self, query: str, top_k: int = 10) -> List[Dict]:
        # 1. 向量检索
        vector_results = self.vector_store.search(query, top_k=top_k)

        # 2. 关键词检索
        keyword_results = self.keyword_index.search(query, top_k=top_k)

        # 3. 合并结果
        combined = self.merge_results(vector_results, keyword_results)

        # 4. 重排序
        reranked = self.rerank(query, combined, top_k=top_k)

        return reranked

    def merge_results(self, vector_results, keyword_results):
        """合并两种检索结果"""
        results = {}

        # 向量检索结果（权重0.7）
        for i, result in enumerate(vector_results):
            doc_id = result['id']
            score = result['score'] * 0.7
            results[doc_id] = {
                'doc': result,
                'score': score,
                'vector_rank': i
            }

        # 关键词检索结果（权重0.3）
        for i, result in enumerate(keyword_results):
            doc_id = result['id']
            score = result['score'] * 0.3

            if doc_id in results:
                results[doc_id]['score'] += score
                results[doc_id]['keyword_rank'] = i
            else:
                results[doc_id] = {
                    'doc': result,
                    'score': score,
                    'keyword_rank': i
                }

        return list(results.values())

    def rerank(self, query: str, results: List[Dict], top_k: int):
        """使用交叉编码器重排序"""
        from sentence_transformers import CrossEncoder

        model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

        # 准备输入
        pairs = [[query, r['doc']['text']] for r in results]

        # 计算相关性分数
        scores = model.predict(pairs)

        # 按分数排序
        for i, result in enumerate(results):
            result['rerank_score'] = scores[i]

        results.sort(key=lambda x: x['rerank_score'], reverse=True)

        return results[:top_k]
```

**RRF (Reciprocal Rank Fusion)**:
```python
def reciprocal_rank_fusion(results_list: List[List[Dict]], k: int = 60):
    """RRF算法合并多个检索结果"""
    scores = {}

    for results in results_list:
        for rank, result in enumerate(results):
            doc_id = result['id']
            score = 1 / (k + rank + 1)

            if doc_id in scores:
                scores[doc_id]['score'] += score
            else:
                scores[doc_id] = {
                    'doc': result,
                    'score': score
                }

    # 按分数排序
    ranked = sorted(scores.values(), key=lambda x: x['score'], reverse=True)
    return ranked
```

---

## 核心20%（2个知识点）

1. ⭐⭐⭐ **Pinecone高级特性**
2. ⭐⭐⭐ **混合检索实现**

---

## 实战项目

### 项目: 高性能RAG检索

**目标**: 构建高性能的混合检索系统。

**架构**:
```
查询 → Embedding → 向量检索 (Pinecone)
                 → 关键词检索 (Elasticsearch)
                 → 合并 → 重排序 → 返回
```

**性能指标**:
- 检索延迟 < 100ms
- 准确率 > 90%
- 支持10M+文档

---

## 学习资源

- [Pinecone文档](https://docs.pinecone.io/)
- [Weaviate文档](https://weaviate.io/developers/weaviate)
- [Qdrant文档](https://qdrant.tech/documentation/)
- [Faiss文档](https://github.com/facebookresearch/faiss)

---

## 常见误区

1. ❌ "向量检索一定比关键词好" → ✅ 混合检索更优
2. ❌ "维度越高越好" → ✅ 权衡精度和性能
3. ❌ "不需要重排序" → ✅ 重排序显著提升效果

---

**版本**: v1.0 | **最后更新**: 2026-02-12
