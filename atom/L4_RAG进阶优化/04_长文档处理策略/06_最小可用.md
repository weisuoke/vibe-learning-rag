# 最小可用知识

掌握以下内容，就能开始处理长文档场景的 RAG 开发。

---

## 核心原则

**长文档处理的本质**：在有限的 Context Window 内，完整理解超出限制的文档。

**三种策略的选择**：
- **分层索引**：需要快速定位 → 构建树形索引
- **摘要链**：需要全文理解 → 递归总结压缩
- **Map-Reduce**：可以分解为独立子问题 → 并行处理聚合

---

## 最小可用1：判断是否需要长文档处理

### 何时需要？

```python
def need_long_document_processing(document, context_window=128000):
    """判断是否需要长文档处理"""

    # 估算文档的 token 数
    estimated_tokens = len(document) / 0.75  # 平均每个 token 0.75 个字符

    # 如果文档超过 Context Window 的 80%，就需要长文档处理
    if estimated_tokens > context_window * 0.8:
        return True

    return False
```

### 快速判断表

| 文档类型 | 页数 | 是否需要 | 推荐策略 |
|---------|------|---------|---------|
| 技术文档 | < 50 页 | ❌ 不需要 | 普通分块即可 |
| 技术文档 | 50-200 页 | ✅ 需要 | 分层索引 |
| 学术论文 | < 20 页 | ❌ 不需要 | 普通分块即可 |
| 学术论文 | 20-100 页 | ✅ 需要 | 分层索引 + 摘要链 |
| 法律文档 | > 100 页 | ✅ 需要 | 分层索引 |
| 多文档问答 | 任意 | ✅ 需要 | Map-Reduce |

---

## 最小可用2：分层索引的最简实现

### 核心代码（50行）

```python
from typing import List, Dict

class SimpleHierarchicalIndex:
    """最简分层索引"""

    def __init__(self, embedding_func):
        self.embedding_func = embedding_func
        self.chapters = []  # 章节列表

    def build(self, document: str, chapter_delimiter: str = "\n\n## "):
        """构建索引"""
        # 按章节分割
        parts = document.split(chapter_delimiter)
        self.chapters = []

        for i, part in enumerate(parts):
            if not part.strip():
                continue

            # 提取章节标题（第一行）
            lines = part.split("\n", 1)
            title = lines[0].strip()
            content = lines[1] if len(lines) > 1 else ""

            # 生成章节摘要（简化版：取前200字）
            summary = content[:200] + "..." if len(content) > 200 else content

            self.chapters.append({
                "id": i,
                "title": title,
                "summary": summary,
                "content": content,
                "embedding": self.embedding_func(summary)
            })

    def search(self, query: str, top_k: int = 3):
        """检索"""
        query_embedding = self.embedding_func(query)

        # 计算相似度
        scores = []
        for chapter in self.chapters:
            score = self._cosine_similarity(query_embedding, chapter["embedding"])
            scores.append((chapter, score))

        # 返回 Top-K
        top_results = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]
        return [chapter for chapter, _ in top_results]

    def _cosine_similarity(self, vec1, vec2):
        import numpy as np
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

### 使用示例

```python
# 1. 准备文档（Markdown 格式，用 ## 分隔章节）
document = """
## 第1章：神经网络基础
神经网络是由大量神经元组成的计算模型...

## 第2章：卷积神经网络
CNN 专门用于处理图像数据...

## 第3章：循环神经网络
RNN 用于处理序列数据...
"""

# 2. 构建索引
index = SimpleHierarchicalIndex(embedding_func=get_embedding)
index.build(document)

# 3. 检索
results = index.search("什么是 RNN？", top_k=2)
for chapter in results:
    print(f"章节: {chapter['title']}")
    print(f"内容: {chapter['content'][:100]}...")
```

**这就够了！** 这个最简实现已经可以处理大部分长文档场景。

---

## 最小可用3：摘要链的最简实现

### 核心代码（40行）

```python
class SimpleSummaryChain:
    """最简摘要链"""

    def __init__(self, summarize_func):
        self.summarize_func = summarize_func
        self.layers = []

    def build(self, document: str, chunk_size: int = 1000):
        """构建摘要链"""
        # 第1层：分块
        chunks = [document[i:i+chunk_size] for i in range(0, len(document), chunk_size)]
        self.layers = [chunks]

        # 递归总结
        current_layer = chunks
        while len(current_layer) > 1:
            next_layer = []

            # 每5个块总结一次
            for i in range(0, len(current_layer), 5):
                batch = current_layer[i:i+5]
                summary = self.summarize_func(batch)
                next_layer.append(summary)

            self.layers.append(next_layer)
            current_layer = next_layer

    def get_summary(self, level: int = -1):
        """获取指定层级的摘要"""
        if level == -1:
            # 返回最顶层（全文摘要）
            return self.layers[-1][0]
        return self.layers[level]
```

### 使用示例

```python
# 1. 定义总结函数
def summarize(texts):
    combined = "\n\n".join(texts)
    prompt = f"请总结以下内容：\n\n{combined}"
    return llm(prompt)

# 2. 构建摘要链
chain = SimpleSummaryChain(summarize_func=summarize)
chain.build(long_document, chunk_size=1000)

# 3. 获取全文摘要
full_summary = chain.get_summary()
print(f"全文摘要: {full_summary}")

# 4. 获取详细摘要（第1层）
detailed_summaries = chain.get_summary(level=1)
for i, summary in enumerate(detailed_summaries):
    print(f"摘要 {i+1}: {summary}")
```

**这就够了！** 这个最简实现已经可以处理任意长度的文档。

---

## 最小可用4：Map-Reduce 的最简实现

### 核心代码（30行）

```python
from concurrent.futures import ThreadPoolExecutor

class SimpleMapReduce:
    """最简 Map-Reduce"""

    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers

    def process(self, documents, map_func, reduce_func):
        """执行 Map-Reduce"""

        # Map 阶段：并行处理
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            map_results = list(executor.map(map_func, documents))

        # Reduce 阶段：聚合结果
        final_result = reduce_func(map_results)

        return final_result
```

### 使用示例

```python
# 1. 准备文档
chapters = ["第1章...", "第2章...", "第3章..."]

# 2. 定义 Map 和 Reduce 函数
def map_summarize(chapter):
    return llm(f"总结这一章：{chapter}")

def reduce_aggregate(summaries):
    combined = "\n".join(summaries)
    return llm(f"汇总以下摘要：{combined}")

# 3. 执行 Map-Reduce
mr = SimpleMapReduce(max_workers=5)
result = mr.process(chapters, map_summarize, reduce_aggregate)
print(f"最终结果: {result}")
```

**这就够了！** 这个最简实现已经可以并行处理多个文档。

---

## 最小可用5：三种策略的组合使用

### 实战场景：学术论文问答

```python
def answer_paper_question(paper, question):
    """回答学术论文问题"""

    # 第1步：判断问题类型
    if "每章" in question or "各章" in question:
        # 可以分解为独立子问题 → 用 Map-Reduce
        return use_map_reduce(paper, question)

    elif "核心创新" in question or "主要贡献" in question:
        # 需要全文理解 → 用摘要链
        return use_summary_chain(paper, question)

    else:
        # 需要快速定位 → 用分层索引
        return use_hierarchical_index(paper, question)


def use_map_reduce(paper, question):
    """使用 Map-Reduce"""
    chapters = split_into_chapters(paper)

    def map_func(chapter):
        return llm(f"根据这一章回答问题：\n\n{chapter}\n\n问题：{question}")

    def reduce_func(answers):
        return llm(f"汇总以下答案：\n\n{chr(10).join(answers)}")

    mr = SimpleMapReduce()
    return mr.process(chapters, map_func, reduce_func)


def use_summary_chain(paper, question):
    """使用摘要链"""
    chain = SimpleSummaryChain(summarize_func=llm_summarize)
    chain.build(paper)

    # 先用全文摘要回答
    full_summary = chain.get_summary()
    prompt = f"根据以下摘要回答问题：\n\n{full_summary}\n\n问题：{question}"
    return llm(prompt)


def use_hierarchical_index(paper, question):
    """使用分层索引"""
    index = SimpleHierarchicalIndex(embedding_func=get_embedding)
    index.build(paper)

    # 检索相关章节
    relevant_chapters = index.search(question, top_k=2)

    # 用相关章节回答
    context = "\n\n".join([c["content"] for c in relevant_chapters])
    prompt = f"根据以下内容回答问题：\n\n{context}\n\n问题：{question}"
    return llm(prompt)
```

---

## 最小可用总结

### 三种策略的快速选择

```python
def choose_strategy(question, document):
    """快速选择策略"""

    # 判断1：问题是否可以分解？
    if can_decompose(question):
        return "Map-Reduce"

    # 判断2：是否需要全文理解？
    if need_full_understanding(question):
        return "摘要链"

    # 判断3：是否需要快速定位？
    return "分层索引"


def can_decompose(question):
    """判断问题是否可以分解"""
    keywords = ["每章", "各章", "每个", "分别", "对比"]
    return any(kw in question for kw in keywords)


def need_full_understanding(question):
    """判断是否需要全文理解"""
    keywords = ["核心", "主要", "整体", "总体", "全文"]
    return any(kw in question for kw in keywords)
```

### 快速决策表

| 问题类型 | 关键词 | 推荐策略 | 原因 |
|---------|--------|---------|------|
| "每章的核心观点是什么？" | 每章、各章 | Map-Reduce | 可以分解为独立子问题 |
| "这篇论文的核心创新是什么？" | 核心、主要 | 摘要链 | 需要全文理解 |
| "如何配置网络策略？" | 如何、配置 | 分层索引 | 需要快速定位 |
| "对比3篇论文的方法" | 对比、分别 | Map-Reduce | 可以分解为独立子问题 |
| "论文的整体逻辑结构" | 整体、结构 | 摘要链 | 需要全文理解 |

---

## 实战检查清单

### 开始前检查

- [ ] 文档是否超过 Context Window 的 80%？
- [ ] 文档是否有清晰的结构（章节、段落）？
- [ ] 问题类型是什么（定位、理解、分解）？

### 实现时检查

- [ ] 是否选择了正确的策略？
- [ ] 是否实现了最简版本（不要过度设计）？
- [ ] 是否测试了边界情况（空文档、单章节）？

### 完成后检查

- [ ] 是否能正确回答用户问题？
- [ ] 是否在 Context Window 限制内？
- [ ] 是否有明显的性能问题？

---

## 常见错误与修正

### 错误1：过度设计

```python
# ❌ 错误：实现了复杂的多层索引
class ComplexHierarchicalIndex:
    def __init__(self):
        self.root = Node()
        self.levels = []
        self.cache = {}
        # ... 100+ 行代码

# ✅ 正确：从最简版本开始
class SimpleHierarchicalIndex:
    def __init__(self, embedding_func):
        self.embedding_func = embedding_func
        self.chapters = []
    # ... 50 行代码
```

### 错误2：选错策略

```python
# ❌ 错误：用 Map-Reduce 处理需要全文理解的问题
question = "这篇论文的核心创新是什么？"
# 用 Map-Reduce 会丢失全局视角

# ✅ 正确：用摘要链
chain = SimpleSummaryChain(summarize_func=llm_summarize)
chain.build(paper)
full_summary = chain.get_summary()
```

### 错误3：忽略文档结构

```python
# ❌ 错误：直接按字符数分块
chunks = [document[i:i+1000] for i in range(0, len(document), 1000)]

# ✅ 正确：按章节分割
chapters = document.split("\n\n## ")
```

---

## 下一步学习

掌握了这些最小可用知识后，你可以：

1. **处理 90% 的长文档场景**
   - 学术论文问答
   - 技术文档检索
   - 多文档综合问答

2. **根据需要深入学习**
   - 分层索引的高级优化（剪枝、缓存）
   - 摘要链的质量提升（更好的总结 Prompt）
   - Map-Reduce 的性能优化（异步、错误处理）

3. **组合使用策略**
   - 分层索引 + 摘要链
   - Map-Reduce + 摘要链
   - 三种策略混合

---

**记住**：
- 从最简版本开始
- 根据问题类型选择策略
- 不要过度设计
- 先让它工作，再优化

---

**下一步：** [07_双重类比](./07_双重类比.md) - 用前端和日常生活类比理解长文档处理
