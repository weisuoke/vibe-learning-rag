# 限流中间件 - 化骨绵掌

> 10个2分钟知识卡片，系统掌握限流中间件

---

## 卡片1：限流的本质

**一句话：** 限流是在单位时间内限制操作次数，本质是时间与资源的权衡。

**举例：**
```python
# 最简单的限流
count = 0
if count < 10:  # 限制10次
    count += 1
    allow()
else:
    reject()
```

**应用：** 在 AI Agent 中，限流用于控制 LLM API 调用频率，防止成本失控和服务过载。

---

## 卡片2：固定窗口算法

**一句话：** 在固定时间窗口内计数，窗口重置时清零，实现简单但有窗口边界问题。

**举例：**
```python
class FixedWindow:
    def __init__(self, limit, window):
        self.limit = limit
        self.window = window
        self.count = 0
        self.window_start = time.time()

    def acquire(self):
        now = time.time()
        if now - self.window_start >= self.window:
            self.count = 0  # 重置
            self.window_start = now

        if self.count < self.limit:
            self.count += 1
            return True
        return False
```

**问题：** 窗口边界可能出现双倍流量（0:59秒100个 + 1:00秒100个 = 1秒内200个）。

**应用：** 适合粗粒度限流（每天、每月），不适合细粒度限流（每秒、每分钟）。

---

## 卡片3：滑动窗口算法

**一句话：** 记录每个请求的时间戳，统计最近N秒内的请求数，精确但内存占用大。

**举例：**
```python
class SlidingWindow:
    def __init__(self, limit, window):
        self.limit = limit
        self.window = window
        self.requests = []  # 存储时间戳

    def acquire(self):
        now = time.time()
        # 移除过期请求
        self.requests = [t for t in self.requests if now - t < self.window]

        if len(self.requests) < self.limit:
            self.requests.append(now)
            return True
        return False
```

**优化：** 滑动计数器——将窗口分成多个小格子，只存储每个格子的计数，空间复杂度从 O(n) 降到 O(k)。

**应用：** 适合需要精确限流的场景，如金融交易、支付接口。

---

## 卡片4：令牌桶算法

**一句话：** 以固定速率生成令牌，请求消耗令牌，桶满了就不再生成，支持突发流量。

**举例：**
```python
class TokenBucket:
    def __init__(self, rate, capacity):
        self.rate = rate        # 令牌生成速率（个/秒）
        self.capacity = capacity  # 桶容量
        self.tokens = capacity   # 当前令牌数
        self.last_time = time.time()

    def acquire(self):
        now = time.time()
        elapsed = now - self.last_time

        # 生成新令牌
        self.tokens = min(self.capacity, self.tokens + elapsed * self.rate)
        self.last_time = now

        # 消耗令牌
        if self.tokens >= 1:
            self.tokens -= 1
            return True
        return False
```

**优点：** 支持突发流量（桶里有余量），实现简单，性能高。

**应用：** API 网关、LLM API 调用（大多数场景的最佳选择）。

---

## 卡片5：漏桶算法

**一句话：** 请求进入队列，以固定速率处理，队列满了就拒绝，用于流量整形。

**举例：**
```python
class LeakyBucket:
    def __init__(self, rate, capacity):
        self.rate = rate        # 处理速率（个/秒）
        self.capacity = capacity  # 队列容量
        self.queue = []
        self.last_time = time.time()

    def acquire(self):
        now = time.time()
        elapsed = now - self.last_time

        # 处理队列中的请求
        processed = int(elapsed * self.rate)
        self.queue = self.queue[processed:]
        self.last_time = now

        # 添加新请求
        if len(self.queue) < self.capacity:
            self.queue.append(now)
            return True
        return False
```

**区别：** 令牌桶控制"发送速率"，漏桶控制"处理速率"。

**应用：** 消息队列、流量整形、需要平滑流量的场景。

---

## 卡片6：Redis 分布式限流

**一句话：** 使用 Redis + Lua 脚本实现分布式限流，保证原子性和一致性。

**举例：**
```python
lua_script = """
local key = KEYS[1]
local rate = tonumber(ARGV[1])
local capacity = tonumber(ARGV[2])
local now = tonumber(ARGV[3])

local state = redis.call('HMGET', key, 'tokens', 'last_time')
local tokens = tonumber(state[1]) or capacity
local last_time = tonumber(state[2]) or now

local elapsed = now - last_time
tokens = math.min(capacity, tokens + elapsed * rate)

if tokens >= 1 then
    tokens = tokens - 1
    redis.call('HMSET', key, 'tokens', tokens, 'last_time', now)
    redis.call('EXPIRE', key, 3600)
    return 1
end
return 0
"""

result = redis.eval(lua_script, 1, key, rate, capacity, time.time())
```

**为什么用 Lua？** 保证原子性（读取、计算、写入是一个事务），避免并发问题。

**应用：** 分布式环境下的限流（多个服务器实例需要共享状态）。

---

## 卡片7：多维度限流

**一句话：** 按用户、IP、端点、资源等多个维度限流，保护服务和实现差异化服务。

**举例：**
```python
class MultiDimensionLimiter:
    async def check_limit(self, user_id, ip, endpoint, user_tier):
        # 1. 全局限流（保护服务器）
        await self._check_global_limit()

        # 2. IP 限流（防止 DDoS）
        await self._check_ip_limit(ip)

        # 3. 用户限流（按等级）
        limits = {
            "free": {"rate": 1, "capacity": 10},
            "pro": {"rate": 100, "capacity": 1000}
        }
        await self._check_user_limit(user_id, limits[user_tier])

        # 4. 端点限流（保护昂贵操作）
        await self._check_endpoint_limit(endpoint)
```

**维度选择：**
- 用户：防止单个用户滥用
- IP：防止 DDoS 攻击
- 端点：保护昂贵操作
- 资源：控制 LLM 模型调用

**应用：** 生产环境的 AI Agent API，需要多维度保护。

---

## 卡片8：FastAPI 集成

**一句话：** 使用依赖注入或装饰器在 FastAPI 中集成限流中间件。

**举例：**
```python
# 方式1：依赖注入（推荐）
async def rate_limit(user_id: str = "anonymous"):
    limiter = RedisTokenBucket(
        redis_client=redis_client,
        key=f"user:{user_id}:rate_limit",
        rate=10,
        capacity=10
    )
    if not limiter.acquire():
        raise HTTPException(status_code=429, detail="Too many requests")

@app.post("/chat")
async def chat(message: str, _: None = Depends(rate_limit)):
    return {"response": f"Echo: {message}"}


# 方式2：装饰器
def rate_limit_decorator(rate, capacity):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            limiter = TokenBucket(rate, capacity)
            if not limiter.acquire():
                raise HTTPException(status_code=429)
            return await func(*args, **kwargs)
        return wrapper
    return decorator

@app.post("/chat")
@rate_limit_decorator(rate=10, capacity=10)
async def chat(message: str):
    return {"response": f"Echo: {message}"}
```

**推荐：** 依赖注入（代码清晰、可复用、易测试）。

**应用：** 在 AI Agent API 中保护每个端点。

---

## 卡片9：限流与熔断、降级

**一句话：** 限流保护自己，熔断保护下游，降级保证可用，三者组合使用。

**举例：**
```python
@app.post("/chat")
async def chat(message: str, user: User = Depends(get_current_user)):
    # 1. 限流：保护自己
    if not await limiter.check(user.id):
        raise HTTPException(status_code=429, detail="Too many requests")

    # 2. 熔断：保护下游
    if circuit_breaker.is_open():
        # 3. 降级：返回缓存
        cached = await cache.get(message)
        if cached:
            return {"response": cached, "from_cache": True}
        raise HTTPException(status_code=503, detail="Service unavailable")

    # 正常调用
    try:
        response = await llm.chat(message)
        circuit_breaker.record_success()
        await cache.set(message, response)
        return {"response": response, "from_cache": False}
    except Exception:
        circuit_breaker.record_failure()
        # 降级：返回缓存
        cached = await cache.get(message)
        if cached:
            return {"response": cached, "from_cache": True}
        raise
```

**区别：**
- 限流：请求频率过高 → 拒绝请求
- 熔断：下游服务异常 → 停止调用
- 降级：服务压力大 → 返回简化结果

**应用：** 生产环境的完整保护策略。

---

## 卡片10：性能优化与监控

**一句话：** 使用混合限流（内存+Redis）提升性能，使用 Prometheus 监控限流指标。

**举例：**
```python
# 混合限流：内存预检 + Redis 精确控制
class HybridLimiter:
    def __init__(self, rate, capacity):
        self.memory_limiter = TokenBucket(rate * 1.2, capacity * 1.2)
        self.redis_limiter = RedisTokenBucket(rate, capacity)

    async def acquire(self):
        # 1. 内存预检（快速拒绝）
        if not self.memory_limiter.acquire():
            return False

        # 2. Redis 精确检查
        return await self.redis_limiter.acquire()


# 监控指标
from prometheus_client import Counter, Histogram

rate_limit_requests = Counter(
    'rate_limit_requests_total',
    'Total rate limit checks',
    ['user_tier', 'endpoint', 'result']
)

rate_limit_latency = Histogram(
    'rate_limit_check_duration_seconds',
    'Rate limit check latency'
)

async def check_limit_with_metrics(user_id, user_tier, endpoint):
    with rate_limit_latency.time():
        try:
            await limiter.check_limit(user_id)
            rate_limit_requests.labels(user_tier, endpoint, 'allowed').inc()
        except HTTPException:
            rate_limit_requests.labels(user_tier, endpoint, 'rejected').inc()
            raise
```

**性能对比：**
- 纯内存：< 1μs
- 纯 Redis：~1ms
- 混合方案：~0.1ms

**监控指标：**
- 限流检查次数（按用户等级、端点、结果）
- 限流检查延迟
- 被拒绝的请求数
- 令牌桶状态（剩余令牌数）

**应用：** 高并发场景下的性能优化和可观测性。

---

## 知识体系总结

### 算法选择决策树

```
需要限流？
  ↓ 是
需要精确控制？
  ↓ 否 → 固定窗口（简单）
  ↓ 是
需要支持突发流量？
  ↓ 是 → 令牌桶（推荐）
  ↓ 否
需要流量整形？
  ↓ 是 → 漏桶
  ↓ 否 → 滑动窗口（精确）
```

### 存储选择决策树

```
单机应用？
  ↓ 是 → 内存限流
  ↓ 否
分布式环境？
  ↓ 是
高并发（>10万QPS）？
  ↓ 是 → 混合方案（内存+Redis）
  ↓ 否 → Redis 限流
```

### 维度选择决策树

```
需要限流？
  ↓ 是
需要防止单个用户滥用？
  ↓ 是 → 用户维度
需要防止 DDoS？
  ↓ 是 → IP 维度
需要保护昂贵操作？
  ↓ 是 → 端点维度
需要控制 LLM 成本？
  ↓ 是 → 资源维度（模型、token）
```

---

## 实战检查清单

完成以下检查，确保你掌握了限流中间件：

### 理论理解
- [ ] 能说明4种限流算法的原理和适用场景
- [ ] 理解固定窗口的窗口边界问题
- [ ] 理解令牌桶和漏桶的区别
- [ ] 理解为什么需要 Redis 分布式限流
- [ ] 理解为什么需要 Lua 脚本保证原子性

### 实现能力
- [ ] 能手写一个简单的令牌桶实现（30行代码）
- [ ] 能用 Redis + Lua 实现分布式令牌桶
- [ ] 能在 FastAPI 中集成限流中间件
- [ ] 能实现多维度限流（用户、IP、端点）
- [ ] 能实现混合限流（内存+Redis）

### 应用能力
- [ ] 能为 AI Agent API 设计限流策略
- [ ] 能根据业务需求选择合适的限流算法
- [ ] 能根据部署环境选择合适的存储方案
- [ ] 能实现限流监控和告警
- [ ] 能优化限流性能（混合方案）

### 生产实践
- [ ] 能处理限流后的用户体验（友好错误提示）
- [ ] 能实现差异化服务（免费/付费用户）
- [ ] 能实现动态调整限流策略
- [ ] 能实现限流降级策略
- [ ] 能集成限流、熔断、降级三种保护机制

---

## 常见问题速查

### Q1: 如何选择限流算法？

**A:**
- 简单场景：固定窗口
- 精确控制：滑动窗口
- 支持突发：令牌桶（推荐）
- 流量整形：漏桶

### Q2: 如何选择存储方案？

**A:**
- 单机：内存
- 分布式：Redis
- 高并发：混合（内存+Redis）

### Q3: 如何设置 rate 和 capacity？

**A:**
- 免费用户：rate=1, capacity=10
- 付费用户：rate=10, capacity=100
- 内部服务：rate=100, capacity=1000

### Q4: 限流后如何提升用户体验？

**A:**
- 友好的错误提示（告知重试时间）
- 渐进式限流（先降级，再拒绝）
- 差异化服务（引导升级）

### Q5: 如何监控限流效果？

**A:**
- 限流检查次数（按维度）
- 被拒绝的请求数
- 限流检查延迟
- 令牌桶状态

---

## 学习路径

### 初学者（1-2天）
1. 理解限流的本质和价值
2. 学习令牌桶算法（最实用）
3. 实现一个简单的内存限流
4. 在 FastAPI 中集成限流

### 进阶（3-5天）
5. 学习其他3种算法（固定窗口、滑动窗口、漏桶）
6. 实现 Redis 分布式限流
7. 实现多维度限流
8. 实现限流监控

### 高级（1-2周）
9. 实现混合限流（性能优化）
10. 实现动态限流调整
11. 集成熔断、降级
12. 完整的生产级 AI Agent API

---

## 延伸学习

### 相关概念
- **熔断器（Circuit Breaker）**：保护下游服务
- **降级（Degradation）**：保证核心功能可用
- **背压（Backpressure）**：反向流量控制
- **令牌桶变种**：分层令牌桶、动态令牌桶

### 相关技术
- **API 网关**：Kong、Nginx、Envoy
- **限流库**：slowapi、fastapi-limiter
- **监控**：Prometheus、Grafana
- **分布式协调**：Redis、Etcd

### 实际案例
- **OpenAI API**：多维度限流（组织、模型、用户）
- **AWS API Gateway**：令牌桶限流
- **Cloudflare**：分布式限流
- **Stripe API**：滑动窗口限流

---

**记住：** 限流不是"拒绝用户"，而是"在有限资源下，保护服务和所有用户"。选择合适的算法、存储、维度，才能在性能、成本、用户体验之间找到平衡。
