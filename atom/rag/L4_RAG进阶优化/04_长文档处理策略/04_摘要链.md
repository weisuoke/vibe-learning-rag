# 核心概念2：摘要链（Summary Chain）

**摘要链是通过逐层总结压缩信息的技术，将长文档递归地总结成多层摘要，在有限的 Context Window 内保留全文的核心内容。**

---

## 什么是摘要链？

### 直觉理解

想象你在读一本 500 页的书：

```
第1遍：快速翻阅，记录每章的核心观点（章节摘要）
第2遍：把所有章节摘要再总结一遍（全书摘要）
第3遍：用一句话概括全书（一句话摘要）

结果：
- 原文：500 页 = 500,000 字
- 章节摘要：10 章 × 500 字 = 5,000 字
- 全书摘要：1,000 字
- 一句话摘要：50 字

压缩比：500,000 → 50 = 10,000 倍压缩
```

**摘要链就是这样的过程**：通过递归总结，将长文档压缩到可以放入 Context Window 的大小。

### 形式化定义

```
摘要链 = 递归总结 + 层次保留

递归总结：
- 第1层：原文片段（叶节点）
- 第2层：片段摘要（每 N 个片段总结一次）
- 第3层：摘要的摘要（每 N 个摘要再总结一次）
- ...
- 第K层：全文摘要（根节点）

层次保留：
- 每层摘要都保留，形成摘要链
- 可以根据需要访问不同粒度的摘要
```

---

## 为什么需要摘要链？

### 问题：Context Window 限制

假设一篇 500 页的论文：

```python
# 文档信息
total_pages = 500
chars_per_page = 1000
total_chars = 500 * 1000  # 500,000 字

# LLM 限制
context_window = 128000  # GPT-4 的 128K tokens
chars_per_token = 0.75  # 平均每个 token 0.75 个字符
max_chars = 128000 * 0.75  # 96,000 字

# 问题：500,000 字 > 96,000 字
# 无法一次性处理全文
```

**传统方案的问题**：
- **直接截断**：只读前 96,000 字 → 丢失后面的内容
- **随机采样**：随机选择部分内容 → 可能漏掉关键信息
- **分块检索**：检索相关块 → 无法回答需要全文理解的问题

### 解决：摘要链

```python
# 摘要链方案
层级1：原文片段（500,000 字，分成 500 个片段，每个 1,000 字）
层级2：片段摘要（500 个片段 → 50 个摘要，每个摘要 200 字 = 10,000 字）
层级3：摘要的摘要（50 个摘要 → 5 个摘要，每个 200 字 = 1,000 字）
层级4：全文摘要（5 个摘要 → 1 个摘要，200 字）

# 现在可以：
# 1. 用 200 字的全文摘要理解全文大意
# 2. 用 1,000 字的层级3摘要理解各部分内容
# 3. 用 10,000 字的层级2摘要深入理解细节
# 4. 根据需要访问原文片段
```

**优势**：
- ✅ 突破 Context Window 限制
- ✅ 保留全文信息（虽然有损失）
- ✅ 支持多粒度理解（从粗到细）
- ✅ 适合需要全文理解的问题

---

## 摘要链的核心原理

### 原理1：递归总结

```
原文片段1 → 摘要1
原文片段2 → 摘要2    } → 摘要A
原文片段3 → 摘要3
原文片段4 → 摘要4    } → 摘要B
...

摘要A →
摘要B →  } → 全文摘要
摘要C →
```

**关键特性**：
- 每次总结都压缩信息（如 10:1 压缩比）
- 递归进行，直到可以放入 Context Window
- 每层摘要都保留，形成链条

### 原理2：信息压缩与保留

```python
# 信息压缩
原文：1000 字 → 摘要：100 字（10:1 压缩）

# 信息保留（核心内容）
原文：
  "神经网络是由大量神经元组成的计算模型。每个神经元接收多个输入，
   进行加权求和，然后通过激活函数输出。常见的激活函数包括 Sigmoid、
   ReLU、Tanh 等。反向传播算法是训练神经网络的核心，通过梯度下降
   更新权重，最小化损失函数。"

摘要：
  "神经网络由神经元组成，通过加权求和和激活函数处理输入。
   反向传播算法用梯度下降训练网络。"

# 保留了核心概念：神经元、激活函数、反向传播
# 丢失了细节：具体的激活函数名称、详细的训练过程
```

### 原理3：自底向上构建

```python
def build_summary_chain(document, chunk_size=1000, summary_ratio=10):
    """构建摘要链"""

    # 第1层：分块
    chunks = split_into_chunks(document, chunk_size)
    layers = [chunks]  # 保存所有层

    # 递归总结
    current_layer = chunks
    while len(current_layer) > 1:
        # 总结当前层
        next_layer = []
        for i in range(0, len(current_layer), summary_ratio):
            batch = current_layer[i:i+summary_ratio]
            summary = summarize(batch)  # 用 LLM 总结
            next_layer.append(summary)

        layers.append(next_layer)
        current_layer = next_layer

    return layers  # 返回所有层的摘要链
```

---

## Python 手写实现

### 完整实现：摘要链系统

```python
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class SummaryNode:
    """摘要节点"""
    text: str
    level: int  # 层级（0=原文，1=第一层摘要，2=第二层摘要...）
    children: List['SummaryNode'] = None

    def __post_init__(self):
        if self.children is None:
            self.children = []

class SummaryChain:
    """摘要链"""

    def __init__(self, summarize_func, chunk_size=1000, summary_ratio=10):
        """
        Args:
            summarize_func: 总结函数（接收文本列表，返回摘要）
            chunk_size: 分块大小（字符数）
            summary_ratio: 总结比例（每 N 个块总结一次）
        """
        self.summarize_func = summarize_func
        self.chunk_size = chunk_size
        self.summary_ratio = summary_ratio
        self.layers = []  # 所有层的节点

    def build(self, document: str) -> SummaryNode:
        """
        构建摘要链

        Args:
            document: 原文

        Returns:
            根节点（全文摘要）
        """
        # 第1层：分块
        chunks = self._split_into_chunks(document)
        chunk_nodes = [SummaryNode(text=chunk, level=0) for chunk in chunks]
        self.layers.append(chunk_nodes)

        # 递归总结
        current_layer = chunk_nodes
        level = 1

        while len(current_layer) > 1:
            next_layer = []

            # 每 summary_ratio 个节点总结一次
            for i in range(0, len(current_layer), self.summary_ratio):
                batch = current_layer[i:i+self.summary_ratio]

                # 总结这一批节点
                texts = [node.text for node in batch]
                summary_text = self.summarize_func(texts)

                # 创建摘要节点
                summary_node = SummaryNode(
                    text=summary_text,
                    level=level,
                    children=batch
                )
                next_layer.append(summary_node)

            self.layers.append(next_layer)
            current_layer = next_layer
            level += 1

        # 返回根节点（全文摘要）
        return current_layer[0]

    def get_summary_at_level(self, level: int) -> List[str]:
        """获取指定层级的所有摘要"""
        if level >= len(self.layers):
            return []
        return [node.text for node in self.layers[level]]

    def get_full_summary(self) -> str:
        """获取全文摘要（最顶层）"""
        if not self.layers:
            return ""
        return self.layers[-1][0].text

    def _split_into_chunks(self, text: str) -> List[str]:
        """将文本分块"""
        chunks = []
        for i in range(0, len(text), self.chunk_size):
            chunk = text[i:i+self.chunk_size]
            chunks.append(chunk)
        return chunks


# ===== 使用示例 =====

# 模拟总结函数（实际应使用 LLM）
def mock_summarize(texts: List[str]) -> str:
    """
    模拟总结函数（实际应使用 OpenAI API）

    实际实现：
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{
            "role": "user",
            "content": f"请总结以下内容：\n\n{chr(10).join(texts)}"
        }]
    )
    return response.choices[0].message.content
    """
    # 简化版：取每段的前 50 字
    summaries = [text[:50] + "..." for text in texts]
    return " | ".join(summaries)

# 准备长文档
long_document = """
第1章：神经网络基础

神经网络是由大量神经元组成的计算模型。每个神经元接收多个输入，进行加权求和，然后通过激活函数输出。
常见的激活函数包括 Sigmoid、ReLU、Tanh 等，它们引入非线性，使神经网络能够拟合复杂函数。
反向传播算法是训练神经网络的核心，通过梯度下降更新权重，最小化损失函数。

第2章：卷积神经网络

卷积神经网络（CNN）专门用于处理图像数据。卷积层通过卷积核提取局部特征，保留空间结构。
池化层用于降低特征图的维度，减少计算量，同时保留重要特征。
经典的 CNN 架构包括 LeNet、AlexNet、VGG、ResNet 等，它们在图像分类任务上取得了突破性进展。

第3章：循环神经网络

循环神经网络（RNN）用于处理序列数据，如文本、时间序列等。RNN 通过隐藏状态保存历史信息。
LSTM 和 GRU 是 RNN 的改进版本，解决了梯度消失问题，能够捕捉长期依赖。
Transformer 架构通过自注意力机制，彻底取代了 RNN，成为 NLP 的主流模型。

第4章：生成对抗网络

生成对抗网络（GAN）由生成器和判别器组成。生成器生成假数据，判别器判断数据真假。
两者通过对抗训练，生成器不断提升生成质量，判别器不断提升判别能力。
GAN 在图像生成、风格迁移、数据增强等任务上有广泛应用。

第5章：强化学习

强化学习是通过与环境交互学习最优策略的方法。智能体根据状态选择动作，环境返回奖励。
Q-Learning 和 DQN 是经典的强化学习算法，通过 Q 值函数评估动作的价值。
AlphaGo 使用深度强化学习，在围棋上战胜了人类世界冠军。
""" * 10  # 重复 10 次，模拟长文档

print(f"原文长度: {len(long_document)} 字符")

# 构建摘要链
print("\n=== 构建摘要链 ===")
chain = SummaryChain(
    summarize_func=mock_summarize,
    chunk_size=500,  # 每块 500 字符
    summary_ratio=5   # 每 5 个块总结一次
)

root = chain.build(long_document)

# 查看各层摘要
print(f"\n摘要链层数: {len(chain.layers)}")
for i, layer in enumerate(chain.layers):
    print(f"  层级 {i}: {len(layer)} 个节点")

# 获取全文摘要
print(f"\n=== 全文摘要 ===")
full_summary = chain.get_full_summary()
print(f"全文摘要长度: {len(full_summary)} 字符")
print(f"全文摘要内容:\n{full_summary[:200]}...")

# 获取第1层摘要（最详细）
print(f"\n=== 第1层摘要（最详细）===")
level1_summaries = chain.get_summary_at_level(1)
print(f"第1层摘要数量: {len(level1_summaries)}")
if level1_summaries:
    print(f"第1个摘要:\n{level1_summaries[0][:200]}...")
```

---

## 摘要链的优缺点

### 优点

| 优点 | 说明 |
|------|------|
| ✅ 突破 Context Window 限制 | 通过压缩，任意长度的文档都能处理 |
| ✅ 保留全文信息 | 虽然有损失，但核心内容被保留 |
| ✅ 支持多粒度理解 | 可以访问不同层级的摘要 |
| ✅ 适合全文理解问题 | 如"这篇论文的核心贡献是什么？" |

### 缺点

| 缺点 | 说明 |
|------|------|
| ❌ 信息损失 | 摘要无法包含所有细节 |
| ❌ 构建成本高 | 需要多次调用 LLM 生成摘要 |
| ❌ 摘要质量依赖 LLM | 如果 LLM 总结不好，整个链条质量下降 |
| ❌ 不适合精确检索 | 如"第3章第2段的公式是什么？" |

---

## 在 RAG 开发中的应用

### 应用场景1：学术论文问答

```python
# 场景：用户问"这篇论文的核心创新是什么？"
#
# 摘要链流程：
# 1. 构建摘要链（离线完成）
# 2. 用户提问时，先读全文摘要
# 3. 如果全文摘要不够详细，读第2层摘要
# 4. 如果需要更多细节，读第1层摘要
# 5. 最后访问原文片段

def answer_with_summary_chain(question, chain):
    """使用摘要链回答问题"""

    # 第1步：读全文摘要
    full_summary = chain.get_full_summary()
    prompt = f"根据以下摘要回答问题：\n\n{full_summary}\n\n问题：{question}"
    answer = llm(prompt)

    # 如果回答不够详细，读更详细的摘要
    if "需要更多细节" in answer:
        level1_summaries = chain.get_summary_at_level(1)
        prompt = f"根据以下详细摘要回答问题：\n\n{chr(10).join(level1_summaries)}\n\n问题：{question}"
        answer = llm(prompt)

    return answer
```

### 应用场景2：多文档综合问答

```python
# 场景：用户问"对比 3 篇论文的方法"
#
# 摘要链流程：
# 1. 为每篇论文构建摘要链
# 2. 提取每篇论文的"方法"部分摘要
# 3. 让 LLM 对比

def compare_papers(papers, question):
    """对比多篇论文"""

    summaries = []
    for paper in papers:
        # 构建摘要链
        chain = SummaryChain(summarize_func=llm_summarize)
        chain.build(paper.content)

        # 提取相关部分的摘要
        relevant_summary = extract_relevant_section(chain, "方法")
        summaries.append(f"{paper.title}:\n{relevant_summary}")

    # 让 LLM 对比
    prompt = f"对比以下论文的方法：\n\n{chr(10).join(summaries)}\n\n问题：{question}"
    return llm(prompt)
```

### 应用场景3：渐进式问答

```python
# 场景：用户先问大问题，再问细节问题
#
# 摘要链流程：
# 1. 第1个问题：用全文摘要回答
# 2. 第2个问题：用第1层摘要回答
# 3. 第3个问题：用原文片段回答

class ProgressiveQA:
    """渐进式问答"""

    def __init__(self, chain):
        self.chain = chain
        self.current_level = len(chain.layers) - 1  # 从最顶层开始

    def answer(self, question):
        """回答问题"""

        # 获取当前层级的摘要
        summaries = self.chain.get_summary_at_level(self.current_level)
        context = "\n\n".join(summaries)

        # 生成回答
        prompt = f"根据以下内容回答问题：\n\n{context}\n\n问题：{question}"
        answer = llm(prompt)

        # 如果用户需要更多细节，下次使用更详细的层级
        if "需要更多细节" in answer:
            self.current_level = max(0, self.current_level - 1)

        return answer
```

---

## 摘要链 vs 分层索引

| 维度 | 摘要链 | 分层索引 |
|------|--------|---------|
| **核心思想** | 递归总结，压缩信息 | 构建树形索引，逐层检索 |
| **适用场景** | 需要全文理解的问题 | 需要快速定位的问题 |
| **信息保留** | 有损压缩，保留核心 | 无损，保留原文 |
| **检索方式** | 从粗到细，逐层深入 | 从粗到细，逐层检索 |
| **构建成本** | 高（需要多次调用 LLM） | 中（需要提取结构） |
| **适用文档** | 任何文档 | 需要有结构的文档 |

---

## 实战技巧

### 技巧1：自适应压缩比

```python
def adaptive_summarize(texts, target_length):
    """自适应压缩比总结"""

    total_length = sum(len(t) for t in texts)
    compression_ratio = total_length / target_length

    if compression_ratio < 2:
        # 压缩比太小，直接拼接
        return " ".join(texts)
    elif compression_ratio < 10:
        # 中等压缩比，提取关键句
        return extract_key_sentences(texts, target_length)
    else:
        # 高压缩比，用 LLM 总结
        return llm_summarize(texts, max_length=target_length)
```

### 技巧2：保留关键信息

```python
def summarize_with_keywords(texts, keywords):
    """总结时保留关键词"""

    prompt = f"""
    请总结以下内容，确保包含这些关键词：{', '.join(keywords)}

    内容：
    {chr(10).join(texts)}
    """

    return llm(prompt)
```

### 技巧3：混合策略（摘要链 + 检索）

```python
def hybrid_summary_retrieval(chain, query):
    """混合策略：先用摘要链理解全文，再检索细节"""

    # 第1步：用全文摘要理解大意
    full_summary = chain.get_full_summary()

    # 第2步：判断需要哪些细节
    prompt = f"根据摘要：{full_summary}\n\n问题：{query}\n\n需要哪些章节的细节？"
    needed_sections = llm(prompt)

    # 第3步：检索相关章节的详细摘要
    detailed_summaries = []
    for section in needed_sections:
        summary = chain.get_section_summary(section)
        detailed_summaries.append(summary)

    # 第4步：用详细摘要回答
    context = "\n\n".join(detailed_summaries)
    prompt = f"根据以下内容回答问题：\n\n{context}\n\n问题：{query}"
    return llm(prompt)
```

---

## 总结

**摘要链的核心**：
1. **递归总结**：原文 → 摘要 → 摘要的摘要 → ...
2. **信息压缩**：每层压缩 10:1，最终可以放入 Context Window
3. **层次保留**：所有层的摘要都保留，支持多粒度访问

**适用场景**：
- ✅ 需要全文理解的问题（如"核心创新是什么？"）
- ✅ 多文档综合问答
- ✅ 渐进式问答（从粗到细）

**不适用场景**：
- ❌ 需要精确检索的问题（如"第3章第2段的公式"）
- ❌ 需要保留所有细节的场景
- ❌ 实时性要求高的场景（构建摘要链需要时间）

---

**下一步：** [05_MapReduce策略](./05_MapReduce策略.md) - 学习如何并行处理长文档
