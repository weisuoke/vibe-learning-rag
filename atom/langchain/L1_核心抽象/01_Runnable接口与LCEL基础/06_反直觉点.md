# 反直觉点

> 揭示 Runnable 和 LCEL 的 3 个常见误区

---

## 误区 1: "LCEL 比传统 Chain 慢" ❌

### 为什么错？

**事实恰恰相反：LCEL 通常比传统 Chain 更快。**

根据 2025-2026 年的实际测试数据和社区反馈[^1][^2]：

1. **自动并行优化**: LCEL 使用 `RunnableParallel` 自动并行执行独立步骤
2. **流式优化**: LCEL 默认支持流式传输，降低首 token 延迟
3. **执行引擎优化**: LangChain 1.0 对 LCEL 执行引擎进行了大量优化

**性能对比示例**：

```python
import time
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableParallel

# 场景：同时调用两个独立的 LLM 任务

# ❌ 传统方式：顺序执行
def traditional_approach():
    llm = ChatOpenAI(model="gpt-4o-mini")

    start = time.time()
    result1 = llm.invoke("翻译成英文: 你好")
    result2 = llm.invoke("翻译成法文: 你好")
    end = time.time()

    print(f"传统方式耗时: {end - start:.2f}秒")
    return result1, result2

# ✅ LCEL 方式：自动并行
def lcel_approach():
    llm = ChatOpenAI(model="gpt-4o-mini")

    parallel_chain = RunnableParallel(
        english=ChatPromptTemplate.from_template("翻译成英文: {text}") | llm,
        french=ChatPromptTemplate.from_template("翻译成法文: {text}") | llm
    )

    start = time.time()
    result = parallel_chain.invoke({"text": "你好"})
    end = time.time()

    print(f"LCEL 方式耗时: {end - start:.2f}秒")
    return result

# 运行对比
traditional_approach()  # 约 2-3 秒（顺序执行）
lcel_approach()         # 约 1-1.5 秒（并行执行）
```

**实际数据**：
- 独立步骤并行执行可节省 **40-60%** 的时间[^2]
- 流式输出可降低 **50-70%** 的感知延迟[^3]

### 为什么人们容易这样错？

1. **早期版本印象**: LangChain 早期版本（0.1.x）确实存在性能问题，但 1.0 版本已大幅优化
2. **抽象层误解**: 人们认为抽象层会带来性能损失，但 LCEL 的抽象实际上带来了优化机会
3. **对比不公平**: 很多对比测试使用的是简单场景，无法体现 LCEL 的并行优势

### 正确理解

**LCEL 的性能优势来自于：**

1. **声明式优化**: 声明式编程让 LangChain 有机会优化执行计划
2. **自动并行**: 框架自动识别可并行的步骤
3. **流式传输**: 默认支持流式，无需手动实现

**何时 LCEL 可能较慢？**

- 极简单的单步骤调用（但差异可忽略）
- 需要极致性能优化的场景（此时应考虑 vLLM 等专用工具）

---

## 误区 2: "Runnable 只能串行执行" ❌

### 为什么错？

**Runnable 协议天然支持并行、条件路由和复杂控制流。**

很多人认为 LCEL 的管道操作符 `|` 只能串行执行，但实际上 LangChain 提供了丰富的并行和路由组件。

### 并行执行示例

```python
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# 定义多个独立任务
def analyze_sentiment(text: str) -> str:
    return f"情感分析: {text}"

def extract_keywords(text: str) -> str:
    return f"关键词提取: {text}"

def summarize(text: str) -> str:
    return f"摘要: {text}"

# ✅ 并行执行三个任务
parallel_chain = RunnableParallel(
    sentiment=RunnableLambda(analyze_sentiment),
    keywords=RunnableLambda(extract_keywords),
    summary=RunnableLambda(summarize)
)

result = parallel_chain.invoke("这是一段很长的文本...")
print(result)
# {
#   "sentiment": "情感分析: ...",
#   "keywords": "关键词提取: ...",
#   "summary": "摘要: ..."
# }
```

### 条件路由示例

```python
from langchain_core.runnables import RunnableBranch

# 根据输入长度选择不同的处理策略
def is_short(input: dict) -> bool:
    return len(input["text"]) < 100

def is_medium(input: dict) -> bool:
    return 100 <= len(input["text"]) < 500

# ✅ 条件路由
router = RunnableBranch(
    (is_short, simple_chain),      # 短文本：简单处理
    (is_medium, medium_chain),     # 中等文本：标准处理
    complex_chain                  # 长文本：复杂处理
)

result = router.invoke({"text": "..."})
```

### 为什么人们容易这样错？

1. **管道符误导**: `|` 操作符看起来像 Unix 管道，给人串行的印象
2. **文档不足**: 早期文档主要展示简单的串行示例
3. **概念混淆**: 将 LCEL 语法和执行模式混为一谈

### 正确理解

**Runnable 支持的执行模式：**

| 模式 | 组件 | 用途 |
|------|------|------|
| **串行** | `\|` 操作符 | 数据依次流动 |
| **并行** | `RunnableParallel` | 独立任务同时执行 |
| **条件** | `RunnableBranch` | 根据条件选择路径 |
| **循环** | LangGraph | 多轮迭代（需要 LangGraph） |

**实际应用场景**：

```python
# 复杂工作流：并行 + 串行 + 条件
chain = (
    preprocess                          # 串行：预处理
    | RunnableParallel(                 # 并行：多任务
        task1=llm_chain1,
        task2=llm_chain2,
        task3=llm_chain3
    )
    | merge_results                     # 串行：合并结果
    | RunnableBranch(                   # 条件：根据结果路由
        (needs_refinement, refine_chain),
        final_output
    )
)
```

---

## 误区 3: "所有组件都必须用 LCEL" ❌

### 为什么错？

**LCEL 和传统方式可以混用，选择最合适的工具完成任务。**

LangChain 1.0 明确支持混合使用 LCEL 和传统 API，不强制全部使用 LCEL[^4]。

### 混合使用示例

```python
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain  # 传统 Chain

llm = ChatOpenAI(model="gpt-4o-mini")

# ✅ 混合使用：LCEL + 传统 API
def custom_logic(input: dict) -> dict:
    """使用传统 Python 代码处理复杂逻辑"""
    # 复杂的业务逻辑
    if input["type"] == "special":
        # 使用传统 LLMChain
        legacy_chain = LLMChain(llm=llm, prompt=special_prompt)
        result = legacy_chain.run(input["text"])
    else:
        # 使用 LCEL
        result = (prompt | llm).invoke(input["text"])

    return {"result": result}

# 包装成 Runnable
hybrid_chain = RunnableLambda(custom_logic)

# 继续用 LCEL 组合
final_chain = preprocess | hybrid_chain | postprocess
```

### 何时使用传统方式？

| 场景 | 推荐方式 | 原因 |
|------|----------|------|
| 简单单次调用 | 直接用 SDK | 更简洁 |
| 复杂业务逻辑 | 传统 Python | 更灵活 |
| 已有代码迁移 | 混合使用 | 渐进式迁移 |
| 新项目工作流 | LCEL | 可组合性强 |

### 实际案例：渐进式迁移

```python
# 阶段 1: 传统方式（已有代码）
def legacy_process(text: str) -> str:
    # 100 行复杂业务逻辑
    result = complex_business_logic(text)
    return result

# 阶段 2: 包装成 Runnable
legacy_runnable = RunnableLambda(legacy_process)

# 阶段 3: 与 LCEL 组合
modern_chain = (
    new_preprocess          # 新代码：LCEL
    | legacy_runnable       # 旧代码：包装
    | new_postprocess       # 新代码：LCEL
)

# 阶段 4: 逐步重构（可选）
# 根据需要逐步将 legacy_process 重构为 LCEL
```

### 为什么人们容易这样错？

1. **文档导向**: 官方文档大量展示 LCEL，给人"必须用"的印象
2. **社区压力**: 社区讨论中 LCEL 被过度推崇
3. **完美主义**: 开发者希望代码"纯粹"，全部使用同一种方式

### 正确理解

**选择标准：**

```
简单任务（单次调用）
    ↓
直接用 SDK（最简单）

中等复杂度（多步骤工作流）
    ↓
优先 LCEL（可组合、可观测）

极复杂业务逻辑
    ↓
传统 Python + RunnableLambda 包装

已有代码
    ↓
混合使用，渐进式迁移
```

**2025-2026 年最佳实践**[^5]：

1. **新项目**: 优先使用 LCEL，享受可组合性和可观测性
2. **已有项目**: 混合使用，逐步迁移关键路径
3. **简单脚本**: 直接用 SDK，不必过度工程化
4. **复杂逻辑**: 用最合适的工具，不强求 LCEL

---

## 总结：三个关键认知转变

| 误区 | 正确理解 |
|------|----------|
| ❌ LCEL 比传统 Chain 慢 | ✅ LCEL 通过并行和流式优化，通常更快 |
| ❌ Runnable 只能串行执行 | ✅ 支持并行、条件路由和复杂控制流 |
| ❌ 所有组件都必须用 LCEL | ✅ 可混用，选择最合适的工具 |

---

## 实战建议

### 性能优化

```python
# ✅ 识别可并行的步骤
chain = RunnableParallel(
    task1=independent_task1,
    task2=independent_task2
) | merge_and_continue

# ✅ 使用流式降低延迟
for chunk in chain.stream(input):
    print(chunk, end="", flush=True)
```

### 灵活组合

```python
# ✅ 根据场景选择执行模式
chain = (
    preprocess
    | RunnableBranch(
        (is_simple, simple_path),
        (is_complex, complex_path),
        default_path
    )
    | postprocess
)
```

### 渐进式采用

```python
# ✅ 从关键路径开始使用 LCEL
critical_chain = prompt | llm | parser  # LCEL

# ✅ 非关键路径保持原样
legacy_result = legacy_function(input)  # 传统方式

# ✅ 最终组合
final_result = combine(critical_chain.invoke(input), legacy_result)
```

---

## 参考资料

[^1]: [LangChain Expression Language Performance](https://www.linkedin.com/pulse/langchain-expression-language-lcel-modern-approach-building-patil-v2k5f) - LinkedIn, 2025
[^2]: [How to Make LangChain Apps 10x Faster](https://medium.com/@vinodkrane/langchain-in-production-performance-security-and-cost-optimization-d5e0b44a26fd) - Medium, 2025
[^3]: [Streaming and Batching LLM Inference](https://levelup.gitconnected.com/streaming-and-batching-llm-inference-using-nvidia-nim-and-langchain-e0afdc031543) - Level Up Coding, 2025
[^4]: [LangChain 1.0 Release Notes](https://blog.langchain.com/langchain-langgraph-1dot0) - LangChain Blog, 2025年10月
[^5]: [LangChain Best Practices 2025-2026](https://www.swarnendu.de/blog/langchain-best-practices) - Swarnendu De, 2025-2026

---

**下一步**: 阅读 [07_实战代码_01_基础调用.md](./07_实战代码_01_基础调用.md) 通过实战巩固理解
