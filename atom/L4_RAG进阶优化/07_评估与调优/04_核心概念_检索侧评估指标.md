# 核心概念：检索侧评估指标

> 衡量 RAG 系统检索环节质量的核心指标 — Precision@K, Recall@K, MRR, NDCG, Hit Rate

---

## 一句话定义

**检索侧评估指标是衡量 RAG 系统"找文档"能力的量化工具，核心回答三个问题：找得准不准（Precision）、找得全不全（Recall）、排得好不好（MRR/NDCG）。**

---

## 为什么需要单独评估检索质量？

RAG 系统的工作流程是"先检索、再生成"。如果检索环节出了问题，后面的生成环节再强也无济于事——大模型只能基于你给它的上下文来回答，如果上下文本身就不对，答案自然也不对。

这就好比考试时你带了一本错误百出的参考书，就算你阅读理解能力再强，也会被错误信息误导。

### 三个核心原因

**1. 检索是 RAG 的地基，地基不稳大厦必倒**

```
RAG Pipeline:
用户提问 → [检索] → [生成] → 答案
              ↑
         如果这里出问题
         后面再好也没用
```

检索返回了不相关的文档，LLM 要么编造答案（幻觉），要么给出错误答案。无论哪种情况，用户体验都很差。

**2. 检索问题和生成问题的优化方向完全不同**

- 检索不好 → 换 Embedding 模型、调整 Chunk 大小、加 ReRank
- 生成不好 → 优化 Prompt、调整 Temperature、换 LLM

如果不单独评估检索，你根本不知道问题出在哪个环节，优化就像无头苍蝇。

**3. 检索评估不需要 LLM，成本低、速度快**

检索评估只需要比对"检索到的文档"和"真正相关的文档"，不需要调用大模型，几毫秒就能算出结果。而生成评估往往需要 LLM-as-Judge，成本高得多。

---

## 前置知识：什么是 Ground Truth？

在讲具体指标之前，先理解一个关键概念：**Ground Truth（标准答案）**。

对于检索评估来说，Ground Truth 就是"对于每个查询，哪些文档是真正相关的"。这通常需要人工标注。

```python
# Ground Truth 示例
ground_truth = {
    "什么是RAG？": {"doc_001", "doc_015", "doc_023"},      # 这3个文档是相关的
    "如何选择Embedding模型？": {"doc_007", "doc_042"},       # 这2个文档是相关的
    "Chunk大小怎么设置？": {"doc_003", "doc_011", "doc_019", "doc_025"},  # 这4个文档是相关的
}
```

有了 Ground Truth，我们才能计算各种评估指标。下面逐一讲解。

---

## 指标1：Precision@K（精确率）

### 定义

**前 K 个检索结果中，相关文档的比例。**

### 公式

```
Precision@K = |检索到的前K个文档 ∩ 相关文档| / K
```

用大白话说：**"我找到的 K 个文档里，有多少是真正有用的？"**

生活类比：你去图书馆借了 5 本书回来写论文，其中 3 本确实和你的论文主题相关，那你的 Precision@5 = 3/5 = 0.6。

### 手写实现

```python
def precision_at_k(retrieved_ids: list, relevant_ids: set, k: int) -> float:
    """
    计算 Precision@K

    参数:
        retrieved_ids: 检索到的文档ID列表（按排名顺序）
        relevant_ids: 相关文档ID集合（ground truth）
        k: 取前K个结果

    返回:
        Precision@K 分数 (0-1)
    """
    # 取前K个检索结果
    retrieved_k = set(retrieved_ids[:k])
    # 计算交集（既被检索到又是相关的）
    relevant_retrieved = retrieved_k & relevant_ids
    # Precision = 相关的数量 / K
    return len(relevant_retrieved) / k


# ===== 示例 =====
retrieved = ["doc1", "doc3", "doc5", "doc2", "doc7"]  # 检索到的5个文档（按排名）
relevant = {"doc1", "doc2", "doc3", "doc4"}            # 真正相关的4个文档

print(f"Precision@1 = {precision_at_k(retrieved, relevant, 1):.3f}")  # 1/1 = 1.000
print(f"Precision@3 = {precision_at_k(retrieved, relevant, 3):.3f}")  # 2/3 = 0.667
print(f"Precision@5 = {precision_at_k(retrieved, relevant, 5):.3f}")  # 3/5 = 0.600
```

### 直觉理解

```
检索结果（按排名顺序）:
  排名1: doc1 ✅ 相关
  排名2: doc3 ✅ 相关
  排名3: doc5 ❌ 不相关
  排名4: doc2 ✅ 相关
  排名5: doc7 ❌ 不相关

相关文档集合: {doc1, doc2, doc3, doc4}

Precision@1 = 1个相关(doc1) / 1个结果 = 1.000
→ "第1个结果是有用的"

Precision@3 = 2个相关(doc1, doc3) / 3个结果 = 0.667
→ "前3个结果中，67%是有用的"

Precision@5 = 3个相关(doc1, doc3, doc2) / 5个结果 = 0.600
→ "前5个结果中，60%是有用的"
```

### 适用场景

| 场景 | 推荐 K 值 | 原因 |
|------|-----------|------|
| 精准问答（只需最佳答案） | K=1 或 K=3 | 用户只看前几个结果 |
| 文档检索（需要多个参考） | K=5 或 K=10 | 需要更多参考文档 |
| 知识库搜索（覆盖面要广） | K=10 或 K=20 | 不能遗漏重要信息 |

### Precision 的局限性

Precision 只关心"找到的里面有多少是对的"，不关心"有多少相关文档没被找到"。比如总共有 100 个相关文档，你只检索了 1 个且是相关的，Precision@1 = 1.0（满分），但你漏掉了 99 个！这就需要 Recall 来补充。

---

## 指标2：Recall@K（召回率）

### 定义

**前 K 个检索结果覆盖了多少相关文档。**

### 公式

```
Recall@K = |检索到的前K个文档 ∩ 相关文档| / |相关文档总数|
```

用大白话说：**"所有有用的文档，我找到了百分之多少？"**

生活类比：期末考试有 4 道重点题，你复习到了其中 3 道，那你的 Recall = 3/4 = 0.75。你覆盖了 75% 的重点。

### 手写实现

```python
def recall_at_k(retrieved_ids: list, relevant_ids: set, k: int) -> float:
    """
    计算 Recall@K

    参数:
        retrieved_ids: 检索到的文档ID列表（按排名顺序）
        relevant_ids: 相关文档ID集合（ground truth）
        k: 取前K个结果

    返回:
        Recall@K 分数 (0-1)
    """
    if not relevant_ids:
        return 0.0
    # 取前K个检索结果
    retrieved_k = set(retrieved_ids[:k])
    # 计算交集
    relevant_retrieved = retrieved_k & relevant_ids
    # Recall = 找到的相关文档数 / 相关文档总数
    return len(relevant_retrieved) / len(relevant_ids)


# ===== 示例（沿用上面的数据） =====
retrieved = ["doc1", "doc3", "doc5", "doc2", "doc7"]
relevant = {"doc1", "doc2", "doc3", "doc4"}  # 共4个相关文档

print(f"Recall@1 = {recall_at_k(retrieved, relevant, 1):.3f}")  # 1/4 = 0.250
print(f"Recall@3 = {recall_at_k(retrieved, relevant, 3):.3f}")  # 2/4 = 0.500
print(f"Recall@5 = {recall_at_k(retrieved, relevant, 5):.3f}")  # 3/4 = 0.750
```

### Precision vs Recall 的权衡

这两个指标天然存在"跷跷板"关系：K 越大，Recall 越高（找到更多相关文档），但 Precision 往往越低（混入更多噪音）。

```
K值   Precision   Recall    解读
─────────────────────────────────────────────
K=1   1.000       0.250     找得很准，但只找到1/4
K=3   0.667       0.500     开始平衡
K=5   0.600       0.750     找得多了，但混入噪音
K=20  0.200       1.000     全找到了，但80%是噪音

直觉理解：
Precision ████████░░  →  ████████░░  →  ██████░░░░  →  ██░░░░░░░░
Recall    ██░░░░░░░░  →  █████░░░░░  →  ████████░░  →  ██████████
          K=1             K=3             K=5             K=20
```

**在 RAG 开发中如何取舍？**

- **精准问答场景**（如客服机器人）：偏重 Precision，宁可少给也不要给错
- **法律/医疗场景**（不能遗漏）：偏重 Recall，宁可多给也不能漏掉关键信息
- **通用场景**：两者兼顾，通常 K=5 是一个不错的起点

---

## 指标3：MRR（Mean Reciprocal Rank，平均倒数排名）

### 定义

**第一个相关文档排在第几位的倒数，多个查询取平均。**

### 公式

```
对单个查询:  RR = 1 / (第一个相关文档的排名位置)
对多个查询:  MRR = 所有查询的 RR 之和 / 查询总数
```

用大白话说：**"最好的结果排在第几？排得越前分越高。"**

生活类比：你在搜索引擎搜一个问题，如果第 1 条就是你要的答案，体验最好（RR=1.0）；如果翻到第 3 条才找到，体验就差一些（RR=0.33）；如果翻了一页都没找到，体验很差（RR=0.0）。

### 手写实现

```python
def reciprocal_rank(retrieved_ids: list, relevant_ids: set) -> float:
    """
    计算单个查询的 Reciprocal Rank

    参数:
        retrieved_ids: 检索到的文档ID列表（按排名顺序）
        relevant_ids: 相关文档ID集合

    返回:
        RR 分数 (0-1)
    """
    for i, doc_id in enumerate(retrieved_ids):
        if doc_id in relevant_ids:
            return 1.0 / (i + 1)  # 排名从1开始，所以 i+1
    return 0.0  # 没有找到任何相关文档


def mean_reciprocal_rank(queries_results: list) -> float:
    """
    计算多个查询的 MRR

    参数:
        queries_results: [(retrieved_ids, relevant_ids), ...]
    """
    rr_scores = []
    for retrieved, relevant in queries_results:
        rr_scores.append(reciprocal_rank(retrieved, relevant))
    return sum(rr_scores) / len(rr_scores) if rr_scores else 0.0


# ===== 示例 =====
queries = [
    # 查询1：第1个结果就是相关的 → RR = 1/1 = 1.0
    (["doc1", "doc3", "doc5"], {"doc1", "doc2"}),
    # 查询2：第2个结果才是相关的 → RR = 1/2 = 0.5
    (["doc4", "doc2", "doc1"], {"doc1", "doc2"}),
    # 查询3：没有相关结果 → RR = 0.0
    (["doc4", "doc5", "doc6"], {"doc1", "doc2"}),
]

for i, (ret, rel) in enumerate(queries):
    rr = reciprocal_rank(ret, rel)
    print(f"查询{i+1}: RR = {rr:.2f}")

mrr = mean_reciprocal_rank(queries)
print(f"\nMRR = ({1.0} + {0.5} + {0.0}) / 3 = {mrr:.3f}")
```

**运行输出：**
```
查询1: RR = 1.00
查询2: RR = 0.50
查询3: RR = 0.00

MRR = (1.0 + 0.5 + 0.0) / 3 = 0.500
```

### 直觉理解

```
查询1的检索结果:  [✅ doc1] [doc3] [doc5]  → 第1个就命中！RR = 1/1 = 1.0
查询2的检索结果:  [❌ doc4] [✅ doc2] [doc1]  → 第2个才命中  RR = 1/2 = 0.5
查询3的检索结果:  [❌ doc4] [❌ doc5] [❌ doc6]  → 全部未命中  RR = 0.0

MRR = 三个查询的 RR 取平均 = (1.0 + 0.5 + 0.0) / 3 = 0.5
```

### MRR 的特点

- **只关心第一个相关结果的位置**：不管后面还有多少相关文档
- **特别适合问答场景**：用户通常只需要一个最佳答案
- **对排名非常敏感**：第1名和第2名的差距（1.0 vs 0.5）远大于第9名和第10名（0.11 vs 0.10）

---

## 指标4：NDCG@K（Normalized Discounted Cumulative Gain）

### 定义

**考虑排名位置的综合评估指标。相关文档排在前面得分高，排在后面得分低，最终归一化到 0-1 之间。**

NDCG 是信息检索领域最经典的指标之一，名字虽然长，但拆开来看并不难。

### 公式拆解（一步步来）

**第一步：CG（Cumulative Gain，累积增益）**

把所有检索结果的相关度分数加起来，不考虑排名。

```
CG = 相关度1 + 相关度2 + ... + 相关度K
```

问题：CG 不考虑排名顺序，把相关文档排在第1位和第10位得分一样，这不合理。

**第二步：DCG（Discounted Cumulative Gain，折扣累积增益）**

给排名靠后的结果打折扣——排名越靠后，折扣越大。

```
DCG@K = Σ (相关度_i / log2(排名_i + 1))
```

`log2` 是折扣因子：排名第1位除以 `log2(2)=1`（不打折），排名第2位除以 `log2(3)=1.58`（打了折），排名越靠后折扣越大。

**第三步：NDCG（Normalized DCG，归一化 DCG）**

用"理想排序下的 DCG"来归一化，使得分数在 0-1 之间。

```
IDCG@K = 把相关文档按相关度从高到低排列后的 DCG（最好情况）
NDCG@K = DCG@K / IDCG@K
```

用大白话说：**"你的排序和理想排序相比，有多接近？1.0 表示完美排序。"**

### 手写实现

```python
import math


def dcg_at_k(relevance_scores: list, k: int) -> float:
    """
    计算 DCG@K

    参数:
        relevance_scores: 每个位置的相关度分数（按检索排名顺序）
                          例如 [1, 0, 1, 0, 1] 表示第1、3、5个结果相关
        k: 取前K个结果
    """
    dcg = 0.0
    for i in range(min(k, len(relevance_scores))):
        # log2(i+2) 是因为排名从1开始，log2(1)=0 会导致除零
        # 所以用 log2(排名+1) = log2(i+1+1) = log2(i+2)
        dcg += relevance_scores[i] / math.log2(i + 2)
    return dcg


def ndcg_at_k(relevance_scores: list, k: int) -> float:
    """
    计算 NDCG@K
    """
    # 实际的 DCG
    dcg = dcg_at_k(relevance_scores, k)

    # 理想的 DCG（把相关度从高到低排序，这是最好的情况）
    ideal_scores = sorted(relevance_scores, reverse=True)
    idcg = dcg_at_k(ideal_scores, k)

    # 归一化
    return dcg / idcg if idcg > 0 else 0.0


# ===== 示例1：二元相关度（0或1） =====
print("=== 二元相关度示例 ===")

# 实际排序：第1、3、5个结果相关
actual = [1, 0, 1, 0, 1]
print(f"实际排序 {actual}")
print(f"  DCG@5  = {dcg_at_k(actual, 5):.3f}")
print(f"  NDCG@5 = {ndcg_at_k(actual, 5):.3f}")

# 理想排序：相关文档全部排在前面
ideal = [1, 1, 1, 0, 0]
print(f"理想排序 {ideal}")
print(f"  DCG@5  = {dcg_at_k(ideal, 5):.3f}")
print(f"  NDCG@5 = {ndcg_at_k(ideal, 5):.3f}")  # 一定是 1.0

# 最差排序：相关文档全部排在后面
worst = [0, 0, 1, 1, 1]
print(f"最差排序 {worst}")
print(f"  DCG@5  = {dcg_at_k(worst, 5):.3f}")
print(f"  NDCG@5 = {ndcg_at_k(worst, 5):.3f}")


# ===== 示例2：多级相关度（0/1/2） =====
print("\n=== 多级相关度示例 ===")

# 相关度：2=非常相关，1=部分相关，0=不相关
graded = [2, 0, 1, 0, 1]
print(f"实际排序 {graded}")
print(f"  NDCG@5 = {ndcg_at_k(graded, 5):.3f}")
```

**运行输出：**
```
=== 二元相关度示例 ===
实际排序 [1, 0, 1, 0, 1]
  DCG@5  = 1.887
  NDCG@5 = 0.886
理想排序 [1, 1, 1, 0, 0]
  DCG@5  = 2.131
  NDCG@5 = 1.000
最差排序 [0, 0, 1, 1, 1]
  DCG@5  = 1.317
  NDCG@5 = 0.618

=== 多级相关度示例 ===
实际排序 [2, 0, 1, 0, 1]
  NDCG@5 = 0.922
```

### 直觉理解

```
为什么 NDCG 比 Precision 更好？

场景：3个相关文档，5个检索结果

排序A: [✅] [✅] [✅] [❌] [❌]  → Precision@5=0.6, NDCG@5=1.0
排序B: [❌] [❌] [✅] [✅] [✅]  → Precision@5=0.6, NDCG@5=0.62

Precision 认为两种排序一样好（都是0.6），
但 NDCG 知道排序A更好（相关文档排在前面）！
```

### NDCG 的优势

- **考虑排名位置**：不像 Precision 只看数量，NDCG 还看排在哪
- **支持多级相关度**：不只是"相关/不相关"，还能区分"非常相关/部分相关/不相关"
- **归一化到 0-1**：方便不同查询之间比较

---

## 指标5：Hit Rate@K（命中率）

### 定义

**前 K 个结果中是否至少包含一个相关文档。结果只有 0（未命中）或 1（命中）。**

这是所有指标中最简单的一个。它不关心找到了几个相关文档，也不关心排在第几位，只关心一个问题：**"有没有找到？"**

生活类比：你去超市买牛奶，不管货架上有多少种牛奶、摆在哪个位置，只要你最终买到了牛奶就算"命中"。

### 手写实现

```python
def hit_at_k(retrieved_ids: list, relevant_ids: set, k: int) -> int:
    """
    计算单个查询的 Hit@K（是否命中）

    返回:
        1 表示命中（前K个结果中有相关文档）
        0 表示未命中
    """
    retrieved_k = set(retrieved_ids[:k])
    return 1 if retrieved_k & relevant_ids else 0


def average_hit_rate(queries_results: list, k: int) -> float:
    """
    计算多个查询的平均命中率

    参数:
        queries_results: [(retrieved_ids, relevant_ids), ...]
        k: 取前K个结果
    """
    hits = [hit_at_k(ret, rel, k) for ret, rel in queries_results]
    return sum(hits) / len(hits) if hits else 0.0


# ===== 示例 =====
queries = [
    (["doc1", "doc3", "doc5"], {"doc1", "doc2"}),  # 命中（doc1相关）
    (["doc4", "doc2", "doc1"], {"doc1", "doc2"}),  # 命中（doc2相关）
    (["doc4", "doc5", "doc6"], {"doc1", "doc2"}),  # 未命中
    (["doc7", "doc1", "doc8"], {"doc1", "doc3"}),  # 命中（doc1相关）
]

for i, (ret, rel) in enumerate(queries):
    h = hit_at_k(ret, rel, k=3)
    print(f"查询{i+1}: Hit@3 = {h}")

print(f"\n平均 Hit Rate@3 = {average_hit_rate(queries, k=3):.2f}")
# 3个命中 / 4个查询 = 0.75
```

**运行输出：**
```
查询1: Hit@3 = 1
查询2: Hit@3 = 1
查询3: Hit@3 = 0
查询4: Hit@3 = 1

平均 Hit Rate@3 = 0.75
```

### Hit Rate 的特点

- **最简单的指标**：只有 0 和 1，计算快、理解容易
- **适合快速筛选**：在大量实验中快速判断哪些配置"完全不行"
- **信息量最少**：不区分"找到1个"和"找到10个"，也不关心排名

---

## 五大指标对比总结

| 指标 | 衡量什么 | 优点 | 缺点 | 最佳适用场景 |
|------|----------|------|------|--------------|
| **Precision@K** | 结果中相关的比例 | 简单直观 | 不考虑排序 | 通用场景 |
| **Recall@K** | 相关文档的覆盖率 | 衡量完整性 | 需要知道全部相关文档 | 法律/医疗（不能遗漏） |
| **MRR** | 第一个相关结果的位置 | 关注最佳结果 | 只看第一个相关文档 | 问答系统 |
| **NDCG@K** | 排序质量（位置加权） | 考虑位置权重 | 计算相对复杂 | 搜索排序优化 |
| **Hit Rate@K** | 是否至少命中一个 | 最简单 | 信息量最少 | 快速筛选实验 |

---

## 如何选择指标？

不同的 RAG 应用场景，应该侧重不同的指标：

```
你的 RAG 场景是什么？
│
├── 精准问答（只需要最佳答案）
│   └── 推荐：MRR + Precision@3
│       原因：用户只看第一个答案，排名很重要
│
├── 文档检索（需要多个参考文档）
│   └── 推荐：NDCG@10 + Recall@10
│       原因：需要多个相关文档，且排序要好
│
├── 知识库搜索（绝对不能遗漏）
│   └── 推荐：Recall@K + Hit Rate@K
│       原因：覆盖率比精确率更重要
│
└── 通用场景（不确定用哪个）
    └── 推荐：Precision@5 + Recall@5 + MRR
        原因：三个指标互补，全面评估
```

**实用建议：** 不要只看一个指标！至少选 2-3 个指标组合使用，才能全面了解检索质量。

---

## 在 RAG 开发中的实际应用

### 应用1：评估 Embedding 模型选型

选择 Embedding 模型时，用同一批测试数据对比不同模型的检索指标：

```python
# 伪代码：对比不同 Embedding 模型的检索效果
models = ["text-embedding-3-small", "text-embedding-3-large", "bge-large-zh"]
test_queries = load_test_queries()  # 测试查询集
ground_truth = load_ground_truth()  # 标准答案

for model_name in models:
    # 用该模型构建索引并检索
    results = retrieve_with_model(model_name, test_queries, k=5)
    # 计算指标
    p5 = avg_precision_at_k(results, ground_truth, k=5)
    r5 = avg_recall_at_k(results, ground_truth, k=5)
    mrr = compute_mrr(results, ground_truth)
    print(f"{model_name}: P@5={p5:.3f}, R@5={r5:.3f}, MRR={mrr:.3f}")
```

```
预期输出（示意）:
text-embedding-3-small: P@5=0.620, R@5=0.710, MRR=0.780
text-embedding-3-large: P@5=0.680, R@5=0.760, MRR=0.830
bge-large-zh:           P@5=0.700, R@5=0.780, MRR=0.850
```

### 应用2：评估 Chunk 大小的影响

不同的 Chunk 大小会直接影响检索质量：

```
Chunk大小    Precision@5    Recall@5    MRR
──────────────────────────────────────────────
128 tokens   0.72           0.45        0.81
256 tokens   0.68           0.62        0.79
512 tokens   0.60           0.75        0.73
1024 tokens  0.52           0.82        0.65

观察：
- Chunk 越小 → Precision 越高（更精准），但 Recall 可能下降
- Chunk 越大 → Recall 越高（覆盖更多内容），但 Precision 下降
- 需要根据场景找到平衡点
```

### 应用3：评估 ReRank 的效果

ReRank 主要改善排序质量，所以 NDCG 和 MRR 的提升最明显：

```
方案              Precision@5    MRR      NDCG@5
──────────────────────────────────────────────────
无 ReRank          0.64          0.72     0.68
+ BM25 ReRank      0.66          0.78     0.74
+ Cross-Encoder    0.68          0.85     0.82

观察：
- ReRank 对 Precision 提升有限（因为候选集没变）
- ReRank 对 MRR 和 NDCG 提升显著（排序变好了）
- Cross-Encoder 效果最好，但速度最慢
```

---

## 一句话记住

**检索评估的核心就三个问题：找得准不准（Precision）、找得全不全（Recall）、排得好不好（MRR/NDCG）——搞清楚这三个维度，检索优化就有方向了。**

---
