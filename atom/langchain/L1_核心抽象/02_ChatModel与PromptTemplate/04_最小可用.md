# 最小可用知识

> **核心理念**: 掌握 20% 的核心知识，解决 80% 的实际问题

---

## 快速上手：5个核心知识点

掌握以下内容，就能开始使用 ChatModel 和 PromptTemplate 构建 AI 应用：

---

### 4.1 ChatModel 的基础调用

**核心概念**: ChatModel 接收消息列表，返回 AI 消息

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# 1. 创建模型实例
model = ChatOpenAI(
    model="gpt-4o-mini",  # 模型名称
    temperature=0.7        # 创造性（0-2）
)

# 2. 构建消息列表
messages = [
    SystemMessage(content="你是一个Python专家"),
    HumanMessage(content="什么是列表推导式？")
]

# 3. 调用模型
response = model.invoke(messages)

# 4. 获取结果
print(response.content)
```

**实际应用**: 这是所有 AI 对话应用的基础，无论是聊天机器人、代码助手还是智能客服。

**记住**: `SystemMessage` 定义行为，`HumanMessage` 是用户输入，`invoke()` 执行调用。

---

### 4.2 三种消息类型的使用

**核心概念**: 不同消息类型有不同的语义作用

```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    # SystemMessage: 定义 AI 的角色和行为规则
    SystemMessage(content="你是一个友好的助手，回答要简洁明了"),

    # HumanMessage: 用户的输入
    HumanMessage(content="Python 和 JavaScript 有什么区别？"),

    # AIMessage: AI 的历史回复（用于多轮对话）
    AIMessage(content="Python 是解释型语言，主要用于..."),

    # HumanMessage: 用户的后续问题
    HumanMessage(content="能举个例子吗？")
]

response = model.invoke(messages)
```

**实际应用**:
- **SystemMessage**: 定义 Agent 的专业领域和回答风格
- **HumanMessage**: 接收用户输入
- **AIMessage**: 维护对话历史，实现多轮对话

**记住**: SystemMessage 只需要一条，AIMessage 用于保存历史对话。

---

### 4.3 ChatPromptTemplate 的基础使用

**核心概念**: 模板让提示词可复用、可维护

```python
from langchain_core.prompts import ChatPromptTemplate

# 1. 创建模板（定义一次）
template = ChatPromptTemplate.from_messages([
    ("system", "你是一个{role}"),
    ("human", "{question}")
])

# 2. 使用模板（多次复用）
messages = template.invoke({
    "role": "Python专家",
    "question": "什么是装饰器？"
})

# 3. 与模型组合
chain = template | model
response = chain.invoke({
    "role": "Python专家",
    "question": "什么是装饰器？"
})
```

**实际应用**:
- 统一管理提示词
- 不同场景复用同一模板
- 方便 A/B 测试和版本控制

**记住**: `("system", "...")` 是简写，等价于 `SystemMessage(...)`。

---

### 4.4 MessagesPlaceholder 处理对话历史

**核心概念**: 动态注入不定长度的对话历史

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# 1. 创建带历史的模板
template = ChatPromptTemplate.from_messages([
    ("system", "你是一个助手"),
    MessagesPlaceholder(variable_name="history"),  # 动态历史
    ("human", "{question}")
])

# 2. 使用时注入历史
from langchain_core.messages import HumanMessage, AIMessage

history = [
    HumanMessage(content="我叫张三"),
    AIMessage(content="你好张三！"),
    HumanMessage(content="我喜欢Python"),
    AIMessage(content="Python是很棒的语言！")
]

response = (template | model).invoke({
    "history": history,
    "question": "我叫什么名字？"  # AI 能记住：张三
})
```

**实际应用**:
- 实现有记忆的对话机器人
- 维护多轮对话上下文
- 动态管理对话历史长度

**记住**: `MessagesPlaceholder` 接收消息列表，长度可变。

---

### 4.5 三种调用方式的选择

**核心概念**: 根据场景选择 invoke/stream/batch

```python
# 方式1: invoke - 同步调用（等待完整响应）
response = model.invoke(messages)
print(response.content)  # 一次性输出

# 方式2: stream - 流式调用（逐字输出）
for chunk in model.stream(messages):
    print(chunk.content, end="", flush=True)  # 逐字打印

# 方式3: batch - 批量调用（多个请求）
messages_list = [messages1, messages2, messages3]
responses = model.batch(messages_list)  # 批量处理
```

**选择标准**:

| 场景 | 推荐方式 | 原因 |
|------|----------|------|
| 简单问答 | invoke | 代码简单，等待时间可接受 |
| 聊天界面 | stream | 降低感知延迟，用户体验好 |
| 批量评估 | batch | 降低成本（2025+ 支持批处理 API） |
| 异步场景 | ainvoke/astream | 不阻塞主线程 |

**实际应用**:
- **invoke**: 后端 API、批量处理、测试脚本
- **stream**: Web 聊天界面、命令行工具、实时对话
- **batch**: 数据标注、批量评估、离线分析

**记住**: 流式不是为了快，而是为了降低感知延迟。

---

## 最小可用代码模板

### 模板1: 简单问答

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

model = ChatOpenAI(model="gpt-4o-mini")
messages = [
    SystemMessage(content="你是助手"),
    HumanMessage(content="你好")
]
response = model.invoke(messages)
print(response.content)
```

### 模板2: 可复用模板

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

model = ChatOpenAI(model="gpt-4o-mini")
template = ChatPromptTemplate.from_messages([
    ("system", "你是{role}"),
    ("human", "{question}")
])

chain = template | model
response = chain.invoke({"role": "助手", "question": "你好"})
print(response.content)
```

### 模板3: 多轮对话

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

model = ChatOpenAI(model="gpt-4o-mini")
template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}")
])

chain = template | model
response = chain.invoke({
    "history": [],  # 首次对话为空
    "question": "你好"
})
print(response.content)
```

### 模板4: 流式输出

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

model = ChatOpenAI(model="gpt-4o-mini")
template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

chain = template | model
for chunk in chain.stream({"question": "讲个笑话"}):
    print(chunk.content, end="", flush=True)
```

---

## 这些知识足以

掌握以上 5 个核心知识点后，你可以：

### ✅ 能做什么

1. **构建基础对话应用**
   - 简单的聊天机器人
   - 问答系统
   - 代码助手

2. **实现多轮对话**
   - 维护对话历史
   - 上下文感知回复
   - 个性化对话

3. **优化用户体验**
   - 流式输出降低等待感
   - 批量处理降低成本
   - 模板复用提高效率

4. **开始 AI Agent 开发**
   - 理解 Agent 的对话基础
   - 为工具调用做准备
   - 为状态管理做准备

### ✅ 为后续学习打基础

- **OutputParser**: 结构化输出（下一个知识点）
- **链式组合**: LCEL 表达式
- **Agent 系统**: 工具调用和推理
- **LangGraph**: 有状态的工作流

---

## 快速检查清单

完成最小可用知识学习后，你应该能够：

**基础能力**
- [ ] 创建 ChatModel 实例并调用
- [ ] 使用 SystemMessage/HumanMessage/AIMessage
- [ ] 创建 ChatPromptTemplate 并使用变量
- [ ] 使用 MessagesPlaceholder 管理历史
- [ ] 选择合适的调用方式（invoke/stream/batch）

**实战能力**
- [ ] 写出简单问答的完整代码
- [ ] 写出可复用模板的完整代码
- [ ] 写出多轮对话的完整代码
- [ ] 写出流式输出的完整代码

**理解能力**
- [ ] 解释为什么需要 ChatModel 而不是直接调用 API
- [ ] 解释为什么需要不同的消息类型
- [ ] 解释为什么需要 PromptTemplate
- [ ] 解释什么时候用 invoke/stream/batch

---

## 常见问题速查

### Q1: ChatModel 和 LLM 有什么区别？

**简答**: ChatModel 用消息列表，LLM 用字符串。

```python
# ChatModel - 消息列表
messages = [SystemMessage("..."), HumanMessage("...")]
response = chatmodel.invoke(messages)

# LLM - 字符串
text = "你是助手。\n\n用户: 你好"
response = llm.invoke(text)
```

**推荐**: 现代应用优先用 ChatModel。

### Q2: SystemMessage 必须要吗？

**简答**: 不是必须，但强烈推荐。

```python
# 没有 SystemMessage - 行为不可控
messages = [HumanMessage("你好")]

# 有 SystemMessage - 行为可控
messages = [
    SystemMessage("你是专业的Python助手，回答要简洁"),
    HumanMessage("你好")
]
```

**推荐**: 生产环境必须加 SystemMessage。

### Q3: 模板变量必须都提供吗？

**简答**: 是的，否则会报错。

```python
template = ChatPromptTemplate.from_messages([
    ("system", "你是{role}"),
    ("human", "{question}")
])

# ❌ 错误 - 缺少 role
template.invoke({"question": "你好"})  # KeyError

# ✅ 正确 - 提供所有变量
template.invoke({"role": "助手", "question": "你好"})
```

**技巧**: 使用 `partial()` 预填充部分变量。

### Q4: 对话历史会无限增长吗？

**简答**: 会的，需要手动管理。

```python
# 问题：历史无限增长
history.append(HumanMessage("..."))
history.append(AIMessage("..."))
# ... 最终超过 context window

# 解决：限制历史长度
MAX_HISTORY = 10
history = history[-MAX_HISTORY:]  # 只保留最近10条
```

**推荐**: 使用 LangChain 的 Memory 组件自动管理。

### Q5: stream 比 invoke 更快吗？

**简答**: 总时间一样，但感知延迟更低。

```python
# invoke - 等待完整响应（感觉慢）
response = model.invoke(messages)  # 等待 5 秒
print(response.content)  # 一次性输出

# stream - 逐字输出（感觉快）
for chunk in model.stream(messages):  # 0.1 秒后开始输出
    print(chunk.content, end="")  # 逐字打印
# 总时间还是 5 秒，但用户 0.1 秒就看到响应
```

**推荐**: 用户界面优先用 stream。

---

## 下一步学习建议

### 如果你想深入理解

1. **阅读核心概念**: `03_核心概念_01_ChatModel抽象.md`
2. **理解消息系统**: `03_核心概念_02_消息类型系统.md`
3. **掌握模板进阶**: `03_核心概念_04_ChatPromptTemplate.md`

### 如果你想动手实践

1. **基础调用**: `07_实战代码_01_基础ChatModel调用.md`
2. **模板使用**: `07_实战代码_02_ChatPromptTemplate使用.md`
3. **对话管理**: `07_实战代码_03_对话历史管理.md`

### 如果你想准备面试

1. **常见误区**: `06_反直觉点.md`
2. **面试题库**: `08_面试必问.md`
3. **知识卡片**: `09_化骨绵掌.md`

---

## 实践建议

### 立即动手

1. **复制模板代码**，在本地运行
2. **修改变量**，观察输出变化
3. **尝试不同模型**（gpt-4o-mini, gpt-4o）
4. **对比 invoke 和 stream** 的体验差异

### 循序渐进

1. 先掌握 **简单问答**（模板1）
2. 再学习 **可复用模板**（模板2）
3. 然后实现 **多轮对话**（模板3）
4. 最后优化 **用户体验**（模板4）

### 避免过度学习

- ❌ 不要一开始就学习所有高级特性
- ❌ 不要纠结于性能优化细节
- ❌ 不要过早引入复杂的状态管理
- ✅ 先让代码跑起来，再逐步优化

---

**记住**: 这 20% 的知识足以解决 80% 的问题。先掌握基础，再深入进阶！
