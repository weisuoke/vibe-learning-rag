# 化骨绵掌

10个2分钟知识卡片，覆盖文本分块的完整知识体系（包含 2025-2026 最新技术）。

---

## 卡片1：直觉理解 - 什么是文本分块？

**一句话：** 文本分块是把长文档切成小片段的技术，就像把披萨切成合适大小的块，方便"消化"（检索和理解）。

**核心问题：** 为什么不能直接用整个文档？
- Embedding 模型有长度限制（512-8191 tokens）
- LLM Context Window 有限且成本高
- 长文档向量无法精准匹配查询

**类比：**
- **前端类比**：虚拟列表分页（每页显示多少条）
- **日常类比**：切披萨（块太小不够吃，块太大拿不动）

**应用：** RAG 系统的核心环节，位于文档加载和向量化之间。

---

## 卡片2：形式化定义 - 分块的本质

**一句话：** 分块是在"检索精度"和"上下文完整性"之间寻找平衡的技术。

**数学表达：**
```
文档 D = {c₁, c₂, ..., cₙ}  (n 个 chunks)

约束条件：
1. size(cᵢ) ≤ max_chunk_size
2. overlap(cᵢ, cᵢ₊₁) = chunk_overlap
3. ∪cᵢ = D (完整覆盖)
```

**核心权衡：**
- 块太小 → 精度高但上下文不完整
- 块太大 → 上下文全但检索不精准
- **NVIDIA 2025 发现**：页面级分块（较大块）准确率最高（0.648）

**应用：** 理解分块不是简单切分，而是优化问题。

---

## 卡片3：固定大小分块 - 最简单的方法

**一句话：** 按固定字符数或 token 数切分，实现简单但可能破坏语义。

**实现：**
```python
def fixed_chunk(text, size=512, overlap=77):
    chunks = []
    for i in range(0, len(text), size - overlap):
        chunks.append(text[i:i+size])
    return chunks
```

**优缺点：**
- ✅ 实现简单、速度快、无额外成本
- ❌ 可能切断句子、不考虑语义边界

**NVIDIA 2025 推荐：**
- chunk_size=512（事实查询）
- chunk_overlap=77（15% 重叠率最优）

**应用：** 快速原型、成本敏感场景。

---

## 卡片4：递归字符分块 - 通用首选

**一句话：** 按分隔符优先级递归切分（段落→句子→词→字符），是 LangChain 默认策略。

**分隔符优先级：**
```python
separators = [
    "\n\n",   # 段落（最优先）
    "\n",     # 换行
    "。",     # 句子（中文）
    ".",      # 句子（英文）
    " ",      # 单词
    "",       # 字符（最后手段）
]
```

**优势：** 保持语义完整性，在自然边界切分。

**2025-2026 推荐配置：**
```python
RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,  # 15% NVIDIA 最优
)
```

**应用：** 90% 的通用场景，生产环境首选。

---

## 卡片5：语义分块 - 基于相似度

**一句话：** 计算句子 embedding 相似度，在语义变化处切分，质量高但成本也高。

**工作原理：**
```
1. 按句子切分文本
2. 计算每个句子的 embedding
3. 计算相邻句子的余弦相似度
4. 在相似度低于阈值处切分
```

**成本效益分析（2025-2026）：**
- 准确率：0.655（仅比递归分块提升 2-3%）
- 成本：高（需要 embedding API）
- **更优方案**：递归分块 + 上下文感知（准确率 0.720，成本仅 1/10）

**应用：** 主题频繁变化的文档（新闻、百科）。

---

## 卡片6：代理式分块 - AI 智能理解 ⭐ NEW 2025-2026

**一句话：** 让 LLM 像人类编辑一样理解文档结构，智能确定分块边界，是质量最高的方法。

**IBM 2025-2026 研究：**
- 提升 15-20% 检索准确率
- 自动生成元数据（主题、关键词、章节）
- 成本：$0.82/10万字

**4步流程：**
1. 文本准备：清洗格式化
2. 智能分块：LLM 理解结构，动态确定边界
3. 元数据标注：自动生成标签
4. 向量化：chunk + 元数据一起向量化

**ROI 分析：** 医疗场景误诊成本 $10,000+ vs 分块成本 $0.82 = 12,000x 回报

**应用：** 高价值文档（医疗、法律、金融）。

**研究来源**: [IBM Agentic Chunking](https://www.ibm.com/think/topics/agentic-chunking)

---

## 卡片7：上下文感知分块 - 减少失败 ⭐ NEW 2024-2025

**一句话：** 为每个 chunk 添加 50-100 token 的文档级上下文说明，减少 49-67% 检索失败。

**Anthropic 2024-2025 研究：**
- 单独使用：减少 49% 检索失败
- 结合 reranking：减少 67% 检索失败
- 成本：$0.02/10万字（使用 Claude 3 Haiku）

**示例：**
```
传统 chunk：
"变量是存储数据的容器。"

上下文感知 chunk：
"[上下文] 这是 Python 编程教程第一章关于变量的内容。

变量是存储数据的容器。"
```

**成本优化：** Prompt Caching 降低 90% 成本。

**应用：** 生产环境标配，性价比极高。

**研究来源**: [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)

---

## 卡片8：NVIDIA 2025 研究 - 三大发现

**一句话：** NVIDIA 2025 年研究推翻了传统观念，验证了最优分块策略。

**三大核心发现：**

**1. 页面级分块最优**
- 准确率：0.648（最高）
- 方差：0.107（最稳定）
- 推翻"块越小越精准"的观念

**2. 查询类型决定块大小**
- 事实查询：256-512 tokens
- 分析查询：1024+ tokens
- 混合查询：512-768 tokens

**3. 15% 重叠率最优**
- 过低（<10%）：边界信息丢失
- 过高（>20%）：成本增加但收益递减
- 最优：15%（平衡成本和效果）

**实践建议：** 根据查询类型动态调整 chunk_size，固定使用 15% 重叠率。

**研究来源**: [NVIDIA 2025 Chunking Benchmark](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)

---

## 卡片9：实战应用 - 如何选择策略

**一句话：** 根据文档价值、查询类型、成本预算三个维度选择最优策略。

**决策框架（2025-2026）：**

**按文档价值：**
- **高价值**（医疗、法律、金融）→ 代理式分块
  - 成本：$0.82/10万字，准确率：+15-20%
- **中等价值**（企业知识库）→ 递归 + 上下文感知
  - 成本：$0.10/10万字，准确率：+12%
- **普通文档**（一般内容）→ 递归分块
  - 成本：$0，准确率：基准

**按查询类型（NVIDIA 2025）：**
- 事实查询：chunk_size=512
- 分析查询：chunk_size=1024
- 混合查询：chunk_size=768
- 重叠率：15%（固定）

**实际案例：**
- 医疗诊断知识库：代理式分块
- 企业技术文档：递归 + 上下文感知
- 个人博客：递归分块

---

## 卡片10：总结与延伸 - 技术演进

**一句话：** 文本分块从规则驱动演进到智能驱动，2025-2026 年是技术革命年。

**技术演进时间线：**
```
2023年前：固定大小 + 递归字符（规则驱动）
    ↓
2024年：语义分块（embedding 驱动）
    ↓
2024-2025：上下文感知分块（Anthropic，-49-67% 失败）
    ↓
2025-2026：代理式分块（IBM，+15-20% 准确率）
    ↓
2025：NVIDIA 基准测试（页面级最优，15% 重叠）
```

**核心突破：**
- **从规则到智能**：固定规则 → LLM 理解
- **从孤立到上下文**：孤立块 → 上下文注入
- **从固定到自适应**：固定大小 → 查询类型自适应

**未来趋势：**
- 多模态分块（图文混合）
- 实时自适应分块
- 端到端优化（分块 + 检索 + 生成联合优化）

**延伸学习：**
- 向量存储：分块后如何存储和索引
- 检索器设计：如何高效检索分块
- Reranking：如何重排序检索结果

---

## 核心研究来源

1. **NVIDIA 2025**: [Finding the Best Chunking Strategy](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)
2. **Anthropic 2024-2025**: [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)
3. **IBM 2025-2026**: [Agentic Chunking](https://www.ibm.com/think/topics/agentic-chunking)

---

**下一步：** [10_一句话总结](./10_一句话总结.md) - 完整知识体系总结
