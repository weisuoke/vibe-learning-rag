# 实战代码1：基础重试示例

## 概述

本文提供基础重试的完整可运行示例，涵盖：
- 简单 API 重试
- 网络错误处理
- with_retry 基础用法
- 实际应用场景

所有代码都可以直接复制运行。

---

## 示例1：最简单的重试

**场景：** OpenAI API 调用可能因速率限制失败

```python
"""
最简单的重试示例
演示：为 LLM 调用添加基础重试
"""

from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

# ===== 1. 创建不带重试的 LLM =====
print("=== 不带重试的 LLM ===")
llm_no_retry = ChatOpenAI(model="gpt-4")

try:
    response = llm_no_retry.invoke("你好，请介绍一下自己")
    print(f"成功: {response.content[:50]}...")
except Exception as e:
    print(f"失败: {type(e).__name__}: {e}")

# ===== 2. 创建带重试的 LLM =====
print("\n=== 带重试的 LLM ===")
llm_with_retry = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3  # 最多重试 3 次
)

try:
    response = llm_with_retry.invoke("你好，请介绍一下自己")
    print(f"成功: {response.content[:50]}...")
except Exception as e:
    print(f"失败（重试3次后）: {type(e).__name__}: {e}")

# ===== 3. 对比 =====
print("\n=== 对比 ===")
print("不带重试：失败后立即抛出异常")
print("带重试：失败后自动重试，最多 3 次")
```

**运行输出示例：**
```
=== 不带重试的 LLM ===
成功: 你好！我是 Claude，一个由 Anthropic 开发的 AI 助手...

=== 带重试的 LLM ===
成功: 你好！我是 Claude，一个由 Anthropic 开发的 AI 助手...

=== 对比 ===
不带重试：失败后立即抛出异常
带重试：失败后自动重试，最多 3 次
```

---

## 示例2：处理速率限制

**场景：** OpenAI API 有每分钟请求限制

```python
"""
处理速率限制
演示：使用重试处理 RateLimitError
"""

from langchain_openai import ChatOpenAI
from openai import RateLimitError
from dotenv import load_dotenv
import time

load_dotenv()

# ===== 1. 创建带重试的 LLM =====
print("=== 处理速率限制 ===")

llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,              # 速率限制可能需要更多重试
    wait_exponential_jitter=True,      # 指数退避
    retry_if_exception_type=(RateLimitError,)  # 只重试速率限制
)

# ===== 2. 批量请求（可能触发速率限制）=====
prompts = [
    "什么是 Python？",
    "什么是 JavaScript？",
    "什么是 Go？",
    "什么是 Rust？",
    "什么是 Java？"
]

print(f"\n处理 {len(prompts)} 个请求...")
start_time = time.time()

results = []
for i, prompt in enumerate(prompts):
    try:
        print(f"\n[{i+1}/{len(prompts)}] 处理: {prompt}")
        response = llm.invoke(prompt)
        results.append(response.content[:50] + "...")
        print(f"✅ 成功")
    except RateLimitError as e:
        print(f"❌ 速率限制（重试5次后仍失败）")
        results.append(None)
    except Exception as e:
        print(f"❌ 其他错误: {type(e).__name__}")
        results.append(None)

elapsed_time = time.time() - start_time

# ===== 3. 统计结果 =====
print("\n=== 结果统计 ===")
success_count = len([r for r in results if r])
print(f"成功: {success_count}/{len(prompts)}")
print(f"失败: {len(prompts) - success_count}/{len(prompts)}")
print(f"总耗时: {elapsed_time:.2f} 秒")

# ===== 4. 显示结果 =====
print("\n=== 结果详情 ===")
for i, (prompt, result) in enumerate(zip(prompts, results)):
    if result:
        print(f"{i+1}. {prompt}")
        print(f"   {result}")
    else:
        print(f"{i+1}. {prompt}")
        print(f"   ❌ 失败")
```

**运行输出示例：**
```
=== 处理速率限制 ===

处理 5 个请求...

[1/5] 处理: 什么是 Python？
✅ 成功

[2/5] 处理: 什么是 JavaScript？
✅ 成功

[3/5] 处理: 什么是 Go？
✅ 成功

[4/5] 处理: 什么是 Rust？
✅ 成功

[5/5] 处理: 什么是 Java？
✅ 成功

=== 结果统计 ===
成功: 5/5
失败: 0/5
总耗时: 12.34 秒

=== 结果详情 ===
1. 什么是 Python？
   Python 是一种高级编程语言，以其简洁的语法和强大的功能而闻名...
2. 什么是 JavaScript？
   JavaScript 是一种主要用于 Web 开发的编程语言...
...
```

---

## 示例3：处理网络超时

**场景：** 网络不稳定导致请求超时

```python
"""
处理网络超时
演示：使用重试处理 Timeout 和 ConnectionError
"""

from langchain_openai import ChatOpenAI
from openai import Timeout, APIConnectionError
from dotenv import load_dotenv
import time

load_dotenv()

# ===== 1. 创建带超时重试的 LLM =====
print("=== 处理网络超时 ===")

llm = ChatOpenAI(
    model="gpt-4",
    timeout=10  # 设置 10 秒超时
).with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True,
    retry_if_exception_type=(Timeout, APIConnectionError)
)

# ===== 2. 测试不同长度的请求 =====
test_cases = [
    ("短请求", "你好"),
    ("中等请求", "请详细解释什么是机器学习，包括其主要类型和应用场景"),
    ("长请求", "请写一篇关于人工智能发展历史的文章，包括从图灵测试到现代深度学习的演变过程")
]

print("\n测试不同长度的请求...")

for name, prompt in test_cases:
    print(f"\n--- {name} ---")
    print(f"Prompt: {prompt[:50]}...")

    start_time = time.time()
    try:
        response = llm.invoke(prompt)
        elapsed = time.time() - start_time
        print(f"✅ 成功 (耗时: {elapsed:.2f}秒)")
        print(f"响应长度: {len(response.content)} 字符")
    except Timeout:
        elapsed = time.time() - start_time
        print(f"❌ 超时 (耗时: {elapsed:.2f}秒)")
    except APIConnectionError:
        elapsed = time.time() - start_time
        print(f"❌ 连接错误 (耗时: {elapsed:.2f}秒)")
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"❌ 其他错误: {type(e).__name__} (耗时: {elapsed:.2f}秒)")
```

**运行输出示例：**
```
=== 处理网络超时 ===

测试不同长度的请求...

--- 短请求 ---
Prompt: 你好...
✅ 成功 (耗时: 1.23秒)
响应长度: 45 字符

--- 中等请求 ---
Prompt: 请详细解释什么是机器学习，包括其主要类型和应用场景...
✅ 成功 (耗时: 3.45秒)
响应长度: 523 字符

--- 长请求 ---
Prompt: 请写一篇关于人工智能发展历史的文章，包括从图灵测试到现代深度学习的演变过程...
✅ 成功 (耗时: 8.67秒)
响应长度: 1245 字符
```

---

## 示例4：在 LCEL 链中使用重试

**场景：** 为整个 LCEL 链添加重试

```python
"""
在 LCEL 链中使用重试
演示：为链中的每个组件添加重试
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from openai import RateLimitError, APIError, Timeout
from dotenv import load_dotenv

load_dotenv()

# ===== 1. 创建带重试的组件 =====
print("=== LCEL 链中的重试 ===")

# LLM 组件（关键组件，更多重试）
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,
    wait_exponential_jitter=True,
    retry_if_exception_type=(RateLimitError, APIError, Timeout)
)

# Prompt 模板
prompt = ChatPromptTemplate.from_template(
    "请用一句话总结: {topic}"
)

# 输出解析器
parser = StrOutputParser()

# ===== 2. 构建链 =====
chain = prompt | llm | parser

# ===== 3. 测试链 =====
topics = [
    "Python 编程语言",
    "机器学习",
    "区块链技术",
    "量子计算"
]

print(f"\n处理 {len(topics)} 个主题...")

for i, topic in enumerate(topics):
    print(f"\n[{i+1}/{len(topics)}] 主题: {topic}")
    try:
        result = chain.invoke({"topic": topic})
        print(f"✅ 总结: {result}")
    except Exception as e:
        print(f"❌ 失败: {type(e).__name__}: {e}")

# ===== 4. 批量处理 =====
print("\n=== 批量处理 ===")
try:
    results = chain.batch([{"topic": t} for t in topics])
    print(f"✅ 批量处理成功，处理了 {len(results)} 个主题")
    for topic, result in zip(topics, results):
        print(f"- {topic}: {result[:50]}...")
except Exception as e:
    print(f"❌ 批量处理失败: {type(e).__name__}: {e}")
```

**运行输出示例：**
```
=== LCEL 链中的重试 ===

处理 4 个主题...

[1/4] 主题: Python 编程语言
✅ 总结: Python 是一种简洁、易学、功能强大的高级编程语言

[2/4] 主题: 机器学习
✅ 总结: 机器学习是让计算机从数据中学习并做出预测的技术

[3/4] 主题: 区块链技术
✅ 总结: 区块链是一种去中心化的分布式账本技术

[4/4] 主题: 量子计算
✅ 总结: 量子计算利用量子力学原理进行超高速计算

=== 批量处理 ===
✅ 批量处理成功，处理了 4 个主题
- Python 编程语言: Python 是一种简洁、易学、功能强大的高级编程语言...
- 机器学习: 机器学习是让计算机从数据中学习并做出预测的技术...
- 区块链技术: 区块链是一种去中心化的分布式账本技术...
- 量子计算: 量子计算利用量子力学原理进行超高速计算...
```

---

## 示例5：手动实现重试逻辑（对比）

**场景：** 理解 with_retry() 的内部机制

```python
"""
手动实现重试逻辑
演示：对比手动重试和 with_retry()
"""

from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError
from dotenv import load_dotenv
import time

load_dotenv()

# ===== 1. 手动实现重试 =====
def manual_retry_invoke(llm, prompt, max_attempts=3):
    """手动实现重试逻辑"""
    for attempt in range(max_attempts):
        try:
            print(f"  尝试 {attempt + 1}/{max_attempts}...")
            response = llm.invoke(prompt)
            print(f"  ✅ 成功")
            return response
        except (RateLimitError, APIError) as e:
            print(f"  ❌ 失败: {type(e).__name__}")
            if attempt < max_attempts - 1:
                wait_time = 2 ** attempt  # 指数退避
                print(f"  等待 {wait_time} 秒后重试...")
                time.sleep(wait_time)
            else:
                print(f"  达到最大重试次数")
                raise

# ===== 2. 测试手动重试 =====
print("=== 手动重试 ===")
llm_no_retry = ChatOpenAI(model="gpt-4")

try:
    response = manual_retry_invoke(llm_no_retry, "你好", max_attempts=3)
    print(f"最终结果: {response.content[:50]}...")
except Exception as e:
    print(f"最终失败: {type(e).__name__}")

# ===== 3. 测试 with_retry() =====
print("\n=== with_retry() ===")
llm_with_retry = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True,
    retry_if_exception_type=(RateLimitError, APIError)
)

try:
    response = llm_with_retry.invoke("你好")
    print(f"✅ 成功: {response.content[:50]}...")
except Exception as e:
    print(f"❌ 失败: {type(e).__name__}")

# ===== 4. 对比 =====
print("\n=== 对比 ===")
print("手动重试:")
print("  优点: 完全控制重试逻辑")
print("  缺点: 代码冗长，容易出错")
print("\nwith_retry():")
print("  优点: 简洁，声明式，经过测试")
print("  缺点: 灵活性略低（但足够大多数场景）")
```

**运行输出示例：**
```
=== 手动重试 ===
  尝试 1/3...
  ✅ 成功
最终结果: 你好！我是 Claude，一个由 Anthropic 开发的 AI 助手...

=== with_retry() ===
✅ 成功: 你好！我是 Claude，一个由 Anthropic 开发的 AI 助手...

=== 对比 ===
手动重试:
  优点: 完全控制重试逻辑
  缺点: 代码冗长，容易出错

with_retry():
  优点: 简洁，声明式，经过测试
  缺点: 灵活性略低（但足够大多数场景）
```

---

## 示例6：完整的生产级示例

**场景：** 生产环境中的完整重试配置

```python
"""
生产级重试示例
演示：完整的生产环境配置
"""

from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError, Timeout, APIConnectionError
from tenacity import before_sleep_log, after_log
from dotenv import load_dotenv
import logging

# ===== 1. 配置日志 =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

# ===== 2. 定义可重试的错误 =====
RETRYABLE_ERRORS = (
    RateLimitError,
    APIError,
    Timeout,
    APIConnectionError
)

# ===== 3. 创建生产级 LLM =====
print("=== 生产级重试配置 ===\n")

llm = ChatOpenAI(
    model="gpt-4",
    timeout=30
).with_retry(
    stop_after_attempt=5,
    wait_exponential_jitter=True,
    retry_if_exception_type=RETRYABLE_ERRORS,
    before_sleep=before_sleep_log(logger, logging.WARNING),
    after=after_log(logger, logging.INFO)
)

# ===== 4. 测试 =====
test_prompts = [
    "什么是 LangChain？",
    "解释一下 LCEL 表达式",
    "什么是 Runnable 协议？"
]

print(f"处理 {len(test_prompts)} 个请求...\n")

for i, prompt in enumerate(test_prompts):
    logger.info(f"[{i+1}/{len(test_prompts)}] 处理: {prompt}")
    try:
        response = llm.invoke(prompt)
        logger.info(f"成功，响应长度: {len(response.content)} 字符")
        print(f"✅ [{i+1}] {prompt}")
        print(f"   {response.content[:80]}...\n")
    except Exception as e:
        logger.error(f"失败: {type(e).__name__}: {e}")
        print(f"❌ [{i+1}] {prompt}")
        print(f"   错误: {type(e).__name__}\n")

print("=== 完成 ===")
```

**运行输出示例：**
```
=== 生产级重试配置 ===

处理 3 个请求...

2026-02-21 10:30:15 - __main__ - INFO - [1/3] 处理: 什么是 LangChain？
2026-02-21 10:30:16 - __main__ - INFO - 成功，响应长度: 234 字符
✅ [1] 什么是 LangChain？
   LangChain 是一个用于构建 AI 应用的框架，它提供了一套工具和抽象...

2026-02-21 10:30:17 - __main__ - INFO - [2/3] 处理: 解释一下 LCEL 表达式
2026-02-21 10:30:18 - __main__ - INFO - 成功，响应长度: 312 字符
✅ [2] 解释一下 LCEL 表达式
   LCEL（LangChain Expression Language）是 LangChain 的声明式编程接口...

2026-02-21 10:30:19 - __main__ - INFO - [3/3] 处理: 什么是 Runnable 协议？
2026-02-21 10:30:20 - __main__ - INFO - 成功，响应长度: 278 字符
✅ [3] 什么是 Runnable 协议？
   Runnable 是 LangChain 的核心协议，定义了所有可执行组件的统一接口...

=== 完成 ===
```

---

## 运行环境要求

### 依赖安装

```bash
# 安装依赖
uv add langchain langchain-openai python-dotenv tenacity

# 或使用 pip
pip install langchain langchain-openai python-dotenv tenacity
```

### 环境变量配置

创建 `.env` 文件：

```bash
OPENAI_API_KEY=your_openai_api_key_here
# 可选：使用代理
# OPENAI_BASE_URL=https://your-proxy.com/v1
```

---

## 最佳实践总结

### 1. 重试次数选择

- **临时故障**：3-5 次
- **速率限制**：5-7 次
- **非关键服务**：2-3 次

### 2. 等待策略

- **推荐**：指数退避 + jitter
- **配置**：`wait_exponential_jitter=True`

### 3. 异常过滤

- **可重试**：RateLimitError, APIError, Timeout, APIConnectionError
- **不可重试**：AuthenticationError, InvalidRequestError

### 4. 日志记录

- **生产环境**：必须添加日志
- **工具**：使用 `before_sleep_log` 和 `after_log`

---

**记住：基础重试是生产级应用的第一道防线。**
