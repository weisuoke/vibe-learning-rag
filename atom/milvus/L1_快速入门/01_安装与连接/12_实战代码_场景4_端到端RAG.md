# 12_实战代码_场景4_端到端RAG

> 完整的 RAG 系统实战：从部署到检索

---

## 场景说明

**适用场景：** 快速搭建完整的 RAG 原型系统

**学习目标:**
- 掌握完整的 RAG 开发流程
- 理解各组件的协作方式
- 学会端到端的系统调试
- 能够快速验证 RAG 方案

**前置要求:**
- 完成场景1-3
- 理解 Embedding 和向量检索原理
- 有 OpenAI API Key

---

## 完整代码实现

```python
"""
端到端 RAG 系统实战
演示：部署 Milvus → 文档处理 → 向量存储 → 语义检索
"""

import subprocess
import time
from typing import List, Dict, Any
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
from openai import OpenAI
import os

class EndToEndRAG:
    """端到端 RAG 系统"""
    
    def __init__(
        self,
        milvus_host="localhost",
        milvus_port="19530",
        collection_name="rag_documents",
        embedding_model="text-embedding-3-small",
        llm_model="gpt-4"
    ):
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port
        self.collection_name = collection_name
        self.embedding_model = embedding_model
        self.llm_model = llm_model
        
        # OpenAI 客户端
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Milvus Collection
        self.collection = None
    
    def step1_deploy_milvus(self):
        """步骤1：部署 Milvus"""
        print("\n" + "="*50)
        print("步骤1：部署 Milvus")
        print("="*50)
        
        # 检查容器是否已存在
        result = subprocess.run(
            ["docker", "ps", "-a", "--filter", "name=milvus-rag", "--format", "{{.Names}}"],
            capture_output=True,
            text=True
        )
        
        if "milvus-rag" in result.stdout:
            print("发现已存在的容器，停止并删除...")
            subprocess.run(["docker", "stop", "milvus-rag"], check=True)
            subprocess.run(["docker", "rm", "milvus-rag"], check=True)
        
        # 启动 Milvus
        print("启动 Milvus 容器...")
        subprocess.run([
            "docker", "run", "-d",
            "--name", "milvus-rag",
            "-p", f"{self.milvus_port}:19530",
            "-p", "9091:9091",
            "milvusdb/milvus:latest"
        ], check=True)
        
        # 等待就绪
        print("等待 Milvus 就绪...")
        max_wait = 60
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            try:
                connections.connect(
                    alias="default",
                    host=self.milvus_host,
                    port=self.milvus_port,
                    timeout=5
                )
                version = utility.get_server_version()
                print(f"✅ Milvus 已就绪！版本: {version}")
                return True
            except:
                time.sleep(2)
        
        print("❌ Milvus 启动超时")
        return False
    
    def step2_create_collection(self):
        """步骤2：创建 Collection"""
        print("\n" + "="*50)
        print("步骤2：创建 Collection")
        print("="*50)
        
        # 删除已存在的 Collection
        if utility.has_collection(self.collection_name):
            utility.drop_collection(self.collection_name)
            print(f"已删除旧的 Collection: {self.collection_name}")
        
        # 定义 Schema
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=5000),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1536)
        ]
        schema = CollectionSchema(fields, description="RAG documents")
        
        # 创建 Collection
        self.collection = Collection(self.collection_name, schema)
        print(f"✅ Collection 创建成功: {self.collection_name}")
        
        # 创建索引
        index_params = {
            "index_type": "IVF_FLAT",
            "metric_type": "L2",
            "params": {"nlist": 128}
        }
        self.collection.create_index("embedding", index_params)
        print("✅ 索引创建成功")
    
    def step3_load_documents(self) -> List[Dict[str, str]]:
        """步骤3：加载文档"""
        print("\n" + "="*50)
        print("步骤3：加载文档")
        print("="*50)
        
        # 示例文档
        documents = [
            {
                "text": "Milvus 是一个开源的向量数据库，专门用于存储和检索高维向量数据。它支持多种索引类型，如 IVF_FLAT、HNSW 等。",
                "source": "milvus_intro.txt"
            },
            {
                "text": "RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术，通过检索相关文档来增强大语言模型的生成能力。",
                "source": "rag_intro.txt"
            },
            {
                "text": "向量检索的核心是计算查询向量与数据库中向量的相似度，常用的度量方式包括欧氏距离（L2）、内积（IP）和余弦相似度。",
                "source": "vector_search.txt"
            },
            {
                "text": "Embedding 是将文本转换为向量表示的过程，常用的模型包括 OpenAI 的 text-embedding-3-small 和 text-embedding-3-large。",
                "source": "embedding.txt"
            },
            {
                "text": "在 RAG 系统中，文档分块（Chunking）是一个重要步骤，需要在保持语义完整性的同时控制块的大小。",
                "source": "chunking.txt"
            }
        ]
        
        print(f"✅ 加载了 {len(documents)} 个文档")
        return documents
    
    def step4_generate_embeddings(self, documents: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """步骤4：生成 Embeddings"""
        print("\n" + "="*50)
        print("步骤4：生成 Embeddings")
        print("="*50)
        
        enriched_docs = []
        
        for i, doc in enumerate(documents):
            print(f"处理文档 {i+1}/{len(documents)}: {doc['source']}")
            
            # 生成 Embedding
            response = self.client.embeddings.create(
                input=doc["text"],
                model=self.embedding_model
            )
            embedding = response.data[0].embedding
            
            enriched_docs.append({
                **doc,
                "embedding": embedding
            })
        
        print(f"✅ 生成了 {len(enriched_docs)} 个 Embeddings")
        return enriched_docs
    
    def step5_insert_vectors(self, documents: List[Dict[str, Any]]):
        """步骤5：插入向量"""
        print("\n" + "="*50)
        print("步骤5：插入向量到 Milvus")
        print("="*50)
        
        # 准备数据
        texts = [doc["text"] for doc in documents]
        sources = [doc["source"] for doc in documents]
        embeddings = [doc["embedding"] for doc in documents]
        
        # 插入数据
        self.collection.insert([texts, sources, embeddings])
        self.collection.flush()
        
        print(f"✅ 插入了 {self.collection.num_entities} 条数据")
        
        # 加载 Collection
        self.collection.load()
        print("✅ Collection 已加载到内存")
    
    def step6_semantic_search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """步骤6：语义检索"""
        print("\n" + "="*50)
        print(f"步骤6：语义检索 - '{query}'")
        print("="*50)
        
        # 生成查询 Embedding
        print("生成查询 Embedding...")
        response = self.client.embeddings.create(
            input=query,
            model=self.embedding_model
        )
        query_embedding = response.data[0].embedding
        
        # 执行检索
        print(f"检索 Top-{top_k} 相关文档...")
        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=top_k,
            output_fields=["text", "source"]
        )
        
        # 格式化结果
        retrieved_docs = []
        for hits in results:
            for i, hit in enumerate(hits):
                doc = {
                    "rank": i + 1,
                    "id": hit.id,
                    "distance": hit.distance,
                    "text": hit.entity.get("text"),
                    "source": hit.entity.get("source")
                }
                retrieved_docs.append(doc)
                print(f"\n结果 {i+1}:")
                print(f"  来源: {doc['source']}")
                print(f"  距离: {doc['distance']:.4f}")
                print(f"  内容: {doc['text'][:100]}...")
        
        return retrieved_docs
    
    def step7_generate_answer(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> str:
        """步骤7：生成答案"""
        print("\n" + "="*50)
        print("步骤7：生成答案")
        print("="*50)
        
        # 构建上下文
        context = "\n\n".join([
            f"文档 {i+1} (来源: {doc['source']}):\n{doc['text']}"
            for i, doc in enumerate(retrieved_docs)
        ])
        
        # 构建 Prompt
        prompt = f"""基于以下检索到的文档，回答用户的问题。

检索到的文档：
{context}

用户问题：{query}

请基于上述文档回答问题，如果文档中没有相关信息，请说明。"""
        
        print("调用 LLM 生成答案...")
        
        # 调用 LLM
        response = self.client.chat.completions.create(
            model=self.llm_model,
            messages=[
                {"role": "system", "content": "你是一个专业的 AI 助手，擅长基于提供的文档回答问题。"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )
        
        answer = response.choices[0].message.content
        
        print("\n生成的答案：")
        print("-" * 50)
        print(answer)
        print("-" * 50)
        
        return answer
    
    def cleanup(self):
        """清理资源"""
        print("\n" + "="*50)
        print("清理资源")
        print("="*50)
        
        try:
            connections.disconnect("default")
            print("✅ 已断开 Milvus 连接")
        except:
            pass
    
    def run_complete_pipeline(self):
        """运行完整的 RAG Pipeline"""
        print("\n" + "="*60)
        print("端到端 RAG 系统演示")
        print("="*60)
        
        try:
            # 步骤1：部署 Milvus
            if not self.step1_deploy_milvus():
                return False
            
            # 步骤2：创建 Collection
            self.step2_create_collection()
            
            # 步骤3：加载文档
            documents = self.step3_load_documents()
            
            # 步骤4：生成 Embeddings
            enriched_docs = self.step4_generate_embeddings(documents)
            
            # 步骤5：插入向量
            self.step5_insert_vectors(enriched_docs)
            
            # 步骤6-7：检索和生成（多个查询）
            queries = [
                "什么是 Milvus？",
                "RAG 是如何工作的？",
                "如何选择合适的 Embedding 模型？"
            ]
            
            for query in queries:
                # 检索
                retrieved_docs = self.step6_semantic_search(query, top_k=2)
                
                # 生成答案
                answer = self.step7_generate_answer(query, retrieved_docs)
                
                time.sleep(1)  # 避免 API 限流
            
            print("\n" + "="*60)
            print("✅ RAG Pipeline 完成！")
            print("="*60)
            
            return True
            
        except Exception as e:
            print(f"\n❌ 错误: {e}")
            return False
        finally:
            self.cleanup()

# ===== 主程序 =====
if __name__ == "__main__":
    # 检查环境变量
    if not os.getenv("OPENAI_API_KEY"):
        print("❌ 请设置 OPENAI_API_KEY 环境变量")
        print("   export OPENAI_API_KEY='your-api-key'")
        exit(1)
    
    # 运行 RAG 系统
    rag = EndToEndRAG()
    rag.run_complete_pipeline()
```

---

## 运行输出示例

```
============================================================
端到端 RAG 系统演示
============================================================

==================================================
步骤1：部署 Milvus
==================================================
启动 Milvus 容器...
等待 Milvus 就绪...
✅ Milvus 已就绪！版本: 2.4.0

==================================================
步骤2：创建 Collection
==================================================
✅ Collection 创建成功: rag_documents
✅ 索引创建成功

==================================================
步骤3：加载文档
==================================================
✅ 加载了 5 个文档

==================================================
步骤4：生成 Embeddings
==================================================
处理文档 1/5: milvus_intro.txt
处理文档 2/5: rag_intro.txt
处理文档 3/5: vector_search.txt
处理文档 4/5: embedding.txt
处理文档 5/5: chunking.txt
✅ 生成了 5 个 Embeddings

==================================================
步骤5：插入向量到 Milvus
==================================================
✅ 插入了 5 条数据
✅ Collection 已加载到内存

==================================================
步骤6：语义检索 - '什么是 Milvus？'
==================================================
生成查询 Embedding...
检索 Top-2 相关文档...

结果 1:
  来源: milvus_intro.txt
  距离: 0.3245
  内容: Milvus 是一个开源的向量数据库，专门用于存储和检索高维向量数据...

结果 2:
  来源: vector_search.txt
  距离: 0.5678
  内容: 向量检索的核心是计算查询向量与数据库中向量的相似度...

==================================================
步骤7：生成答案
==================================================
调用 LLM 生成答案...

生成的答案：
--------------------------------------------------
Milvus 是一个开源的向量数据库，专门设计用于存储和检索高维向量数据。它支持多种索引类型，包括 IVF_FLAT 和 HNSW 等，这些索引类型可以帮助提高向量检索的效率和准确性。Milvus 特别适合用于需要处理大规模向量数据的应用场景，如推荐系统、图像检索和自然语言处理等。
--------------------------------------------------

==================================================
清理资源
==================================================
✅ 已断开 Milvus 连接

============================================================
✅ RAG Pipeline 完成！
============================================================
```

---

## 系统架构图

```
用户查询
   ↓
[1. 查询 Embedding]
   ↓
[2. Milvus 向量检索] ← [向量数据库]
   ↓
[3. 检索结果]
   ↓
[4. 构建 Prompt]
   ↓
[5. LLM 生成答案]
   ↓
最终答案
```

---

## 关键技术点

### 1. Embedding 生成

```python
# 使用 OpenAI Embedding API
response = client.embeddings.create(
    input=text,
    model="text-embedding-3-small"  # 1536 维
)
embedding = response.data[0].embedding
```

### 2. 向量检索

```python
# L2 距离检索
results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param={"metric_type": "L2", "params": {"nprobe": 10}},
    limit=top_k
)
```

### 3. Prompt 构建

```python
# 将检索结果注入 Prompt
context = "\n\n".join([doc["text"] for doc in retrieved_docs])
prompt = f"基于以下文档：\n{context}\n\n回答：{query}"
```

---

## 性能优化建议

### 1. 批量处理

```python
# 批量生成 Embeddings
texts = [doc["text"] for doc in documents]
response = client.embeddings.create(
    input=texts,  # 批量输入
    model="text-embedding-3-small"
)
embeddings = [data.embedding for data in response.data]
```

### 2. 缓存机制

```python
# 缓存 Embeddings
import hashlib
import json

def get_cached_embedding(text, cache_file="embeddings_cache.json"):
    text_hash = hashlib.md5(text.encode()).hexdigest()
    
    # 读取缓存
    if os.path.exists(cache_file):
        with open(cache_file, 'r') as f:
            cache = json.load(f)
        if text_hash in cache:
            return cache[text_hash]
    
    # 生成新的 Embedding
    embedding = generate_embedding(text)
    
    # 保存缓存
    cache[text_hash] = embedding
    with open(cache_file, 'w') as f:
        json.dump(cache, f)
    
    return embedding
```

### 3. 异步处理

```python
import asyncio

async def async_generate_embeddings(documents):
    tasks = [
        asyncio.create_task(async_generate_embedding(doc["text"]))
        for doc in documents
    ]
    embeddings = await asyncio.gather(*tasks)
    return embeddings
```

---

## 检查清单

- [ ] 理解完整的 RAG 流程
- [ ] 掌握 Embedding 生成和向量检索
- [ ] 能够构建有效的 Prompt
- [ ] 理解各组件的协作方式
- [ ] 能够调试和优化 RAG 系统
- [ ] 掌握性能优化技巧

---

**记住:** 端到端的 RAG 系统需要协调多个组件，理解每个环节的作用是优化系统的关键！
