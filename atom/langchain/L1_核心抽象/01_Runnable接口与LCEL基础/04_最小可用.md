# 最小可用知识

> 掌握以下内容，就能开始使用 Runnable 和 LCEL 构建 AI 应用

---

## 核心理念

**20% 的核心知识解决 80% 的问题**

本文档提炼出 Runnable 和 LCEL 最核心的 5 个知识点，让你快速上手并应用到实际项目中。

---

## 4.1 理解 Runnable 协议的三大方法

### 核心概念

Runnable 协议定义了三个核心方法，覆盖了 AI 应用的所有执行场景：

```python
from typing import Protocol, TypeVar, Iterator

Input = TypeVar("Input")
Output = TypeVar("Output")

class Runnable(Protocol[Input, Output]):
    def invoke(self, input: Input) -> Output:
        """同步单次执行"""

    def batch(self, inputs: list[Input]) -> list[Output]:
        """批量并发执行"""

    def stream(self, input: Input) -> Iterator[Output]:
        """流式实时输出"""
```

### 使用场景

| 方法 | 使用场景 | 示例 |
|------|----------|------|
| **invoke** | 单次查询、简单转换 | 翻译一段文本 |
| **batch** | 批量处理、评估测试 | 批量分类 100 个文档 |
| **stream** | 实时对话、长文本生成 | 聊天机器人逐字输出 |

### 快速示例

```python
from langchain_core.runnables import RunnableLambda

# 定义一个简单的 Runnable
uppercase = RunnableLambda(lambda x: x.upper())

# 1. invoke: 单次执行
result = uppercase.invoke("hello")
print(result)  # "HELLO"

# 2. batch: 批量执行
results = uppercase.batch(["hello", "world", "python"])
print(results)  # ["HELLO", "WORLD", "PYTHON"]

# 3. stream: 流式执行
for chunk in uppercase.stream("hello"):
    print(chunk, end="")  # H E L L O
```

### 为什么重要？

- **统一接口**: 所有 LangChain 组件都实现这三个方法
- **可组合性**: 任何 Runnable 都可以无缝组合
- **灵活切换**: 根据场景选择最合适的执行方式

---

## 4.2 掌握 LCEL 管道操作符

### 核心概念

LCEL 使用管道操作符 `|` 将多个 Runnable 组合成链：

```python
chain = component1 | component2 | component3
```

这种声明式语法类似于 Unix 管道，数据从左到右流动。

### 基础示例

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 定义组件
prompt = ChatPromptTemplate.from_template("将以下文本翻译成英文: {text}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# LCEL 组合
chain = prompt | llm | parser

# 执行
result = chain.invoke({"text": "你好世界"})
print(result)  # "Hello World"
```

### 数据流转

```
输入 {"text": "你好世界"}
    ↓
prompt: 生成提示词
    ↓
llm: 调用 LLM
    ↓
parser: 解析输出
    ↓
输出 "Hello World"
```

### 为什么使用 LCEL？

1. **可读性**: 代码清晰表达数据流向
2. **可维护性**: 组件独立，易于修改和测试
3. **自动优化**: LangChain 自动优化执行（并行、流式）

### 对比传统方式

```python
# ❌ 传统方式：命令式编程
prompt_value = prompt.format(text="你好世界")
llm_output = llm.invoke(prompt_value)
result = parser.parse(llm_output)

# ✅ LCEL 方式：声明式编程
chain = prompt | llm | parser
result = chain.invoke({"text": "你好世界"})
```

---

## 4.3 实现自定义 Runnable

### 核心概念

通过继承 `Runnable` 基类，可以将任何逻辑包装成 Runnable 组件。

### 最小实现

```python
from langchain_core.runnables import Runnable
from typing import Optional

class TextCleaner(Runnable[str, str]):
    """文本清理 Runnable"""

    def invoke(
        self,
        input: str,
        config: Optional[dict] = None
    ) -> str:
        # 移除多余空格
        cleaned = " ".join(input.split())
        # 转小写
        cleaned = cleaned.lower()
        return cleaned

# 使用
cleaner = TextCleaner()
result = cleaner.invoke("  Hello   World  ")
print(result)  # "hello world"
```

### 集成到 LCEL 链

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 自定义 Runnable
cleaner = TextCleaner()

# 组合到链中
chain = (
    cleaner
    | ChatPromptTemplate.from_template("分析以下文本: {text}")
    | ChatOpenAI(model="gpt-4o-mini")
)

# 执行
result = chain.invoke("  Messy   Text  ")
```

### 为什么重要？

- **扩展性**: 将任何逻辑包装成 Runnable
- **复用性**: 自定义组件可在多个链中复用
- **一致性**: 统一的接口便于维护

---

## 4.4 使用 RunnableLambda 快速包装函数

### 核心概念

`RunnableLambda` 是最简单的 Runnable 包装器，可以将任何函数转换为 Runnable。

### 基础用法

```python
from langchain_core.runnables import RunnableLambda

# 方式 1: 使用 lambda
uppercase = RunnableLambda(lambda x: x.upper())

# 方式 2: 使用普通函数
def add_prefix(text: str) -> str:
    return f"[PREFIX] {text}"

prefixer = RunnableLambda(add_prefix)

# 组合使用
chain = uppercase | prefixer
result = chain.invoke("hello")
print(result)  # "[PREFIX] HELLO"
```

### 实际应用场景

#### 场景 1: 数据预处理

```python
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI

# 预处理函数
def preprocess(data: dict) -> dict:
    return {
        "text": data["raw_text"].strip().lower(),
        "metadata": data.get("metadata", {})
    }

# 构建链
chain = (
    RunnableLambda(preprocess)
    | ChatPromptTemplate.from_template("处理: {text}")
    | ChatOpenAI(model="gpt-4o-mini")
)
```

#### 场景 2: 后处理输出

```python
def extract_json(llm_output: str) -> dict:
    """从 LLM 输出中提取 JSON"""
    import json
    import re

    # 提取 JSON 代码块
    match = re.search(r'```json\n(.*?)\n```', llm_output, re.DOTALL)
    if match:
        return json.loads(match.group(1))
    return {}

# 构建链
chain = (
    prompt
    | llm
    | RunnableLambda(extract_json)
)
```

### 为什么使用 RunnableLambda？

- **快速原型**: 无需定义完整的类
- **灵活性**: 适用于简单的转换逻辑
- **即插即用**: 轻松集成到 LCEL 链中

---

## 4.5 理解 Config 参数传递

### 核心概念

`config` 参数用于在 Runnable 链中传递运行时配置，如回调、标签、元数据等。

### 基础用法

```python
from langchain_core.runnables import RunnableConfig

# 定义配置
config = RunnableConfig(
    tags=["production", "translation"],
    metadata={"user_id": "123", "session_id": "abc"},
    max_concurrency=5
)

# 执行时传递配置
result = chain.invoke(
    {"text": "你好世界"},
    config=config
)
```

### 常用配置项

| 配置项 | 用途 | 示例 |
|--------|------|------|
| **tags** | 标记执行，便于追踪 | `["production", "v2"]` |
| **metadata** | 附加元数据 | `{"user_id": "123"}` |
| **callbacks** | 自定义回调 | `[MyCallback()]` |
| **max_concurrency** | 并发限制 | `5` |
| **recursion_limit** | 递归深度限制 | `10` |

### 实际应用：可观测性

```python
from langchain_core.callbacks import StdOutCallbackHandler

# 配置回调
config = RunnableConfig(
    callbacks=[StdOutCallbackHandler()],
    tags=["debug"],
    metadata={"environment": "development"}
)

# 执行时会打印详细日志
result = chain.invoke(
    {"text": "你好世界"},
    config=config
)
```

### 为什么重要？

- **可观测性**: 追踪执行过程和性能
- **调试**: 快速定位问题
- **生产环境**: 区分不同环境和用户

---

## 最小可用检查清单

掌握以上 5 个知识点后，你应该能够：

- [ ] 理解 invoke/batch/stream 三大方法的使用场景
- [ ] 使用 LCEL 管道操作符组合简单的链
- [ ] 实现自定义 Runnable 组件
- [ ] 使用 RunnableLambda 快速包装函数
- [ ] 使用 Config 参数进行配置和调试

---

## 快速实战：构建翻译链

综合运用以上知识点，构建一个完整的翻译链：

```python
"""
最小可用示例：翻译链
演示 Runnable 和 LCEL 的核心用法
"""

from langchain_core.runnables import RunnableLambda, RunnableConfig
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# ===== 1. 定义预处理 Runnable =====
def preprocess(text: str) -> dict:
    """清理文本并准备输入"""
    cleaned = text.strip()
    return {"text": cleaned, "target_lang": "英文"}

preprocessor = RunnableLambda(preprocess)

# ===== 2. 定义提示词模板 =====
prompt = ChatPromptTemplate.from_template(
    "将以下文本翻译成{target_lang}:\n\n{text}"
)

# ===== 3. 定义 LLM =====
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ===== 4. 定义输出解析器 =====
parser = StrOutputParser()

# ===== 5. 组合成链 =====
translation_chain = preprocessor | prompt | llm | parser

# ===== 6. 执行 =====
# 单次执行
result = translation_chain.invoke("  你好，世界！  ")
print(f"翻译结果: {result}")

# 批量执行
texts = ["早上好", "晚安", "谢谢"]
results = translation_chain.batch(texts)
for text, translation in zip(texts, results):
    print(f"{text} -> {translation}")

# 流式执行
print("\n流式输出:")
for chunk in translation_chain.stream("这是一个很长的句子"):
    print(chunk, end="", flush=True)
print()

# 带配置执行
config = RunnableConfig(
    tags=["translation", "production"],
    metadata={"user_id": "user_123"}
)
result = translation_chain.invoke("你好", config=config)
print(f"\n带配置执行: {result}")
```

**运行输出示例：**
```
翻译结果: Hello, World!
早上好 -> Good morning
晚安 -> Good night
谢谢 -> Thank you

流式输出:
This is a very long sentence

带配置执行: Hello
```

---

## 这些知识足以

完成以上学习后，你已经可以：

### 基础应用
- ✅ 构建简单的 LLM 调用链
- ✅ 实现文本预处理和后处理
- ✅ 组合多个组件形成工作流

### 实际项目
- ✅ 构建翻译、摘要、分类等基础应用
- ✅ 实现批量处理和流式输出
- ✅ 集成到 FastAPI 等 Web 框架

### 为后续学习打基础
- ✅ 理解 Runnable 协议的核心设计
- ✅ 掌握 LCEL 的基本语法
- ✅ 为学习高级特性（并行、条件路由）做准备

---

## 下一步学习

掌握最小可用知识后，推荐继续学习：

1. **03_核心概念_03_batch方法**: 深入学习批量处理和成本优化
2. **03_核心概念_04_stream方法**: 掌握流式输出和 UI 集成
3. **07_实战代码_01_基础调用**: 通过完整示例巩固知识
4. **L2_LCEL表达式**: 学习 RunnableParallel、RunnableBranch 等高级用法

---

## 常见问题

### Q1: 什么时候用 invoke，什么时候用 batch？

**A**:
- **invoke**: 单次查询、实时响应场景
- **batch**: 批量处理、评估测试、成本敏感场景

### Q2: LCEL 和传统 Chain 有什么区别？

**A**:
- **LCEL**: 声明式、自动优化、更灵活
- **传统 Chain**: 命令式、手动优化、较固定

推荐新项目使用 LCEL。

### Q3: 自定义 Runnable 需要实现所有三个方法吗？

**A**:
不需要。只需实现 `invoke` 方法，`batch` 和 `stream` 会自动基于 `invoke` 实现。但为了性能优化，建议手动实现 `batch`。

### Q4: Config 参数是必需的吗？

**A**:
不是必需的。Config 主要用于可观测性和调试，生产环境推荐使用。

---

## 参考资料

### 官方文档
- [LangChain Runnable Basics](https://python.langchain.com/docs/concepts/runnables) - 2025-2026
- [LCEL Quick Start](https://python.langchain.com/docs/concepts/lcel) - 2025-2026

### 实战案例
- [Building Production-Ready AI Pipelines](https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557) - 2026
- [LangChain Best Practices](https://www.swarnendu.de/blog/langchain-best-practices) - 2025-2026

---

**完成学习**: 继续阅读 [06_反直觉点.md](./06_反直觉点.md) 了解常见误区
