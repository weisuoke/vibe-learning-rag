# 最小可用知识

> 掌握以下内容，就能开始构建基础的RAG生成系统

---

## 4.1 System/User Prompt分离

**核心原则：** 角色定义与任务指令分离

```python
from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# System Prompt：定义角色和约束
system_prompt = """
你是一个严谨的知识助手。
你只能基于提供的参考资料回答问题。
如果参考资料不足，明确说明。
"""

# User Prompt：具体任务
user_prompt = """
参考资料：
Python是一种解释型、面向对象的编程语言。

问题：Python有什么特点？

回答：
"""

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    temperature=0.1
)

print(response.choices[0].message.content)
```

**为什么重要：**
- System Prompt设定全局规则，User Prompt处理具体任务
- 避免指令混淆和冲突
- 便于复用和维护

---

## 4.2 Temperature参数控制

**核心原则：** 事实性回答用低温度，创意性任务用高温度

```python
# 事实性RAG回答：Temperature = 0.1-0.3
response_factual = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "基于参考资料回答"},
        {"role": "user", "content": "Python的特点是什么？"}
    ],
    temperature=0.1  # 低温度，减少随机性
)

# 创意性内容生成：Temperature = 0.7-0.9
response_creative = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "创作一个故事"},
        {"role": "user", "content": "写一个关于AI的故事"}
    ],
    temperature=0.8  # 高温度，增加创造性
)
```

**RAG场景推荐：**
- 文档问答：0.1-0.2
- 知识总结：0.2-0.3
- 内容改写：0.5-0.7

---

## 4.3 基础引用标记

**核心原则：** 为关键信息添加来源标记

```python
def generate_with_citations(query: str, docs: list) -> str:
    """
    生成带引用的答案
    """
    # 构建带编号的上下文
    context = "\n\n".join([
        f"[文档{i+1}] {doc['content']}"
        for i, doc in enumerate(docs)
    ])

    prompt = f"""
参考资料：
{context}

问题：{query}

要求：
1. 基于参考资料回答
2. 为关键信息添加引用标记，格式：[文档X]
3. 如果信息不足，明确说明

回答：
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "你是一个严谨的知识助手"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1
    )

    return response.choices[0].message.content

# 使用示例
docs = [
    {"content": "Python是一种解释型语言", "id": "doc1"},
    {"content": "Python支持面向对象编程", "id": "doc2"}
]

answer = generate_with_citations("Python有什么特点？", docs)
print(answer)
# 输出示例：
# Python是一种解释型语言[文档1]，支持面向对象编程[文档2]。
```

**为什么重要：**
- 用户可以追溯信息来源
- 增强答案可信度
- 便于验证和审核

---

## 4.4 Groundedness基础检查

**核心原则：** 验证答案是否基于参考资料

```python
def check_groundedness_simple(answer: str, context: str) -> bool:
    """
    简单的Groundedness检查
    使用LLM判断答案是否基于上下文
    """
    check_prompt = f"""
判断以下答案是否完全基于参考资料，没有添加额外信息。

参考资料：
{context}

答案：
{answer}

判断（只回答"是"或"否"）：
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "你是一个严谨的评估者"},
            {"role": "user", "content": check_prompt}
        ],
        temperature=0
    )

    result = response.choices[0].message.content.strip()
    return "是" in result

# 使用示例
context = "Python是一种解释型语言"
answer1 = "Python是一种解释型语言"  # 基于上下文
answer2 = "Python是世界上最好的语言"  # 添加了额外信息

print(f"答案1 Grounded: {check_groundedness_simple(answer1, context)}")
print(f"答案2 Grounded: {check_groundedness_simple(answer2, context)}")
```

**为什么重要：**
- 防止幻觉和错误信息
- 确保答案可靠性
- 生产环境必需

---

## 4.5 上下文组装模板

**核心原则：** 结构化组织检索结果和用户问题

```python
def build_rag_prompt(query: str, retrieved_docs: list) -> dict:
    """
    构建标准RAG Prompt
    """
    # 组装上下文
    context = "\n\n".join([
        f"### 文档 {i+1}\n{doc['content']}"
        for i, doc in enumerate(retrieved_docs)
    ])

    # System Prompt
    system = """
你是一个专业的知识助手。
你的任务是基于提供的参考资料回答用户问题。

规则：
1. 只使用参考资料中的信息
2. 为关键信息添加引用 [文档X]
3. 如果资料不足，明确说明
4. 保持客观和准确
"""

    # User Prompt
    user = f"""
## 参考资料

{context}

## 用户问题

{query}

## 你的回答

请基于以上参考资料回答用户问题：
"""

    return {
        "system": system,
        "user": user
    }

# 使用示例
docs = [
    {"content": "RAG是检索增强生成技术"},
    {"content": "RAG结合了检索和生成两个步骤"}
]

prompts = build_rag_prompt("什么是RAG？", docs)

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": prompts["system"]},
        {"role": "user", "content": prompts["user"]}
    ],
    temperature=0.1
)

print(response.choices[0].message.content)
```

**为什么重要：**
- 标准化Prompt结构
- 便于维护和调试
- 提高生成质量

---

## 这些知识足以

掌握以上5个核心知识点，你就能：

✅ **构建基础RAG生成系统**
- System/User分离确保角色清晰
- Temperature控制输出质量
- 引用标记提供可追溯性
- Groundedness检查防止幻觉
- 标准模板保证一致性

✅ **处理常见RAG场景**
- 文档问答
- 知识库检索
- 内容总结
- 信息提取

✅ **为进阶学习打基础**
- 理解Context Engineering理念
- 掌握Multi-Stage验证流程
- 学习Context Pruning技术
- 优化生产环境性能

---

## 快速实战：完整RAG生成示例

```python
"""
最小可用RAG生成系统
演示：文档问答场景
"""

from openai import OpenAI
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def minimal_rag_generate(query: str, docs: list) -> dict:
    """
    最小可用RAG生成函数
    """
    # 1. 组装上下文
    context = "\n\n".join([
        f"[文档{i+1}] {doc}"
        for i, doc in enumerate(docs)
    ])

    # 2. 构建Prompt
    system = "你是一个严谨的知识助手，只基于参考资料回答问题。"

    user = f"""
参考资料：
{context}

问题：{query}

要求：为关键信息添加引用[文档X]

回答：
"""

    # 3. 生成答案
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        temperature=0.1  # 低温度，事实性回答
    )

    answer = response.choices[0].message.content

    # 4. 简单Groundedness检查
    check_prompt = f"""
判断答案是否基于参考资料（只回答"是"或"否"）：

参考资料：{context}
答案：{answer}
"""

    check_response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "你是评估者"},
            {"role": "user", "content": check_prompt}
        ],
        temperature=0
    )

    is_grounded = "是" in check_response.choices[0].message.content

    return {
        "answer": answer,
        "is_grounded": is_grounded,
        "sources": [f"文档{i+1}" for i in range(len(docs))]
    }

# ===== 使用示例 =====

docs = [
    "RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术",
    "RAG通过检索相关文档来增强LLM的生成能力",
    "RAG可以减少幻觉，提高答案的准确性"
]

result = minimal_rag_generate("什么是RAG？", docs)

print("=== RAG生成结果 ===")
print(f"答案：{result['answer']}")
print(f"Grounded：{result['is_grounded']}")
print(f"来源：{', '.join(result['sources'])}")
```

**运行输出示例：**
```
=== RAG生成结果 ===
答案：RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术[文档1]，
通过检索相关文档来增强LLM的生成能力[文档2]，可以减少幻觉，提高答案的准确性[文档3]。
Grounded：True
来源：文档1, 文档2, 文档3
```

---

## 学习检查清单

- [ ] 理解System/User Prompt的区别和作用
- [ ] 掌握Temperature参数对输出的影响
- [ ] 能够为答案添加引用标记
- [ ] 了解Groundedness的概念和重要性
- [ ] 能够构建标准的RAG Prompt模板
- [ ] 完成上面的完整示例代码

---

## 下一步学习

掌握最小可用知识后，可以深入学习：

1. **Context Engineering** - 系统化的上下文管理
2. **Multi-Stage RAG** - Draft → Critique → Synthesis
3. **Context Pruning** - 避免上下文过载
4. **高级引用技术** - Citation-aware RAG with spatial metadata
5. **生产级质量控制** - RAG Triad评估体系

---

**版本：** v1.0 (2025-2026最新实践)
**最后更新：** 2026-02-16
