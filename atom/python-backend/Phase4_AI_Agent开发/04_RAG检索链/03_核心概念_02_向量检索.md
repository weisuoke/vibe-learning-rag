# RAG检索链 - 核心概念2：向量检索

> 在向量空间中快速找到最相似的文档

---

## 一句话定义

**向量检索是在向量数据库中，通过计算查询向量与文档向量的相似度，快速找到最相关的 Top-K 个文档的技术。**

---

## 为什么需要向量检索？

### 问题：如何在海量文档中找到相关内容？

```python
# 假设有 10000 个文档
documents = ["文档1", "文档2", ..., "文档10000"]

# 用户问题
question = "Python 如何读取文件？"

# 如何找到最相关的文档？
# 方法1：逐个比较（太慢）
for doc in documents:
    if is_relevant(question, doc):  # 需要 10000 次比较
        print(doc)

# 方法2：向量检索（快速）
question_vector = embed(question)
top_docs = vector_db.search(question_vector, k=3)  # 毫秒级
```

**关键洞察**：
- 向量检索利用向量空间的几何特性
- 通过索引结构（如 HNSW、IVF）加速检索
- 时间复杂度从 O(n) 降到 O(log n)

---

## 相似度计算方法

### 1. 余弦相似度（Cosine Similarity）

**定义**：计算两个向量夹角的余弦值

```python
import numpy as np

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """计算余弦相似度"""
    # 公式：cos(θ) = (A · B) / (||A|| * ||B||)
    dot_product = np.dot(vec1, vec2)
    norm_a = np.linalg.norm(vec1)
    norm_b = np.linalg.norm(vec2)
    return dot_product / (norm_a * norm_b)

# 示例
vec1 = np.array([1, 2, 3])
vec2 = np.array([2, 3, 4])
vec3 = np.array([-1, -2, -3])

print(f"vec1 vs vec2: {cosine_similarity(vec1, vec2):.4f}")  # 0.9926（非常相似）
print(f"vec1 vs vec3: {cosine_similarity(vec1, vec3):.4f}")  # -1.0000（完全相反）
```

**特点**：
- ✅ 不受向量长度影响（归一化）
- ✅ 值域 [-1, 1]，1 表示完全相同，-1 表示完全相反
- ✅ 最常用的相似度度量
- ❌ 不考虑向量的绝对大小

**几何直觉**：
```
向量夹角越小 → 余弦值越大 → 越相似

vec1 ──→
       ╲ θ=10°
        ╲
         ╲
          vec2 ──→
余弦相似度 = cos(10°) ≈ 0.98（非常相似）

vec1 ──→
       ╲ θ=90°
        ╲
         ↓
        vec3
余弦相似度 = cos(90°) = 0（不相关）
```

### 2. 欧氏距离（Euclidean Distance）

**定义**：计算两个向量在空间中的直线距离

```python
def euclidean_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """计算欧氏距离"""
    # 公式：d = sqrt(Σ(ai - bi)²)
    return np.linalg.norm(vec1 - vec2)

# 示例
vec1 = np.array([1, 2, 3])
vec2 = np.array([2, 3, 4])
vec3 = np.array([10, 20, 30])

print(f"vec1 vs vec2: {euclidean_distance(vec1, vec2):.4f}")  # 1.7321（近）
print(f"vec1 vs vec3: {euclidean_distance(vec1, vec3):.4f}")  # 31.3050（远）
```

**特点**：
- ✅ 直观，容易理解
- ✅ 考虑向量的绝对大小
- ❌ 受向量长度影响
- ❌ 高维空间中效果不如余弦相似度

**几何直觉**：
```
二维空间中的欧氏距离：

  vec2 (2,3)
    ●
   /|
  / |
 /  | 距离 = √((2-1)² + (3-2)²) = √2 ≈ 1.41
/   |
●───┘
vec1 (1,2)
```

### 3. 点积（Dot Product）

**定义**：两个向量对应元素相乘后求和

```python
def dot_product(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """计算点积"""
    # 公式：A · B = Σ(ai * bi)
    return np.dot(vec1, vec2)

# 示例
vec1 = np.array([1, 2, 3])
vec2 = np.array([2, 3, 4])

print(f"点积: {dot_product(vec1, vec2)}")  # 20
```

**特点**：
- ✅ 计算最快（不需要开方）
- ✅ 如果向量已归一化，等价于余弦相似度
- ❌ 受向量长度影响

**使用场景**：
- 向量已归一化（如 OpenAI Embeddings）
- 需要极致性能

### 4. 相似度方法对比

| 方法 | 公式 | 值域 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|------|----------|
| **余弦相似度** | cos(θ) | [-1, 1] | 不受长度影响 | 计算稍慢 | 文本检索（最常用） |
| **欧氏距离** | √Σ(ai-bi)² | [0, ∞) | 直观 | 受长度影响 | 图像检索 |
| **点积** | Σ(ai*bi) | (-∞, ∞) | 最快 | 受长度影响 | 归一化向量 |

**最佳实践**：
```python
# 文本检索：使用余弦相似度
similarity = cosine_similarity(query_vec, doc_vec)

# 如果向量已归一化：使用点积（更快）
if is_normalized(query_vec) and is_normalized(doc_vec):
    similarity = np.dot(query_vec, doc_vec)  # 等价于余弦相似度
```

---

## 向量数据库

### 1. 为什么需要向量数据库？

**问题**：暴力检索太慢

```python
# 暴力检索：逐个计算相似度
def brute_force_search(query_vec, all_vecs, k=3):
    similarities = []
    for i, doc_vec in enumerate(all_vecs):
        sim = cosine_similarity(query_vec, doc_vec)
        similarities.append((i, sim))

    # 排序，返回 Top-K
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:k]

# 时间复杂度：O(n * d)
# n = 文档数量，d = 向量维度
# 10000 个文档，1536 维 → 需要 15,360,000 次乘法
```

**解决方案**：向量数据库使用索引结构加速检索

```python
# 向量数据库：使用索引
vectorstore = Chroma()
vectorstore.add(all_vecs)  # 构建索引

# 检索：利用索引快速找到候选
results = vectorstore.search(query_vec, k=3)

# 时间复杂度：O(log n * d)
# 10000 个文档 → 只需要检查约 100 个候选
```

### 2. ChromaDB（推荐入门）

**特点**：
- ✅ 简单易用，零配置
- ✅ 支持持久化存储
- ✅ 内置元数据过滤
- ❌ 性能不如 FAISS（小规模够用）

**基础使用**：
```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# 1. 创建向量库
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(
    embedding_function=embeddings,
    persist_directory="./chroma_db"  # 持久化目录
)

# 2. 添加文档
documents = [
    "Python 是一门编程语言",
    "FastAPI 是一个 Web 框架",
    "RAG 是检索增强生成"
]
vectorstore.add_texts(documents)

# 3. 检索
query = "什么是 Python？"
results = vectorstore.similarity_search(query, k=2)

for doc in results:
    print(doc.page_content)
```

**高级功能：元数据过滤**
```python
# 添加文档时附带元数据
vectorstore.add_texts(
    texts=["Python 教程", "Java 教程", "Go 教程"],
    metadatas=[
        {"language": "Python", "type": "tutorial"},
        {"language": "Java", "type": "tutorial"},
        {"language": "Go", "type": "tutorial"}
    ]
)

# 检索时过滤元数据
results = vectorstore.similarity_search(
    query="编程教程",
    k=2,
    filter={"language": "Python"}  # 只检索 Python 相关
)
```

### 3. FAISS（推荐生产环境）

**特点**：
- ✅ 性能极高（Facebook 开发）
- ✅ 支持多种索引类型
- ✅ 支持 GPU 加速
- ❌ 配置复杂
- ❌ 不支持元数据过滤（需要自己实现）

**基础使用**：
```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# 1. 创建向量库
embeddings = OpenAIEmbeddings()
documents = ["文档1", "文档2", "文档3"]

vectorstore = FAISS.from_texts(documents, embeddings)

# 2. 检索
query = "查询问题"
results = vectorstore.similarity_search(query, k=2)

# 3. 保存和加载
vectorstore.save_local("faiss_index")
vectorstore = FAISS.load_local("faiss_index", embeddings)
```

**索引类型选择**：
```python
import faiss

# Flat 索引：精确检索，适合小规模（<10K）
index = faiss.IndexFlatL2(dimension)

# IVF 索引：近似检索，适合中等规模（10K-1M）
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist=100)

# HNSW 索引：高性能，适合大规模（>1M）
index = faiss.IndexHNSWFlat(dimension, M=32)
```

### 4. 向量数据库对比

| 数据库 | 性能 | 易用性 | 持久化 | 元数据过滤 | 适用场景 |
|--------|------|--------|--------|------------|----------|
| **ChromaDB** | 中 | ⭐⭐⭐⭐⭐ | ✅ | ✅ | 原型开发、小规模 |
| **FAISS** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ✅ | ❌ | 生产环境、大规模 |
| **Milvus** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ✅ | ✅ | 企业级、分布式 |
| **Pinecone** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ✅ | ✅ | 云服务、托管 |

**选择建议**：
```python
def choose_vector_db():
    if 文档数量 < 10000:
        return "ChromaDB"  # 简单够用
    elif 文档数量 < 1000000:
        return "FAISS"  # 高性能
    else:
        return "Milvus"  # 分布式
```

---

## Top-K 检索

### 1. 什么是 Top-K？

**定义**：返回相似度最高的 K 个文档

```python
# 所有文档的相似度
similarities = [
    (doc1, 0.95),  # 最相似
    (doc2, 0.87),
    (doc3, 0.82),
    (doc4, 0.76),
    (doc5, 0.65),
    ...
]

# Top-3：只返回前 3 个
top_3 = similarities[:3]
# [(doc1, 0.95), (doc2, 0.87), (doc3, 0.82)]
```

### 2. K 值的选择

**权衡**：
- **K 太小**：可能漏掉重要信息
- **K 太大**：引入噪音，浪费 token

```python
# 实验：不同 K 值的效果
question = "Python 如何读取文件？"

# K=1：只返回最相关的 1 个文档
results_k1 = vectorstore.similarity_search(question, k=1)
# 优点：精准
# 缺点：信息可能不够全面

# K=3：返回 3 个文档（推荐）
results_k3 = vectorstore.similarity_search(question, k=3)
# 优点：平衡精度和召回率
# 缺点：可能有 1-2 个不太相关

# K=10：返回 10 个文档
results_k10 = vectorstore.similarity_search(question, k=10)
# 优点：召回率高
# 缺点：噪音多，浪费 token
```

**经验法则**：
```python
# 根据问题复杂度调整 K
def choose_k(question: str) -> int:
    if is_simple_question(question):
        return 2  # 简单问题：如"什么是 Python？"
    elif is_complex_question(question):
        return 5  # 复杂问题：如"对比 Python、Java、Go"
    else:
        return 3  # 默认值
```

### 3. 相似度阈值过滤

```python
# 问题：Top-K 可能包含不相关的文档
results = vectorstore.similarity_search_with_score(question, k=5)
# [
#   (doc1, 0.92),  # 相关
#   (doc2, 0.85),  # 相关
#   (doc3, 0.78),  # 相关
#   (doc4, 0.45),  # 不相关！
#   (doc5, 0.32),  # 不相关！
# ]

# 解决方案：设置相似度阈值
threshold = 0.7
filtered_results = [
    (doc, score) for doc, score in results
    if score >= threshold
]
# [(doc1, 0.92), (doc2, 0.85), (doc3, 0.78)]
```

---

## 检索策略

### 1. 基础向量检索

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(documents, embeddings)

# 基础检索
results = vectorstore.similarity_search(query, k=3)
```

**优点**：
- ✅ 简单直接
- ✅ 速度快

**缺点**：
- ❌ 只考虑语义相似度
- ❌ 可能漏掉关键词匹配

### 2. 混合检索（Hybrid Search）

**原理**：结合向量检索和关键词检索

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# 1. 向量检索器
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# 2. 关键词检索器（BM25）
bm25_retriever = BM25Retriever.from_texts(documents)
bm25_retriever.k = 10

# 3. 混合检索器
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]  # 各占 50% 权重
)

# 检索
results = ensemble_retriever.get_relevant_documents(query)
```

**适用场景**：
- 用户问题包含专有名词（如"GPT-4"）
- 需要精确匹配（如代码、数字）

### 3. MMR（最大边际相关性）

**原理**：在相关性和多样性之间平衡

```python
# 问题：Top-K 可能返回很多相似的文档
results = vectorstore.similarity_search("Python 教程", k=5)
# [
#   "Python 入门教程",
#   "Python 基础教程",  # 与第1个很相似
#   "Python 初学者教程",  # 与第1个很相似
#   "Python 进阶教程",
#   "Python 实战教程"
# ]

# 解决方案：使用 MMR
results = vectorstore.max_marginal_relevance_search(
    query="Python 教程",
    k=5,
    fetch_k=20,  # 先检索 20 个候选
    lambda_mult=0.5  # 0=最大多样性，1=最大相关性
)
# [
#   "Python 入门教程",
#   "Python 进阶教程",  # 与第1个不同
#   "Python Web 开发",  # 不同主题
#   "Python 数据分析",  # 不同主题
#   "Python 机器学习"   # 不同主题
# ]
```

**参数说明**：
- `fetch_k`：先检索多少个候选（通常是 k 的 3-5 倍）
- `lambda_mult`：相关性和多样性的权衡
  - 0：最大多样性（可能不相关）
  - 1：最大相关性（可能重复）
  - 0.5：平衡（推荐）

---

## 手写简化版向量检索

理解检索原理，手写一个简化版：

```python
import numpy as np
from typing import List, Tuple

class SimpleVectorStore:
    """简化版向量数据库（仅用于理解原理）"""

    def __init__(self):
        self.vectors = []  # 存储向量
        self.documents = []  # 存储原始文档

    def add(self, document: str, vector: np.ndarray):
        """添加文档和向量"""
        self.documents.append(document)
        self.vectors.append(vector)

    def search(self, query_vector: np.ndarray, k: int = 3) -> List[Tuple[str, float]]:
        """检索 Top-K 最相似的文档"""
        if not self.vectors:
            return []

        # 1. 计算查询向量与所有文档向量的相似度
        similarities = []
        for i, doc_vector in enumerate(self.vectors):
            sim = self._cosine_similarity(query_vector, doc_vector)
            similarities.append((i, sim))

        # 2. 排序，返回 Top-K
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_k = similarities[:k]

        # 3. 返回文档和相似度
        results = [
            (self.documents[i], score)
            for i, score in top_k
        ]
        return results

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """计算余弦相似度"""
        dot_product = np.dot(vec1, vec2)
        norm_a = np.linalg.norm(vec1)
        norm_b = np.linalg.norm(vec2)
        return dot_product / (norm_a * norm_b)

# 使用示例
store = SimpleVectorStore()

# 添加文档（假设已经向量化）
store.add("Python 是一门编程语言", np.array([0.2, 0.8, 0.3]))
store.add("FastAPI 是一个 Web 框架", np.array([0.3, 0.7, 0.4]))
store.add("今天天气真好", np.array([0.9, 0.1, 0.2]))

# 检索
query_vector = np.array([0.25, 0.75, 0.35])  # 类似"编程语言"
results = store.search(query_vector, k=2)

for doc, score in results:
    print(f"相似度: {score:.4f} - {doc}")
```

**注意**：
- 这是暴力检索，时间复杂度 O(n)
- 真实的向量数据库使用索引（HNSW、IVF）加速到 O(log n)

---

## 在 RAG 中的应用

### 完整的检索流程

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA

# 1. 准备文档
documents = [
    "Python 是一门编程语言，由 Guido van Rossum 创建。",
    "FastAPI 是一个现代、快速的 Web 框架。",
    "RAG 是检索增强生成技术。"
]

# 2. 创建向量库
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(documents, embeddings)

# 3. 创建检索器
retriever = vectorstore.as_retriever(
    search_type="similarity",  # 检索类型
    search_kwargs={"k": 2}     # Top-2
)

# 4. 创建 RAG 链
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=retriever
)

# 5. 提问
question = "谁创建了 Python？"
answer = qa_chain.invoke({"query": question})

print(f"问题: {question}")
print(f"答案: {answer['result']}")
```

---

## 常见问题

### Q1: 向量检索一定比关键词检索好吗？

**A**: 不一定，各有优劣。

```python
# 场景1：语义相似但词不同 → 向量检索更好
query = "如何提升性能？"
doc1 = "优化速度的方法"  # 关键词不匹配，但语义相似
# 向量检索：✅ 能找到
# 关键词检索：❌ 找不到

# 场景2：专有名词精确匹配 → 关键词检索更好
query = "GPT-4 的参数量"
doc1 = "GPT-4 有 1.76 万亿参数"  # 精确匹配
doc2 = "大模型的参数规模"  # 语义相似但不精确
# 向量检索：可能返回 doc2
# 关键词检索：✅ 精确返回 doc1

# 最佳实践：混合检索
```

### Q2: 如何调试检索质量？

**A**: 查看检索结果和相似度分数。

```python
# 检索时返回相似度分数
results = vectorstore.similarity_search_with_score(query, k=5)

for doc, score in results:
    print(f"相似度: {score:.4f}")
    print(f"内容: {doc.page_content[:100]}...")
    print(f"是否相关: {is_relevant(doc, query)}")  # 人工判断
    print("---")

# 如果相似度高但不相关 → 需要 ReRank 或混合检索
# 如果相似度低 → 可能是 Embedding 模型问题
```

### Q3: 向量数据库需要多少内存？

**A**: 取决于文档数量和向量维度。

```python
# 计算公式
memory_mb = (num_docs * vector_dim * 4) / (1024 * 1024)

# 示例
num_docs = 10000
vector_dim = 1536  # OpenAI text-embedding-3-small
memory_mb = (10000 * 1536 * 4) / (1024 * 1024)
print(f"需要内存: {memory_mb:.2f} MB")  # 约 58 MB

# 100 万文档 → 约 5.8 GB
# 1000 万文档 → 约 58 GB
```

---

## 总结

**向量检索的核心价值**：
1. 快速找到语义相似的文档
2. 支持大规模文档检索
3. 不依赖关键词匹配

**在 RAG 中的作用**：
- 根据用户问题检索相关文档
- 为 LLM 提供上下文
- 决定 RAG 的检索质量

**最佳实践**：
- 使用余弦相似度（文本检索）
- K 值通常 3-5
- 考虑混合检索提升精度
- 使用相似度阈值过滤噪音

---

**下一步**：学习 [03_核心概念_03_LLM生成](./03_核心概念_03_LLM生成.md)，了解如何基于检索到的文档生成答案。
