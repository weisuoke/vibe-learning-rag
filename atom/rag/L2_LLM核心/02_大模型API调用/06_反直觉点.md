# 大模型API调用 - 反直觉点

这些是最容易踩的坑，提前了解能少走很多弯路。

---

## 误区1：API 调用很简单，生产环境直接用就行 ❌

### 为什么错？

**Demo 代码：**
```python
# 看起来很简单
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "你好"}]
)
print(response.choices[0].message.content)
```

**生产环境需要考虑：**
```python
# 实际要处理的问题
1. 网络超时怎么办？
2. API 限流怎么办？
3. 响应格式异常怎么办？
4. 成本如何控制？
5. 如何记录日志？
6. 如何监控延迟？
7. 多个模型如何切换？
8. API Key 如何安全管理？
```

### 为什么人们容易这样错？

- Demo 代码太简洁，给人"很简单"的错觉
- 本地测试时网络稳定，感受不到生产环境的复杂性
- 低频调用时不会触发限流

### 正确理解：生产级代码示例

```python
import time
from openai import OpenAI, APIError, RateLimitError, APIConnectionError
from tenacity import retry, stop_after_attempt, wait_exponential

client = OpenAI()

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10)
)
def call_llm_with_retry(messages: list, model: str = "gpt-4o") -> str:
    """带重试机制的 LLM 调用"""
    try:
        start_time = time.time()

        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.1,
            max_tokens=500,
            timeout=30  # 超时设置
        )

        # 记录延迟
        latency = time.time() - start_time
        print(f"API 延迟: {latency:.2f}s, Tokens: {response.usage.total_tokens}")

        return response.choices[0].message.content

    except RateLimitError:
        print("触发限流，等待重试...")
        raise  # 让 tenacity 处理重试

    except APIConnectionError:
        print("网络错误，等待重试...")
        raise

    except APIError as e:
        print(f"API 错误: {e}")
        raise
```

---

## 误区2：temperature=0 就能得到完全相同的输出 ❌

### 为什么错？

```python
# 你以为：
response1 = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "写一句话介绍Python"}],
    temperature=0
)

response2 = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "写一句话介绍Python"}],
    temperature=0
)

# 期望：response1 == response2
# 实际：可能不同！
```

**原因：**
1. **模型更新**：OpenAI 会静默更新模型，即使模型名相同
2. **服务器差异**：不同服务器可能有微小差异
3. **浮点数精度**：GPU 计算存在浮点误差
4. **并行计算**：分布式推理的顺序可能不同

### 为什么人们容易这样错？

- 直觉上 `temperature=0` 意味着"没有随机性"
- 传统软件中，相同输入 = 相同输出
- 短期测试可能恰好得到相同结果

### 正确理解

```python
# temperature=0 的正确理解：
# - 大幅降低随机性
# - 输出更稳定、更可预测
# - 但不保证 100% 相同

# 如果需要完全可复现：
# 1. 固定模型版本（如 gpt-4-0613 而非 gpt-4）
# 2. 使用 seed 参数（OpenAI 支持）
response = client.chat.completions.create(
    model="gpt-4-0613",  # 固定版本
    messages=[...],
    temperature=0,
    seed=42  # 设置随机种子
)

# 即使如此，OpenAI 也只承诺"尽力而为"的确定性
```

---

## 误区3：max_tokens 设大点总没错 ❌

### 为什么错？

```python
# 你以为：设大点保险
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "1+1=?"}],
    max_tokens=4096  # 设很大
)

# 问题1：浪费钱
# - 虽然实际只生成几个 token
# - 但某些场景下模型可能"话痨"

# 问题2：响应变慢
# - 模型可能生成不必要的长内容
# - 增加网络传输时间

# 问题3：超出上下文限制
# - input_tokens + max_tokens 不能超过模型上限
# - 设太大可能导致请求失败
```

### 为什么人们容易这样错？

- 担心答案被截断
- 不了解 token 计费机制
- 不清楚上下文窗口限制

### 正确理解

```python
# 根据场景合理设置 max_tokens

# 场景1：简短问答（RAG 常见）
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    max_tokens=500  # 足够回答大多数问题
)

# 场景2：长文生成
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    max_tokens=2000  # 根据需要设置
)

# 场景3：动态计算
def calculate_max_tokens(input_text: str, model_limit: int = 128000) -> int:
    """根据输入长度动态计算 max_tokens"""
    # 粗略估算：1 个中文字 ≈ 2 tokens
    estimated_input_tokens = len(input_text) * 2
    # 留出余量
    available = model_limit - estimated_input_tokens - 100
    # 设置合理上限
    return min(available, 2000)
```

---

## 误区总结

| 误区 | 现实 | 建议 |
|------|------|------|
| API 调用很简单 | 生产环境需要重试、超时、监控 | 使用成熟的封装库 |
| temperature=0 完全确定 | 只是大幅降低随机性 | 固定模型版本 + seed |
| max_tokens 设大点保险 | 可能浪费钱、变慢、报错 | 根据场景合理设置 |

---

## 额外的反直觉点

### 4. 模型越新不一定越好

```python
# gpt-4o 比 gpt-4-turbo 更快更便宜
# 但某些任务 gpt-4-turbo 效果更好

# 建议：针对你的场景做 A/B 测试
```

### 5. 长 Prompt 不一定效果好

```python
# 你以为：给更多上下文，回答更准确
# 实际：太长的上下文可能导致"迷失在中间"

# RAG 建议：检索结果控制在 3-5 个 chunk
```

### 6. API 响应时间波动很大

```python
# 同样的请求，响应时间可能从 1s 到 30s 不等
# 原因：服务器负载、网络状况、生成长度

# 建议：设置合理的超时，做好用户体验优化
```

---

**上一节：** [05_双重类比.md](./05_双重类比.md)
**下一节：** [07_实战代码.md](./07_实战代码.md) - 完整可运行的示例
