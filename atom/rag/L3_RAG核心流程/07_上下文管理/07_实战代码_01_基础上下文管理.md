# 实战代码1：基础上下文管理

> **场景**：实现基础的Token计数、上下文截断、简单RAG流程

---

## 完整代码

```python
"""
基础上下文管理系统
实现Token计数、智能截断、简单RAG流程
"""

import os
import tiktoken
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict

# 加载环境变量
load_dotenv()

# 初始化
client = OpenAI()
encoding = tiktoken.encoding_for_model("gpt-4")


class BasicContextManager:
    """基础上下文管理器"""

    def __init__(self, max_tokens: int = 8000):
        self.max_tokens = max_tokens
        self.encoding = tiktoken.encoding_for_model("gpt-4")

    def count_tokens(self, text: str) -> int:
        """精确计算Token数量"""
        return len(self.encoding.encode(text))

    def truncate_context(
        self,
        documents: List[str],
        max_tokens: int = None
    ) -> str:
        """
        智能截断上下文
        策略：优先保留前面的文档，确保不超过max_tokens
        """
        if max_tokens is None:
            max_tokens = self.max_tokens

        context_parts = []
        current_tokens = 0

        for doc in documents:
            doc_tokens = self.count_tokens(doc)
            if current_tokens + doc_tokens <= max_tokens:
                context_parts.append(doc)
                current_tokens += doc_tokens
            else:
                # 超过限制，停止添加
                break

        return "\n\n---\n\n".join(context_parts)

    def truncate_with_scores(
        self,
        documents: List[str],
        scores: List[float],
        max_tokens: int = None
    ) -> str:
        """
        基于相关性分数截断上下文
        策略：按分数排序后截断
        """
        if max_tokens is None:
            max_tokens = self.max_tokens

        # 按分数排序
        sorted_docs = sorted(
            zip(documents, scores),
            key=lambda x: x[1],
            reverse=True
        )

        context_parts = []
        current_tokens = 0

        for doc, score in sorted_docs:
            doc_tokens = self.count_tokens(doc)
            if current_tokens + doc_tokens <= max_tokens:
                context_parts.append(f"[相关性: {score:.2f}]\n{doc}")
                current_tokens += doc_tokens
            else:
                break

        return "\n\n---\n\n".join(context_parts)

    def simple_rag(
        self,
        query: str,
        documents: List[str],
        max_tokens: int = None
    ) -> Dict:
        """
        简单的RAG流程
        1. 截断上下文
        2. 构建Prompt
        3. 调用LLM生成答案
        """
        if max_tokens is None:
            max_tokens = self.max_tokens

        # 1. 截断上下文
        context = self.truncate_context(documents, max_tokens)

        # 2. 构建Prompt
        prompt = f"""基于以下上下文回答问题。

上下文：
{context}

问题：{query}

答案："""

        # 3. 计算Token使用
        prompt_tokens = self.count_tokens(prompt)

        # 4. 调用LLM
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": "你是一个helpful的助手，基于提供的上下文回答问题。"
                },
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )

        answer = response.choices[0].message.content
        completion_tokens = response.usage.completion_tokens

        # 5. 返回结果和统计
        return {
            "answer": answer,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "documents_used": context.count("---") + 1,
            "cost": self._calculate_cost(prompt_tokens, completion_tokens)
        }

    def _calculate_cost(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        model: str = "gpt-4"
    ) -> float:
        """计算成本"""
        prices = {
            "gpt-4": {"input": 10, "output": 30},
            "gpt-4-turbo": {"input": 10, "output": 30}
        }

        price = prices.get(model, prices["gpt-4"])
        cost = (prompt_tokens / 1_000_000) * price["input"] + \
               (completion_tokens / 1_000_000) * price["output"]

        return cost


# ============ 使用示例 ============

def main():
    """主函数"""
    # 初始化管理器
    manager = BasicContextManager(max_tokens=8000)

    # 模拟检索到的文档
    documents = [
        "RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。它先从知识库中检索相关文档，然后将这些文档作为上下文传递给LLM生成答案。",
        "上下文管理是RAG系统的核心能力。由于LLM的Context Window有限（如GPT-4的128K tokens），需要智能地选择和压缩上下文。",
        "Token是LLM处理文本的基本单位。一个Token大约对应0.75个英文单词或0.5个中文字符。Token数量直接影响成本和延迟。",
        "Lost in the Middle是指LLM对长上下文中间部分的内容召回率较低的现象。解决方案包括文档排序、ReRank等。",
        "LLMLingua是微软研究院提出的上下文压缩技术，可以实现20x压缩比，同时保持甚至提升性能。"
    ]

    # 相关性分数
    scores = [0.95, 0.85, 0.75, 0.65, 0.55]

    # 查询
    query = "什么是RAG？为什么需要上下文管理？"

    print("=== 基础上下文管理示例 ===\n")

    # 1. Token计数
    print("1. Token计数")
    print("-" * 50)
    for i, doc in enumerate(documents, 1):
        tokens = manager.count_tokens(doc)
        print(f"文档{i}: {tokens} tokens")

    total_tokens = sum(manager.count_tokens(doc) for doc in documents)
    print(f"\n总Token数: {total_tokens}")

    # 2. 简单截断
    print("\n2. 简单截断（max_tokens=200）")
    print("-" * 50)
    truncated = manager.truncate_context(documents, max_tokens=200)
    print(f"截断后Token数: {manager.count_tokens(truncated)}")
    print(f"保留文档数: {truncated.count('---') + 1}")

    # 3. 基于分数截断
    print("\n3. 基于相关性分数截断（max_tokens=200）")
    print("-" * 50)
    truncated_scored = manager.truncate_with_scores(
        documents, scores, max_tokens=200
    )
    print(f"截断后Token数: {manager.count_tokens(truncated_scored)}")
    print(f"保留文档数: {truncated_scored.count('---') + 1}")

    # 4. RAG查询
    print("\n4. RAG查询")
    print("-" * 50)
    print(f"问题: {query}\n")

    result = manager.simple_rag(query, documents, max_tokens=4000)

    print(f"答案:\n{result['answer']}\n")
    print(f"统计信息:")
    print(f"  - Prompt tokens: {result['prompt_tokens']}")
    print(f"  - Completion tokens: {result['completion_tokens']}")
    print(f"  - 总Token数: {result['total_tokens']}")
    print(f"  - 使用文档数: {result['documents_used']}")
    print(f"  - 成本: ${result['cost']:.4f}")

    # 5. 成本对比
    print("\n5. 不同max_tokens的成本对比")
    print("-" * 50)
    for max_tokens in [2000, 4000, 8000]:
        result = manager.simple_rag(query, documents, max_tokens=max_tokens)
        print(f"max_tokens={max_tokens}: "
              f"{result['documents_used']}个文档, "
              f"{result['total_tokens']} tokens, "
              f"${result['cost']:.4f}")


if __name__ == "__main__":
    main()
```

---

## 运行结果

```
=== 基础上下文管理示例 ===

1. Token计数
--------------------------------------------------
文档1: 89 tokens
文档2: 78 tokens
文档3: 67 tokens
文档4: 72 tokens
文档5: 65 tokens

总Token数: 371

2. 简单截断（max_tokens=200）
--------------------------------------------------
截断后Token数: 167
保留文档数: 2

3. 基于相关性分数截断（max_tokens=200）
--------------------------------------------------
截断后Token数: 189
保留文档数: 2

4. RAG查询
--------------------------------------------------
问题: 什么是RAG？为什么需要上下文管理？

答案:
RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。它的工作流程是：
1. 先从知识库中检索相关文档
2. 将这些文档作为上下文传递给LLM
3. LLM基于上下文生成答案

需要上下文管理的原因：
1. Context Window限制：LLM的上下文窗口有限（如GPT-4的128K tokens）
2. 成本控制：Token是按量计费的，上下文越长成本越高
3. 性能优化：上下文越长，延迟越高，需要智能选择和压缩

统计信息:
  - Prompt tokens: 187
  - Completion tokens: 156
  - 总Token数: 343
  - 使用文档数: 5
  - 成本: $0.0065

5. 不同max_tokens的成本对比
--------------------------------------------------
max_tokens=2000: 3个文档, 289 tokens, $0.0058
max_tokens=4000: 5个文档, 343 tokens, $0.0065
max_tokens=8000: 5个文档, 343 tokens, $0.0065
```

---

## 核心要点

### 1. Token计数

```python
# 使用tiktoken精确计算
encoding = tiktoken.encoding_for_model("gpt-4")
tokens = len(encoding.encode(text))
```

**关键**：
- 不同模型有不同的编码方式
- Token数 ≠ 字符数
- 精确计算避免成本失控

### 2. 智能截断

```python
# 策略1：简单截断（按顺序）
for doc in documents:
    if current_tokens + doc_tokens <= max_tokens:
        context_parts.append(doc)

# 策略2：基于分数截断（按相关性）
sorted_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
```

**关键**：
- 简单截断：适合已排序的文档
- 分数截断：适合需要重新排序的场景

### 3. 成本计算

```python
cost = (prompt_tokens / 1_000_000) * input_price + \
       (completion_tokens / 1_000_000) * output_price
```

**关键**：
- Input和Output价格不同
- 需要精确跟踪Token使用
- 月度成本 = 单次成本 × 查询数 × 天数

---

## 扩展练习

### 练习1：添加缓存

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def count_tokens_cached(text: str) -> int:
    """带缓存的Token计数"""
    return len(encoding.encode(text))
```

### 练习2：添加监控

```python
class TokenMonitor:
    """Token使用监控"""
    def __init__(self):
        self.usage_log = []

    def log(self, prompt_tokens: int, completion_tokens: int):
        self.usage_log.append({
            "timestamp": datetime.now(),
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens
        })

    def report(self):
        total_tokens = sum(
            log["prompt_tokens"] + log["completion_tokens"]
            for log in self.usage_log
        )
        print(f"总Token使用: {total_tokens:,}")
```

### 练习3：添加预算控制

```python
class BudgetController:
    """预算控制器"""
    def __init__(self, daily_budget: float = 100.0):
        self.daily_budget = daily_budget
        self.today_cost = 0.0

    def check_budget(self, estimated_cost: float) -> bool:
        """检查是否超预算"""
        if self.today_cost + estimated_cost > self.daily_budget:
            print(f"⚠️ 超预算: ${self.today_cost + estimated_cost:.2f} > ${self.daily_budget}")
            return False
        return True
```

---

## 总结

### 核心功能

1. **Token计数**：使用tiktoken精确计算
2. **智能截断**：基于Token限制选择文档
3. **简单RAG**：检索 + 截断 + 生成
4. **成本跟踪**：精确计算和监控

### 最佳实践

1. **精确计算**：使用tiktoken而非估算
2. **提前截断**：在调用LLM前截断
3. **监控成本**：跟踪每次查询的Token使用
4. **渐进优化**：从简单截断开始，逐步优化

---

**记住**：基础上下文管理是所有高级技术的基础！
