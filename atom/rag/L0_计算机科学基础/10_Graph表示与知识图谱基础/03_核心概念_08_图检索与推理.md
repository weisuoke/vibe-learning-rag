# 核心概念8：图检索与推理

> 局部检索、全局检索、多跳推理 - GraphRAG的核心检索策略

---

## 一句话定义

**图检索与推理是基于知识图谱结构进行信息检索和多跳推理的技术，通过局部检索（实体邻居）、全局检索（社区摘要）和混合检索实现精准的知识发现。**

---

## 核心原理

### 三种检索模式

```python
# 1. 局部检索（Local Retrieval）
# 基于实体的邻居查询
query = "张三在哪工作？"
entity = "张三"
neighbors = kg.get_neighbors(entity)  # ["阿里巴巴", "杭州"]

# 2. 全局检索（Global Retrieval）
# 基于社区摘要查询
query = "互联网公司有哪些？"
relevant_communities = find_communities_by_topic("互联网")
summaries = [community_summaries[c] for c in relevant_communities]

# 3. 混合检索（Hybrid Retrieval）
# 组合局部、全局和向量检索
context = local_retrieval(query) + global_retrieval(query) + vector_retrieval(query)
```

---

## 方法1：局部检索（精确查询）⭐⭐⭐⭐⭐

### 基础实现

```python
class LocalRetriever:
    """局部检索器：基于实体邻居"""

    def __init__(self, knowledge_graph):
        self.kg = knowledge_graph

    def retrieve(self, entities, max_hops=2):
        """检索实体的邻居（支持多跳）"""
        context = []

        for entity in entities:
            # 1跳邻居
            neighbors_1hop = self._get_neighbors_with_relations(entity)
            context.extend(neighbors_1hop)

            # 2跳邻居（可选）
            if max_hops >= 2:
                for neighbor, _ in neighbors_1hop:
                    neighbors_2hop = self._get_neighbors_with_relations(neighbor)
                    context.extend(neighbors_2hop)

        # 去重
        return list(set(context))

    def _get_neighbors_with_relations(self, entity):
        """获取邻居及关系"""
        neighbors = []

        if entity in self.kg:
            for neighbor in self.kg.neighbors(entity):
                relation = self.kg[entity][neighbor].get('relation', 'related_to')
                neighbors.append((entity, relation, neighbor))

        return neighbors

# 使用
retriever = LocalRetriever(kg)
entities = ["张三"]
context = retriever.retrieve(entities, max_hops=2)

for s, p, o in context:
    print(f"{s} {p} {o}")
# 张三 工作于 阿里巴巴
# 阿里巴巴 位于 杭州
# 阿里巴巴 创始人 马云
```

### 带权重的局部检索

```python
class WeightedLocalRetriever:
    """带权重的局部检索"""

    def retrieve(self, entities, max_hops=2):
        """检索并计算相关性权重"""
        scored_triples = []

        for entity in entities:
            # 1跳：权重1.0
            neighbors_1hop = self._get_neighbors(entity)
            for triple in neighbors_1hop:
                scored_triples.append((triple, 1.0))

            # 2跳：权重0.5
            if max_hops >= 2:
                for _, _, neighbor in neighbors_1hop:
                    neighbors_2hop = self._get_neighbors(neighbor)
                    for triple in neighbors_2hop:
                        scored_triples.append((triple, 0.5))

        # 按权重排序
        scored_triples.sort(key=lambda x: x[1], reverse=True)

        return [triple for triple, _ in scored_triples[:20]]
```

---

## 方法2：全局检索（主题查询）⭐⭐⭐⭐⭐

### 基于社区摘要

```python
class GlobalRetriever:
    """全局检索器：基于社区摘要"""

    def __init__(self, communities, community_summaries, embedding_model):
        self.communities = communities
        self.summaries = community_summaries
        self.embedding_model = embedding_model

        # 预计算社区摘要的向量
        self.summary_embeddings = {}
        for comm_id, summary in community_summaries.items():
            self.summary_embeddings[comm_id] = embedding_model.embed(summary)

    def retrieve(self, query, top_k=3):
        """检索最相关的社区"""
        query_embedding = self.embedding_model.embed(query)

        # 计算相似度
        similarities = []
        for comm_id, summary_emb in self.summary_embeddings.items():
            similarity = self._cosine_similarity(query_embedding, summary_emb)
            similarities.append((comm_id, similarity))

        # 排序并返回top-k
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_communities = similarities[:top_k]

        # 返回社区摘要和实体
        context = []
        for comm_id, score in top_communities:
            context.append({
                'summary': self.summaries[comm_id],
                'entities': self.communities[comm_id],
                'score': score
            })

        return context

    def _cosine_similarity(self, vec1, vec2):
        import numpy as np
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# 使用
retriever = GlobalRetriever(communities, summaries, embedding_model)
context = retriever.retrieve("互联网公司有哪些？", top_k=3)

for item in context:
    print(f"社区摘要: {item['summary']}")
    print(f"相关实体: {item['entities']}")
    print(f"相关性: {item['score']:.2f}")
```

---

## 方法3：混合检索（最佳实践）⭐⭐⭐⭐⭐

### 完整实现

```python
class HybridRetriever:
    """混合检索器：局部+全局+向量"""

    def __init__(self, kg, communities, summaries, vector_store, embedding_model):
        self.local_retriever = LocalRetriever(kg)
        self.global_retriever = GlobalRetriever(communities, summaries, embedding_model)
        self.vector_store = vector_store
        self.embedding_model = embedding_model

    def retrieve(self, query, strategy='adaptive'):
        """混合检索策略"""

        if strategy == 'adaptive':
            # 自适应：根据查询类型选择策略
            return self._adaptive_retrieval(query)
        elif strategy == 'weighted':
            # 加权：组合三种检索结果
            return self._weighted_retrieval(query)
        elif strategy == 'cascade':
            # 级联：先局部，不足再全局
            return self._cascade_retrieval(query)

    def _adaptive_retrieval(self, query):
        """自适应检索"""
        # 提取查询中的实体
        entities = self._extract_entities(query)

        if entities:
            # 有明确实体：使用局部检索
            return self.local_retriever.retrieve(entities, max_hops=2)
        else:
            # 无明确实体：使用全局检索
            global_context = self.global_retriever.retrieve(query, top_k=3)
            return [item['summary'] for item in global_context]

    def _weighted_retrieval(self, query, weights=None):
        """加权检索"""
        if weights is None:
            weights = {'local': 0.5, 'global': 0.3, 'vector': 0.2}

        # 提取实体
        entities = self._extract_entities(query)

        # 局部检索
        local_results = []
        if entities:
            local_results = self.local_retriever.retrieve(entities, max_hops=2)

        # 全局检索
        global_results = self.global_retriever.retrieve(query, top_k=3)

        # 向量检索
        vector_results = self._vector_retrieval(query, top_k=5)

        # 加权合并
        scored_results = []

        for result in local_results:
            scored_results.append((result, weights['local']))

        for item in global_results:
            scored_results.append((item['summary'], weights['global'] * item['score']))

        for result in vector_results:
            scored_results.append((result, weights['vector']))

        # 排序并去重
        scored_results.sort(key=lambda x: x[1], reverse=True)
        unique_results = []
        seen = set()

        for result, score in scored_results:
            result_str = str(result)
            if result_str not in seen:
                seen.add(result_str)
                unique_results.append(result)

        return unique_results[:10]

    def _cascade_retrieval(self, query, threshold=3):
        """级联检索"""
        # 先尝试局部检索
        entities = self._extract_entities(query)
        if entities:
            local_results = self.local_retriever.retrieve(entities, max_hops=2)
            if len(local_results) >= threshold:
                return local_results

        # 局部检索不足，使用全局检索
        global_results = self.global_retriever.retrieve(query, top_k=3)
        return [item['summary'] for item in global_results]

    def _vector_retrieval(self, query, top_k=5):
        """向量检索"""
        query_embedding = self.embedding_model.embed(query)

        similarities = []
        for chunk, embedding in self.vector_store.items():
            similarity = self._cosine_similarity(query_embedding, embedding)
            similarities.append((chunk, similarity))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return [chunk for chunk, _ in similarities[:top_k]]

    def _extract_entities(self, query):
        """提取查询中的实体（简化版）"""
        # 实际应用中应使用NER或LLM提取
        # 这里简化为关键词匹配
        entities = []
        for node in self.local_retriever.kg.nodes():
            if node.lower() in query.lower():
                entities.append(node)
        return entities

    def _cosine_similarity(self, vec1, vec2):
        import numpy as np
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

---

## 多跳推理

### BFS多跳推理

```python
def multi_hop_reasoning_bfs(kg, start, end, max_hops=3):
    """BFS多跳推理：找最短路径"""
    from collections import deque

    queue = deque([(start, [start])])
    visited = {start}

    while queue:
        node, path = queue.popleft()

        # 达到目标
        if node == end:
            return path

        # 超过最大跳数
        if len(path) > max_hops:
            continue

        # 扩展邻居
        for neighbor in kg.neighbors(node):
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, path + [neighbor]))

    return None  # 未找到路径

# 使用
path = multi_hop_reasoning_bfs(kg, "张三", "杭州", max_hops=3)
if path:
    print(" → ".join(path))
    # 张三 → 阿里巴巴 → 杭州
```

### DFS多跳推理

```python
def multi_hop_reasoning_dfs(kg, start, end, max_hops=3):
    """DFS多跳推理：找所有路径"""

    def dfs(node, path, visited):
        # 达到目标
        if node == end:
            all_paths.append(path[:])
            return

        # 超过最大跳数
        if len(path) > max_hops:
            return

        # 扩展邻居
        for neighbor in kg.neighbors(node):
            if neighbor not in visited:
                visited.add(neighbor)
                path.append(neighbor)
                dfs(neighbor, path, visited)
                path.pop()
                visited.remove(neighbor)

    all_paths = []
    dfs(start, [start], {start})
    return all_paths

# 使用
paths = multi_hop_reasoning_dfs(kg, "张三", "杭州", max_hops=3)
for path in paths:
    print(" → ".join(path))
# 张三 → 杭州
# 张三 → 阿里巴巴 → 杭州
```

### 带约束的推理

```python
def constrained_reasoning(kg, start, end, relation_constraints):
    """带关系约束的推理"""

    def dfs(node, path, relations):
        if node == end:
            # 检查关系约束
            if all(r in relations for r in relation_constraints):
                all_paths.append((path[:], relations[:]))
            return

        if len(path) > 3:
            return

        for neighbor in kg.neighbors(node):
            if neighbor not in path:
                relation = kg[node][neighbor].get('relation')
                path.append(neighbor)
                relations.append(relation)
                dfs(neighbor, path, relations)
                path.pop()
                relations.pop()

    all_paths = []
    dfs(start, [start], [])
    return all_paths

# 使用：查找"张三通过工作关系到达的地点"
paths = constrained_reasoning(kg, "张三", "杭州", ["工作于"])
for path, relations in paths:
    print(f"路径: {' → '.join(path)}")
    print(f"关系: {' → '.join(relations)}")
```

---

## 在AI Agent中的应用

### 应用1：复杂问答

```python
class ComplexQASystem:
    """复杂问答系统"""

    def __init__(self, hybrid_retriever, llm):
        self.retriever = hybrid_retriever
        self.llm = llm

    def answer(self, question):
        """回答复杂问题"""

        # 1. 分析问题类型
        question_type = self._analyze_question_type(question)

        # 2. 选择检索策略
        if question_type == 'factual':
            # 事实性问题：局部检索
            context = self.retriever.retrieve(question, strategy='adaptive')
        elif question_type == 'analytical':
            # 分析性问题：全局检索
            context = self.retriever._global_retrieval(question)
        else:
            # 混合问题：混合检索
            context = self.retriever.retrieve(question, strategy='weighted')

        # 3. 生成答案
        answer = self._generate_answer(question, context)

        return answer

    def _analyze_question_type(self, question):
        """分析问题类型"""
        if any(word in question for word in ["谁", "什么", "哪里"]):
            return 'factual'
        elif any(word in question for word in ["为什么", "如何", "分析"]):
            return 'analytical'
        else:
            return 'mixed'

    def _generate_answer(self, question, context):
        """生成答案"""
        prompt = f"""
基于以下知识回答问题：

知识：
{chr(10).join(str(c) for c in context)}

问题：{question}

答案：
"""
        return self.llm.generate(prompt)
```

### 应用2：关系发现

```python
def discover_relationships(kg, entity1, entity2, max_hops=3):
    """发现两个实体之间的关系"""

    # 找所有路径
    paths = multi_hop_reasoning_dfs(kg, entity1, entity2, max_hops)

    if not paths:
        return f"{entity1}和{entity2}之间没有直接关系"

    # 分析路径
    relationships = []
    for path in paths:
        # 提取关系链
        relations = []
        for i in range(len(path) - 1):
            relation = kg[path[i]][path[i+1]].get('relation', 'related_to')
            relations.append(relation)

        relationships.append({
            'path': path,
            'relations': relations,
            'hops': len(path) - 1
        })

    return relationships

# 使用
relationships = discover_relationships(kg, "张三", "杭州")
for rel in relationships:
    print(f"{rel['hops']}跳: {' → '.join(rel['path'])}")
    print(f"关系: {' → '.join(rel['relations'])}")
```

### 应用3：推荐系统

```python
class GraphBasedRecommender:
    """基于图的推荐系统"""

    def __init__(self, kg):
        self.kg = kg

    def recommend(self, user, top_k=5):
        """推荐相似实体"""

        # 1. 获取用户的邻居
        user_neighbors = set(self.kg.neighbors(user))

        # 2. 计算候选实体的相似度
        candidates = {}

        for neighbor in user_neighbors:
            # 获取邻居的邻居
            for candidate in self.kg.neighbors(neighbor):
                if candidate != user and candidate not in user_neighbors:
                    # 计算共同邻居数
                    candidate_neighbors = set(self.kg.neighbors(candidate))
                    common = len(user_neighbors & candidate_neighbors)

                    if candidate not in candidates:
                        candidates[candidate] = 0
                    candidates[candidate] += common

        # 3. 排序并返回top-k
        sorted_candidates = sorted(
            candidates.items(),
            key=lambda x: x[1],
            reverse=True
        )

        return [entity for entity, score in sorted_candidates[:top_k]]

# 使用
recommender = GraphBasedRecommender(kg)
recommendations = recommender.recommend("张三", top_k=5)
print(f"推荐给张三: {recommendations}")
```

---

## 性能优化

### 优化1：索引加速

```python
class IndexedRetriever:
    """带索引的检索器"""

    def __init__(self, kg):
        self.kg = kg
        # 预计算索引
        self.entity_index = self._build_entity_index()
        self.relation_index = self._build_relation_index()

    def _build_entity_index(self):
        """构建实体索引"""
        index = {}
        for node in self.kg.nodes():
            # 索引实体的所有关系
            index[node] = {
                'out_edges': list(self.kg.out_edges(node, data=True)),
                'in_edges': list(self.kg.in_edges(node, data=True)),
                'degree': self.kg.degree(node)
            }
        return index

    def _build_relation_index(self):
        """构建关系索引"""
        index = {}
        for u, v, data in self.kg.edges(data=True):
            relation = data.get('relation')
            if relation not in index:
                index[relation] = []
            index[relation].append((u, v))
        return index

    def retrieve_by_relation(self, relation):
        """按关系类型检索"""
        return self.relation_index.get(relation, [])
```

### 优化2：缓存结果

```python
from functools import lru_cache

class CachedRetriever:
    """带缓存的检索器"""

    def __init__(self, kg):
        self.kg = kg

    @lru_cache(maxsize=1000)
    def retrieve_neighbors(self, entity, max_hops):
        """缓存邻居查询结果"""
        return self._retrieve_neighbors_impl(entity, max_hops)

    def _retrieve_neighbors_impl(self, entity, max_hops):
        """实际检索逻辑"""
        # ... 实现
        pass
```

---

## 评估指标

```python
def evaluate_retrieval(retriever, test_cases):
    """评估检索质量"""

    metrics = {
        'precision': [],
        'recall': [],
        'f1': [],
        'mrr': []  # Mean Reciprocal Rank
    }

    for query, relevant_docs in test_cases:
        # 检索
        retrieved = retriever.retrieve(query)

        # 计算指标
        relevant_set = set(relevant_docs)
        retrieved_set = set(retrieved)

        # 精确率
        precision = len(relevant_set & retrieved_set) / len(retrieved_set) if retrieved_set else 0

        # 召回率
        recall = len(relevant_set & retrieved_set) / len(relevant_set) if relevant_set else 0

        # F1
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # MRR
        for i, doc in enumerate(retrieved):
            if doc in relevant_set:
                mrr = 1 / (i + 1)
                break
        else:
            mrr = 0

        metrics['precision'].append(precision)
        metrics['recall'].append(recall)
        metrics['f1'].append(f1)
        metrics['mrr'].append(mrr)

    # 平均指标
    return {
        'avg_precision': sum(metrics['precision']) / len(metrics['precision']),
        'avg_recall': sum(metrics['recall']) / len(metrics['recall']),
        'avg_f1': sum(metrics['f1']) / len(metrics['f1']),
        'avg_mrr': sum(metrics['mrr']) / len(metrics['mrr'])
    }
```

---

## 总结

**图检索与推理核心方法：**

| 方法 | 适用场景 | 优势 | 劣势 |
|------|----------|------|------|
| **局部检索** | 精确查询、实体明确 | 精准、快速 | 覆盖范围小 |
| **全局检索** | 主题查询、开放问题 | 覆盖全面 | 可能不够精确 |
| **混合检索** | 复杂问题 | 平衡精准和覆盖 | 实现复杂 ⭐ |

**多跳推理策略：**
- BFS：找最短路径
- DFS：找所有路径
- 带约束：满足特定关系

**在AI Agent中的应用：**
- 复杂问答系统
- 关系发现和推荐
- 推理链可视化
- 知识图谱导航

**关键洞察：**
- 混合检索是最佳实践
- 索引和缓存提升性能
- 多跳推理支持复杂问答
- 评估指标指导优化

---

**下一步：** 学习 `07_实战代码_场景1_基础图实现.md` - 进入实战代码部分
