# 核心概念 01：自动压缩触发机制

> **核心概念**：Compaction 如何自动检测上下文使用情况并在合适的时机触发压缩

---

## 概述

**自动压缩触发机制**是 Compaction 的第一个核心概念，它解决了"何时压缩"的问题。

**核心问题：**
- 如何知道上下文快满了？
- 何时触发压缩最合适？
- 如何避免频繁压缩？
- 如何确保不会超限？

**解决方案：**
```typescript
// 核心触发逻辑
if (contextTokens > contextWindow - reserveTokens) {
  await compactSession();
}
```

---

## 1. Token 计算公式

### 1.1 核心公式

**触发条件：**
```typescript
contextTokens > contextWindow - reserveTokens
```

**变量说明：**
- `contextTokens`: 当前对话使用的 token 数
- `contextWindow`: LLM 的上下文窗口大小
- `reserveTokens`: 预留空间（默认 16384）

**示例计算：**
```typescript
// Claude Opus 4
const contextWindow = 200000;      // 200K tokens
const reserveTokens = 16384;       // 16K tokens
const threshold = 200000 - 16384;  // 183,616 tokens

// 当 contextTokens > 183,616 时触发压缩
```

**触发阈值百分比：**
```typescript
const thresholdPercentage = (threshold / contextWindow) * 100;
// (183,616 / 200,000) * 100 = 91.8%

// 意味着：使用超过 91.8% 的上下文窗口时触发压缩
```

### 1.2 为什么需要 reserveTokens？

**问题：如果不预留空间会怎样？**
```typescript
// 没有预留空间
if (contextTokens > contextWindow) {
  await compactSession(); // 太晚了！
}

// 问题：
// 1. 压缩本身需要调用 LLM（需要空间）
// 2. 用户的下一条消息需要空间
// 3. Agent 的响应需要空间
// 4. 可能在压缩前就超限了
```

**解决方案：预留空间**
```typescript
// 预留 16K tokens
const reserveTokens = 16384;

// 空间分配：
// - 压缩提示词：~2K tokens
// - 用户消息：~2K tokens
// - Agent 响应：~10K tokens
// - 缓冲：~2K tokens
// 总计：~16K tokens
```

**类比：**
> 就像餐厅预留座位：
> - 不预留：客人来了没座位，尴尬
> - 预留：确保有空间接待新客人

### 1.3 reserveTokens 的配置

**默认值：**
```typescript
const defaultReserveTokens = 16384; // 16K tokens
```

**调整建议：**

| 场景 | reserveTokens | 说明 |
|------|---------------|------|
| **默认** | 16384 | 适合大多数场景 |
| **长响应** | 32768 | Agent 响应很长（如生成大量代码） |
| **短响应** | 8192 | Agent 响应很短（如简单问答） |
| **成本优化** | 8192 | 更激进的压缩，降低 token 使用 |
| **质量优先** | 32768 | 更少压缩，保留更多上下文 |

**配置示例：**
```typescript
// .pi/settings.json
{
  "compaction": {
    "reserveTokens": 20000  // 自定义预留空间
  }
}
```

---

## 2. 使用量追踪

### 2.1 从 Assistant 消息获取使用量

**核心函数：`getLastAssistantUsage()`**
```typescript
function getLastAssistantUsage(messages: Message[]): Usage | undefined {
  // 从后往前遍历消息
  for (let i = messages.length - 1; i >= 0; i--) {
    const message = messages[i];

    // 找到最后一条 assistant 消息
    if (message.role === 'assistant' && message.usage) {
      return message.usage;
    }
  }

  return undefined;
}
```

**Usage 结构：**
```typescript
interface Usage {
  input_tokens: number;   // 输入 token 数
  output_tokens: number;  // 输出 token 数
}
```

**示例：**
```typescript
const messages = [
  { role: 'user', content: 'Hello' },
  {
    role: 'assistant',
    content: 'Hi there!',
    usage: { input_tokens: 1000, output_tokens: 50 }
  },
  { role: 'user', content: 'How are you?' },
  {
    role: 'assistant',
    content: 'I am fine',
    usage: { input_tokens: 1100, output_tokens: 30 }
  },
];

const usage = getLastAssistantUsage(messages);
// { input_tokens: 1100, output_tokens: 30 }
```

### 2.2 为什么使用 input_tokens？

**关键理解：**
```typescript
// input_tokens 包含了整个上下文
const usage = {
  input_tokens: 50000,  // 包含所有历史消息 + 系统提示词
  output_tokens: 500,   // 只是这次响应
};

// 所以 input_tokens 就是 contextTokens
const contextTokens = usage.input_tokens;
```

**验证：**
```typescript
// 手动计算上下文 tokens
const manualCount = messages.reduce((sum, msg) => {
  return sum + estimateTokens(msg.content);
}, 0);

// 应该接近 input_tokens
console.log('Manual:', manualCount);
console.log('Usage:', usage.input_tokens);
// 差异来自：系统提示词、工具定义等
```

### 2.3 Usage 追踪的时机

**在 Agent 执行循环中：**
```typescript
async function agentLoop() {
  while (true) {
    // 1. 调用 LLM
    const response = await llm.chat(messages);

    // 2. 记录 usage
    messages.push({
      role: 'assistant',
      content: response.content,
      usage: response.usage,  // ← 关键：记录 usage
    });

    // 3. 检查是否需要压缩
    const usage = getLastAssistantUsage(messages);
    if (usage && shouldCompact(usage.input_tokens)) {
      await compactSession();
    }

    // 4. 执行工具调用（如果有）
    if (response.tool_calls) {
      await executeTools(response.tool_calls);
    } else {
      break; // 没有工具调用，结束循环
    }
  }
}
```

---

## 3. 上下文估算算法

### 3.1 estimateContextTokens() 函数

**为什么需要估算？**
```typescript
// 问题：不是所有消息都有 usage
const messages = [
  { role: 'user', content: 'Hello' },           // 没有 usage
  { role: 'assistant', content: '...', usage }, // 有 usage
  { role: 'tool', content: '...' },             // 没有 usage
  { role: 'user', content: 'Continue' },        // 没有 usage
];

// 需要估算总 token 数
```

**估算算法：**
```typescript
function estimateContextTokens(messages: Message[]): number {
  // 1. 尝试从最后一条 assistant 消息获取 usage
  const usage = getLastAssistantUsage(messages);
  if (usage) {
    return usage.input_tokens;
  }

  // 2. 如果没有 usage，手动估算
  let totalTokens = 0;

  for (const message of messages) {
    // 估算每条消息的 token 数
    totalTokens += estimateMessageTokens(message);
  }

  // 3. 加上系统提示词和工具定义的估算
  totalTokens += estimateSystemTokens();

  return totalTokens;
}
```

**estimateMessageTokens() 实现：**
```typescript
function estimateMessageTokens(message: Message): number {
  // 简单估算：1 token ≈ 4 字符
  const contentLength = message.content?.length || 0;
  const estimatedTokens = Math.ceil(contentLength / 4);

  // 加上消息结构的开销（role、metadata 等）
  const overhead = 10;

  return estimatedTokens + overhead;
}
```

**更精确的估算（使用 tiktoken）：**
```typescript
import { encoding_for_model } from 'tiktoken';

function estimateMessageTokens(message: Message): number {
  const encoding = encoding_for_model('gpt-4');
  const tokens = encoding.encode(message.content || '');
  encoding.free();

  return tokens.length;
}
```

### 3.2 估算的准确性

**估算 vs 实际：**
```typescript
// 估算
const estimated = estimateContextTokens(messages);
// 50,000 tokens

// 实际（从 usage）
const actual = getLastAssistantUsage(messages)?.input_tokens;
// 52,000 tokens

// 误差
const error = Math.abs(estimated - actual) / actual;
// 3.8%
```

**误差来源：**
1. **系统提示词**：估算可能不准确
2. **工具定义**：JSON schema 的 token 数
3. **消息格式**：XML 标签、JSON 结构等
4. **Tokenizer 差异**：不同模型的 tokenizer 不同

**为什么误差可以接受？**
```typescript
// 即使有 5% 的误差
const estimated = 50000;
const actual = 52500;
const threshold = 183616;

// 两者都远低于阈值，不影响触发判断
if (estimated > threshold) { /* 不会触发 */ }
if (actual > threshold) { /* 不会触发 */ }

// 只有在接近阈值时，误差才可能影响判断
// 但此时已经很接近了，早触发或晚触发影响不大
```

---

## 4. 触发时机

### 4.1 在 Agent 生命周期中的位置

**Agent 执行流程：**
```typescript
async function agentTurn() {
  // 1. 接收用户消息
  const userMessage = await getUserInput();
  messages.push(userMessage);

  // 2. 检查是否需要压缩（在调用 LLM 前）
  const contextTokens = estimateContextTokens(messages);
  if (contextTokens > contextWindow - reserveTokens) {
    await compactSession(); // ← 触发点 1
  }

  // 3. 调用 LLM
  const response = await llm.chat(messages);
  messages.push(response);

  // 4. 检查是否需要压缩（在调用 LLM 后）
  const usage = getLastAssistantUsage(messages);
  if (usage && usage.input_tokens > contextWindow - reserveTokens) {
    await compactSession(); // ← 触发点 2
  }

  // 5. 执行工具调用
  if (response.tool_calls) {
    await executeTools(response.tool_calls);
  }
}
```

**两个触发点的区别：**

| 触发点 | 时机 | 优势 | 劣势 |
|--------|------|------|------|
| **触发点 1** | 调用 LLM 前 | 确保 LLM 调用不会超限 | 估算可能不准确 |
| **触发点 2** | 调用 LLM 后 | 使用实际 usage，准确 | 可能已经接近超限 |

**Pi-mono 的选择：**
```typescript
// Pi-mono 使用触发点 2（调用 LLM 后）
// 原因：
// 1. 使用实际 usage，更准确
// 2. reserveTokens 足够大，不会超限
// 3. 避免不必要的压缩
```

### 4.2 压缩的频率

**理想情况：**
```typescript
// 对话长度 vs 压缩次数
const conversationLength = 1000; // 1000 条消息
const compactionCount = 5;       // 5 次压缩

// 平均每 200 条消息压缩一次
const avgMessagesPerCompaction = conversationLength / compactionCount;
// 200 条消息
```

**实际情况：**
```typescript
// 取决于消息长度
const shortMessages = 100; // 平均 100 tokens/消息
const longMessages = 1000; // 平均 1000 tokens/消息

// 短消息：更多消息才触发压缩
const shortCompactionFrequency = threshold / shortMessages;
// 183,616 / 100 = 1836 条消息

// 长消息：更少消息就触发压缩
const longCompactionFrequency = threshold / longMessages;
// 183,616 / 1000 = 183 条消息
```

**监控压缩频率：**
```typescript
let compactionCount = 0;
let messageCount = 0;

session.on('session_before_compact', () => {
  compactionCount++;
  console.log(`第 ${compactionCount} 次压缩`);
  console.log(`消息数: ${messageCount}`);
  console.log(`平均: ${messageCount / compactionCount} 条消息/次压缩`);
});

session.on('message_added', () => {
  messageCount++;
});
```

---

## 5. 默认阈值配置

### 5.1 Pi-mono 的默认配置

**默认值：**
```typescript
const defaultConfig = {
  contextWindow: 200000,      // 200K tokens (Claude Opus 4)
  reserveTokens: 16384,       // 16K tokens
  keepRecentTokens: 20000,    // 20K tokens
};
```

**配置来源：**
```typescript
// 从模型信息获取 contextWindow
const modelInfo = getModelInfo(model);
const contextWindow = modelInfo.contextWindow;

// 从配置文件获取 reserveTokens 和 keepRecentTokens
const config = loadConfig('.pi/settings.json');
const reserveTokens = config.compaction?.reserveTokens || 16384;
const keepRecentTokens = config.compaction?.keepRecentTokens || 20000;
```

### 5.2 不同模型的配置

**模型差异：**

| 模型 | Context Window | reserveTokens | keepRecentTokens |
|------|----------------|---------------|------------------|
| **Claude Opus 4** | 200K | 16K | 20K |
| **Claude Sonnet 4** | 200K | 16K | 20K |
| **GPT-4 Turbo** | 128K | 10K | 15K |
| **Gemini Pro** | 1M | 50K | 100K |

**自动调整：**
```typescript
function getOptimalConfig(contextWindow: number) {
  // reserveTokens = 8% of context window
  const reserveTokens = Math.floor(contextWindow * 0.08);

  // keepRecentTokens = 10% of context window
  const keepRecentTokens = Math.floor(contextWindow * 0.10);

  return { reserveTokens, keepRecentTokens };
}

// Claude Opus 4 (200K)
getOptimalConfig(200000);
// { reserveTokens: 16000, keepRecentTokens: 20000 }

// Gemini Pro (1M)
getOptimalConfig(1000000);
// { reserveTokens: 80000, keepRecentTokens: 100000 }
```

---

## 6. 实际示例

### 6.1 完整的触发流程

```typescript
import { Session } from '@mariozechner/pi-agent-core';

// 创建 Session
const session = new Session({
  model: 'claude-opus-4',
  compaction: {
    enabled: true,
    reserveTokens: 16384,
    keepRecentTokens: 20000,
  },
});

// 监听压缩事件
session.on('session_before_compact', (event) => {
  console.log('=== 即将触发压缩 ===');
  console.log(`当前 tokens: ${event.contextTokens}`);
  console.log(`上下文窗口: ${event.contextWindow}`);
  console.log(`预留空间: ${event.reserveTokens}`);
  console.log(`阈值: ${event.contextWindow - event.reserveTokens}`);
  console.log(`使用率: ${(event.contextTokens / event.contextWindow * 100).toFixed(2)}%`);
});

// 进行对话
for (let i = 0; i < 1000; i++) {
  await session.sendMessage(`请读取文件 ${i}.ts 并总结`);

  // 每 100 条消息输出一次状态
  if (i % 100 === 0) {
    const contextTokens = estimateContextTokens(session.messages);
    console.log(`消息 ${i}: ${contextTokens} tokens`);
  }
}
```

**输出示例：**
```
消息 0: 5000 tokens
消息 100: 50000 tokens
消息 200: 100000 tokens
消息 300: 150000 tokens
消息 400: 185000 tokens
=== 即将触发压缩 ===
当前 tokens: 185000
上下文窗口: 200000
预留空间: 16384
阈值: 183616
使用率: 92.50%
消息 500: 25000 tokens (压缩后)
消息 600: 75000 tokens
消息 700: 125000 tokens
消息 800: 175000 tokens
消息 900: 186000 tokens
=== 即将触发压缩 ===
当前 tokens: 186000
上下文窗口: 200000
预留空间: 16384
阈值: 183616
使用率: 93.00%
```

### 6.2 调试触发逻辑

```typescript
// 调试函数
function debugCompactionTrigger(session: Session) {
  const messages = session.messages;
  const contextTokens = estimateContextTokens(messages);
  const contextWindow = session.config.contextWindow;
  const reserveTokens = session.config.reserveTokens;
  const threshold = contextWindow - reserveTokens;

  console.log('=== Compaction Trigger Debug ===');
  console.log(`Context Tokens: ${contextTokens}`);
  console.log(`Context Window: ${contextWindow}`);
  console.log(`Reserve Tokens: ${reserveTokens}`);
  console.log(`Threshold: ${threshold}`);
  console.log(`Usage: ${(contextTokens / contextWindow * 100).toFixed(2)}%`);
  console.log(`Will Trigger: ${contextTokens > threshold ? 'YES' : 'NO'}`);
  console.log(`Remaining: ${threshold - contextTokens} tokens`);
}

// 使用
debugCompactionTrigger(session);
```

---

## 7. 2025-2026 前沿实践

### 7.1 动态阈值调整

> **2025-2026 Industry Practice**: 根据对话特征动态调整压缩阈值，而不是使用固定值。
> ([Context Engineering Best Practices](https://github.com/langchain-ai/context_engineering))

**实现思路：**
```typescript
function getDynamicReserveTokens(session: Session): number {
  // 分析最近消息的平均长度
  const recentMessages = session.messages.slice(-10);
  const avgLength = recentMessages.reduce((sum, msg) => {
    return sum + estimateTokens(msg.content);
  }, 0) / recentMessages.length;

  // 根据平均长度调整预留空间
  if (avgLength > 2000) {
    return 32768; // 长消息，预留更多空间
  } else if (avgLength > 1000) {
    return 16384; // 中等消息，默认空间
  } else {
    return 8192;  // 短消息，预留较少空间
  }
}
```

### 7.2 成本感知的触发策略

> **2025-2026 Research**: 考虑 LLM 调用成本，优化压缩触发策略。
> ([Reddit: LLM Summarization Costs](https://www.reddit.com/r/LocalLLaMA/comments/...))

**实现思路：**
```typescript
function shouldCompactWithCostAwareness(
  contextTokens: number,
  threshold: number,
  costPerToken: number
): boolean {
  // 基本触发条件
  if (contextTokens <= threshold) {
    return false;
  }

  // 计算压缩成本
  const compactionCost = estimateCompactionCost(contextTokens, costPerToken);

  // 计算不压缩的成本（继续使用大上下文）
  const continueCost = estimateContinueCost(contextTokens, costPerToken);

  // 只有在压缩更划算时才触发
  return compactionCost < continueCost;
}
```

---

## 总结

**自动压缩触发机制的核心要点：**

1. **触发公式**：`contextTokens > contextWindow - reserveTokens`
2. **Usage 追踪**：从最后一条 assistant 消息获取 `input_tokens`
3. **上下文估算**：使用 `estimateContextTokens()` 估算总 token 数
4. **触发时机**：在 LLM 调用后检查 usage
5. **默认配置**：`reserveTokens=16384`, `keepRecentTokens=20000`

**关键理解：**
- 预留空间确保不会超限
- 使用实际 usage 比估算更准确
- 适中的压缩频率最优
- 可以根据场景调整配置

---

**下一步学习：**
- **核心概念 02**：压缩策略与切点算法（03_核心概念_02_压缩策略与切点算法.md）
- **实战代码 01**：基础压缩配置（07_实战代码_01_基础压缩配置.md）

---

**版本：** v1.0
**最后更新：** 2026-02-20
**维护者：** Claude Code
