# 核心概念 - 基础架构层（6个组件）

基础架构层是RAG系统的地基，负责数据的摄取、处理、存储和查询准备。

---

## 组件1：数据源层

**一句话定义：** 数据源层是RAG系统的输入端，负责提供和管理各种格式的原始数据。

### 核心功能

1. **多格式支持**
   - 文档：PDF、Word、Markdown、HTML
   - 结构化数据：JSON、CSV、数据库
   - 实时数据：API、WebSocket、RSS

2. **数据连接器**
   - 本地文件系统
   - 云存储（S3、Google Drive）
   - 数据库（PostgreSQL、MongoDB）
   - API端点

3. **元数据管理**
   - 文档标题、作者、日期
   - 文件路径、版本信息
   - 访问权限、更新时间

### 代码示例

```python
from pathlib import Path
from typing import List, Dict
import json

class DataSource:
    """统一的数据源接口"""

    def __init__(self, source_type: str, config: Dict):
        self.source_type = source_type
        self.config = config

    def load(self) -> List[Dict]:
        """加载数据"""
        if self.source_type == "local":
            return self._load_local()
        elif self.source_type == "api":
            return self._load_api()
        elif self.source_type == "database":
            return self._load_database()

    def _load_local(self) -> List[Dict]:
        """加载本地文件"""
        path = Path(self.config["path"])
        documents = []

        for file_path in path.glob(self.config.get("pattern", "**/*.pdf")):
            documents.append({
                "content": file_path.read_text(),
                "metadata": {
                    "source": str(file_path),
                    "type": file_path.suffix,
                    "size": file_path.stat().st_size
                }
            })

        return documents

    def _load_api(self) -> List[Dict]:
        """从API加载数据"""
        import requests
        response = requests.get(
            self.config["url"],
            headers=self.config.get("headers", {})
        )
        return response.json()

    def _load_database(self) -> List[Dict]:
        """从数据库加载数据"""
        # 示例：PostgreSQL
        import psycopg2
        conn = psycopg2.connect(self.config["connection_string"])
        cursor = conn.cursor()
        cursor.execute(self.config["query"])
        return cursor.fetchall()

# 使用示例
local_source = DataSource("local", {
    "path": "data/documents",
    "pattern": "**/*.pdf"
})
documents = local_source.load()
```

### 在RAG开发中

- **场景1**：企业知识库（本地PDF文档）
- **场景2**：实时新闻（API数据源）
- **场景3**：客服系统（数据库历史记录）

---

## 组件2：摄取与预处理

**一句话定义：** 摄取与预处理负责解析、清洗和标准化原始数据，为后续处理做准备。

### 核心功能

1. **文档解析**
   - PDF提取（pypdf、pdfplumber）
   - Word解析（python-docx）
   - HTML清洗（BeautifulSoup）

2. **文本清洗**
   - 去除特殊字符
   - 标准化空白
   - 去除停用词（可选）

3. **结构化提取**
   - 标题层级
   - 表格数据
   - 图片描述

### 代码示例

```python
import re
from typing import Dict, List
from pypdf import PdfReader
from bs4 import BeautifulSoup

class DocumentProcessor:
    """文档预处理器"""

    def process(self, document: Dict) -> Dict:
        """处理文档"""
        # 1. 解析内容
        content = self._parse_content(document)

        # 2. 清洗文本
        cleaned = self._clean_text(content)

        # 3. 提取元数据
        metadata = self._extract_metadata(document, cleaned)

        return {
            "content": cleaned,
            "metadata": metadata
        }

    def _parse_content(self, document: Dict) -> str:
        """根据文件类型解析内容"""
        file_type = document["metadata"]["type"]

        if file_type == ".pdf":
            return self._parse_pdf(document["metadata"]["source"])
        elif file_type == ".html":
            return self._parse_html(document["content"])
        else:
            return document["content"]

    def _parse_pdf(self, file_path: str) -> str:
        """解析PDF"""
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text

    def _parse_html(self, html: str) -> str:
        """解析HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        # 移除script和style标签
        for script in soup(["script", "style"]):
            script.decompose()
        return soup.get_text()

    def _clean_text(self, text: str) -> str:
        """清洗文本"""
        # 1. 去除多余空白
        text = re.sub(r'\s+', ' ', text)

        # 2. 去除特殊字符（保留基本标点）
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\;]', '', text)

        # 3. 标准化换行
        text = text.strip()

        return text

    def _extract_metadata(self, document: Dict, content: str) -> Dict:
        """提取元数据"""
        metadata = document["metadata"].copy()

        # 添加内容统计
        metadata["char_count"] = len(content)
        metadata["word_count"] = len(content.split())

        # 提取标题（简单示例）
        lines = content.split('\n')
        if lines:
            metadata["title"] = lines[0][:100]

        return metadata

# 使用示例
processor = DocumentProcessor()
processed_doc = processor.process(documents[0])
print(f"清洗后字符数: {processed_doc['metadata']['char_count']}")
```

### 在RAG开发中

- **重要性**：清洗质量直接影响检索效果
- **权衡**：过度清洗可能丢失重要信息
- **最佳实践**：保留原始文档，清洗后的文档单独存储

---

## 组件3：文本分块（Chunking）

**一句话定义：** 文本分块将长文档切分成适合检索和生成的小块，平衡语义完整性和检索粒度。

### 核心功能

1. **分块策略**
   - 固定大小分块
   - 语义分块
   - 滑动窗口分块

2. **重叠控制**
   - 避免信息丢失
   - 保持上下文连贯

3. **元数据继承**
   - 保留原文档信息
   - 添加块位置信息

### 代码示例

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict

class ChunkingStrategy:
    """分块策略"""

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # 使用LangChain的分块器
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def chunk(self, document: Dict) -> List[Dict]:
        """分块文档"""
        content = document["content"]
        metadata = document["metadata"]

        # 分块
        chunks = self.splitter.split_text(content)

        # 为每个块添加元数据
        chunked_docs = []
        for i, chunk in enumerate(chunks):
            chunked_docs.append({
                "content": chunk,
                "metadata": {
                    **metadata,
                    "chunk_id": i,
                    "chunk_count": len(chunks),
                    "chunk_size": len(chunk)
                }
            })

        return chunked_docs

# 使用示例
chunker = ChunkingStrategy(chunk_size=500, chunk_overlap=50)
chunks = chunker.chunk(processed_doc)
print(f"文档被分成 {len(chunks)} 个块")
print(f"第一个块: {chunks[0]['content'][:100]}...")
```

### 分块策略对比

| 策略 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| 固定大小 | 简单快速 | 可能切断语义 | 结构化文档 |
| 语义分块 | 保持语义完整 | 块大小不均 | 长文章、论文 |
| 滑动窗口 | 信息不丢失 | 存储冗余 | 关键信息密集 |

### 在RAG开发中

- **chunk_size选择**：
  - 小块（200-300）：精确检索，但可能缺少上下文
  - 中块（500-800）：平衡选择
  - 大块（1000+）：完整上下文，但检索粗糙

- **chunk_overlap选择**：
  - 10-20%的chunk_size
  - 避免关键信息被切断

---

## 组件4：嵌入模型（Embedding）

**一句话定义：** 嵌入模型将文本转换为高维向量，使计算机能够理解和计算文本的语义相似度。

### 核心功能

1. **文本向量化**
   - 将文本映射到向量空间
   - 保留语义信息

2. **相似度计算**
   - 余弦相似度
   - 欧氏距离

3. **批量处理**
   - 提高效率
   - 降低API调用成本

### 代码示例

```python
from openai import OpenAI
from typing import List
import numpy as np

class EmbeddingModel:
    """嵌入模型封装"""

    def __init__(self, model: str = "text-embedding-3-small"):
        self.client = OpenAI()
        self.model = model

    def embed(self, texts: List[str]) -> List[List[float]]:
        """批量嵌入文本"""
        response = self.client.embeddings.create(
            model=self.model,
            input=texts
        )
        return [item.embedding for item in response.data]

    def embed_single(self, text: str) -> List[float]:
        """嵌入单个文本"""
        return self.embed([text])[0]

    def similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """计算余弦相似度"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# 使用示例
embedder = EmbeddingModel()

# 嵌入文档块
chunk_texts = [chunk["content"] for chunk in chunks]
embeddings = embedder.embed(chunk_texts)

print(f"向量维度: {len(embeddings[0])}")
print(f"第一个向量前5维: {embeddings[0][:5]}")

# 计算相似度
query = "什么是RAG？"
query_embedding = embedder.embed_single(query)
similarity = embedder.similarity(query_embedding, embeddings[0])
print(f"查询与第一个块的相似度: {similarity:.4f}")
```

### 嵌入模型选择

| 模型 | 维度 | 性能 | 成本 | 适用场景 |
|------|------|------|------|----------|
| text-embedding-3-small | 1536 | 快 | 低 | 通用场景 |
| text-embedding-3-large | 3072 | 慢 | 高 | 高精度需求 |
| BGE-large | 1024 | 中 | 免费 | 本地部署 |
| E5-large | 1024 | 中 | 免费 | 多语言 |

### 在RAG开发中

- **选择原则**：
  - 通用场景：OpenAI text-embedding-3-small
  - 高精度：text-embedding-3-large
  - 本地部署：BGE或E5

- **优化技巧**：
  - 批量处理（减少API调用）
  - 缓存常用查询的embedding
  - 使用更小的维度（如果精度够用）

---

## 组件5：向量存储（Vector Store）

**一句话定义：** 向量存储负责高效存储和检索向量，是RAG系统的核心数据库。

### 核心功能

1. **向量索引**
   - HNSW（分层可导航小世界图）
   - IVF（倒排文件索引）
   - Flat（暴力搜索）

2. **相似度检索**
   - Top-K检索
   - 阈值过滤
   - 元数据过滤

3. **持久化存储**
   - 磁盘存储
   - 增量更新
   - 备份恢复

### 代码示例

```python
import chromadb
from typing import List, Dict

class VectorStore:
    """向量存储封装"""

    def __init__(self, collection_name: str = "documents"):
        self.client = chromadb.Client()
        self.collection = self.client.create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

    def add(self, chunks: List[Dict], embeddings: List[List[float]]):
        """添加文档块"""
        self.collection.add(
            embeddings=embeddings,
            documents=[chunk["content"] for chunk in chunks],
            metadatas=[chunk["metadata"] for chunk in chunks],
            ids=[f"doc_{i}" for i in range(len(chunks))]
        )

    def search(self, query_embedding: List[float], k: int = 3) -> List[Dict]:
        """检索相似文档"""
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )

        # 格式化结果
        documents = []
        for i in range(len(results["documents"][0])):
            documents.append({
                "content": results["documents"][0][i],
                "metadata": results["metadatas"][0][i],
                "distance": results["distances"][0][i]
            })

        return documents

    def search_with_filter(
        self,
        query_embedding: List[float],
        filter_dict: Dict,
        k: int = 3
    ) -> List[Dict]:
        """带元数据过滤的检索"""
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k,
            where=filter_dict
        )
        return results

# 使用示例
vectorstore = VectorStore("my_documents")

# 添加文档
vectorstore.add(chunks, embeddings)

# 检索
query = "什么是RAG？"
query_embedding = embedder.embed_single(query)
results = vectorstore.search(query_embedding, k=3)

print(f"找到 {len(results)} 个相关文档")
for i, doc in enumerate(results):
    print(f"\n文档 {i+1}:")
    print(f"内容: {doc['content'][:100]}...")
    print(f"距离: {doc['distance']:.4f}")
```

### 向量数据库对比

| 数据库 | 类型 | 性能 | 易用性 | 适用场景 |
|--------|------|------|--------|----------|
| ChromaDB | 嵌入式 | 中 | 高 | 开发测试 |
| Milvus | 分布式 | 高 | 中 | 生产环境 |
| Pinecone | 云服务 | 高 | 高 | 快速上线 |
| Qdrant | 独立服务 | 高 | 中 | 本地部署 |

### 在RAG开发中

- **选择原则**：
  - 开发阶段：ChromaDB（简单快速）
  - 生产环境：Milvus或Qdrant（高性能）
  - 云服务：Pinecone（免运维）

---

## 组件6：查询处理（Query Processing）

**一句话定义：** 查询处理负责理解和优化用户查询，提高检索效果。

### 核心功能

1. **查询理解**
   - 意图识别
   - 实体提取
   - 关键词提取

2. **查询优化**
   - 查询扩展（添加同义词）
   - 查询改写（优化表达）
   - 查询分解（复杂查询拆分）

3. **查询路由**
   - 根据查询类型选择检索策略
   - 多索引路由

### 代码示例

```python
from typing import List, Dict
from openai import OpenAI

class QueryProcessor:
    """查询处理器"""

    def __init__(self):
        self.client = OpenAI()

    def process(self, query: str) -> Dict:
        """处理查询"""
        return {
            "original": query,
            "cleaned": self._clean_query(query),
            "expanded": self._expand_query(query),
            "intent": self._detect_intent(query)
        }

    def _clean_query(self, query: str) -> str:
        """清洗查询"""
        # 去除多余空白
        query = " ".join(query.split())
        # 转小写
        query = query.lower()
        return query

    def _expand_query(self, query: str) -> List[str]:
        """查询扩展"""
        # 使用LLM生成查询变体
        prompt = f"""
        为以下查询生成3个语义相似的变体查询：
        原查询：{query}

        要求：
        1. 保持原意
        2. 使用不同表达方式
        3. 每行一个查询

        变体查询：
        """

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )

        variants = response.choices[0].message.content.strip().split('\n')
        return [query] + [v.strip() for v in variants if v.strip()]

    def _detect_intent(self, query: str) -> str:
        """检测查询意图"""
        # 简单的规则匹配
        if any(word in query.lower() for word in ["什么", "是什么", "定义"]):
            return "definition"
        elif any(word in query.lower() for word in ["如何", "怎么", "步骤"]):
            return "how_to"
        elif any(word in query.lower() for word in ["为什么", "原因"]):
            return "explanation"
        else:
            return "general"

# 使用示例
query_processor = QueryProcessor()
processed = query_processor.process("什么是RAG架构？")

print(f"原查询: {processed['original']}")
print(f"清洗后: {processed['cleaned']}")
print(f"意图: {processed['intent']}")
print(f"扩展查询:")
for variant in processed['expanded']:
    print(f"  - {variant}")
```

### 在RAG开发中

- **查询理解**：帮助选择合适的检索策略
- **查询扩展**：提高召回率（找到更多相关文档）
- **查询改写**：提高精确度（找到更准确的文档）

---

## 基础层总结

**6个组件的协同工作：**

```
数据源 → 摄取预处理 → 分块 → 嵌入 → 向量存储
                                        ↓
                                    查询处理
```

**关键要点：**
1. **数据源**：支持多格式，统一接口
2. **预处理**：清洗质量影响检索效果
3. **分块**：平衡语义完整性和检索粒度
4. **嵌入**：选择合适的模型和维度
5. **存储**：根据场景选择向量数据库
6. **查询**：理解和优化用户意图

**下一步：** 学习检索优化层，提升检索质量
