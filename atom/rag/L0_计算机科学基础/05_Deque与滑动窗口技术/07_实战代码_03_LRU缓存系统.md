# 实战代码 03：LRU 缓存系统

> 生产级 LRU 缓存实现 + AI Agent API 响应缓存

---

## 1. OrderedDict 实现（推荐）

```python
from collections import OrderedDict
from typing import Optional

class LRUCache:
    """使用 OrderedDict 实现的 LRU 缓存"""
    
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity
    
    def get(self, key: int) -> int:
        if key not in self.cache:
            return -1
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def put(self, key: int, value: int):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)

# 测试
cache = LRUCache(2)
cache.put(1, 1)
cache.put(2, 2)
print(cache.get(1))  # 1
cache.put(3, 3)      # 淘汰 key 2
print(cache.get(2))  # -1
```

---

## 2. AI Agent LLM API 响应缓存

```python
from collections import OrderedDict
import hashlib
import json
from typing import Optional, Dict, Any

class LLMResponseCache:
    """LLM API 响应缓存"""
    
    def __init__(self, capacity: int = 100):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.hits = 0
        self.misses = 0
    
    def _generate_key(self, prompt: str, model: str, **kwargs) -> str:
        """生成缓存键"""
        cache_data = {
            "prompt": prompt,
            "model": model,
            **kwargs
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def get(self, prompt: str, model: str, **kwargs) -> Optional[str]:
        """获取缓存的响应"""
        key = self._generate_key(prompt, model, **kwargs)
        
        if key not in self.cache:
            self.misses += 1
            return None
        
        self.hits += 1
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def put(self, prompt: str, model: str, response: str, **kwargs):
        """缓存响应"""
        key = self._generate_key(prompt, model, **kwargs)
        
        if key in self.cache:
            self.cache.move_to_end(key)
        
        self.cache[key] = response
        
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)
    
    def get_stats(self) -> Dict[str, Any]:
        """获取缓存统计"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": f"{hit_rate:.2%}",
            "size": len(self.cache),
            "capacity": self.capacity
        }

# 使用示例
cache = LLMResponseCache(capacity=50)

# 模拟 API 调用
def call_llm_with_cache(prompt: str, model: str = "gpt-4") -> str:
    # 尝试从缓存获取
    cached_response = cache.get(prompt, model)
    if cached_response:
        print(f"缓存命中: {prompt[:30]}...")
        return cached_response
    
    # 缓存未命中，调用 API
    print(f"调用 API: {prompt[:30]}...")
    response = f"Response to: {prompt}"  # 实际应该调用 LLM API
    
    # 缓存响应
    cache.put(prompt, model, response)
    
    return response

# 测试
call_llm_with_cache("What is Python?")
call_llm_with_cache("What is Python?")  # 缓存命中
call_llm_with_cache("What is JavaScript?")

print("\n缓存统计:")
print(cache.get_stats())
```

---

## 3. 带 TTL 的 LRU 缓存

```python
from collections import OrderedDict
import time
from typing import Optional, Any

class TTLLRUCache:
    """带 TTL（Time To Live）的 LRU 缓存"""
    
    def __init__(self, capacity: int, ttl_seconds: int = 3600):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.ttl_seconds = ttl_seconds
        self.timestamps = {}
    
    def get(self, key: str) -> Optional[Any]:
        """获取值，检查是否过期"""
        if key not in self.cache:
            return None
        
        # 检查是否过期
        if time.time() - self.timestamps[key] > self.ttl_seconds:
            del self.cache[key]
            del self.timestamps[key]
            return None
        
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def put(self, key: str, value: Any):
        """添加/更新值"""
        if key in self.cache:
            self.cache.move_to_end(key)
        
        self.cache[key] = value
        self.timestamps[key] = time.time()
        
        if len(self.cache) > self.capacity:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            del self.timestamps[oldest_key]
    
    def cleanup_expired(self):
        """清理过期条目"""
        now = time.time()
        expired_keys = [
            key for key, timestamp in self.timestamps.items()
            if now - timestamp > self.ttl_seconds
        ]
        
        for key in expired_keys:
            del self.cache[key]
            del self.timestamps[key]
        
        return len(expired_keys)

# 使用示例
cache = TTLLRUCache(capacity=100, ttl_seconds=60)

cache.put("key1", "value1")
time.sleep(2)
print(cache.get("key1"))  # "value1"

time.sleep(59)
print(cache.get("key1"))  # None (已过期)
```

---

## 4. 线程安全的 LRU 缓存

```python
import threading
from collections import OrderedDict
from typing import Optional, Any

class ThreadSafeLRUCache:
    """线程安全的 LRU 缓存"""
    
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.lock = threading.Lock()
    
    def get(self, key: Any) -> Optional[Any]:
        """线程安全地获取值"""
        with self.lock:
            if key not in self.cache:
                return None
            self.cache.move_to_end(key)
            return self.cache[key]
    
    def put(self, key: Any, value: Any):
        """线程安全地添加/更新值"""
        with self.lock:
            if key in self.cache:
                self.cache.move_to_end(key)
            self.cache[key] = value
            if len(self.cache) > self.capacity:
                self.cache.popitem(last=False)
    
    def clear(self):
        """清空缓存"""
        with self.lock:
            self.cache.clear()

# 多线程测试
cache = ThreadSafeLRUCache(capacity=100)

def worker(thread_id: int):
    for i in range(100):
        cache.put(f"key_{thread_id}_{i}", f"value_{thread_id}_{i}")
        cache.get(f"key_{thread_id}_{i}")

threads = [threading.Thread(target=worker, args=(i,)) for i in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()

print(f"缓存大小: {len(cache.cache)}")
```

---

## 5. Embedding 缓存系统

```python
from collections import OrderedDict
import hashlib
from typing import List, Optional

class EmbeddingCache:
    """Embedding 缓存系统"""
    
    def __init__(self, capacity: int = 1000):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.hits = 0
        self.misses = 0
    
    def _generate_key(self, text: str, model: str) -> str:
        """生成缓存键"""
        cache_str = f"{model}:{text}"
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def get(self, text: str, model: str) -> Optional[List[float]]:
        """获取缓存的 embedding"""
        key = self._generate_key(text, model)
        
        if key not in self.cache:
            self.misses += 1
            return None
        
        self.hits += 1
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def put(self, text: str, model: str, embedding: List[float]):
        """缓存 embedding"""
        key = self._generate_key(text, model)
        
        if key in self.cache:
            self.cache.move_to_end(key)
        
        self.cache[key] = embedding
        
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)
    
    def get_hit_rate(self) -> float:
        """获取缓存命中率"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0

# 使用示例
cache = EmbeddingCache(capacity=500)

def get_embedding_with_cache(text: str, model: str = "text-embedding-3-small") -> List[float]:
    # 尝试从缓存获取
    cached_embedding = cache.get(text, model)
    if cached_embedding:
        return cached_embedding
    
    # 缓存未命中，生成 embedding
    embedding = [0.1, 0.2, 0.3]  # 实际应该调用 embedding API
    
    # 缓存 embedding
    cache.put(text, model, embedding)
    
    return embedding

# 测试
get_embedding_with_cache("Hello world")
get_embedding_with_cache("Hello world")  # 缓存命中

print(f"命中率: {cache.get_hit_rate():.2%}")
```

---

## 6. 性能测试

```python
import time
from collections import OrderedDict

def benchmark_lru_cache():
    """LRU 缓存性能测试"""
    cache = LRUCache(capacity=1000)
    
    # 测试写入性能
    start = time.time()
    for i in range(10000):
        cache.put(i, i)
    write_time = time.time() - start
    
    # 测试读取性能
    start = time.time()
    for i in range(10000):
        cache.get(i % 1000)
    read_time = time.time() - start
    
    print(f"写入 10000 次: {write_time:.4f}s")
    print(f"读取 10000 次: {read_time:.4f}s")
    print(f"平均写入时间: {write_time / 10000 * 1000:.4f}ms")
    print(f"平均读取时间: {read_time / 10000 * 1000:.4f}ms")

benchmark_lru_cache()
```

---

**版本**: v1.0
**最后更新**: 2026-02-13
