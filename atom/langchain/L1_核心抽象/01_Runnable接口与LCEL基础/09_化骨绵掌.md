# 化骨绵掌

> 10 个 2 分钟知识卡片，快速掌握 Runnable 和 LCEL

---

## 使用说明

每个卡片独立完整，可单独阅读，也可按顺序学习。每个卡片约 2 分钟阅读时间。

---

## 卡片 1: 直觉理解 - Runnable 是什么

**一句话：** Runnable 是可以执行、可以组合的计算单元。

**举例：**

想象一个数据处理流水线：

```
原始数据 → [清洗] → [转换] → [分析] → 结果
```

每个方括号里的步骤就是一个 Runnable：
- 接收输入（原始数据或上一步的输出）
- 执行计算（清洗、转换、分析）
- 产生输出（传给下一步）

**关键特征：**
- **可执行**：有 invoke/batch/stream 方法
- **可组合**：用 `|` 连接
- **统一接口**：所有 Runnable 都一样

**应用：** 在 AI Agent 开发中，每个组件（LLM、Prompt、Parser）都是 Runnable，可以灵活组合成复杂工作流。

---

## 卡片 2: 形式化定义 - Runnable 协议

**一句话：** Runnable 是 Python Protocol，定义了三个核心方法。

**精确定义：**

```python
from typing import Protocol, TypeVar

Input = TypeVar("Input")
Output = TypeVar("Output")

class Runnable(Protocol[Input, Output]):
    def invoke(self, input: Input) -> Output:
        """同步单次执行"""

    def batch(self, inputs: list[Input]) -> list[Output]:
        """批量并发执行"""

    def stream(self, input: Input) -> Iterator[Output]:
        """流式实时输出"""
```

**为什么用 Protocol？**
- 不需要继承，只需实现方法
- 更灵活，降低耦合
- 易于集成第三方库

**应用：** 任何实现这三个方法的对象都是 Runnable，可以无缝集成到 LangChain 生态。

---

## 卡片 3: invoke 方法 - 同步单次执行

**一句话：** invoke 是最基础的执行方法，同步处理单个输入。

**方法签名：**

```python
def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output
```

**使用场景：**
- 单次查询
- 简单脚本
- 调试测试

**代码示例：**

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# 同步执行
response = llm.invoke("你好")
print(response.content)  # "你好！有什么我可以帮助你的吗？"
```

**应用：** 适合实时查询场景，如单次翻译、单个问题回答。

---

## 卡片 4: batch 方法 - 批量并发处理

**一句话：** batch 并发处理多个输入，比循环 invoke 快 2-3 倍。

**方法签名：**

```python
def batch(self, inputs: list[Input], config: Optional[RunnableConfig] = None) -> list[Output]
```

**性能对比：**

```python
# ❌ 慢：顺序执行
results = [llm.invoke(x) for x in inputs]  # 10秒

# ✅ 快：并发执行
results = llm.batch(inputs)  # 3-4秒
```

**成本优化：**
- 使用 langasync 工具
- 结合批处理 API
- 成本降低 50%

**应用：** 批量评估、数据标注、离线分析。

---

## 卡片 5: stream 方法 - 流式实时输出

**一句话：** stream 逐块输出结果，降低 50-70% 感知延迟。

**方法签名：**

```python
def stream(self, input: Input, config: Optional[RunnableConfig] = None) -> Iterator[Output]
```

**使用示例：**

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)

# 流式输出
for chunk in llm.stream("写一首诗"):
    print(chunk.content, end="", flush=True)

# 输出：逐字显示，而非等待完整诗歌
```

**UI 集成：**
- FastAPI + StreamingResponse
- 前端 EventSource
- 实时聊天应用

**应用：** 聊天机器人、长文本生成、需要实时反馈的场景。

---

## 卡片 6: LCEL 管道 - 声明式组合

**一句话：** LCEL 用 `|` 操作符声明式组合 Runnable，自动优化执行。

**基础语法：**

```python
# 定义组件
prompt = ChatPromptTemplate.from_template("翻译: {text}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# LCEL 组合
chain = prompt | llm | parser

# 执行
result = chain.invoke({"text": "你好"})
```

**自动优化：**
- 并行执行：`RunnableParallel` 自动并发
- 流式传输：自动支持 stream
- 异步执行：自动支持 ainvoke

**应用：** 构建 RAG、Agent、多步推理等复杂 AI 工作流。

---

## 卡片 7: Config 配置 - 运行时参数传递

**一句话：** Config 在 Runnable 链中传递配置，实现可观测性和监控。

**配置项：**

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    tags=["production", "v2"],           # 标签
    metadata={"user_id": "123"},         # 元数据
    callbacks=[monitoring_handler],      # 回调
    max_concurrency=5,                   # 并发限制
    run_name="my_chain"                  # 运行名称
)

result = chain.invoke(input, config=config)
```

**传递机制：**
- Config 自动传递给链中所有组件
- 每个 Runnable 都能访问 config
- 便于统一监控和调试

**应用：** 生产环境监控、性能追踪、成本计算、用户追踪。

---

## 卡片 8: 类型系统 - Input/Output 泛型

**一句话：** Runnable 使用泛型确保类型安全，IDE 可以自动推断类型。

**类型定义：**

```python
from langchain_core.runnables import Runnable

# 明确类型
class TextProcessor(Runnable[str, dict]):
    def invoke(self, input: str) -> dict:
        return {"result": input.upper()}

# 类型推断
prompt: Runnable[dict, PromptValue]
llm: Runnable[PromptValue, AIMessage]
parser: Runnable[AIMessage, str]

# 组合后自动推断
chain: Runnable[dict, str] = prompt | llm | parser
```

**好处：**
- IDE 类型提示
- 编译时类型检查
- 减少运行时错误

**应用：** 大型项目中确保类型安全，减少 bug。

---

## 卡片 9: 生产实践 - 2025-2026 最佳实践

**一句话：** 生产环境使用 Runnable 需要关注性能、成本、监控、安全。

**最佳实践清单：**

**1. 性能优化**
```python
# ✅ 使用 batch 而非循环
results = chain.batch(inputs)

# ✅ 识别可并行的步骤
chain = RunnableParallel(task1=..., task2=...)
```

**2. 成本优化**
```python
# ✅ 使用 langasync（50% 成本降低）
from langasync import wrap_chain
async_chain = wrap_chain(chain, batch_size=10)
```

**3. 监控**
```python
# ✅ 集成 LangSmith
config = RunnableConfig(
    tags=["production"],
    callbacks=[monitoring_handler]
)
```

**4. 安全**
```python
# ✅ 升级到 langchain-core 1.2.5+
# 修复 CVE-2025-68664 序列化漏洞
```

**应用：** 企业级 AI 应用的生产部署标准。

---

## 卡片 10: 总结与延伸 - 与 LangGraph 的关系

**一句话：** Runnable 是基础，LangGraph 是高级工作流引擎。

**关系图：**

```
Runnable 协议（基础层）
    ↓
LCEL 表达式（组合层）
    ↓
LangGraph 工作流（高级层）
```

**Runnable vs LangGraph：**

| 维度 | Runnable + LCEL | LangGraph |
|------|-----------------|-----------|
| **复杂度** | 简单 | 复杂 |
| **状态** | 无状态 | 有状态 |
| **控制流** | 线性/并行 | 循环/条件 |
| **适用场景** | RAG、简单 Agent | 多代理、复杂工作流 |

**何时使用 LangGraph？**
- 需要状态持久化
- 需要循环和回溯
- 多代理协作
- 长时间运行任务

**学习路径：**
1. 掌握 Runnable 和 LCEL（本文档）
2. 学习 LangGraph 基础
3. 构建有状态的 Agent 应用

**2025-2026 年趋势：**
- LangGraph 成为企业级标准
- 工作流优先，代理是组件
- 状态管理和检查点机制

**应用：** 复杂的多步推理、需要回溯的任务、多代理协作系统。

---

## 快速复习

### 核心方法

| 方法 | 用途 | 场景 |
|------|------|------|
| **invoke** | 同步单次 | 实时查询 |
| **batch** | 并发批量 | 批量处理 |
| **stream** | 流式输出 | 聊天应用 |

### 关键概念

| 概念 | 定义 | 价值 |
|------|------|------|
| **Runnable** | 可组合的计算单元 | 统一接口 |
| **LCEL** | 声明式编程语言 | 自动优化 |
| **Protocol** | 结构化类型 | 灵活解耦 |
| **Config** | 运行时配置 | 可观测性 |

### 2025-2026 关键数据

- **57%** 企业生产部署
- **2-3倍** 并行性能提升
- **50%** 成本降低（langasync）
- **80%** 查询时间缩短（Klarna）

---

## 学习检查清单

完成本文档学习后，你应该能够：

### 理解层面
- [ ] 理解 Runnable 协议的设计哲学
- [ ] 理解 LCEL 声明式编程的优势
- [ ] 理解 invoke/batch/stream 的使用场景
- [ ] 理解 Config 参数的作用

### 应用层面
- [ ] 能够实现自定义 Runnable
- [ ] 能够使用 LCEL 组合复杂工作流
- [ ] 能够选择合适的执行方法
- [ ] 能够使用 Config 进行监控

### 生产层面
- [ ] 能够优化性能和成本
- [ ] 能够集成 LangSmith 监控
- [ ] 能够处理错误和异常
- [ ] 能够进行单元测试

---

## 下一步学习

### 深入学习
1. **L2_LCEL表达式**: 学习 RunnableParallel、RunnableBranch 等高级用法
2. **L3_组件生态**: 了解 LangChain 的丰富组件
3. **L4_Agent系统**: 学习 Agent 的推理和工具调用
4. **L7_LangGraph**: 学习有状态的工作流引擎

### 实战项目
1. 构建一个 RAG 文档问答系统
2. 实现一个多步推理 Agent
3. 开发一个流式聊天应用
4. 集成 LangSmith 进行监控

### 持续关注
- LangChain 官方博客
- GitHub releases
- 企业案例分享
- 社区最佳实践

---

**完成学习**: 恭喜完成 Runnable 和 LCEL 的学习！继续探索 LangChain 的更多功能。
