# 化骨绵掌

> 10 个 2 分钟知识卡片，碎片化掌握 RAG 架构

---

## 卡片1：RAG 是什么

**一句话：** RAG = Retrieval-Augmented Generation = 检索增强生成

**直觉理解：**
```
闭卷考试（纯 LLM）：只能靠记忆，不会的就瞎编
开卷考试（RAG）：可以查资料，答案有据可查
```

**核心公式：**
```
RAG = 检索相关知识 + 注入到 Prompt + LLM 生成答案
```

**应用：** 企业知识库问答、智能客服、文档助手

---

## 卡片2：为什么需要 RAG

**一句话：** LLM 的知识是静态的，RAG 让它能获取动态知识

**LLM 的三个致命问题：**

| 问题 | 表现 | RAG 如何解决 |
|-----|------|-------------|
| 知识截止 | 不知道最新信息 | 检索最新文档 |
| 知识缺失 | 不知道私有数据 | 检索企业文档 |
| 幻觉问题 | 不知道就编造 | 基于真实内容生成 |

**应用：** 任何需要 LLM 回答"它不知道的事情"的场景

---

## 卡片3：双阶段架构

**一句话：** RAG 分为离线索引和在线查询两个独立阶段

```
离线索引（提前做好）:
文档 → 分块 → 向量化 → 存储
       ↓
    向量数据库
       ↓
在线查询（实时响应）:
问题 → 检索 → 生成 → 答案
```

**类比：**
- 离线索引 = 图书馆编目录（只做一次）
- 在线查询 = 读者借书（每次都做）

**应用：** 理解 RAG 系统的整体结构，知道什么时候做什么

---

## 卡片4：六大核心组件

**一句话：** RAG 有 6 个组件，口诀"加分向存检生"

| 组件 | 口诀 | 职责 |
|-----|------|------|
| Loader | **加**载 | 把文档变成文本 |
| Chunker | **分**块 | 把长文本切成小块 |
| Embedder | **向**量化 | 把文本变成向量 |
| VectorStore | **存**储 | 把向量存起来 |
| Retriever | **检**索 | 找到最相关的块 |
| Generator | **生**成 | 调用 LLM 生成答案 |

**应用：** 出问题时知道从哪个组件开始排查

---

## 卡片5：数据流转换

**一句话：** 数据在 RAG 中经历 5 次关键转换

```
文档 → 文本 → 块 → 向量 → 答案
 ↓      ↓     ↓     ↓      ↓
解析   分块  向量化  检索   生成
```

**每次转换的关键决策：**
- 解析：选什么解析器
- 分块：块多大、重叠多少
- 向量化：用什么 Embedding 模型
- 检索：返回多少个结果（Top-K）
- 生成：Prompt 怎么设计

**应用：** 理解数据如何在系统中流动，优化每个环节

---

## 卡片6：RAG vs Fine-tuning

**一句话：** RAG 解决知识问题，Fine-tuning 解决能力问题

| 维度 | RAG | Fine-tuning |
|-----|-----|-------------|
| 解决问题 | 知识获取 | 行为调整 |
| 更新成本 | 低（改文档） | 高（重新训练） |
| 可解释性 | 高（可追溯） | 低（黑盒） |

**选择建议：**
- 知识问答 → RAG
- 风格调整 → Fine-tuning
- 两者可以结合使用

**应用：** 技术选型时做出正确决策

---

## 卡片7：向量检索原理

**一句话：** 向量检索通过语义相似度找到相关内容，而非关键词匹配

```
关键词搜索：
"苹果手机" → 只能匹配包含"苹果手机"的文档

向量检索：
"苹果手机" → 能匹配 "iPhone"、"Apple 手机"、"iOS 设备"
```

**原理：**
1. 文本 → Embedding 模型 → 向量（如 1536 维）
2. 语义相似的文本，向量也相似
3. 用余弦相似度计算向量距离

**应用：** 理解为什么 RAG 能做语义搜索

---

## 卡片8：Chunking 策略

**一句话：** 分块策略直接影响检索质量，是 RAG 调优的关键

**块大小的权衡：**
```
块太大：
- 检索不精准（返回一大段，只有一句相关）
- 浪费 Context Window

块太小：
- 丢失上下文（句子被切断）
- 检索结果碎片化
```

**推荐参数：**
- chunk_size: 500-1000 字符
- overlap: 50-100 字符（保持上下文连贯）

**应用：** 调优 RAG 系统时，分块是第一个要调的参数

---

## 卡片9：常见误区

**一句话：** 避开这 3 个坑，少走弯路

| 误区 | 正确理解 |
|-----|---------|
| RAG 很简单 | 每个环节都需要精心设计 |
| RAG 万能 | 只解决知识问题，不提升推理能力 |
| 检索越多越好 | 质量 > 数量，3 个精准 > 10 个一般 |

**记住：**
- Garbage In, Garbage Out
- 检索质量决定生成质量
- 80% 的问题出在检索环节

**应用：** 避免在错误方向上浪费时间

---

## 卡片10：最简 RAG 实现

**一句话：** 理解这 6 行伪代码，就理解了 RAG 的本质

```python
# 离线索引
chunks = split(load(documents))      # 加载 + 分块
vectors = embed(chunks)              # 向量化
store(vectors, chunks)               # 存储

# 在线查询
relevant = search(embed(question))   # 检索
answer = llm(question + relevant)    # 生成
```

**关键决策：**
- `split`: chunk_size=500
- `embed`: text-embedding-3-small
- `search`: top_k=3
- `llm`: gpt-4o-mini

**应用：** 快速搭建 RAG 原型，验证想法

---

## 卡片速查表

| 卡片 | 主题 | 核心要点 |
|-----|------|---------|
| 1 | RAG 是什么 | 检索 + 增强 + 生成 |
| 2 | 为什么需要 | 解决 LLM 知识局限 |
| 3 | 双阶段架构 | 离线索引 + 在线查询 |
| 4 | 六大组件 | 加分向存检生 |
| 5 | 数据流转换 | 文档→文本→块→向量→答案 |
| 6 | RAG vs FT | 知识 vs 能力 |
| 7 | 向量检索 | 语义相似度匹配 |
| 8 | Chunking | 块大小是关键参数 |
| 9 | 常见误区 | 不简单、不万能、不是越多越好 |
| 10 | 最简实现 | 6 行代码理解本质 |

---

**下一步：** [10_一句话总结](./10_一句话总结.md) - 最终总结
