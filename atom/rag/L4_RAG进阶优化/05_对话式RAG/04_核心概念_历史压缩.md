# 核心概念：历史压缩

> 在Context Window限制下保留关键信息的策略

---

## 概念定义

### 什么是历史压缩？

**历史压缩**是指在对话式RAG系统中，当对话历史超过一定长度时，通过各种策略减少历史信息的存储和传输量，同时尽可能保留关键信息的技术。

**核心目标：**
1. **控制Token消耗**：避免超过LLM的Context Window限制
2. **降低成本**：减少每次API调用的Token数量
3. **保留关键信息**：不能简单截断，要保留重要内容
4. **提升效果**：去除冗余信息，让LLM更聚焦

### 为什么需要历史压缩？

**问题1：Context Window限制**
```
GPT-4: 128K tokens (约10万字)
GPT-3.5: 16K tokens (约1.2万字)
Claude 3: 200K tokens (约15万字)

一轮对话平均: 200-500 tokens
10轮对话: 2000-5000 tokens
100轮对话: 20000-50000 tokens ← 可能超过限制
```

**问题2：成本线性增长**
```
假设GPT-4价格: $0.03/1K tokens (输入)

无压缩:
- 第1轮: 500 tokens → $0.015
- 第10轮: 5000 tokens → $0.15
- 第100轮: 50000 tokens → $1.5

有压缩(保持5000 tokens):
- 每轮: 5000 tokens → $0.15
- 节省: 90%成本(第100轮)
```

**问题3：信息噪音**
```
用户: "什么是RAG？"
助手: [详细解释RAG...]
用户: "它有什么优势？"
助手: [详细解释优势...]
...
用户: "如何部署到生产环境？"  ← 第50轮

如果把前49轮全部传给LLM:
- LLM需要处理大量无关信息
- 可能"迷失"在历史中
- 生成质量反而下降
```

### 压缩 vs 截断的区别

| 维度 | 简单截断 | 智能压缩 |
|------|---------|---------|
| 实现 | 只保留最近N轮 | 提取关键信息 |
| 信息损失 | 完全丢失早期信息 | 保留关键信息 |
| 成本 | 无额外成本 | 需要额外LLM调用 |
| 效果 | 可能丢失重要上下文 | 更好地保留上下文 |
| 适用场景 | 短对话、简单场景 | 长对话、复杂场景 |

---

## 压缩策略

### 策略1：滑动窗口截断

#### 原理

**最简单的压缩策略：只保留最近N轮对话，丢弃更早的历史。**

```
完整历史: [M1, M2, M3, M4, M5, M6, M7, M8, M9, M10]
                                    ↑
                              window_size=3
                                    ↓
压缩后:                         [M5, M6, M7, M8, M9, M10]
                                 ↑___保留3轮___↑
```

#### 实现代码

```python
from typing import List
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Message:
    role: str
    content: str
    timestamp: datetime

class SlidingWindowCompressor:
    def __init__(self, window_size: int = 5):
        """
        window_size: 保留最近N轮对话（每轮=1个user+1个assistant）
        """
        self.window_size = window_size

    def compress(self, messages: List[Message]) -> List[Message]:
        """
        滑动窗口压缩
        """
        # 每轮对话=2条消息（user + assistant）
        keep_count = self.window_size * 2

        if len(messages) <= keep_count:
            return messages  # 不需要压缩

        # 只保留最近的消息
        compressed = messages[-keep_count:]

        print(f"[压缩] {len(messages)}条 → {len(compressed)}条")
        return compressed

# ===== 使用示例 =====
compressor = SlidingWindowCompressor(window_size=3)

# 模拟10轮对话
messages = []
for i in range(10):
    messages.append(Message(role="user", content=f"问题{i+1}", timestamp=datetime.now()))
    messages.append(Message(role="assistant", content=f"回答{i+1}", timestamp=datetime.now()))

print(f"原始消息数: {len(messages)}")

# 压缩
compressed = compressor.compress(messages)
print(f"压缩后消息数: {len(compressed)}")
print(f"保留的对话: {[m.content for m in compressed]}")
```

**运行输出：**
```
原始消息数: 20
[压缩] 20条 → 6条
压缩后消息数: 6
保留的对话: ['问题8', '回答8', '问题9', '回答9', '问题10', '回答10']
```

#### 优缺点分析

**优点：**
- ✅ 实现极其简单（一行代码）
- ✅ 零额外成本（无需LLM调用）
- ✅ 延迟低（毫秒级）
- ✅ 可预测（固定保留N轮）

**缺点：**
- ❌ 完全丢失早期信息
- ❌ 无法保留重要的历史事实
- ❌ 不适合需要长期记忆的场景

**适用场景：**
- 短对话（<10轮）
- 对历史依赖低的场景
- 原型开发和测试
- 成本敏感的应用

#### 窗口大小选择

```python
# 根据场景选择窗口大小
scenarios = {
    "简单问答": 2,      # 只需要记住上一轮
    "文档助手": 3-5,    # 需要追踪讨论主题
    "代码助手": 5-7,    # 需要记住代码修改历史
    "智能客服": 3-5,    # 需要记住用户问题和订单信息
}
```

---

### 策略2：LLM摘要压缩

#### 原理

**使用LLM总结历史对话，生成简洁的摘要，保留摘要+最近N轮原文。**

```
完整历史: [M1, M2, M3, M4, M5, M6, M7, M8, M9, M10]
           ↓___LLM总结___↓
摘要: "用户询问了RAG的定义、优势和应用场景..."
                                    ↓
压缩后: [摘要, M7, M8, M9, M10]
         ↑     ↑___保留最近2轮___↑
```

#### 实现代码

```python
from openai import OpenAI
from typing import List

class LLMSummaryCompressor:
    def __init__(self, window_size: int = 3, summary_threshold: int = 10):
        """
        window_size: 保留最近N轮原文
        summary_threshold: 超过N轮就触发摘要
        """
        self.client = OpenAI()
        self.window_size = window_size
        self.summary_threshold = summary_threshold

    def compress(self, messages: List[Message]) -> List[Message]:
        """
        LLM摘要压缩
        """
        # 不需要压缩
        if len(messages) <= self.summary_threshold * 2:
            return messages

        # 分割：需要摘要的部分 + 保留的最近部分
        keep_count = self.window_size * 2
        to_summarize = messages[:-keep_count]
        to_keep = messages[-keep_count:]

        # 使用LLM生成摘要
        summary = self._generate_summary(to_summarize)

        # 构建摘要消息
        summary_message = Message(
            role="system",
            content=f"[历史摘要] {summary}",
            timestamp=datetime.now()
        )

        # 返回：摘要 + 最近N轮
        compressed = [summary_message] + to_keep

        print(f"[压缩] {len(messages)}条 → {len(compressed)}条 (摘要+{self.window_size}轮)")
        return compressed

    def _generate_summary(self, messages: List[Message]) -> str:
        """
        使用LLM生成对话摘要
        """
        # 构建历史文本
        history_text = "\n".join([
            f"{msg.role}: {msg.content}"
            for msg in messages
        ])

        # 调用LLM生成摘要
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{
                "role": "system",
                "content": """你是一个对话摘要助手。
                请总结以下对话的关键信息，包括：
                1. 讨论的主要主题
                2. 用户提出的关键问题
                3. 重要的事实和结论
                4. 用户的偏好和需求

                要求：
                - 简洁明了（200字以内）
                - 保留关键信息
                - 使用第三人称描述
                """
            }, {
                "role": "user",
                "content": f"对话历史:\n{history_text}\n\n请生成摘要:"
            }],
            temperature=0.3  # 降低随机性
        )

        summary = response.choices[0].message.content.strip()
        return summary

# ===== 使用示例 =====
compressor = LLMSummaryCompressor(window_size=2, summary_threshold=5)

# 模拟10轮对话
messages = []
topics = ["RAG定义", "RAG优势", "RAG应用", "RAG实现", "RAG优化",
          "RAG评估", "RAG部署", "RAG成本", "RAG未来", "RAG总结"]

for i, topic in enumerate(topics):
    messages.append(Message(
        role="user",
        content=f"请介绍{topic}",
        timestamp=datetime.now()
    ))
    messages.append(Message(
        role="assistant",
        content=f"关于{topic}的详细解释...",
        timestamp=datetime.now()
    ))

print(f"原始消息数: {len(messages)}")

# 压缩
compressed = compressor.compress(messages)
print(f"压缩后消息数: {len(compressed)}")

# 查看摘要
if compressed[0].role == "system":
    print(f"\n摘要内容:\n{compressed[0].content}")
```

**运行输出示例：**
```
原始消息数: 20
[压缩] 20条 → 5条 (摘要+2轮)
压缩后消息数: 5

摘要内容:
[历史摘要] 用户系统地询问了RAG（检索增强生成）的多个方面，包括基本定义、
核心优势、实际应用场景、技术实现方法、性能优化策略、效果评估指标、生产环境
部署方案和成本控制。助手提供了详细的技术解释和实践建议。用户展现出对RAG
技术的全面学习需求，从理论到实践逐步深入。
```

#### 优缺点分析

**优点：**
- ✅ 保留关键信息（不是简单截断）
- ✅ 压缩比高（10轮→1条摘要）
- ✅ 适合长对话
- ✅ 保持上下文连贯性

**缺点：**
- ❌ 额外LLM调用成本（每次压缩~$0.01-0.05）
- ❌ 增加延迟（1-2秒）
- ❌ 摘要质量依赖LLM
- ❌ 可能丢失细节信息

**适用场景：**
- 长对话（>15轮）
- 需要保留上下文的场景
- 对成本不敏感的应用
- 复杂的多主题讨论

#### 成本优化

```python
# 优化1：批量压缩（减少API调用次数）
def compress_batch(self, sessions: Dict[str, List[Message]]):
    """一次API调用压缩多个会话"""
    # 将多个会话的历史合并到一个请求
    pass

# 优化2：使用更便宜的模型
self.client.chat.completions.create(
    model="gpt-3.5-turbo",  # 比GPT-4便宜10倍
    ...
)

# 优化3：缓存摘要（避免重复压缩）
def compress_with_cache(self, messages: List[Message]):
    cache_key = hash(tuple(m.content for m in messages))
    if cache_key in self.summary_cache:
        return self.summary_cache[cache_key]
    # 生成摘要并缓存
    ...
```

---

### 策略3：关键信息提取

#### 原理

**使用NLP技术提取对话中的关键实体、意图、事实，结构化存储。**

```
完整历史:
user: "我的订单12345什么时候发货？"
assistant: "订单12345预计明天发货"
user: "能加急吗？"
assistant: "可以加急，需要额外10元"

↓ 提取关键信息 ↓

结构化存储:
{
  "entities": {"order_id": "12345"},
  "facts": ["订单12345预计明天发货", "加急需要额外10元"],
  "intent": "查询订单并请求加急",
  "topic": "订单配送"
}
```

#### 实现代码

```python
from typing import Dict, List, Set
import json

class KeyInfoExtractor:
    def __init__(self):
        self.client = OpenAI()

    def compress(self, messages: List[Message]) -> List[Message]:
        """
        提取关键信息并结构化存储
        """
        # 提取关键信息
        key_info = self._extract_key_info(messages)

        # 构建结构化摘要
        summary_content = self._format_key_info(key_info)

        # 创建摘要消息
        summary_message = Message(
            role="system",
            content=f"[关键信息]\n{summary_content}",
            timestamp=datetime.now()
        )

        # 保留最近2轮原文
        recent = messages[-4:]

        compressed = [summary_message] + recent

        print(f"[压缩] {len(messages)}条 → {len(compressed)}条 (关键信息+2轮)")
        return compressed

    def _extract_key_info(self, messages: List[Message]) -> Dict:
        """
        使用LLM提取关键信息
        """
        history_text = "\n".join([
            f"{msg.role}: {msg.content}"
            for msg in messages
        ])

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{
                "role": "system",
                "content": """你是一个信息提取助手。
                从对话中提取以下信息，以JSON格式返回：
                {
                  "entities": {"实体类型": "实体值"},
                  "facts": ["关键事实1", "关键事实2"],
                  "intent": "用户意图",
                  "topic": "讨论主题"
                }

                示例：
                对话: "我的订单12345什么时候发货？"
                输出: {
                  "entities": {"order_id": "12345"},
                  "facts": [],
                  "intent": "查询订单状态",
                  "topic": "订单配送"
                }
                """
            }, {
                "role": "user",
                "content": f"对话:\n{history_text}\n\n请提取关键信息:"
            }],
            temperature=0.1
        )

        # 解析JSON
        try:
            key_info = json.loads(response.choices[0].message.content)
        except:
            key_info = {"entities": {}, "facts": [], "intent": "", "topic": ""}

        return key_info

    def _format_key_info(self, key_info: Dict) -> str:
        """
        格式化关键信息为文本
        """
        lines = []

        if key_info.get("topic"):
            lines.append(f"主题: {key_info['topic']}")

        if key_info.get("intent"):
            lines.append(f"意图: {key_info['intent']}")

        if key_info.get("entities"):
            entities_str = ", ".join([f"{k}={v}" for k, v in key_info['entities'].items()])
            lines.append(f"实体: {entities_str}")

        if key_info.get("facts"):
            lines.append("关键事实:")
            for fact in key_info['facts']:
                lines.append(f"  - {fact}")

        return "\n".join(lines)

# ===== 使用示例 =====
extractor = KeyInfoExtractor()

# 模拟客服对话
messages = [
    Message(role="user", content="我的订单12345什么时候发货？", timestamp=datetime.now()),
    Message(role="assistant", content="订单12345预计明天发货", timestamp=datetime.now()),
    Message(role="user", content="能加急吗？", timestamp=datetime.now()),
    Message(role="assistant", content="可以加急，需要额外10元", timestamp=datetime.now()),
    Message(role="user", content="好的，帮我加急", timestamp=datetime.now()),
    Message(role="assistant", content="已为您开启加急服务", timestamp=datetime.now()),
]

print(f"原始消息数: {len(messages)}")

# 压缩
compressed = extractor.compress(messages)
print(f"压缩后消息数: {len(compressed)}")

# 查看关键信息
if compressed[0].role == "system":
    print(f"\n关键信息:\n{compressed[0].content}")
```

**运行输出示例：**
```
原始消息数: 6
[压缩] 6条 → 5条 (关键信息+2轮)
压缩后消息数: 5

关键信息:
[关键信息]
主题: 订单配送
意图: 查询订单状态并请求加急
实体: order_id=12345
关键事实:
  - 订单12345预计明天发货
  - 加急需要额外10元
  - 用户已确认加急
```

#### 优缺点分析

**优点：**
- ✅ 精准保留重要信息
- ✅ 结构化存储，可查询
- ✅ 适合需要精确追踪信息的场景
- ✅ 可以与数据库结合

**缺点：**
- ❌ 实现复杂
- ❌ 需要额外的LLM调用或NLP模型
- ❌ 提取质量依赖模型能力
- ❌ 可能丢失上下文语义

**适用场景：**
- 客服系统（需要追踪订单号、用户信息）
- 任务型对话（需要记录任务状态）
- 需要精确信息的场景

---

## RAG应用

### 压缩时机的判断

**何时触发压缩？**

```python
class AdaptiveCompressor:
    def __init__(self):
        self.sliding_window = SlidingWindowCompressor(window_size=5)
        self.llm_summary = LLMSummaryCompressor(window_size=3)

    def should_compress(self, messages: List[Message]) -> bool:
        """
        判断是否需要压缩
        """
        # 策略1：基于消息数量
        if len(messages) > 20:  # 超过10轮
            return True

        # 策略2：基于Token数量
        total_tokens = sum(len(m.content.split()) * 1.3 for m in messages)
        if total_tokens > 3000:  # 超过3000 tokens
            return True

        # 策略3：基于时间跨度
        if messages:
            time_span = (messages[-1].timestamp - messages[0].timestamp).total_seconds()
            if time_span > 3600:  # 超过1小时
                return True

        return False

    def compress(self, messages: List[Message]) -> List[Message]:
        """
        自适应压缩
        """
        if not self.should_compress(messages):
            return messages

        # 根据消息数量选择策略
        if len(messages) < 30:
            # 短对话：使用滑动窗口
            return self.sliding_window.compress(messages)
        else:
            # 长对话：使用LLM摘要
            return self.llm_summary.compress(messages)
```

### 长对话场景的压缩策略选择

**场景对比：**

| 场景 | 对话轮次 | 推荐策略 | 原因 |
|------|---------|---------|------|
| 简单问答 | <10轮 | 滑动窗口 | 成本低，足够用 |
| 文档助手 | 10-30轮 | LLM摘要 | 需要保留讨论主题 |
| 智能客服 | 任意 | 关键信息提取 | 需要精确追踪订单等信息 |
| 代码助手 | 10-50轮 | LLM摘要 | 需要记住代码修改历史 |

**混合策略：**

```python
class HybridCompressor:
    def compress(self, messages: List[Message]) -> List[Message]:
        """
        混合压缩策略
        """
        # 第1-5轮：不压缩
        if len(messages) <= 10:
            return messages

        # 第6-15轮：滑动窗口
        if len(messages) <= 30:
            return self.sliding_window.compress(messages)

        # 第16轮以上：LLM摘要
        # 但每5轮才压缩一次（避免频繁调用LLM）
        if len(messages) % 10 == 0:
            return self.llm_summary.compress(messages)

        return messages
```

### 压缩质量评估方法

**评估指标：**

1. **信息保留率**
```python
def evaluate_information_retention(original: List[Message],
                                   compressed: List[Message]) -> float:
    """
    评估信息保留率
    """
    # 提取原始对话的关键实体
    original_entities = extract_entities(original)

    # 提取压缩后的关键实体
    compressed_entities = extract_entities(compressed)

    # 计算保留率
    retention = len(compressed_entities & original_entities) / len(original_entities)

    return retention
```

2. **压缩比**
```python
def compression_ratio(original: List[Message],
                     compressed: List[Message]) -> float:
    """
    计算压缩比
    """
    original_tokens = sum(len(m.content.split()) for m in original)
    compressed_tokens = sum(len(m.content.split()) for m in compressed)

    ratio = compressed_tokens / original_tokens
    return ratio
```

3. **下游任务效果**
```python
def evaluate_rag_quality(messages: List[Message],
                        query: str,
                        ground_truth: str) -> float:
    """
    评估压缩后的RAG效果
    """
    # 使用压缩后的历史进行RAG
    answer = rag_system.chat(messages, query)

    # 与标准答案对比
    similarity = compute_similarity(answer, ground_truth)

    return similarity
```

### 完整代码示例：带压缩的对话管理器

```python
class ConversationManagerWithCompression:
    def __init__(self):
        self.sessions: Dict[str, List[Message]] = {}
        self.compressor = AdaptiveCompressor()

    def add_message(self, session_id: str, role: str, content: str):
        """
        添加消息（自动压缩）
        """
        if session_id not in self.sessions:
            self.sessions[session_id] = []

        message = Message(role=role, content=content, timestamp=datetime.now())
        self.sessions[session_id].append(message)

        # 检查是否需要压缩
        if self.compressor.should_compress(self.sessions[session_id]):
            print(f"[触发压缩] Session {session_id}")
            self.sessions[session_id] = self.compressor.compress(self.sessions[session_id])

    def get_context(self, session_id: str) -> List[Message]:
        """
        获取上下文（已压缩）
        """
        return self.sessions.get(session_id, [])

# ===== 使用示例 =====
manager = ConversationManagerWithCompression()

# 模拟长对话
for i in range(25):
    manager.add_message("user_123", "user", f"问题{i+1}")
    manager.add_message("user_123", "assistant", f"回答{i+1}")

# 获取上下文
context = manager.get_context("user_123")
print(f"最终上下文长度: {len(context)}条消息")
```

---

## 总结

**历史压缩的三种策略：**

1. **滑动窗口截断**
   - 最简单，零成本
   - 适合短对话
   - 会丢失早期信息

2. **LLM摘要压缩**
   - 保留关键信息
   - 适合长对话
   - 有额外成本

3. **关键信息提取**
   - 精准保留重要信息
   - 适合需要精确追踪的场景
   - 实现复杂

**选择建议：**
- 原型开发：滑动窗口
- 生产环境（短对话）：滑动窗口
- 生产环境（长对话）：LLM摘要
- 特殊场景（客服、任务型）：关键信息提取

**下一步学习：**
- `05_核心概念_指代消解.md` - 如何理解"它"、"这个"等指代词
- `10_实战代码_场景2_历史压缩实现.md` - 完整的压缩代码实现
