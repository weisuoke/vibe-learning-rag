# 实战代码07：生产级日志系统

## 学习目标

构建完整的生产级日志系统，包括日志轮转、性能优化、安全性和监控集成。

---

## 第一步：日志轮转配置

### 示例1：基于大小的日志轮转

```python
# config/log_rotation.py
"""
日志轮转配置
"""

import logging
from logging.handlers import RotatingFileHandler, TimedRotatingFileHandler
from pathlib import Path

def setup_rotating_file_handler(
    filename: str,
    max_bytes: int = 10 * 1024 * 1024,  # 10MB
    backup_count: int = 5
) -> RotatingFileHandler:
    """
    创建基于大小的轮转handler

    Args:
        filename: 日志文件名
        max_bytes: 单个文件最大大小
        backup_count: 保留的备份文件数量
    """
    handler = RotatingFileHandler(
        filename,
        maxBytes=max_bytes,
        backupCount=backup_count,
        encoding='utf-8'
    )

    return handler

# 使用
handler = setup_rotating_file_handler(
    "logs/app.log",
    max_bytes=10 * 1024 * 1024,  # 10MB
    backup_count=5
)

# 日志文件结构：
# logs/app.log        (当前日志)
# logs/app.log.1      (最近的备份)
# logs/app.log.2
# logs/app.log.3
# logs/app.log.4
# logs/app.log.5      (最旧的备份)
```

### 示例2：基于时间的日志轮转

```python
# config/timed_rotation.py
"""
基于时间的日志轮转
"""

from logging.handlers import TimedRotatingFileHandler

def setup_timed_rotating_handler(
    filename: str,
    when: str = 'midnight',  # 每天午夜轮转
    interval: int = 1,
    backup_count: int = 30  # 保留30天
) -> TimedRotatingFileHandler:
    """
    创建基于时间的轮转handler

    Args:
        filename: 日志文件名
        when: 轮转时机 ('S', 'M', 'H', 'D', 'midnight', 'W0'-'W6')
        interval: 轮转间隔
        backup_count: 保留的备份数量
    """
    handler = TimedRotatingFileHandler(
        filename,
        when=when,
        interval=interval,
        backupCount=backup_count,
        encoding='utf-8'
    )

    return handler

# 使用
handler = setup_timed_rotating_handler(
    "logs/app.log",
    when='midnight',  # 每天午夜轮转
    backup_count=30   # 保留30天
)

# 日志文件结构：
# logs/app.log                (当前日志)
# logs/app.log.2024-01-15     (昨天的日志)
# logs/app.log.2024-01-14
# ...
```

---

## 第二步：性能优化

### 示例3：异步日志写入

```python
# config/async_logging.py
"""
异步日志写入
"""

import logging
from logging.handlers import QueueHandler, QueueListener
from queue import Queue
import structlog

def setup_async_logging(handlers: list):
    """
    配置异步日志写入

    Args:
        handlers: 实际的日志handlers列表
    """
    # 创建队列
    log_queue = Queue(-1)  # 无限大小

    # 创建队列handler（主线程使用）
    queue_handler = QueueHandler(log_queue)

    # 创建队列监听器（后台线程使用）
    queue_listener = QueueListener(
        log_queue,
        *handlers,
        respect_handler_level=True
    )

    # 启动监听器
    queue_listener.start()

    # 配置标准库logging使用队列handler
    logging.basicConfig(
        handlers=[queue_handler],
        level=logging.INFO
    )

    return queue_listener

# 使用
from logging.handlers import RotatingFileHandler

# 创建实际的handlers
file_handler = RotatingFileHandler("logs/app.log", maxBytes=10*1024*1024)
console_handler = logging.StreamHandler()

# 配置异步日志
listener = setup_async_logging([file_handler, console_handler])

# 配置structlog
structlog.configure(
    processors=[
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.stdlib.LoggerFactory()
)

# 使用
logger = structlog.get_logger()
logger.info("test_message")

# 应用退出时停止监听器
# listener.stop()
```

### 示例4：日志采样减少I/O

```python
# middleware/sampling_logger.py
"""
采样日志中间件
"""

import random
import structlog
from fastapi import FastAPI, Request

app = FastAPI()
logger = structlog.get_logger()

# 采样配置
SAMPLING_CONFIG = {
    "/health": 0.01,      # 1%
    "/metrics": 0.01,     # 1%
    "/api/search": 0.1,   # 10%
    # 默认：100%
}

@app.middleware("http")
async def sampling_middleware(request: Request, call_next):
    """采样日志中间件"""
    path = request.url.path
    sampling_rate = SAMPLING_CONFIG.get(path, 1.0)

    # 决定是否记录
    should_log = random.random() < sampling_rate

    if should_log:
        logger.info("request", path=path, sampled=True)

    response = await call_next(request)

    return response
```

---

## 第三步：安全性配置

### 示例5：敏感信息脱敏

```python
# processors/security.py
"""
安全相关的日志处理器
"""

import re
from typing import Dict, Any

def mask_sensitive_data(logger, method_name, event_dict: Dict[str, Any]) -> Dict[str, Any]:
    """
    脱敏敏感信息

    自动检测和脱敏：
    - API密钥
    - 密码
    - Token
    - 邮箱
    - 手机号
    - 身份证号
    """
    # 敏感字段名
    sensitive_keys = {
        "password", "passwd", "pwd",
        "api_key", "apikey", "api_secret",
        "token", "access_token", "refresh_token",
        "secret", "private_key",
        "credit_card", "card_number"
    }

    # 脱敏字段
    for key in list(event_dict.keys()):
        if key.lower() in sensitive_keys:
            value = event_dict[key]
            if isinstance(value, str):
                if len(value) > 8:
                    event_dict[key] = value[:4] + "***" + value[-4:]
                else:
                    event_dict[key] = "***REDACTED***"

    # 脱敏字符串值中的敏感模式
    for key, value in event_dict.items():
        if isinstance(value, str):
            # 邮箱脱敏
            value = re.sub(
                r'([a-zA-Z0-9._%+-]+)@([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'\1***@\2',
                value
            )

            # 手机号脱敏（中国）
            value = re.sub(
                r'1[3-9]\d{9}',
                lambda m: m.group()[:3] + "****" + m.group()[-4:],
                value
            )

            # 身份证号脱敏
            value = re.sub(
                r'\d{17}[\dXx]',
                lambda m: m.group()[:6] + "********" + m.group()[-4:],
                value
            )

            event_dict[key] = value

    return event_dict

def filter_large_payloads(logger, method_name, event_dict: Dict[str, Any]) -> Dict[str, Any]:
    """
    截断大payload
    """
    max_size = 1000

    for key in ["body", "response", "data", "payload"]:
        if key in event_dict:
            value = event_dict[key]
            if isinstance(value, str) and len(value) > max_size:
                event_dict[key] = value[:max_size] + "...[truncated]"
                event_dict[f"{key}_truncated"] = True
                event_dict[f"{key}_original_size"] = len(value)

    return event_dict

# 使用
import structlog

structlog.configure(
    processors=[
        mask_sensitive_data,      # 脱敏
        filter_large_payloads,    # 截断
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
```

---

## 第四步：监控集成

### 示例6：集成Prometheus指标

```python
# monitoring/metrics.py
"""
日志指标监控
"""

from prometheus_client import Counter, Histogram, Gauge
import structlog
import time

# 定义指标
log_messages_total = Counter(
    'log_messages_total',
    'Total number of log messages',
    ['level']
)

log_errors_total = Counter(
    'log_errors_total',
    'Total number of error logs',
    ['error_type']
)

request_duration_seconds = Histogram(
    'request_duration_seconds',
    'Request duration in seconds',
    ['method', 'path', 'status']
)

active_requests = Gauge(
    'active_requests',
    'Number of active requests'
)

def metrics_processor(logger, method_name, event_dict):
    """
    记录日志指标的处理器
    """
    # 记录日志级别
    level = event_dict.get('level', 'info')
    log_messages_total.labels(level=level).inc()

    # 记录错误
    if level == 'error':
        error_type = event_dict.get('error_type', 'unknown')
        log_errors_total.labels(error_type=error_type).inc()

    # 记录请求耗时
    if event_dict.get('event') == 'request_end':
        duration_ms = event_dict.get('duration_ms', 0)
        method = event_dict.get('method', 'unknown')
        path = event_dict.get('path', 'unknown')
        status = event_dict.get('status_code', 0)

        request_duration_seconds.labels(
            method=method,
            path=path,
            status=status
        ).observe(duration_ms / 1000)

    return event_dict

# 配置
structlog.configure(
    processors=[
        metrics_processor,  # 添加指标处理器
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)

# FastAPI集成
from fastapi import FastAPI
from prometheus_client import make_asgi_app

app = FastAPI()

# 添加Prometheus端点
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)
```

### 示例7：告警集成

```python
# monitoring/alerting.py
"""
日志告警
"""

import structlog
from typing import Dict, Any
import requests

class AlertManager:
    """告警管理器"""

    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url

    def send_alert(self, title: str, message: str, level: str = "warning"):
        """发送告警"""
        payload = {
            "title": title,
            "message": message,
            "level": level,
            "timestamp": structlog.processors.TimeStamper(fmt="iso")(None, None, {})["timestamp"]
        }

        try:
            requests.post(self.webhook_url, json=payload, timeout=5)
        except Exception as e:
            print(f"Failed to send alert: {e}")

# 全局告警管理器
alert_manager = AlertManager(webhook_url="https://your-webhook-url.com")

def alerting_processor(logger, method_name, event_dict: Dict[str, Any]) -> Dict[str, Any]:
    """
    告警处理器
    """
    level = event_dict.get('level', 'info')

    # 错误告警
    if level == 'error':
        error = event_dict.get('error', 'Unknown error')
        alert_manager.send_alert(
            title="Application Error",
            message=f"Error occurred: {error}",
            level="error"
        )

    # 慢请求告警
    if event_dict.get('event') == 'request_end':
        duration_ms = event_dict.get('duration_ms', 0)
        if duration_ms > 3000:  # 超过3秒
            path = event_dict.get('path', 'unknown')
            alert_manager.send_alert(
                title="Slow Request",
                message=f"Request to {path} took {duration_ms}ms",
                level="warning"
            )

    return event_dict

# 配置
structlog.configure(
    processors=[
        alerting_processor,  # 添加告警处理器
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
```

---

## 第五步：完整的生产级配置

### 示例8：生产级日志系统

```python
# config/production_logging.py
"""
生产级日志系统配置

特性：
- 日志轮转
- 异步写入
- 敏感信息脱敏
- 性能监控
- 告警集成
- 多目标输出
"""

import os
import sys
import structlog
import logging
from logging.handlers import RotatingFileHandler, QueueHandler, QueueListener
from queue import Queue
from pathlib import Path
from typing import List, Optional

# 导入自定义处理器
from processors.security import mask_sensitive_data, filter_large_payloads
from monitoring.metrics import metrics_processor
from monitoring.alerting import alerting_processor

class ProductionLoggingConfig:
    """生产日志配置"""

    def __init__(
        self,
        log_dir: str = "logs",
        log_level: str = "INFO",
        max_bytes: int = 10 * 1024 * 1024,  # 10MB
        backup_count: int = 5,
        enable_async: bool = True,
        enable_metrics: bool = True,
        enable_alerting: bool = True
    ):
        self.log_dir = Path(log_dir)
        self.log_level = getattr(logging, log_level.upper())
        self.max_bytes = max_bytes
        self.backup_count = backup_count
        self.enable_async = enable_async
        self.enable_metrics = enable_metrics
        self.enable_alerting = enable_alerting

        # 创建日志目录
        self.log_dir.mkdir(parents=True, exist_ok=True)

    def create_handlers(self) -> List[logging.Handler]:
        """创建日志handlers"""
        handlers = []

        # 控制台handler（只在开发环境）
        if os.getenv("ENV") == "development":
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setLevel(self.log_level)
            handlers.append(console_handler)

        # 所有日志文件
        all_handler = RotatingFileHandler(
            self.log_dir / "all.log",
            maxBytes=self.max_bytes,
            backupCount=self.backup_count,
            encoding='utf-8'
        )
        all_handler.setLevel(logging.INFO)
        handlers.append(all_handler)

        # 错误日志文件
        error_handler = RotatingFileHandler(
            self.log_dir / "error.log",
            maxBytes=self.max_bytes,
            backupCount=self.backup_count,
            encoding='utf-8'
        )
        error_handler.setLevel(logging.ERROR)
        handlers.append(error_handler)

        return handlers

    def setup(self) -> Optional[QueueListener]:
        """配置日志系统"""

        # 创建handlers
        handlers = self.create_handlers()

        # 异步日志
        queue_listener = None
        if self.enable_async:
            log_queue = Queue(-1)
            queue_handler = QueueHandler(log_queue)

            queue_listener = QueueListener(
                log_queue,
                *handlers,
                respect_handler_level=True
            )
            queue_listener.start()

            # 使用队列handler
            logging.basicConfig(
                handlers=[queue_handler],
                level=self.log_level,
                format="%(message)s"
            )
        else:
            # 直接使用handlers
            logging.basicConfig(
                handlers=handlers,
                level=self.log_level,
                format="%(message)s"
            )

        # 构建处理器链
        processors = [
            # 上下文管理
            structlog.contextvars.merge_contextvars,

            # 安全处理
            mask_sensitive_data,
            filter_large_payloads,

            # 添加日志级别
            structlog.processors.add_log_level,

            # 添加时间戳
            structlog.processors.TimeStamper(fmt="iso"),
        ]

        # 可选：指标监控
        if self.enable_metrics:
            processors.insert(0, metrics_processor)

        # 可选：告警
        if self.enable_alerting:
            processors.insert(0, alerting_processor)

        # JSON渲染器
        processors.append(structlog.processors.JSONRenderer())

        # 配置structlog
        structlog.configure(
            processors=processors,
            wrapper_class=structlog.make_filtering_bound_logger(self.log_level),
            logger_factory=structlog.stdlib.LoggerFactory(),
            cache_logger_on_first_use=True,
        )

        # 记录配置信息
        logger = structlog.get_logger()
        logger.info("logging_configured",
            log_dir=str(self.log_dir),
            log_level=logging.getLevelName(self.log_level),
            async_enabled=self.enable_async,
            metrics_enabled=self.enable_metrics,
            alerting_enabled=self.enable_alerting
        )

        return queue_listener


def setup_production_logging(
    log_dir: str = "logs",
    log_level: str = None,
    **kwargs
) -> Optional[QueueListener]:
    """
    快速配置生产日志

    Args:
        log_dir: 日志目录
        log_level: 日志级别，默认根据环境自动选择
        **kwargs: 其他配置参数
    """
    # 根据环境自动选择日志级别
    if log_level is None:
        env = os.getenv("ENV", "development")
        log_level = "DEBUG" if env == "development" else "INFO"

    config = ProductionLoggingConfig(
        log_dir=log_dir,
        log_level=log_level,
        **kwargs
    )

    return config.setup()


# 使用示例
if __name__ == "__main__":
    # 配置生产日志
    listener = setup_production_logging(
        log_dir="logs",
        enable_async=True,
        enable_metrics=True,
        enable_alerting=True
    )

    # 获取logger
    logger = structlog.get_logger()

    # 使用
    logger.info("app_started", version="1.0.0")
    logger.info("user_login", user_id="user_123", ip="192.168.1.1")
    logger.error("api_failed", error="timeout", retry_count=3)

    # 应用退出时停止监听器
    if listener:
        listener.stop()
```

---

## 第六步：FastAPI完整集成

### 示例9：生产级FastAPI应用

```python
# main.py
"""
生产级FastAPI应用

特性：
- 完整的日志系统
- 请求追踪
- 性能监控
- 错误处理
- 健康检查
"""

import os
import uuid
import time
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
import structlog
from contextlib import asynccontextmanager

# 导入日志配置
from config.production_logging import setup_production_logging

# 全局变量
queue_listener = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """应用生命周期管理"""
    global queue_listener

    # 启动：配置日志
    queue_listener = setup_production_logging(
        log_dir="logs",
        enable_async=True,
        enable_metrics=True,
        enable_alerting=True
    )

    logger = structlog.get_logger()
    logger.info("app_startup", version="1.0.0")

    yield

    # 关闭：停止日志监听器
    if queue_listener:
        queue_listener.stop()

    logger.info("app_shutdown")

# 创建应用
app = FastAPI(lifespan=lifespan)

logger = structlog.get_logger()

# 跳过日志的路径
SKIP_PATHS = {"/health", "/metrics"}

@app.middleware("http")
async def logging_middleware(request: Request, call_next):
    """日志中间件"""

    # 跳过特定路径
    if request.url.path in SKIP_PATHS:
        return await call_next(request)

    # 生成请求ID
    request_id = request.headers.get("X-Request-ID") or str(uuid.uuid4())

    # 绑定上下文
    structlog.contextvars.bind_contextvars(
        request_id=request_id,
        method=request.method,
        path=request.url.path,
        client_ip=request.client.host
    )

    # 记录请求开始
    start_time = time.time()
    logger.info("request_start")

    try:
        # 处理请求
        response = await call_next(request)

        # 计算耗时
        duration_ms = (time.time() - start_time) * 1000

        # 记录请求结束
        logger.info("request_end",
            status_code=response.status_code,
            duration_ms=duration_ms
        )

        # 添加响应头
        response.headers["X-Request-ID"] = request_id
        response.headers["X-Response-Time"] = f"{duration_ms:.2f}ms"

        return response

    except Exception as e:
        # 计算耗时
        duration_ms = (time.time() - start_time) * 1000

        # 记录异常
        logger.error("request_failed",
            error=str(e),
            error_type=type(e).__name__,
            duration_ms=duration_ms
        )

        # 返回500错误
        return JSONResponse(
            status_code=500,
            content={"error": "Internal server error"},
            headers={
                "X-Request-ID": request_id,
                "X-Response-Time": f"{duration_ms:.2f}ms"
            }
        )

    finally:
        # 清理上下文
        structlog.contextvars.clear_contextvars()

@app.get("/health")
async def health():
    """健康检查"""
    return {"status": "ok"}

@app.get("/")
async def root():
    """根路径"""
    logger.info("root_endpoint")
    return {"message": "Hello World"}

@app.get("/users/{user_id}")
async def get_user(user_id: str):
    """获取用户"""
    logger.info("get_user", user_id=user_id)
    return {"user_id": user_id, "name": "John"}

@app.post("/users")
async def create_user(user: dict):
    """创建用户"""
    logger.info("create_user", user=user)
    return {"id": "user_123", **user}

@app.get("/error")
async def error():
    """测试错误"""
    logger.warning("about_to_raise_error")
    raise ValueError("Test error")

# 运行
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_config=None  # 使用我们自己的日志配置
    )
```

---

## 第七步：Docker部署配置

### 示例10：Docker环境日志配置

```dockerfile
# Dockerfile
FROM python:3.13-slim

WORKDIR /app

# 安装依赖
COPY pyproject.toml .
RUN pip install uv && uv sync

# 复制代码
COPY . .

# 创建日志目录
RUN mkdir -p /app/logs

# 环境变量
ENV ENV=production
ENV LOG_LEVEL=INFO

# 运行
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENV=production
      - LOG_LEVEL=INFO
    volumes:
      # 挂载日志目录到宿主机
      - ./logs:/app/logs
    restart: unless-stopped

  # 日志收集（可选）
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    volumes:
      - ./logs:/app/logs:ro
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
    depends_on:
      - app
```

---

## 总结

### 核心要点

1. **日志轮转**
   - 基于大小轮转（RotatingFileHandler）
   - 基于时间轮转（TimedRotatingFileHandler）
   - 自动清理旧日志

2. **性能优化**
   - 异步日志写入（QueueHandler）
   - 日志采样减少I/O
   - 避免记录大对象

3. **安全性**
   - 敏感信息脱敏
   - 大payload截断
   - 访问控制

4. **监控集成**
   - Prometheus指标
   - 告警系统
   - 健康检查

5. **生产部署**
   - Docker容器化
   - 日志持久化
   - 环境配置

### 最佳实践

1. 使用异步日志写入提升性能
2. 配置日志轮转避免磁盘满
3. 脱敏敏感信息保护隐私
4. 集成监控系统实时告警
5. 日志目录挂载到宿主机

### 下一步

- 【实战代码08】：AI Agent日志监控
- 集成ELK/Datadog等日志平台
- 实现分布式追踪
