# 化骨绵掌

## 卡片1：BFS核心直觉

**一句话：** BFS像水波纹扩散，一层层向外传播

**举例：**
```python
# 水波纹扩散
起点 → 第1层（1跳邻居） → 第2层（2跳邻居） → ...

# 代码体现
queue = deque([start])  # 起点
while queue:
    node = queue.popleft()  # 当前层
    queue.extend(neighbors)  # 下一层
```

**应用：** 知识图谱广度检索，先找近的实体再找远的

---

## 卡片2：DFS核心直觉

**一句话：** DFS像走迷宫，一条路走到底再回头

**举例：**
```python
# 迷宫探索
起点 → 路径1尽头 → 回溯 → 路径2尽头 → ...

# 代码体现
def dfs(node):
    visit(node)
    for neighbor in neighbors:
        dfs(neighbor)  # 深入
    # 自动回溯
```

**应用：** 推理链构建，沿着因果关系深入探索

---

## 卡片3：队列vs栈

**一句话：** 队列先进先出（FIFO），栈后进先出（LIFO）

**举例：**
```python
# 队列（BFS）
queue = deque([1, 2, 3])
queue.popleft()  # 1（先进先出）

# 栈（DFS）
stack = [1, 2, 3]
stack.pop()  # 3（后进先出）
```

**应用：** BFS用队列保证层序，DFS用栈实现回溯

---

## 卡片4：visited集合

**一句话：** visited防止重复访问，避免无限循环

**举例：**
```python
# 图有环：A → B → A
visited = set()

if node not in visited:
    visited.add(node)  # 标记
    process(node)
```

**应用：** 知识图谱中实体间的双向关系会形成环，必须用visited

---

## 卡片5：时间复杂度O(V+E)

**一句话：** 每个节点访问一次O(V)，每条边检查一次O(E)

**举例：**
```python
# V=5个节点，E=6条边
graph = {
    'A': ['B', 'C'],  # 2条边
    'B': ['D'],       # 1条边
    'C': ['D', 'E'],  # 2条边
    'D': ['E'],       # 1条边
    'E': []
}

# 时间：O(5 + 6) = O(11)
```

**应用：** BFS和DFS时间复杂度相同，选择取决于空间和任务

---

## 卡片6：路径重建

**一句话：** 记录父节点，反向追溯，然后反转

**举例：**
```python
parent = {'C': 'B', 'B': 'A', 'A': None}

# 从C追溯到A
path = []
node = 'C'
while node:
    path.append(node)
    node = parent[node]

path.reverse()  # ['A', 'B', 'C']
```

**应用：** 多跳推理中追溯推理链

---

## 卡片7：KG2RAG混合策略

**一句话：** BFS扩展邻域（广度），DFS组织段落（深度）

**举例：**
```python
# 步骤1：BFS扩展2-hop
expanded = bfs_expand(kg, seeds, m_hops=2)
# 结果：{A, B, C, D, E, F, ...}（无序）

# 步骤2：DFS组织连贯段落
organized = dfs_organize(kg, expanded, max_depth=3)
# 结果：[[A, B, D], [A, C, E]]（连贯）
```

**应用：** 避免上下文碎片化，提高LLM理解准确率

---

## 卡片8：ARK自适应选择

**一句话：** LLM控制的广度-深度权衡，按需扩展

**举例：**
```python
# 浅层查询
query = "Python的创始人？"
→ BFS深度0找到答案 → 停止

# 深层查询
query = "为什么Python成为AI首选？"
→ BFS深度0不足 → LLM决策expand
→ DFS深度2找到推理链 → 停止
```

**应用：** 避免过度检索，节省资源

---

## 卡片9：Agentic查询分类

**一句话：** 根据查询类型（事实/因果/探索）选择策略

**举例：**
```python
# 事实查询 → BFS浅层
"Python的创始人？" → BFS(max_depth=2)

# 因果查询 → DFS深层
"为什么Python成为AI首选？" → DFS(max_depth=5)

# 探索查询 → 混合策略
"Python在AI的应用？" → BFS+DFS
```

**应用：** 精准匹配策略，提高准确率

---

## 卡片10：性能优化要点

**一句话：** 深度限制、双向搜索、缓存、早停

**举例：**
```python
# 优化前：访问10000个节点，523ms
# 优化后：
- 深度限制(max_depth=2)：3125个节点，164ms
- 双向搜索：625个节点，33ms
- A*启发式：312个节点，16ms
- 早停：156个节点，8ms
```

**应用：** 组合使用多种优化，平衡准确率和性能

---

## 学习路径

**10个卡片的递进关系：**
```
卡片1-2：基础直觉（BFS/DFS是什么）
  ↓
卡片3-4：核心机制（队列/栈、visited）
  ↓
卡片5-6：算法分析（复杂度、路径重建）
  ↓
卡片7-9：AI应用（KG2RAG、ARK、Agentic）
  ↓
卡片10：性能优化（实战技巧）
```

---

## 快速复习

**2分钟速记：**
1. BFS=队列+层序，DFS=栈+深度
2. visited必需（避免环）
3. O(V+E)时间，空间不同
4. 路径重建=父节点追溯
5. KG2RAG=BFS扩展+DFS组织
6. ARK=LLM控制的自适应
7. Agentic=查询分类驱动
8. 优化=剪枝+双向+缓存

---

**版本：** v1.0
**最后更新：** 2026-02-14
