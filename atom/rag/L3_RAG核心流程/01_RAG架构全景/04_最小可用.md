# 最小可用知识

掌握以下内容，就能开始构建RAG系统。

---

## 4.1 核心架构理解

**RAG = 检索（Retrieval）+ 增强（Augmented）+ 生成（Generation）**

最小架构包含5个组件：
1. 文档加载
2. 文本分块
3. 向量嵌入
4. 向量存储
5. LLM生成

```python
# 最小RAG流程
docs = load_documents()           # 1. 加载
chunks = split_text(docs)         # 2. 分块
embeddings = embed(chunks)        # 3. 嵌入
vectorstore.add(embeddings)       # 4. 存储
results = vectorstore.search(query)  # 5. 检索
answer = llm.generate(results, query)  # 6. 生成
```

---

## 4.2 关键技术选型

**必需的库：**
```bash
pip install openai chromadb langchain pypdf
```

**推荐配置：**
- 嵌入模型：OpenAI text-embedding-3-small
- 向量数据库：ChromaDB（开发）/ Milvus（生产）
- LLM：GPT-4 / Claude
- 分块大小：500字符，重叠50字符

---

## 4.3 基础实现模板

```python
from openai import OpenAI
import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 初始化
client = OpenAI()
chroma_client = chromadb.Client()
collection = chroma_client.create_collection("docs")
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

# 1. 处理文档
chunks = splitter.split_text(document_text)

# 2. 嵌入并存储
embeddings = client.embeddings.create(
    model="text-embedding-3-small",
    input=chunks
).data

collection.add(
    embeddings=[e.embedding for e in embeddings],
    documents=chunks,
    ids=[f"doc_{i}" for i in range(len(chunks))]
)

# 3. 查询
query_embedding = client.embeddings.create(
    model="text-embedding-3-small",
    input=query
).data[0].embedding

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=3
)

# 4. 生成答案
context = "\n".join(results["documents"][0])
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{
        "role": "user",
        "content": f"Context: {context}\n\nQuestion: {query}"
    }]
)
answer = response.choices[0].message.content
```

---

## 4.4 优化检索质量

**三个关键优化：**

1. **混合检索**（召回率+20%）
```python
# Dense + Sparse
dense_results = vectorstore.search(query)
sparse_results = bm25.search(query)
final = reciprocal_rank_fusion([dense_results, sparse_results])
```

2. **重排序**（精确度+15%）
```python
from sentence_transformers import CrossEncoder
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
scores = reranker.predict([(query, doc) for doc in results])
reranked = sorted(zip(results, scores), key=lambda x: x[1], reverse=True)
```

3. **上下文管理**（信息利用率+10%）
```python
# 控制token数量
def manage_context(docs, max_tokens=4000):
    selected = []
    total = 0
    for doc in docs:
        tokens = count_tokens(doc)
        if total + tokens <= max_tokens:
            selected.append(doc)
            total += tokens
    return selected
```

---

## 4.5 常见问题解决

**问题1：检索结果不相关**
- 解决：优化分块大小（试试300-800字符）
- 解决：使用混合检索
- 解决：添加重排序

**问题2：答案不准确**
- 解决：优化Prompt（明确要求基于上下文）
- 解决：增加检索数量（k=5-10）
- 解决：检查上下文质量

**问题3：响应太慢**
- 解决：减少检索数量
- 解决：使用更小的嵌入模型
- 解决：添加缓存

---

## 这些知识足以

- ✅ 构建基础RAG系统
- ✅ 处理简单问答场景
- ✅ 理解核心工作原理
- ✅ 为进阶学习打基础

**下一步：** 学习实战代码，动手实现完整系统
