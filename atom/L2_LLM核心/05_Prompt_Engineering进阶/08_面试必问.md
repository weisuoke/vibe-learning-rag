# 面试必问

进阶 Prompt 技巧相关的高频面试问题及出彩回答。

---

## 问题1："什么是 Few-shot Learning？在 RAG 中怎么用？"

### 普通回答（❌ 不出彩）

"Few-shot 就是给 LLM 几个例子，让它学会怎么回答。在 RAG 里可以用来统一回答格式。"

### 出彩回答（✅ 推荐）

> **Few-shot Learning 有三层含义：**
>
> 1. **原理层面**：利用 LLM 的 In-context Learning 能力，通过在 Prompt 中提供少量示例，让模型学会任务模式，无需微调
>
> 2. **实现层面**：在 Prompt 中添加 2-5 个高质量的"输入-输出"示例，LLM 会识别模式并应用到新问题
>
> 3. **在 RAG 中的应用**：
>    - **统一回答风格**：示例展示期望的语气、详细程度
>    - **规范输出格式**：示例展示 Markdown 格式、列表结构
>    - **处理特定领域**：示例展示专业术语的使用方式
>
> **实际经验**：示例数量不是越多越好，2-3 个高质量示例通常最优，因为要平衡效果和 Token 消耗。示例的多样性比数量更重要。

### 为什么这个回答出彩？

1. ✅ 分层次解释（原理/实现/应用）
2. ✅ 结合 RAG 实际场景
3. ✅ 提到了实践经验（示例数量、多样性）
4. ✅ 展示了对 Token 成本的意识

---

## 问题2："Chain-of-Thought 的原理是什么？什么时候该用？"

### 普通回答（❌ 不出彩）

"CoT 就是让 LLM 一步步思考，可以提高准确率。复杂问题的时候用。"

### 出彩回答（✅ 推荐）

> **Chain-of-Thought 的原理：**
>
> LLM 是自回归模型，每个 token 的生成依赖于前面的 token。当我们让 LLM 先输出推理步骤，这些步骤会成为生成最终答案的"上下文"，相当于给模型提供了"草稿纸"。
>
> **两种实现方式：**
> - **Zero-shot CoT**：只需加一句"让我们一步步思考"，简单有效
> - **Manual CoT**：在示例中展示完整推理过程，效果更好但成本更高
>
> **什么时候该用：**
> - ✅ 多步数学计算
> - ✅ 需要综合多个信息源的问题
> - ✅ 逻辑推理、因果分析
> - ❌ 简单事实查询（浪费 Token）
> - ❌ 直接从文档提取信息
>
> **在 RAG 中的典型应用**：当用户问题需要综合多个检索文档时，CoT 可以引导 LLM 先分析每个文档的相关信息，再综合得出答案，显著提高准确率。

### 为什么这个回答出彩？

1. ✅ 解释了底层原理（自回归模型）
2. ✅ 区分了两种实现方式
3. ✅ 明确了适用和不适用场景
4. ✅ 结合 RAG 给出具体应用

---

## 问题3："如何让 LLM 输出稳定的 JSON 格式？"

### 普通回答（❌ 不出彩）

"在 Prompt 里要求输出 JSON，或者用 OpenAI 的 JSON Mode。"

### 出彩回答（✅ 推荐）

> **实现 JSON 输出有三个层次：**
>
> **1. Prompt 层面约束**
> - 在 Prompt 中明确 JSON Schema
> - 提供示例展示期望格式
> - 强调"只输出 JSON，不要其他内容"
>
> **2. API 层面约束**
> - OpenAI：`response_format={"type": "json_object"}`
> - Anthropic：使用 Tool Use 定义输出结构
> - 这比 Prompt 约束更可靠
>
> **3. 后处理层面容错**
> - 即使用了 JSON Mode，也要添加解析容错
> - 验证必需字段是否存在
> - 验证字段类型是否正确
> - 提供默认值和降级策略
>
> **实际经验**：JSON Mode 保证格式正确，但不保证内容正确。比如要求 `confidence` 字段是 `high/medium/low`，LLM 可能输出 `very high`。所以后处理验证是必须的。
>
> ```python
> # 防御性代码示例
> def safe_parse(response):
>     result = json.loads(response)
>     if result.get("confidence") not in ["high", "medium", "low"]:
>         result["confidence"] = "medium"  # 默认值
>     return result
> ```

### 为什么这个回答出彩？

1. ✅ 分层次解决方案（Prompt/API/后处理）
2. ✅ 提到了不同厂商的实现方式
3. ✅ 强调了"格式正确≠内容正确"的关键点
4. ✅ 给出了实际代码示例

---

## 问题4："RAG 系统中如何处理复杂的多跳问题？"

### 普通回答（❌ 不出彩）

"可以用 CoT 让 LLM 分步思考，或者把问题拆分成多个子问题。"

### 出彩回答（✅ 推荐）

> **多跳问题的挑战**：用户问题需要综合多个文档的信息，或者需要先回答子问题才能回答主问题。
>
> **解决方案有三个层次：**
>
> **1. Prompt 层面：CoT 引导**
> ```
> 步骤1：分析问题需要哪些信息
> 步骤2：在各文档中定位相关内容
> 步骤3：综合分析得出答案
> ```
>
> **2. 检索层面：Query Decomposition**
> - 先用 LLM 把复杂问题拆成多个子问题
> - 分别检索每个子问题
> - 合并检索结果后再生成答案
>
> **3. 架构层面：Multi-step RAG**
> - 第一轮：检索 + 生成中间答案
> - 第二轮：基于中间答案继续检索
> - 最终：综合所有信息生成最终答案
>
> **实际选择**：
> - 简单多跳：CoT 就够了
> - 中等复杂：Query Decomposition
> - 高度复杂：Multi-step RAG（但延迟和成本都会增加）
>
> 在实际项目中，我会先尝试 CoT，如果效果不好再升级到 Query Decomposition，避免过度工程。

### 为什么这个回答出彩？

1. ✅ 明确了问题的本质（多跳问题的挑战）
2. ✅ 给出了多层次解决方案
3. ✅ 提供了实际选择建议
4. ✅ 展示了工程思维（避免过度工程）

---

## 面试技巧总结

| 问题类型 | 回答策略 |
|---------|---------|
| 概念解释 | 分层次（原理/实现/应用） |
| 技术选型 | 给出多个方案 + 选择建议 |
| 实践经验 | 提到具体数字、踩过的坑 |
| RAG 相关 | 结合检索、生成、评估全流程 |

---

**下一步：** [09_化骨绵掌](./09_化骨绵掌.md) - 10个知识卡片深入学习
