# 实战代码2: RAG检索场景

> **完整可运行的 RAG 并行检索示例**

---

## 环境准备

```bash
# 安装依赖
uv add langchain langchain-openai langchain-community chromadb python-dotenv

# 配置 API Key
echo "OPENAI_API_KEY=your_key_here" > .env
```

---

## 示例1: 基础 RAG 链

```python
"""
最基础的 RAG 检索链
演示并行检索和保持问题
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()

# 准备知识库
docs = [
    Document(page_content="LangChain 是一个 AI 应用开发框架"),
    Document(page_content="LCEL 是 LangChain 的表达式语言"),
    Document(page_content="RunnableParallel 用于并行执行多个任务"),
]

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

model = ChatOpenAI(model="gpt-4")

# RAG 链：并行检索 + 保持问题
rag_chain = (
    {
        "context": retriever,  # 检索相关文档
        "question": RunnablePassthrough(),  # 保持原始问题
    }
    | ChatPromptTemplate.from_template(
        "根据上下文回答问题。如果上下文中没有相关信息，请说不知道。\n\n"
        "上下文:\n{context}\n\n"
        "问题: {question}\n\n"
        "答案:"
    )
    | model
)

# 测试
questions = [
    "LangChain 是什么？",
    "什么是 LCEL？",
    "RunnableParallel 的作用是什么？",
]

for q in questions:
    answer = rag_chain.invoke(q)
    print(f"Q: {q}")
    print(f"A: {answer.content}\n")
```

---

## 示例2: 多源并行检索

```python
"""
从多个数据源并行检索
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()

# 准备多个知识库
docs_tech = [
    Document(page_content="LangChain 使用 Python 开发"),
    Document(page_content="LCEL 提供声明式语法"),
]

docs_business = [
    Document(page_content="LangChain 提高开发效率"),
    Document(page_content="LangChain 降低开发成本"),
]

# 创建多个检索器
vector_retriever = Chroma.from_documents(
    documents=docs_tech,
    embedding=OpenAIEmbeddings(),
    collection_name="tech"
).as_retriever(search_kwargs={"k": 2})

business_retriever = Chroma.from_documents(
    documents=docs_business,
    embedding=OpenAIEmbeddings(),
    collection_name="business"
).as_retriever(search_kwargs={"k": 2})

model = ChatOpenAI(model="gpt-4")

# 多源并行检索
rag_chain = (
    {
        "tech_docs": vector_retriever,  # 技术文档
        "business_docs": business_retriever,  # 商业文档
        "question": RunnablePassthrough(),
    }
    # 合并检索结果
    | RunnableLambda(lambda x: {
        "context": x["tech_docs"] + x["business_docs"],
        "question": x["question"],
    })
    | ChatPromptTemplate.from_template(
        "根据技术和商业文档回答问题\n\n"
        "文档:\n{context}\n\n"
        "问题: {question}\n\n"
        "答案:"
    )
    | model
)

answer = rag_chain.invoke("LangChain 的优势是什么？")
print(answer.content)
```

---

## 示例3: 保留中间结果的 RAG

```python
"""
保留所有中间结果，便于调试和分析
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()

# 准备知识库
docs = [
    Document(page_content="LangChain 是一个 AI 框架"),
    Document(page_content="LCEL 是表达式语言"),
]

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

model = ChatOpenAI(model="gpt-4")

# 保留所有中间结果
rag_chain = (
    # 步骤1: 检索
    RunnablePassthrough.assign(
        context=retriever
    )
    # 步骤2: 生成答案
    | RunnablePassthrough.assign(
        answer=ChatPromptTemplate.from_template(
            "根据上下文回答问题\n\n上下文: {context}\n\n问题: {question}"
        ) | model
    )
    # 步骤3: 添加元数据
    | RunnablePassthrough.assign(
        metadata=lambda x: {
            "context_count": len(x["context"]),
            "answer_length": len(x["answer"].content),
        }
    )
)

result = rag_chain.invoke({"question": "LangChain 是什么？"})

print("问题:", result["question"])
print("检索到的文档数:", result["metadata"]["context_count"])
print("答案:", result["answer"].content)
print("答案长度:", result["metadata"]["answer_length"])
```

---

## 示例4: 带重排序的 RAG

```python
"""
检索后重排序，提高相关性
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()

# 准备知识库
docs = [
    Document(page_content="LangChain 是一个 AI 框架", metadata={"score": 0.9}),
    Document(page_content="Python 是一种编程语言", metadata={"score": 0.3}),
    Document(page_content="LCEL 是 LangChain 的核心", metadata={"score": 0.8}),
]

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

model = ChatOpenAI(model="gpt-4")

def rerank_documents(docs, top_k=2):
    """简单的重排序：按 metadata score 排序"""
    sorted_docs = sorted(docs, key=lambda d: d.metadata.get("score", 0), reverse=True)
    return sorted_docs[:top_k]

# 带重排序的 RAG
rag_chain = (
    {
        "raw_docs": retriever,
        "question": RunnablePassthrough(),
    }
    | RunnableLambda(lambda x: {
        "context": rerank_documents(x["raw_docs"], top_k=2),
        "question": x["question"],
    })
    | ChatPromptTemplate.from_template(
        "根据上下文回答问题\n\n上下文: {context}\n\n问题: {question}"
    )
    | model
)

answer = rag_chain.invoke("LangChain 是什么？")
print(answer.content)
```

---

## 示例5: 混合检索（向量 + 关键词）

```python
"""
结合向量检索和关键词检索
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()

# 准备知识库
docs = [
    Document(page_content="LangChain 是一个 AI 应用开发框架"),
    Document(page_content="LCEL 是 LangChain 的表达式语言"),
    Document(page_content="Python 是 LangChain 的主要开发语言"),
]

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings()
)
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 模拟关键词检索
def keyword_search(query: str):
    """简单的关键词匹配"""
    keywords = query.lower().split()
    results = []
    for doc in docs:
        if any(kw in doc.page_content.lower() for kw in keywords):
            results.append(doc)
    return results[:2]

model = ChatOpenAI(model="gpt-4")

# 混合检索
rag_chain = (
    {
        "vector_docs": vector_retriever,
        "keyword_docs": RunnableLambda(keyword_search),
        "question": RunnablePassthrough(),
    }
    # 合并去重
    | RunnableLambda(lambda x: {
        "context": list({doc.page_content: doc for doc in
                        x["vector_docs"] + x["keyword_docs"]}.values()),
        "question": x["question"],
    })
    | ChatPromptTemplate.from_template(
        "根据上下文回答问题\n\n上下文: {context}\n\n问题: {question}"
    )
    | model
)

answer = rag_chain.invoke("LangChain 使用什么语言开发？")
print(answer.content)
```

---

## 示例6: 对话式 RAG

```python
"""
带历史记录的对话式 RAG
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from dotenv import load_dotenv

load_dotenv()

# 准备知识库
docs = [
    Document(page_content="LangChain 是一个 AI 框架"),
    Document(page_content="LCEL 是表达式语言"),
]

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

model = ChatOpenAI(model="gpt-4")

# 对话式 RAG
conversational_rag = (
    {
        "context": lambda x: retriever.invoke(x["question"]),
        "question": lambda x: x["question"],
        "history": lambda x: x.get("history", []),
    }
    | ChatPromptTemplate.from_template(
        "历史对话:\n{history}\n\n"
        "上下文:\n{context}\n\n"
        "问题: {question}\n\n"
        "答案:"
    )
    | model
)

# 模拟对话
history = []
questions = [
    "LangChain 是什么？",
    "它有什么特点？",  # 需要理解"它"指的是 LangChain
]

for q in questions:
    answer = conversational_rag.invoke({
        "question": q,
        "history": "\n".join(history)
    })
    print(f"Q: {q}")
    print(f"A: {answer.content}\n")

    # 更新历史
    history.append(f"Q: {q}")
    history.append(f"A: {answer.content}")
```

---

## 示例7: 生产级 RAG（带错误处理）

```python
"""
生产环境的 RAG 链
包含错误处理、日志、性能监控
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from dotenv import load_dotenv
import logging
import time

load_dotenv()

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 准备知识库
docs = [
    Document(page_content="LangChain 是一个 AI 框架"),
    Document(page_content="LCEL 是表达式语言"),
]

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

model = ChatOpenAI(model="gpt-4")

def safe_retrieve(query: str):
    """安全的检索，带错误处理"""
    try:
        logger.info(f"检索: {query}")
        start = time.time()
        docs = retriever.invoke(query)
        elapsed = time.time() - start
        logger.info(f"检索成功，耗时: {elapsed:.2f}s，文档数: {len(docs)}")
        return docs
    except Exception as e:
        logger.error(f"检索失败: {e}")
        return []

def safe_generate(context, question):
    """安全的生成，带错误处理"""
    try:
        logger.info(f"生成答案: {question}")
        start = time.time()
        prompt = ChatPromptTemplate.from_template(
            "根据上下文回答问题\n\n上下文: {context}\n\n问题: {question}"
        )
        chain = prompt | model
        answer = chain.invoke({"context": context, "question": question})
        elapsed = time.time() - start
        logger.info(f"生成成功，耗时: {elapsed:.2f}s")
        return answer
    except Exception as e:
        logger.error(f"生成失败: {e}")
        return None

# 生产级 RAG
production_rag = (
    {
        "context": RunnableLambda(safe_retrieve),
        "question": RunnablePassthrough(),
    }
    | RunnableLambda(lambda x: safe_generate(x["context"], x["question"]))
)

# 测试
answer = production_rag.invoke("LangChain 是什么？")
if answer:
    print(f"答案: {answer.content}")
else:
    print("生成失败")
```

---

## 示例8: 完整的企业级 RAG 系统

```python
"""
企业级 RAG 系统
包含：多源检索、重排序、缓存、监控
"""
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from dotenv import load_dotenv
from functools import lru_cache
import logging
import time

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 准备多个知识库
docs_general = [
    Document(page_content="LangChain 是一个 AI 框架"),
]

docs_technical = [
    Document(page_content="LCEL 是表达式语言"),
]

# 创建检索器
general_retriever = Chroma.from_documents(
    documents=docs_general,
    embedding=OpenAIEmbeddings(),
    collection_name="general"
).as_retriever()

technical_retriever = Chroma.from_documents(
    documents=docs_technical,
    embedding=OpenAIEmbeddings(),
    collection_name="technical"
).as_retriever()

model = ChatOpenAI(model="gpt-4")

# 带缓存的检索
@lru_cache(maxsize=100)
def cached_retrieve(query: str, source: str):
    """带缓存的检索"""
    logger.info(f"[{source}] 检索: {query}")
    if source == "general":
        return tuple(general_retriever.invoke(query))
    else:
        return tuple(technical_retriever.invoke(query))

def rerank_docs(docs, top_k=3):
    """重排序文档"""
    # 简单示例：按长度排序
    sorted_docs = sorted(docs, key=lambda d: len(d.page_content), reverse=True)
    return sorted_docs[:top_k]

# 企业级 RAG 链
enterprise_rag = (
    # 步骤1: 并行检索多个源
    {
        "general_docs": RunnableLambda(lambda x: list(cached_retrieve(x, "general"))),
        "technical_docs": RunnableLambda(lambda x: list(cached_retrieve(x, "technical"))),
        "question": RunnablePassthrough(),
    }
    # 步骤2: 合并和重排序
    | RunnableLambda(lambda x: {
        "context": rerank_docs(x["general_docs"] + x["technical_docs"]),
        "question": x["question"],
        "source_count": {
            "general": len(x["general_docs"]),
            "technical": len(x["technical_docs"]),
        }
    })
    # 步骤3: 生成答案
    | RunnablePassthrough.assign(
        answer=ChatPromptTemplate.from_template(
            "根据上下文回答问题\n\n上下文: {context}\n\n问题: {question}"
        ) | model
    )
    # 步骤4: 添加元数据
    | RunnableLambda(lambda x: {
        "question": x["question"],
        "answer": x["answer"].content,
        "context_count": len(x["context"]),
        "source_count": x["source_count"],
    })
)

# 测试
result = enterprise_rag.invoke("LangChain 是什么？")
print("问题:", result["question"])
print("答案:", result["answer"])
print("使用文档数:", result["context_count"])
print("数据源:", result["source_count"])
```

---

## 常见模式总结

### 模式1: 基础 RAG
```python
{
    "context": retriever,
    "question": RunnablePassthrough(),
} | prompt | model
```

### 模式2: 多源检索
```python
{
    "source1": retriever1,
    "source2": retriever2,
    "question": RunnablePassthrough(),
} | merge | prompt | model
```

### 模式3: 保留中间结果
```python
RunnablePassthrough.assign(context=retriever)
| RunnablePassthrough.assign(answer=prompt | model)
```

### 模式4: 带重排序
```python
{
    "raw_docs": retriever,
    "question": RunnablePassthrough(),
} | rerank | prompt | model
```

---

## 性能优化建议

1. **使用缓存**
   ```python
   @lru_cache(maxsize=100)
   def cached_retrieve(query):
       return retriever.invoke(query)
   ```

2. **控制检索数量**
   ```python
   retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
   ```

3. **并行检索多个源**
   ```python
   {
       "source1": retriever1,
       "source2": retriever2,
   }
   ```

4. **重排序提高相关性**
   ```python
   docs = rerank_documents(raw_docs, top_k=3)
   ```

---

## 下一步学习

- [实战代码3: 多任务处理](./07_实战代码_03_多任务处理.md) - 多模型并行调用
- [实战代码4: 上下文保持](./07_实战代码_04_上下文保持.md) - RunnablePassthrough.assign()
- [核心概念1: 字典式并行执行](./03_核心概念_01_字典式并行执行.md) - 深入理解原理

---

**版本**: v1.0
**最后更新**: 2026-02-19
**适用**: LangChain 0.3+, Python 3.13+
