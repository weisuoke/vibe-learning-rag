# 实战代码1：多副本集群部署配置

本示例演示如何在 Kubernetes 上部署一个生产级的 Milvus 高可用集群，包括多副本配置、资源限制、健康检查等。

---

## 场景说明

**目标：** 在 Kubernetes 上部署一个 3 副本的 Milvus 集群，支持：
- Coordinator 组件：3 个副本（Raft 选主）
- Worker 组件：2-3 个副本（负载均衡）
- 外部存储：MinIO（对象存储）+ etcd（元数据）+ Pulsar（消息队列）
- 健康检查：livenessProbe + readinessProbe
- 资源限制：CPU + Memory

**适用场景：**
- 生产环境部署
- 需要 99.9% 可用性
- 每天处理 10 万+ 次查询

---

## 完整部署代码

### 1. 命名空间和配置

```yaml
# namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: milvus-ha
  labels:
    name: milvus-ha
---
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: milvus-config
  namespace: milvus-ha
data:
  milvus.yaml: |
    # Milvus 配置文件
    etcd:
      endpoints:
        - etcd-0.etcd.milvus-ha.svc.cluster.local:2379
        - etcd-1.etcd.milvus-ha.svc.cluster.local:2379
        - etcd-2.etcd.milvus-ha.svc.cluster.local:2379
      rootPath: by-dev
      metaSubPath: meta
      kvSubPath: kv

    minio:
      address: minio.milvus-ha.svc.cluster.local
      port: 9000
      accessKeyID: minioadmin
      secretAccessKey: minioadmin
      useSSL: false
      bucketName: milvus-bucket
      rootPath: file

    pulsar:
      address: pulsar-proxy.milvus-ha.svc.cluster.local
      port: 6650
      maxMessageSize: 5242880

    common:
      chanNamePrefix:
        cluster: by-dev
        rootCoordTimeTick: rootcoord-timetick
        rootCoordStatistics: rootcoord-statistics
        rootCoordDml: rootcoord-dml
        search: search
        searchResult: searchResult
        queryTimeTick: queryTimeTick

    rootCoord:
      dmlChannelNum: 16
      maxPartitionNum: 4096
      minSegmentSizeToEnableIndex: 1024

    queryCoord:
      autoHandoff: true
      autoBalance: true
      balanceIntervalSeconds: 60
      memoryUsageMaxDifferencePercentage: 30

    dataCoord:
      enableCompaction: true
      enableGarbageCollection: true

    indexCoord:
      bindIndexNodeMode:
        enable: false
```

---

### 2. etcd 集群部署（3 副本）

```yaml
# etcd-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: etcd
  namespace: milvus-ha
spec:
  clusterIP: None
  ports:
  - port: 2379
    name: client
  - port: 2380
    name: peer
  selector:
    app: etcd
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: etcd
  namespace: milvus-ha
spec:
  serviceName: etcd
  replicas: 3
  selector:
    matchLabels:
      app: etcd
  template:
    metadata:
      labels:
        app: etcd
    spec:
      containers:
      - name: etcd
        image: quay.io/coreos/etcd:v3.5.5
        ports:
        - containerPort: 2379
          name: client
        - containerPort: 2380
          name: peer
        env:
        - name: ETCD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: ETCD_INITIAL_CLUSTER
          value: "etcd-0=http://etcd-0.etcd.milvus-ha.svc.cluster.local:2380,etcd-1=http://etcd-1.etcd.milvus-ha.svc.cluster.local:2380,etcd-2=http://etcd-2.etcd.milvus-ha.svc.cluster.local:2380"
        - name: ETCD_INITIAL_CLUSTER_STATE
          value: "new"
        - name: ETCD_INITIAL_CLUSTER_TOKEN
          value: "milvus-etcd-cluster"
        - name: ETCD_LISTEN_CLIENT_URLS
          value: "http://0.0.0.0:2379"
        - name: ETCD_ADVERTISE_CLIENT_URLS
          value: "http://$(ETCD_NAME).etcd.milvus-ha.svc.cluster.local:2379"
        - name: ETCD_LISTEN_PEER_URLS
          value: "http://0.0.0.0:2380"
        - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
          value: "http://$(ETCD_NAME).etcd.milvus-ha.svc.cluster.local:2380"
        volumeMounts:
        - name: etcd-data
          mountPath: /etcd-data
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1"
  volumeClaimTemplates:
  - metadata:
      name: etcd-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```

---

### 3. MinIO 部署（对象存储）

```yaml
# minio-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: milvus-ha
spec:
  ports:
  - port: 9000
    name: api
  - port: 9001
    name: console
  selector:
    app: minio
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
  namespace: milvus-ha
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
      - name: minio
        image: minio/minio:RELEASE.2023-03-20T20-16-18Z
        args:
        - server
        - /data
        - --console-address
        - ":9001"
        env:
        - name: MINIO_ROOT_USER
          value: "minioadmin"
        - name: MINIO_ROOT_PASSWORD
          value: "minioadmin"
        ports:
        - containerPort: 9000
        - containerPort: 9001
        volumeMounts:
        - name: minio-data
          mountPath: /data
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
      volumes:
      - name: minio-data
        persistentVolumeClaim:
          claimName: minio-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-pvc
  namespace: milvus-ha
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
```

---

### 4. Root Coordinator 部署（3 副本）

```yaml
# rootcoord-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: milvus-rootcoord
  namespace: milvus-ha
spec:
  ports:
  - port: 53100
    name: grpc
  - port: 9091
    name: metrics
  selector:
    app: milvus-rootcoord
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-rootcoord
  namespace: milvus-ha
spec:
  replicas: 3  # 3 个副本
  selector:
    matchLabels:
      app: milvus-rootcoord
  template:
    metadata:
      labels:
        app: milvus-rootcoord
    spec:
      containers:
      - name: rootcoord
        image: milvusdb/milvus:v2.4.0
        command: ["milvus", "run", "rootcoord"]
        ports:
        - containerPort: 53100
          name: grpc
        - containerPort: 9091
          name: metrics
        env:
        - name: ETCD_ENDPOINTS
          value: "etcd-0.etcd.milvus-ha.svc.cluster.local:2379,etcd-1.etcd.milvus-ha.svc.cluster.local:2379,etcd-2.etcd.milvus-ha.svc.cluster.local:2379"
        - name: MINIO_ADDRESS
          value: "minio.milvus-ha.svc.cluster.local:9000"
        - name: PULSAR_ADDRESS
          value: "pulsar://pulsar-proxy.milvus-ha.svc.cluster.local:6650"
        volumeMounts:
        - name: milvus-config
          mountPath: /milvus/configs/milvus.yaml
          subPath: milvus.yaml
        # 存活探针：检测容器是否存活
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9091
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        # 就绪探针：检测容器是否准备好接收流量
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9091
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
      volumes:
      - name: milvus-config
        configMap:
          name: milvus-config
```

---

### 5. Proxy 部署（2 副本 + 负载均衡）

```yaml
# proxy-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: milvus-proxy
  namespace: milvus-ha
spec:
  type: LoadBalancer  # 或 NodePort/ClusterIP
  ports:
  - port: 19530
    name: grpc
    targetPort: 19530
  - port: 9091
    name: metrics
    targetPort: 9091
  selector:
    app: milvus-proxy
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-proxy
  namespace: milvus-ha
spec:
  replicas: 2  # 2 个副本
  selector:
    matchLabels:
      app: milvus-proxy
  template:
    metadata:
      labels:
        app: milvus-proxy
    spec:
      containers:
      - name: proxy
        image: milvusdb/milvus:v2.4.0
        command: ["milvus", "run", "proxy"]
        ports:
        - containerPort: 19530
          name: grpc
        - containerPort: 9091
          name: metrics
        env:
        - name: ETCD_ENDPOINTS
          value: "etcd-0.etcd.milvus-ha.svc.cluster.local:2379,etcd-1.etcd.milvus-ha.svc.cluster.local:2379,etcd-2.etcd.milvus-ha.svc.cluster.local:2379"
        - name: MINIO_ADDRESS
          value: "minio.milvus-ha.svc.cluster.local:9000"
        - name: PULSAR_ADDRESS
          value: "pulsar://pulsar-proxy.milvus-ha.svc.cluster.local:6650"
        volumeMounts:
        - name: milvus-config
          mountPath: /milvus/configs/milvus.yaml
          subPath: milvus.yaml
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9091
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9091
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
      volumes:
      - name: milvus-config
        configMap:
          name: milvus-config
```

---

### 6. Query Node 部署（3 副本）

```yaml
# querynode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-querynode
  namespace: milvus-ha
spec:
  replicas: 3  # 3 个副本
  selector:
    matchLabels:
      app: milvus-querynode
  template:
    metadata:
      labels:
        app: milvus-querynode
    spec:
      containers:
      - name: querynode
        image: milvusdb/milvus:v2.4.0
        command: ["milvus", "run", "querynode"]
        ports:
        - containerPort: 21123
          name: grpc
        - containerPort: 9091
          name: metrics
        env:
        - name: ETCD_ENDPOINTS
          value: "etcd-0.etcd.milvus-ha.svc.cluster.local:2379,etcd-1.etcd.milvus-ha.svc.cluster.local:2379,etcd-2.etcd.milvus-ha.svc.cluster.local:2379"
        - name: MINIO_ADDRESS
          value: "minio.milvus-ha.svc.cluster.local:9000"
        - name: PULSAR_ADDRESS
          value: "pulsar://pulsar-proxy.milvus-ha.svc.cluster.local:6650"
        volumeMounts:
        - name: milvus-config
          mountPath: /milvus/configs/milvus.yaml
          subPath: milvus.yaml
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9091
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9091
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
      volumes:
      - name: milvus-config
        configMap:
          name: milvus-config
```

---

## 部署脚本

```bash
#!/bin/bash
# deploy-milvus-ha.sh

set -e

echo "=== 部署 Milvus 高可用集群 ==="

# 1. 创建命名空间
echo "1. 创建命名空间..."
kubectl apply -f namespace.yaml

# 2. 创建配置
echo "2. 创建配置..."
kubectl apply -f configmap.yaml

# 3. 部署 etcd 集群
echo "3. 部署 etcd 集群（3 副本）..."
kubectl apply -f etcd-statefulset.yaml
kubectl wait --for=condition=ready pod -l app=etcd -n milvus-ha --timeout=300s

# 4. 部署 MinIO
echo "4. 部署 MinIO..."
kubectl apply -f minio-deployment.yaml
kubectl wait --for=condition=ready pod -l app=minio -n milvus-ha --timeout=300s

# 5. 创建 MinIO bucket
echo "5. 创建 MinIO bucket..."
kubectl run -it --rm minio-client --image=minio/mc --restart=Never -n milvus-ha -- \
  sh -c "mc alias set myminio http://minio.milvus-ha.svc.cluster.local:9000 minioadmin minioadmin && mc mb myminio/milvus-bucket"

# 6. 部署 Pulsar（简化版，生产环境建议使用 Pulsar Operator）
echo "6. 部署 Pulsar..."
kubectl apply -f pulsar-deployment.yaml
kubectl wait --for=condition=ready pod -l app=pulsar -n milvus-ha --timeout=300s

# 7. 部署 Milvus Coordinator 组件
echo "7. 部署 Milvus Coordinator 组件（3 副本）..."
kubectl apply -f rootcoord-deployment.yaml
kubectl apply -f querycoord-deployment.yaml
kubectl apply -f datacoord-deployment.yaml
kubectl apply -f indexcoord-deployment.yaml
kubectl wait --for=condition=ready pod -l app=milvus-rootcoord -n milvus-ha --timeout=300s

# 8. 部署 Milvus Worker 组件
echo "8. 部署 Milvus Worker 组件（2-3 副本）..."
kubectl apply -f proxy-deployment.yaml
kubectl apply -f querynode-deployment.yaml
kubectl apply -f datanode-deployment.yaml
kubectl apply -f indexnode-deployment.yaml
kubectl wait --for=condition=ready pod -l app=milvus-proxy -n milvus-ha --timeout=300s

echo ""
echo "=== 部署完成 ==="
echo ""
echo "查看 Pod 状态："
kubectl get pods -n milvus-ha
echo ""
echo "查看 Service："
kubectl get svc -n milvus-ha
echo ""
echo "获取 Milvus 访问地址："
kubectl get svc milvus-proxy -n milvus-ha
```

---

## 验证部署

```python
"""
验证 Milvus 高可用集群部署
"""
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility
import numpy as np

def verify_milvus_ha_cluster():
    """验证 Milvus 高可用集群"""
    print("=== 验证 Milvus 高可用集群 ===\n")

    # 1. 连接到 Milvus 集群
    print("1. 连接到 Milvus 集群...")
    connections.connect(
        alias="default",
        host="milvus-proxy.milvus-ha.svc.cluster.local",
        port="19530"
    )
    print("✅ 连接成功\n")

    # 2. 创建测试 Collection
    print("2. 创建测试 Collection...")
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)
    ]
    schema = CollectionSchema(fields=fields, description="HA test collection")
    collection = Collection(name="ha_test", schema=schema)
    print("✅ Collection 创建成功\n")

    # 3. 插入测试数据
    print("3. 插入测试数据...")
    vectors = np.random.rand(1000, 128).tolist()
    collection.insert([vectors])
    print("✅ 插入 1000 条数据\n")

    # 4. 创建索引
    print("4. 创建索引...")
    index_params = {
        "index_type": "IVF_FLAT",
        "metric_type": "L2",
        "params": {"nlist": 128}
    }
    collection.create_index(field_name="embedding", index_params=index_params)
    print("✅ 索引创建成功\n")

    # 5. 加载 Collection（3 副本）
    print("5. 加载 Collection（3 副本）...")
    collection.load(replica_number=3)
    print("✅ Collection 加载成功\n")

    # 6. 执行查询
    print("6. 执行查询...")
    query_vector = np.random.rand(1, 128).tolist()
    results = collection.search(
        data=query_vector,
        anns_field="embedding",
        param={"metric_type": "L2", "params": {"nprobe": 10}},
        limit=10
    )
    print(f"✅ 查询成功，返回 {len(results[0])} 条结果\n")

    # 7. 检查集群状态
    print("7. 检查集群状态...")
    collections = utility.list_collections()
    print(f"   当前有 {len(collections)} 个 Collection")
    print(f"   Collection 数据量: {collection.num_entities}")
    print("✅ 集群状态正常\n")

    print("=== 验证完成 ===")

if __name__ == "__main__":
    verify_milvus_ha_cluster()
```

---

## 关键配置说明

### 1. 副本数量配置

```yaml
# Coordinator: 3 个副本（奇数，用于 Raft 选主）
replicas: 3

# Worker: 2-3 个副本（根据负载调整）
replicas: 2
```

### 2. 健康检查配置

```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 9091
  initialDelaySeconds: 30  # 启动后 30 秒开始检查
  periodSeconds: 10        # 每 10 秒检查一次
  timeoutSeconds: 5        # 超时时间 5 秒
  failureThreshold: 3      # 连续失败 3 次则重启

readinessProbe:
  httpGet:
    path: /healthz
    port: 9091
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3
```

### 3. 资源限制配置

```yaml
resources:
  requests:
    memory: "2Gi"  # 最小资源
    cpu: "1"
  limits:
    memory: "4Gi"  # 最大资源
    cpu: "2"
```

---

## 运行输出示例

```bash
$ ./deploy-milvus-ha.sh

=== 部署 Milvus 高可用集群 ===
1. 创建命名空间...
namespace/milvus-ha created
2. 创建配置...
configmap/milvus-config created
3. 部署 etcd 集群（3 副本）...
service/etcd created
statefulset.apps/etcd created
pod/etcd-0 condition met
pod/etcd-1 condition met
pod/etcd-2 condition met
4. 部署 MinIO...
service/minio created
deployment.apps/minio created
persistentvolumeclaim/minio-pvc created
pod/minio-xxx condition met
5. 创建 MinIO bucket...
Bucket created successfully `myminio/milvus-bucket`.
6. 部署 Pulsar...
...
7. 部署 Milvus Coordinator 组件（3 副本）...
deployment.apps/milvus-rootcoord created
...
8. 部署 Milvus Worker 组件（2-3 副本）...
deployment.apps/milvus-proxy created
...

=== 部署完成 ===

查看 Pod 状态：
NAME                                READY   STATUS    RESTARTS   AGE
etcd-0                              1/1     Running   0          5m
etcd-1                              1/1     Running   0          5m
etcd-2                              1/1     Running   0          5m
minio-xxx                           1/1     Running   0          4m
milvus-rootcoord-xxx                1/1     Running   0          3m
milvus-rootcoord-yyy                1/1     Running   0          3m
milvus-rootcoord-zzz                1/1     Running   0          3m
milvus-proxy-xxx                    1/1     Running   0          2m
milvus-proxy-yyy                    1/1     Running   0          2m
milvus-querynode-xxx                1/1     Running   0          2m
milvus-querynode-yyy                1/1     Running   0          2m
milvus-querynode-zzz                1/1     Running   0          2m

查看 Service：
NAME               TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)
etcd               ClusterIP      None            <none>        2379/TCP,2380/TCP
minio              ClusterIP      10.96.1.1       <none>        9000/TCP,9001/TCP
milvus-proxy       LoadBalancer   10.96.1.2       <pending>     19530:30000/TCP
milvus-rootcoord   ClusterIP      10.96.1.3       <none>        53100/TCP,9091/TCP
```
