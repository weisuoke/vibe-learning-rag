# 实战代码5：多格式统一管道

> 构建统一处理所有文档格式的完整管道

---

## 场景描述

演示如何构建一个统一的文档处理管道，自动识别格式、选择解析器、提取元数据，并构建知识库。

**适用场景：**
- 企业文档管理系统
- 多格式知识库构建
- 自动化文档处理

---

## 依赖库

```bash
# 安装所有依赖
uv add pypdf pdfplumber python-docx openpyxl python-pptx beautifulsoup4 requests gitpython langchain langchain-openai chromadb python-dotenv chardet
```

---

## 完整代码

```python
"""
多格式统一管道示例
演示：构建统一处理所有文档格式的完整管道

依赖库：
- pypdf, pdfplumber: PDF解析
- python-docx, openpyxl, python-pptx: Office文档
- beautifulsoup4: HTML解析
- gitpython: Git仓库
- langchain: 文档处理框架
- chardet: 编码检测

参考来源：
- LangChain Document Loaders (2025): https://python.langchain.com/docs/modules/data_connection/document_loaders/
- Unstructured.io (2025): https://docs.unstructured.io/
"""

import os
from typing import List, Dict, Callable
from datetime import datetime
from dotenv import load_dotenv
from langchain.schema import Document
import chardet

load_dotenv()

# ===== 1. 格式检测器 =====
print("=== 1. 格式检测器 ===")

class FormatDetector:
    """文档格式检测器"""

    def detect(self, file_path: str) -> str:
        """
        检测文件格式
        返回: 格式名称 (pdf, docx, xlsx, pptx, html, txt, md, unknown)
        """
        # 1. 基于扩展名
        ext = os.path.splitext(file_path)[1].lower()

        format_map = {
            '.pdf': 'pdf',
            '.docx': 'docx',
            '.xlsx': 'xlsx',
            '.pptx': 'pptx',
            '.html': 'html',
            '.htm': 'html',
            '.md': 'markdown',
            '.txt': 'text',
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript'
        }

        detected_format = format_map.get(ext, 'unknown')
        print(f"检测到格式: {detected_format} ({file_path})")

        return detected_format

# 测试格式检测
detector = FormatDetector()
# test_files = ["doc.pdf", "report.docx", "data.xlsx", "slides.pptx", "page.html"]
# for file in test_files:
#     detector.detect(file)

# ===== 2. 解析器注册表 =====
print("\n=== 2. 解析器注册表 ===")

class ParserRegistry:
    """解析器注册表"""

    def __init__(self):
        self._parsers: Dict[str, Callable] = {}

    def register(self, format: str, parser_func: Callable):
        """注册解析器"""
        self._parsers[format] = parser_func
        print(f"注册解析器: {format}")

    def get_parser(self, format: str) -> Callable:
        """获取解析器"""
        return self._parsers.get(format)

    def list_supported_formats(self) -> List[str]:
        """列出支持的格式"""
        return list(self._parsers.keys())

# 创建全局注册表
parser_registry = ParserRegistry()

# ===== 3. 各格式解析器实现 =====
print("\n=== 3. 各格式解析器实现 ===")

# PDF解析器
def parse_pdf(file_path: str) -> List[Document]:
    """解析PDF文档"""
    from pypdf import PdfReader

    reader = PdfReader(file_path)
    documents = []

    for page_num, page in enumerate(reader.pages):
        text = page.extract_text()
        doc = Document(
            page_content=text,
            metadata={
                "source": file_path,
                "format": "pdf",
                "page": page_num,
                "total_pages": len(reader.pages)
            }
        )
        documents.append(doc)

    return documents

# Word解析器
def parse_docx(file_path: str) -> List[Document]:
    """解析Word文档"""
    from docx import Document as DocxDocument

    doc = DocxDocument(file_path)
    paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]
    full_text = "\n".join(paragraphs)

    return [Document(
        page_content=full_text,
        metadata={
            "source": file_path,
            "format": "docx",
            "paragraph_count": len(doc.paragraphs),
            "table_count": len(doc.tables)
        }
    )]

# Excel解析器
def parse_xlsx(file_path: str) -> List[Document]:
    """解析Excel文档"""
    import pandas as pd

    excel_file = pd.ExcelFile(file_path)
    documents = []

    for sheet_name in excel_file.sheet_names:
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        df = df.dropna(how='all').dropna(axis=1, how='all')

        if len(df) == 0:
            continue

        text = f"工作表: {sheet_name}\n"
        text += f"列名: {', '.join(df.columns)}\n\n"
        text += df.head(10).to_string(index=False)

        documents.append(Document(
            page_content=text,
            metadata={
                "source": file_path,
                "format": "xlsx",
                "sheet_name": sheet_name,
                "row_count": len(df),
                "column_count": len(df.columns)
            }
        ))

    return documents

# PowerPoint解析器
def parse_pptx(file_path: str) -> List[Document]:
    """解析PowerPoint文档"""
    from pptx import Presentation

    prs = Presentation(file_path)
    documents = []

    for slide_idx, slide in enumerate(prs.slides):
        text_content = []
        for shape in slide.shapes:
            if hasattr(shape, "text") and shape.text.strip():
                text_content.append(shape.text)

        documents.append(Document(
            page_content="\n".join(text_content),
            metadata={
                "source": file_path,
                "format": "pptx",
                "slide_number": slide_idx + 1,
                "total_slides": len(prs.slides)
            }
        ))

    return documents

# HTML解析器
def parse_html(file_path: str) -> List[Document]:
    """解析HTML文档"""
    from bs4 import BeautifulSoup

    with open(file_path, 'r', encoding='utf-8') as f:
        html_content = f.read()

    soup = BeautifulSoup(html_content, 'html.parser')

    # 移除噪声
    for tag in soup(['script', 'style', 'nav', 'footer', 'aside']):
        tag.decompose()

    # 提取主要内容
    main_content = soup.find('article') or soup.find('main') or soup.body
    if not main_content:
        return []

    text = main_content.get_text(separator='\n', strip=True)
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    clean_text = '\n'.join(lines)

    title = soup.find('title')

    return [Document(
        page_content=clean_text,
        metadata={
            "source": file_path,
            "format": "html",
            "title": title.text if title else ""
        }
    )]

# 文本解析器
def parse_text(file_path: str) -> List[Document]:
    """解析文本文档（带编码检测）"""
    # 检测编码
    with open(file_path, 'rb') as f:
        raw_data = f.read()
    result = chardet.detect(raw_data)
    encoding = result['encoding'] or 'utf-8'

    # 读取文件
    try:
        with open(file_path, 'r', encoding=encoding) as f:
            content = f.read()
    except:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

    return [Document(
        page_content=content,
        metadata={
            "source": file_path,
            "format": "text",
            "encoding": encoding
        }
    )]

# Markdown解析器
def parse_markdown(file_path: str) -> List[Document]:
    """解析Markdown文档"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    return [Document(
        page_content=content,
        metadata={
            "source": file_path,
            "format": "markdown"
        }
    )]

# 注册所有解析器
parser_registry.register('pdf', parse_pdf)
parser_registry.register('docx', parse_docx)
parser_registry.register('xlsx', parse_xlsx)
parser_registry.register('pptx', parse_pptx)
parser_registry.register('html', parse_html)
parser_registry.register('text', parse_text)
parser_registry.register('markdown', parse_markdown)

print(f"支持的格式: {', '.join(parser_registry.list_supported_formats())}")

# ===== 4. 统一文档加载器 =====
print("\n=== 4. 统一文档加载器 ===")

class UniversalDocumentLoader:
    """统一文档加载器"""

    def __init__(self):
        self.detector = FormatDetector()
        self.registry = parser_registry
        self.failed_files = []

    def load_file(self, file_path: str) -> List[Document]:
        """加载单个文件"""
        # 1. 检测格式
        file_format = self.detector.detect(file_path)

        # 2. 获取解析器
        parser = self.registry.get_parser(file_format)
        if not parser:
            raise ValueError(f"不支持的格式: {file_format}")

        # 3. 解析文档
        documents = parser(file_path)

        # 4. 添加加载时间
        for doc in documents:
            doc.metadata["loaded_at"] = datetime.now().isoformat()

        return documents

    def load_directory(self, directory: str, recursive: bool = True) -> List[Document]:
        """加载目录下所有文档"""
        all_documents = []
        stats = {"total": 0, "success": 0, "failed": 0}

        print(f"\n扫描目录: {directory}")

        # 遍历目录
        if recursive:
            file_iterator = os.walk(directory)
        else:
            file_iterator = [(directory, [], os.listdir(directory))]

        for root, dirs, files in file_iterator:
            # 排除特定目录
            dirs[:] = [d for d in dirs if d not in ['.git', 'node_modules', '__pycache__', '.venv']]

            for file in files:
                file_path = os.path.join(root, file)
                stats["total"] += 1

                try:
                    docs = self.load_file(file_path)
                    all_documents.extend(docs)
                    stats["success"] += 1
                    print(f"  ✅ {file}: {len(docs)} 个文档")
                except Exception as e:
                    stats["failed"] += 1
                    self.failed_files.append({"file": file_path, "error": str(e)})
                    print(f"  ❌ {file}: {e}")

        print(f"\n加载统计:")
        print(f"  总文件数: {stats['total']}")
        print(f"  成功: {stats['success']}")
        print(f"  失败: {stats['failed']}")
        print(f"  文档数: {len(all_documents)}")

        return all_documents

# 测试统一加载器
# loader = UniversalDocumentLoader()
# docs = loader.load_directory("./documents")

# ===== 5. 元数据增强管道 =====
print("\n=== 5. 元数据增强管道 ===")

class MetadataEnhancer:
    """元数据增强器"""

    def enhance(self, documents: List[Document]) -> List[Document]:
        """增强文档元数据"""
        for doc in documents:
            # 内容统计
            doc.metadata["content_length"] = len(doc.page_content)
            doc.metadata["word_count"] = len(doc.page_content.split())

            # 语言检测
            chinese_chars = sum(1 for c in doc.page_content if '\u4e00' <= c <= '\u9fff')
            total_chars = len(doc.page_content)
            doc.metadata["language"] = "zh" if chinese_chars / total_chars > 0.1 else "en"

            # 文档类型
            format_to_type = {
                "pdf": "文档",
                "docx": "文档",
                "xlsx": "表格",
                "pptx": "演示",
                "html": "网页",
                "text": "文本",
                "markdown": "文档"
            }
            doc.metadata["doc_type"] = format_to_type.get(doc.metadata.get("format"), "未知")

        return documents

# ===== 6. 完整处理管道 =====
print("\n=== 6. 完整处理管道 ===")

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

class DocumentProcessingPipeline:
    """文档处理管道"""

    def __init__(self, output_dir: str = "./universal_kb"):
        self.loader = UniversalDocumentLoader()
        self.enhancer = MetadataEnhancer()
        self.output_dir = output_dir

    def process(self, source: str, is_directory: bool = True):
        """
        处理文档并构建知识库

        Args:
            source: 文件路径或目录路径
            is_directory: 是否为目录
        """
        print("=== 文档处理管道 ===\n")

        # 1. 加载文档
        print("步骤1: 加载文档")
        if is_directory:
            documents = self.loader.load_directory(source)
        else:
            documents = self.loader.load_file(source)

        if not documents:
            print("❌ 没有加载到任何文档")
            return None

        # 2. 增强元数据
        print("\n步骤2: 增强元数据")
        documents = self.enhancer.enhance(documents)
        print(f"✅ 元数据增强完成")

        # 3. 文本分块
        print("\n步骤3: 文本分块")
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = splitter.split_documents(documents)
        print(f"✅ 分块完成: {len(chunks)} 个文本块")

        # 4. 向量化并存储
        print("\n步骤4: 向量化并存储")
        try:
            embeddings = OpenAIEmbeddings()
            vectorstore = Chroma.from_documents(
                chunks,
                embeddings,
                persist_directory=self.output_dir
            )
            print(f"✅ 知识库构建成功")

            # 5. 生成报告
            self._generate_report(documents, chunks)

            # 6. 测试检索
            self._test_retrieval(vectorstore)

            return vectorstore

        except Exception as e:
            print(f"❌ 向量化失败: {e}")
            return None

    def _generate_report(self, documents: List[Document], chunks: List[Document]):
        """生成处理报告"""
        print("\n步骤5: 生成报告")

        # 统计信息
        format_stats = {}
        language_stats = {}

        for doc in documents:
            fmt = doc.metadata.get("format", "unknown")
            format_stats[fmt] = format_stats.get(fmt, 0) + 1

            lang = doc.metadata.get("language", "unknown")
            language_stats[lang] = language_stats.get(lang, 0) + 1

        print(f"\n文档统计:")
        print(f"  总文档数: {len(documents)}")
        print(f"  总文本块: {len(chunks)}")

        print(f"\n按格式分布:")
        for fmt, count in format_stats.items():
            print(f"  {fmt}: {count}")

        print(f"\n按语言分布:")
        for lang, count in language_stats.items():
            print(f"  {lang}: {count}")

        # 保存报告
        import json
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_documents": len(documents),
            "total_chunks": len(chunks),
            "format_distribution": format_stats,
            "language_distribution": language_stats,
            "failed_files": self.loader.failed_files
        }

        report_path = os.path.join(self.output_dir, "processing_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\n报告已保存: {report_path}")

    def _test_retrieval(self, vectorstore):
        """测试检索"""
        print("\n步骤6: 测试检索")

        test_queries = [
            "文档的主要内容是什么？",
            "有哪些技术文档？",
            "数据统计信息"
        ]

        for query in test_queries:
            results = vectorstore.similarity_search(query, k=2)
            print(f"\n查询: {query}")
            print(f"找到 {len(results)} 个相关文档:")
            for i, doc in enumerate(results, 1):
                print(f"  {i}. {doc.metadata['source']} ({doc.metadata['format']})")
                print(f"     {doc.page_content[:100]}...")

# 测试完整管道
# pipeline = DocumentProcessingPipeline(output_dir="./universal_kb")
# vectorstore = pipeline.process("./documents", is_directory=True)

# ===== 7. 使用示例 =====
print("\n=== 7. 使用示例 ===")

def example_usage():
    """完整使用示例"""

    # 创建管道
    pipeline = DocumentProcessingPipeline(output_dir="./my_knowledge_base")

    # 处理文档目录
    vectorstore = pipeline.process("./documents", is_directory=True)

    if vectorstore:
        # 使用知识库进行检索
        from langchain.chains import RetrievalQA
        from langchain.llms import OpenAI

        qa_chain = RetrievalQA.from_chain_type(
            llm=OpenAI(),
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )

        # 查询
        result = qa_chain({"query": "总结文档的主要内容"})

        print(f"\n答案: {result['result']}\n")
        print("来源:")
        for doc in result['source_documents']:
            print(f"- {doc.metadata['source']} ({doc.metadata['format']})")

# 运行示例
# example_usage()

print("\n=== 示例代码执行完成 ===")
print("\n使用说明:")
print("1. 准备包含多种格式文档的目录")
print("2. 取消注释测试代码")
print("3. 运行脚本构建知识库")
print("\n支持的格式:")
print("- PDF (.pdf)")
print("- Word (.docx)")
print("- Excel (.xlsx)")
print("- PowerPoint (.pptx)")
print("- HTML (.html, .htm)")
print("- Markdown (.md)")
print("- 文本 (.txt)")
print("\n特性:")
print("- 自动格式检测")
print("- 统一处理接口")
print("- 元数据增强")
print("- 完整的处理报告")
print("- 错误处理和统计")
```

---

## 运行输出示例

```
=== 文档处理管道 ===

步骤1: 加载文档

扫描目录: ./documents
  ✅ report.pdf: 10 个文档
  ✅ data.xlsx: 2 个文档
  ✅ presentation.pptx: 15 个文档
  ✅ guide.md: 1 个文档
  ❌ corrupted.pdf: 解析失败

加载统计:
  总文件数: 5
  成功: 4
  失败: 1
  文档数: 28

步骤2: 增强元数据
✅ 元数据增强完成

步骤3: 文本分块
✅ 分块完成: 85 个文本块

步骤4: 向量化并存储
✅ 知识库构建成功

步骤5: 生成报告

文档统计:
  总文档数: 28
  总文本块: 85

按格式分布:
  pdf: 10
  xlsx: 2
  pptx: 15
  markdown: 1

按语言分布:
  zh: 25
  en: 3

报告已保存: ./universal_kb/processing_report.json
```

---

## 关键要点

### 1. 管道设计模式

```python
# 管道模式：每个阶段独立、可组合
加载 → 增强 → 分块 → 向量化 → 存储
  ↓      ↓      ↓       ↓       ↓
 多格式  元数据  文本块   向量   知识库
```

### 2. 解析器注册表

```python
# 易于扩展新格式
parser_registry.register('new_format', parse_new_format)

# 支持自定义解析器
def custom_parser(file_path: str) -> List[Document]:
    # 自定义解析逻辑
    pass

parser_registry.register('custom', custom_parser)
```

### 3. 错误处理策略

- 单个文件失败不影响其他文件
- 记录所有失败文件
- 生成详细的处理报告

---

## 扩展功能

### 1. 添加新格式支持

```python
# 添加JSON格式支持
def parse_json(file_path: str) -> List[Document]:
    import json
    with open(file_path, 'r') as f:
        data = json.load(f)
    return [Document(
        page_content=json.dumps(data, indent=2),
        metadata={"source": file_path, "format": "json"}
    )]

parser_registry.register('json', parse_json)
```

### 2. 并行处理

```python
from concurrent.futures import ThreadPoolExecutor

def load_directory_parallel(directory: str, max_workers: int = 4):
    """并行加载目录"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(load_file, f) for f in files]
        results = [f.result() for f in futures]
    return results
```

### 3. 增量更新

```python
def update_knowledge_base(new_files: List[str], vectorstore):
    """增量更新知识库"""
    # 只处理新文件
    new_docs = loader.load_files(new_files)
    chunks = splitter.split_documents(new_docs)
    vectorstore.add_documents(chunks)
```

---

## 常见问题

### Q1: 如何处理大量文件？

使用批处理和进度跟踪：
```python
from tqdm import tqdm

for file in tqdm(files, desc="处理文件"):
    process_file(file)
```

### Q2: 如何优化内存使用？

分批处理文档：
```python
batch_size = 100
for i in range(0, len(files), batch_size):
    batch = files[i:i+batch_size]
    process_batch(batch)
```

### Q3: 如何自定义元数据？

```python
# 在加载后添加自定义元数据
for doc in documents:
    doc.metadata["department"] = "技术部"
    doc.metadata["project"] = "RAG系统"
```

---

## 扩展阅读

- [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) (2025)
- [Unstructured.io](https://docs.unstructured.io/) - 统一文档处理 (2025)
- [Pipeline Pattern](https://en.wikipedia.org/wiki/Pipeline_(software)) - 管道模式

---

**版本：** v1.0
**最后更新：** 2026-02-15
**下一步：** 阅读 [07_实战代码_06_元数据提取保留.md](./07_实战代码_06_元数据提取保留.md)
