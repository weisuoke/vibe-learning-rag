# 03_核心概念_03_评估指标体系

## ReRank评估指标概述

评估ReRank效果需要多维度指标体系，主要包括：

1. **排序质量指标**：NDCG、MRR、MAP
2. **召回率指标**：Recall@K、Precision@K
3. **性能指标**：延迟、吞吐量
4. **成本指标**：API费用、计算资源
5. **业务指标**：用户满意度、点击率

---

## 核心指标1：NDCG (Normalized Discounted Cumulative Gain)

### 定义

**NDCG@K**：归一化折损累积增益，衡量Top K结果的排序质量，考虑位置和相关性。

### 公式

```
DCG@K = Σ(rel_i / log2(i+1))  # i从1到K
NDCG@K = DCG@K / IDCG@K       # IDCG是理想排序的DCG
```

**详细计算：**

```python
import numpy as np

def dcg_at_k(relevance_scores, k):
    """计算DCG@K"""
    relevance_scores = np.array(relevance_scores)[:k]
    positions = np.arange(1, len(relevance_scores) + 1)
    dcg = np.sum(relevance_scores / np.log2(positions + 1))
    return dcg

def ndcg_at_k(relevance_scores, k):
    """计算NDCG@K"""
    # 实际排序的DCG
    dcg = dcg_at_k(relevance_scores, k)

    # 理想排序的DCG（按相关性降序）
    ideal_relevance = sorted(relevance_scores, reverse=True)
    idcg = dcg_at_k(ideal_relevance, k)

    # 归一化
    if idcg == 0:
        return 0
    return dcg / idcg

# 示例
relevance_scores = [3, 1, 2, 0, 0]  # 相关性分数
ndcg_10 = ndcg_at_k(relevance_scores, k=10)
print(f"NDCG@10: {ndcg_10:.4f}")
# 输出：NDCG@10: 0.9203
```

### 为什么用NDCG？

**优势：**

1. **考虑位置**：位置越靠前的文档权重越高（通过log2(i+1)折损）
2. **考虑相关性**：支持分级相关性（0-5分），不是二元的
3. **归一化**：值域[0,1]，便于比较不同query
4. **符合实际**：用户更关注Top结果，NDCG反映这一点

**vs其他指标：**

| 指标 | 考虑位置 | 考虑相关性 | 归一化 | 适用场景 |
|------|---------|-----------|--------|---------|
| **NDCG** | ✅ | ✅ 分级 | ✅ | **ReRank评估** |
| Recall | ❌ | ✅ 二元 | ❌ | 初检评估 |
| Precision | ❌ | ✅ 二元 | ❌ | 初检评估 |
| MRR | ✅ 首位 | ✅ 二元 | ✅ | 单答案场景 |

### 实际应用

**2026年ReRank基准：**

| 方法 | NDCG@10 | 提升 |
|------|---------|------|
| BM25（基线） | 0.65 | - |
| 向量检索 | 0.72 | +10.8% |
| 混合检索 | 0.78 | +20.0% |
| **混合检索 + ReRank** | **0.87** | **+33.8%** |

**代码示例：**

```python
from sklearn.metrics import ndcg_score

# 准备测试数据
queries = ["什么是RAG？", "如何使用向量数据库？"]
ground_truth = [
    [3, 1, 2, 0, 0],  # 第1个文档最相关(3分)
    [2, 3, 1, 0, 0]   # 第2个文档最相关(3分)
]

# 初检分数
initial_scores = [
    [0.8, 0.6, 0.5, 0.4, 0.3],
    [0.7, 0.9, 0.6, 0.4, 0.2]
]

# ReRank分数
rerank_scores = [
    [0.95, 0.75, 0.85, 0.12, 0.08],
    [0.88, 0.96, 0.78, 0.15, 0.10]
]

# 计算NDCG@10
for i, query in enumerate(queries):
    ndcg_initial = ndcg_score([ground_truth[i]], [initial_scores[i]])
    ndcg_rerank = ndcg_score([ground_truth[i]], [rerank_scores[i]])

    improvement = (ndcg_rerank - ndcg_initial) / ndcg_initial * 100
    print(f"Query: {query}")
    print(f"  Initial NDCG@10: {ndcg_initial:.4f}")
    print(f"  ReRank NDCG@10: {ndcg_rerank:.4f}")
    print(f"  Improvement: +{improvement:.1f}%\n")
```

---

## 核心指标2：MRR (Mean Reciprocal Rank)

### 定义

**MRR**：平均倒数排名，衡量第一个相关结果的平均位置。

### 公式

```
RR = 1 / rank_of_first_relevant_result
MRR = (1/N) * Σ RR_i  # N是query数量
```

**计算示例：**

```python
def mrr(relevance_lists):
    """计算MRR"""
    reciprocal_ranks = []

    for relevance in relevance_lists:
        # 找到第一个相关结果的位置
        for i, rel in enumerate(relevance, start=1):
            if rel > 0:  # 相关
                reciprocal_ranks.append(1.0 / i)
                break
        else:
            reciprocal_ranks.append(0.0)  # 没有相关结果

    return np.mean(reciprocal_ranks)

# 示例
relevance_lists = [
    [0, 0, 1, 0, 0],  # 第3位是第一个相关结果，RR=1/3
    [1, 0, 0, 0, 0],  # 第1位是第一个相关结果，RR=1/1
    [0, 1, 0, 0, 0]   # 第2位是第一个相关结果，RR=1/2
]

mrr_score = mrr(relevance_lists)
print(f"MRR: {mrr_score:.4f}")
# 输出：MRR: 0.6111 = (1/3 + 1/1 + 1/2) / 3
```

### 适用场景

**MRR适合：**
- 单答案场景（如问答系统）
- 用户只看第一个结果
- 快速评估首位质量

**MRR不适合：**
- 多答案场景（如文档检索）
- 需要评估整体排序质量
- 用户会浏览多个结果

**实际对比：**

```python
# 场景1：单答案问答
query = "2026年中国GDP是多少？"
results = [
    "2026年中国GDP为135万亿元",  # 唯一正确答案
    "2025年中国GDP为126万亿元",
    "中国是世界第二大经济体"
]
# MRR适合：只关心正确答案是否在第1位

# 场景2：文档检索
query = "RAG技术综述"
results = [
    "RAG技术原理",  # 相关
    "RAG应用案例",  # 相关
    "RAG优化方法",  # 相关
    "向量数据库",   # 部分相关
    "LLM基础"      # 不相关
]
# NDCG适合：需要评估所有相关文档的排序质量
```

---

## 核心指标3：MAP (Mean Average Precision)

### 定义

**MAP**：平均精度均值，衡量所有相关结果的平均精度。

### 公式

```
AP = (1/R) * Σ(Precision@k × rel_k)  # R是相关文档总数
MAP = (1/N) * Σ AP_i  # N是query数量
```

**计算示例：**

```python
def average_precision(relevance):
    """计算单个query的AP"""
    relevant_count = 0
    precision_sum = 0.0

    for i, rel in enumerate(relevance, start=1):
        if rel > 0:  # 相关
            relevant_count += 1
            precision_at_i = relevant_count / i
            precision_sum += precision_at_i

    if relevant_count == 0:
        return 0.0
    return precision_sum / relevant_count

def mean_average_precision(relevance_lists):
    """计算MAP"""
    aps = [average_precision(rel) for rel in relevance_lists]
    return np.mean(aps)

# 示例
relevance_lists = [
    [1, 0, 1, 0, 1],  # 3个相关文档在位置1,3,5
    [0, 1, 1, 0, 0],  # 2个相关文档在位置2,3
]

# Query 1的AP计算：
# 位置1：相关，Precision@1 = 1/1 = 1.0
# 位置3：相关，Precision@3 = 2/3 = 0.667
# 位置5：相关，Precision@5 = 3/5 = 0.6
# AP = (1.0 + 0.667 + 0.6) / 3 = 0.756

map_score = mean_average_precision(relevance_lists)
print(f"MAP: {map_score:.4f}")
```

### MAP vs NDCG

| 维度 | MAP | NDCG |
|------|-----|------|
| 相关性 | 二元（相关/不相关） | 分级（0-5分） |
| 位置权重 | 线性 | 对数折损 |
| 适用场景 | 二元相关性任务 | 分级相关性任务 |
| 计算复杂度 | 低 | 中 |

**2026年推荐：** NDCG（支持分级相关性，更符合实际）

---

## 核心指标4：Recall@K 和 Precision@K

### 定义

**Recall@K**：Top K中包含的相关文档比例
**Precision@K**：Top K中相关文档的精确率

### 公式

```
Recall@K = (Top K中相关文档数) / (总相关文档数)
Precision@K = (Top K中相关文档数) / K
```

**计算示例：**

```python
def recall_at_k(relevance, k):
    """计算Recall@K"""
    top_k_relevance = relevance[:k]
    relevant_in_top_k = sum(1 for rel in top_k_relevance if rel > 0)
    total_relevant = sum(1 for rel in relevance if rel > 0)

    if total_relevant == 0:
        return 0.0
    return relevant_in_top_k / total_relevant

def precision_at_k(relevance, k):
    """计算Precision@K"""
    top_k_relevance = relevance[:k]
    relevant_in_top_k = sum(1 for rel in top_k_relevance if rel > 0)
    return relevant_in_top_k / k

# 示例
relevance = [1, 0, 1, 0, 1, 0, 0, 1, 0, 0]  # 4个相关文档

recall_5 = recall_at_k(relevance, k=5)
precision_5 = precision_at_k(relevance, k=5)

print(f"Recall@5: {recall_5:.4f}")    # 3/4 = 0.75
print(f"Precision@5: {precision_5:.4f}")  # 3/5 = 0.60
```

### 适用场景

**Recall@K：**
- 评估初检召回率
- 确保不遗漏相关文档
- 候选集大小优化

**Precision@K：**
- 评估Top K质量
- 用户体验优化
- 减少噪声文档

**实际应用：**

```python
# 初检评估：关注Recall@50
initial_results = vector_search(query, top_k=50)
recall_50 = recall_at_k(initial_results, k=50)
# 目标：Recall@50 > 85%

# ReRank评估：关注Precision@5
reranked_results = reranker.rerank(query, initial_results, top_k=5)
precision_5 = precision_at_k(reranked_results, k=5)
# 目标：Precision@5 > 80%
```

---

## 核心指标5：延迟 (Latency)

### 定义

**延迟**：从query输入到结果返回的时间。

### 关键指标

**P50/P95/P99延迟：**
- P50：50%请求的延迟
- P95：95%请求的延迟
- P99：99%请求的延迟

**计算示例：**

```python
import time
import numpy as np

def measure_latency(reranker, query, candidates, num_runs=100):
    """测量延迟"""
    latencies = []

    for _ in range(num_runs):
        start = time.time()
        reranker.rerank(query, candidates)
        latency = (time.time() - start) * 1000  # 转换为毫秒
        latencies.append(latency)

    latencies = np.array(latencies)
    return {
        'p50': np.percentile(latencies, 50),
        'p95': np.percentile(latencies, 95),
        'p99': np.percentile(latencies, 99),
        'mean': np.mean(latencies),
        'std': np.std(latencies)
    }

# 示例
stats = measure_latency(reranker, query, candidates)
print(f"P50: {stats['p50']:.2f}ms")
print(f"P95: {stats['p95']:.2f}ms")
print(f"P99: {stats['p99']:.2f}ms")
```

### 2026年基准

| 候选集大小 | P50延迟 | P95延迟 | P99延迟 |
|-----------|---------|---------|---------|
| 20 | 80ms | 120ms | 150ms |
| 50 | 200ms | 300ms | 400ms |
| 100 | 400ms | 600ms | 800ms |
| 200 | 800ms | 1200ms | 1500ms |

**目标：**
- 实时系统：P95 < 500ms
- 批处理：P95 < 2s

---

## 核心指标6：吞吐量 (Throughput)

### 定义

**吞吐量**：单位时间内处理的query数量（QPS）。

### 计算示例

```python
import time
import threading

def measure_throughput(reranker, queries, candidates_list, duration=60):
    """测量吞吐量"""
    query_count = 0
    start_time = time.time()

    def worker():
        nonlocal query_count
        while time.time() - start_time < duration:
            for query, candidates in zip(queries, candidates_list):
                reranker.rerank(query, candidates)
                query_count += 1

    # 多线程测试
    threads = [threading.Thread(target=worker) for _ in range(4)]
    for t in threads:
        t.start()
    for t in threads:
        t.join()

    elapsed = time.time() - start_time
    qps = query_count / elapsed
    return qps

# 示例
qps = measure_throughput(reranker, queries, candidates_list)
print(f"Throughput: {qps:.2f} QPS")
```

### 2026年基准

| 模型 | 单线程QPS | 4线程QPS | GPU加速 |
|------|----------|----------|---------|
| BGE-base | 50 | 180 | 10x |
| BGE-v2-m3 | 40 | 150 | 8x |
| BGE-large | 20 | 70 | 6x |

---

## 核心指标7：成本 (Cost)

### 定义

**成本**：每百万tokens的费用（$/M tokens）。

### 成本对比

**2026年ReRank成本：**

| 方法 | 成本/M tokens | 50文档成本 | 10万query/天 |
|------|--------------|-----------|-------------|
| **Cross-Encoder** | **$0.025** | **$0.001** | **$2.5** |
| LLM Pointwise | $0.50 | $0.025 | $2500 |
| LLM Listwise | $1.00 | $0.050 | $5000 |
| 无ReRank | $0 | $0 | $0 |

**ROI计算：**

```python
def calculate_roi(baseline_ndcg, rerank_ndcg, rerank_cost_per_query,
                  queries_per_day, revenue_per_query):
    """计算ROI"""
    # 精度提升
    improvement = (rerank_ndcg - baseline_ndcg) / baseline_ndcg

    # 收入增加（假设精度提升带来收入增加）
    revenue_increase = queries_per_day * revenue_per_query * improvement

    # 成本增加
    cost_increase = queries_per_day * rerank_cost_per_query

    # ROI
    roi = (revenue_increase - cost_increase) / cost_increase

    return {
        'improvement': improvement * 100,
        'revenue_increase': revenue_increase,
        'cost_increase': cost_increase,
        'roi': roi * 100
    }

# 示例
roi = calculate_roi(
    baseline_ndcg=0.72,
    rerank_ndcg=0.87,
    rerank_cost_per_query=0.001,
    queries_per_day=100000,
    revenue_per_query=0.01  # 每个query带来1分钱收入
)

print(f"精度提升: {roi['improvement']:.1f}%")
print(f"收入增加: ${roi['revenue_increase']:.2f}/天")
print(f"成本增加: ${roi['cost_increase']:.2f}/天")
print(f"ROI: {roi['roi']:.1f}%")
```

---

## 综合评估框架

### 评估流程

```python
class ReRankEvaluator:
    def __init__(self, test_queries, ground_truth):
        self.test_queries = test_queries
        self.ground_truth = ground_truth

    def evaluate(self, reranker, initial_results):
        """综合评估"""
        results = {
            'ndcg@10': [],
            'mrr': [],
            'recall@5': [],
            'precision@5': [],
            'latency': []
        }

        for query, truth, candidates in zip(
            self.test_queries, self.ground_truth, initial_results
        ):
            # ReRank
            start = time.time()
            reranked = reranker.rerank(query, candidates, top_k=10)
            latency = (time.time() - start) * 1000

            # 计算指标
            results['ndcg@10'].append(ndcg_score([truth], [reranked]))
            results['mrr'].append(self._calculate_mrr(reranked, truth))
            results['recall@5'].append(recall_at_k(reranked, k=5))
            results['precision@5'].append(precision_at_k(reranked, k=5))
            results['latency'].append(latency)

        # 汇总
        summary = {
            'NDCG@10': np.mean(results['ndcg@10']),
            'MRR': np.mean(results['mrr']),
            'Recall@5': np.mean(results['recall@5']),
            'Precision@5': np.mean(results['precision@5']),
            'P50_latency': np.percentile(results['latency'], 50),
            'P95_latency': np.percentile(results['latency'], 95),
            'P99_latency': np.percentile(results['latency'], 99)
        }

        return summary

    def _calculate_mrr(self, reranked, truth):
        """计算MRR"""
        for i, doc in enumerate(reranked, start=1):
            if doc in truth:
                return 1.0 / i
        return 0.0

# 使用示例
evaluator = ReRankEvaluator(test_queries, ground_truth)
summary = evaluator.evaluate(reranker, initial_results)

print("=== ReRank评估结果 ===")
for metric, value in summary.items():
    print(f"{metric}: {value:.4f}")
```

### 2026年生产基准

| 指标 | 目标值 | 优秀值 |
|------|--------|--------|
| NDCG@10 | > 0.85 | > 0.90 |
| MRR | > 0.80 | > 0.90 |
| Recall@5 | > 0.75 | > 0.85 |
| Precision@5 | > 0.80 | > 0.90 |
| P95延迟 | < 500ms | < 300ms |
| 成本 | < $0.05/M | < $0.03/M |

---

## 关键要点速记

### 核心指标
1. **NDCG@10**：排序质量金标准，考虑位置和相关性
2. **MRR**：首位质量，适合单答案场景
3. **MAP**：平均精度，适合二元相关性
4. **Recall@K**：召回率，评估初检质量
5. **Precision@K**：精确率，评估Top K质量

### 性能指标
6. **P50/P95/P99延迟**：用户体验关键指标
7. **吞吐量**：系统容量指标
8. **成本**：商业可行性指标

### 实战建议
9. **主指标**：NDCG@10（排序质量）
10. **辅助指标**：P95延迟（用户体验）+ 成本（商业可行性）
11. **评估流程**：离线评估 → A/B测试 → 生产监控
12. **基准对比**：vs无ReRank基线，提升>15%才有价值

---

## 参考资料

### 核心论文
- [Normalized Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) - Wikipedia
- [Mean Reciprocal Rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) - Wikipedia
- [Information Retrieval Evaluation](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html) - Stanford NLP

### 技术文档
- [Databricks Reranking Research](https://www.databricks.com/blog/reranking-mosaic-ai-vector-search-faster-smarter-retrieval-rag-agents) - 2026
- [Ultimate Guide to Choosing the Best Reranking Model in 2026](https://www.zeroentropy.dev/articles/ultimate-guide-to-choosing-the-best-reranking-model-in-2025)

### 实现参考
- [scikit-learn NDCG](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html)
- [rank-eval Python Package](https://github.com/AmenRa/rank-eval)

---

**版本：** v1.0 (2026年标准)
**最后更新：** 2026-02-16
**适用场景：** RAG开发、信息检索、模型评估
