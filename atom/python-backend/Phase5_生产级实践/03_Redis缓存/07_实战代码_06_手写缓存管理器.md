# 实战代码6：手写缓存管理器

## 完整可运行示例

```python
"""
手写缓存管理器
演示：封装完整的缓存逻辑，提供统一接口
"""

import redis
import asyncio
import hashlib
import json
import random
import time
from typing import Optional, Callable, List
from dataclasses import dataclass

# ===== 1. 缓存管理器 =====

@dataclass
class CacheConfig:
    """缓存配置"""
    ttl: int = 3600
    use_random_ttl: bool = True
    use_mutex: bool = True
    use_null_cache: bool = True
    null_cache_ttl: int = 60

class CacheManager:
    """生产级缓存管理器"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.locks = {}
        self.stats = {"hits": 0, "misses": 0, "errors": 0}

    async def get_or_set(
        self,
        key: str,
        fetch_func: Callable,
        config: Optional[CacheConfig] = None
    ) -> Optional[str]:
        """获取或设置缓存"""
        config = config or CacheConfig()

        try:
            # 1. 查询缓存
            cached = self.redis.get(key)

            if cached == "NULL" and config.use_null_cache:
                self.stats["hits"] += 1
                return None

            if cached:
                self.stats["hits"] += 1
                return cached

            self.stats["misses"] += 1

            # 2. 使用互斥锁
            if config.use_mutex:
                if key not in self.locks:
                    self.locks[key] = asyncio.Lock()

                async with self.locks[key]:
                    # 双重检查
                    cached = self.redis.get(key)
                    if cached:
                        return cached if cached != "NULL" else None

                    # 获取数据
                    data = await fetch_func()

                    # 设置缓存
                    self._set_cache(key, data, config)

                    return data
            else:
                # 不使用互斥锁
                data = await fetch_func()
                self._set_cache(key, data, config)
                return data

        except Exception as e:
            self.stats["errors"] += 1
            print(f"❌ 缓存错误: {e}")
            return None

    def _set_cache(self, key: str, data: Optional[str], config: CacheConfig):
        """设置缓存"""
        if data:
            # 计算TTL
            ttl = config.ttl
            if config.use_random_ttl:
                ttl = ttl + random.randint(-ttl//10, ttl//10)

            self.redis.setex(key, ttl, data)
        elif config.use_null_cache:
            # 空值缓存
            self.redis.setex(key, config.null_cache_ttl, "NULL")

    def get_stats(self) -> dict:
        """获取统计信息"""
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total if total > 0 else 0

        return {
            **self.stats,
            "total": total,
            "hit_rate": hit_rate
        }

    def clear_pattern(self, pattern: str):
        """清空匹配的缓存"""
        keys = self.redis.keys(pattern)
        if keys:
            self.redis.delete(*keys)
        return len(keys)

# ===== 2. LLM缓存管理器 =====

class LLMCacheManager(CacheManager):
    """LLM专用缓存管理器"""

    def __init__(self, redis_client: redis.Redis, openai_client):
        super().__init__(redis_client)
        self.openai = openai_client

    def _generate_key(self, prompt: str, model: str) -> str:
        """生成缓存key"""
        content = f"{model}:{prompt}"
        return f"llm:{hashlib.md5(content.encode()).hexdigest()}"

    async def get_llm_response(
        self,
        prompt: str,
        model: str = "gpt-4o-mini",
        config: Optional[CacheConfig] = None
    ) -> str:
        """获取LLM响应（带缓存）"""
        cache_key = self._generate_key(prompt, model)

        async def fetch_llm():
            response = self.openai.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content

        return await self.get_or_set(cache_key, fetch_llm, config)

# ===== 3. Embedding缓存管理器 =====

class EmbeddingCacheManager(CacheManager):
    """Embedding专用缓存管理器"""

    def __init__(self, redis_client: redis.Redis, openai_client):
        super().__init__(redis_client)
        self.openai = openai_client

    def _generate_key(self, text: str, model: str) -> str:
        """生成缓存key"""
        content = f"{model}:{text}"
        return f"emb:{hashlib.md5(content.encode()).hexdigest()}"

    async def get_embedding(
        self,
        text: str,
        model: str = "text-embedding-3-small"
    ) -> List[float]:
        """获取Embedding（带缓存）"""
        cache_key = self._generate_key(text, model)

        async def fetch_embedding():
            response = self.openai.embeddings.create(
                model=model,
                input=text
            )
            return json.dumps(response.data[0].embedding)

        cached = await self.get_or_set(
            cache_key,
            fetch_embedding,
            CacheConfig(ttl=86400)  # 24小时
        )

        return json.loads(cached) if cached else []

# ===== 4. 测试 =====

async def test_cache_manager():
    """测试缓存管理器"""
    redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
    manager = CacheManager(redis_client)

    print("=== 测试缓存管理器 ===\n")

    # 模拟数据获取函数
    async def fetch_data():
        await asyncio.sleep(0.1)
        return "test_data"

    # 第一次：未命中
    print("第一次调用:")
    result1 = await manager.get_or_set("test_key", fetch_data)
    print(f"结果: {result1}\n")

    # 第二次：命中
    print("第二次调用:")
    result2 = await manager.get_or_set("test_key", fetch_data)
    print(f"结果: {result2}\n")

    # 统计
    stats = manager.get_stats()
    print(f"统计: {stats}")

    # 清理
    manager.clear_pattern("test_*")
    print("\n✅ 测试完成")

# 运行测试
if __name__ == "__main__":
    asyncio.run(test_cache_manager())
```

## 学习检查清单

- [ ] 理解缓存管理器的设计模式
- [ ] 实现通用缓存管理器
- [ ] 实现LLM专用缓存管理器
- [ ] 实现Embedding专用缓存管理器
- [ ] 封装缓存逻辑，提供统一接口
- [ ] 支持配置化（TTL、互斥锁、空值缓存）
