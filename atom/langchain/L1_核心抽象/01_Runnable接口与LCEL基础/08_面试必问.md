# 面试必问

> 掌握 Runnable 和 LCEL 的高频面试题

---

## 问题 1: "什么是 Runnable 协议？为什么 LangChain 要设计这个协议？"

### 普通回答（❌ 不出彩）

"Runnable 是 LangChain 的基础接口，定义了 invoke、batch、stream 三个方法。它让所有组件都有统一的执行方式，可以用管道操作符组合起来。"

**问题**：
- 太表面，没有深度
- 没有说明设计动机
- 没有体现对架构的理解

### 出彩回答（✅ 推荐）

> **Runnable 协议有三层含义：**
>
> **1. 接口层面**：Runnable 是 Python Protocol（结构化类型），定义了统一的执行契约。任何实现了 `invoke`、`batch`、`stream` 方法的对象都是 Runnable，无需继承。这种设计比传统继承更灵活，降低了耦合度。
>
> **2. 架构层面**：Runnable 解决了 AI 应用的核心问题——如何构建可组合、可观测、可测试的复杂工作流。通过统一接口，LangChain 实现了：
> - **组合性**：任意 Runnable 可以用 `|` 操作符组合，形成 LCEL 表达式
> - **可观测性**：统一的 Config 参数支持回调和监控
> - **可测试性**：标准化的输入输出便于单元测试和 mock
>
> **3. 生态层面**：Runnable 协议是 LangChain 生态的基石。2025年 LangChain 1.0 发布后，Runnable 协议保持稳定，承诺到 2.0 无重大变更。这让企业可以放心采用，第三方库可以基于 Runnable 构建。目前 57% 的企业已在生产环境部署基于 Runnable 的 Agent 应用。
>
> **与传统 Chain 的区别**：
> - 传统 Chain 使用继承，Runnable 使用 Protocol
> - 传统 Chain 接口不统一，Runnable 接口标准化
> - 传统 Chain 难以组合，Runnable 天然支持 LCEL 组合
>
> **在实际工作中的应用**：
> 我在项目中使用 Runnable 构建了一个 RAG 系统，通过组合 retriever、prompt、llm、parser 等 Runnable 组件，实现了灵活的文档问答功能。使用 LangSmith 的回调系统监控每个 Runnable 的性能，快速定位了检索环节的瓶颈并优化。

**为什么这个回答出彩？**

1. ✅ **多层次解释**：从接口、架构、生态三个层面深入分析
2. ✅ **对比说明**：与传统 Chain 对比，突出 Runnable 的优势
3. ✅ **数据支撑**：引用 2025-2026 年的实际数据（57% 企业采用）
4. ✅ **实战经验**：结合实际项目说明应用场景
5. ✅ **技术深度**：提到 Protocol、LCEL、LangSmith 等关键技术

---

## 问题 2: "LCEL 和传统 Chain 有什么区别？什么时候用哪个？"

### 普通回答（❌ 不出彩）

"LCEL 用管道操作符 `|` 组合组件，比传统 Chain 更简洁。LCEL 是新的方式，所以应该优先使用 LCEL。"

**问题**：
- 只说了语法差异，没有本质区别
- 没有说明使用场景
- 没有性能和架构层面的对比

### 出彩回答（✅ 推荐）

> **LCEL 和传统 Chain 的本质区别是编程范式：**
>
> **1. 编程范式**：
> - **传统 Chain**：命令式编程，告诉计算机"如何做"
>   ```python
>   chain = LLMChain(llm=llm, prompt=prompt)
>   result = chain.run(input)
>   ```
> - **LCEL**：声明式编程，告诉计算机"做什么"
>   ```python
>   chain = prompt | llm | parser
>   result = chain.invoke(input)
>   ```
>
> **2. 性能特征**：
> - **传统 Chain**：顺序执行，需要手动实现并行和流式
> - **LCEL**：自动优化执行计划，支持：
>   - 自动并行：`RunnableParallel` 识别独立步骤并发执行
>   - 默认流式：无需修改代码即可流式输出
>   - 异步支持：自动支持 `ainvoke`、`abatch`、`astream`
>
> 根据 2025-2026 年的实测数据，LCEL 在并行场景下比传统 Chain 快 2-3 倍。
>
> **3. 架构灵活性**：
> - **传统 Chain**：固定结构，难以修改和扩展
> - **LCEL**：组合式架构，可以任意组合和调整
>
> **4. 可观测性**：
> - **传统 Chain**：每个 Chain 有不同的监控方式
> - **LCEL**：统一的 Config 和回调系统，便于集成 LangSmith
>
> **使用场景建议**：
>
> | 场景 | 推荐方式 | 原因 |
> |------|----------|------|
> | **新项目** | LCEL | 灵活、可维护、性能好 |
> | **简单单次调用** | 直接用 SDK | 无需框架抽象 |
> | **已有项目迁移** | 混合使用 | 渐进式迁移，用 `RunnableLambda` 包装旧代码 |
> | **需要极致性能** | 考虑 vLLM | 专用推理引擎 |
>
> **迁移策略**：
> 在实际项目中，我采用渐进式迁移：
> 1. 新功能使用 LCEL 开发
> 2. 关键路径优先迁移（如 RAG 检索链）
> 3. 非关键路径保持原样，用 `RunnableLambda` 包装
> 4. 逐步重构，最终实现全 LCEL
>
> **常见误区**：
> - ❌ "LCEL 比传统 Chain 慢"：实际上 LCEL 通过并行优化更快
> - ❌ "必须全部用 LCEL"：可以混用，选择最合适的工具
> - ❌ "LCEL 只是语法糖"：LCEL 有执行引擎优化，不只是语法

**为什么这个回答出彩？**

1. ✅ **本质区别**：从编程范式角度分析，而非表面语法
2. ✅ **性能数据**：引用实测数据（2-3倍提升）
3. ✅ **场景对比**：清晰的使用场景建议表格
4. ✅ **实战策略**：提供渐进式迁移方案
5. ✅ **纠正误区**：澄清常见误解
6. ✅ **技术深度**：涉及执行引擎、并行优化等底层机制

---

## 加分项：深度问题

### 问题 3: "Runnable 的 batch 方法是如何实现并发的？"

**出彩回答**：

> LangChain 的 batch 方法默认使用**线程池**实现并发：
>
> ```python
> # 简化的实现原理
> from concurrent.futures import ThreadPoolExecutor
>
> def batch(self, inputs, config=None):
>     max_concurrency = config.get("max_concurrency", None) if config else None
>
>     with ThreadPoolExecutor(max_workers=max_concurrency) as executor:
>         futures = [executor.submit(self.invoke, input, config) for input in inputs]
>         results = [future.result() for future in futures]
>
>     return results
> ```
>
> **关键点**：
> 1. 使用线程池而非进程池（因为 LLM 调用是 IO 密集型）
> 2. 通过 `max_concurrency` 控制并发数，避免超过 API 限制
> 3. 保证输出顺序与输入顺序一致
>
> **性能优化**：
> - 对于异步操作，推荐使用 `abatch` 配合 `asyncio`，性能更好
> - 2025-2026 年引入的 langasync 工具通过批处理 API 实现 50% 成本降低

### 问题 4: "如何在生产环境中监控 Runnable 的性能？"

**出彩回答**：

> 生产环境监控 Runnable 有三个层次：
>
> **1. LangSmith 集成**（推荐）：
> ```python
> from langchain_core.runnables import RunnableConfig
>
> config = RunnableConfig(
>     tags=["production", "rag", "v2"],
>     metadata={"user_id": "123", "environment": "prod"},
>     run_name="rag_query"
> )
>
> result = chain.invoke(input, config=config)
> ```
>
> LangSmith 自动追踪：
> - 每个 Runnable 的输入输出
> - 执行时间和 token 使用
> - 错误和异常
> - 完整的调用链路
>
> **2. 自定义回调**：
> ```python
> from langchain_core.callbacks import BaseCallbackHandler
>
> class MetricsHandler(BaseCallbackHandler):
>     def on_chain_start(self, serialized, inputs, **kwargs):
>         # 记录开始时间
>         self.start_time = time.time()
>
>     def on_chain_end(self, outputs, **kwargs):
>         # 计算耗时，发送到监控系统
>         elapsed = time.time() - self.start_time
>         send_to_prometheus({"latency": elapsed})
> ```
>
> **3. 成本追踪**：
> ```python
> class CostTracker(BaseCallbackHandler):
>     def on_llm_end(self, response, **kwargs):
>         token_usage = response.llm_output.get('token_usage', {})
>         cost = calculate_cost(token_usage)
>         log_cost(cost)
> ```
>
> **实际经验**：
> 在我们的生产系统中，通过 LangSmith 发现检索环节占总耗时的 60%，优化后整体性能提升 40%。

### 问题 5: "Runnable 的类型系统是如何工作的？"

**出彩回答**：

> Runnable 使用 Python 泛型实现类型安全：
>
> ```python
> from typing import TypeVar
>
> Input = TypeVar("Input", contravariant=True)
> Output = TypeVar("Output", covariant=True)
>
> class Runnable(Protocol[Input, Output]):
>     def invoke(self, input: Input) -> Output: ...
> ```
>
> **关键概念**：
> 1. **协变（covariant）**：Output 可以是子类型
> 2. **逆变（contravariant）**：Input 可以是父类型
> 3. **类型推断**：组合时自动推断类型
>
> ```python
> prompt: Runnable[dict, PromptValue]
> llm: Runnable[PromptValue, AIMessage]
> parser: Runnable[AIMessage, str]
>
> # 组合后自动推断
> chain: Runnable[dict, str] = prompt | llm | parser
> ```
>
> **2025-2026 年改进**：
> LangChain 1.0 增强了类型安全，IDE 可以更好地提示类型错误。

---

## 面试准备建议

### 1. 理解层次

**表面理解**（不够）：
- 知道 Runnable 有三个方法
- 会用 `|` 操作符组合

**深度理解**（推荐）：
- 理解 Protocol 设计的优势
- 理解 LCEL 的执行优化机制
- 理解生产环境的最佳实践
- 了解 2025-2026 年的最新发展

### 2. 实战经验

**准备实际案例**：
- 你用 Runnable 构建过什么应用？
- 遇到过什么问题？如何解决？
- 如何优化性能和成本？
- 如何监控和调试？

**示例**：
> "我用 Runnable 构建了一个 RAG 文档问答系统。最初检索速度慢，通过 LangSmith 发现是向量检索的瓶颈。优化后使用 `RunnableParallel` 并行检索多个数据源，性能提升 3 倍。同时集成 langasync 进行批量评估，成本降低 50%。"

### 3. 技术广度

**关联知识**：
- Unix 管道和函数式编程
- 中间件模式和 AOP
- Protocol vs 继承
- 异步编程和并发
- 可观测性和监控

### 4. 最新动态

**2025-2026 年关键更新**：
- LangChain 1.0 稳定性承诺
- CVE-2025-68664 安全修复
- langasync 成本优化
- 企业级应用案例（Klarna、Elastic）

---

## 快速复习卡片

### 核心概念

| 概念 | 一句话总结 |
|------|-----------|
| **Runnable** | 可组合的计算单元，统一执行接口 |
| **LCEL** | 声明式编程语言，管道操作符组合 |
| **invoke** | 同步单次执行 |
| **batch** | 并发批量处理 |
| **stream** | 流式实时输出 |
| **Protocol** | 结构化类型，无需继承 |

### 关键数据

| 数据 | 来源 |
|------|------|
| **57%** 企业生产部署 | State of Agent Engineering 2026 |
| **2-3倍** 并行性能提升 | 2025-2026 实测 |
| **50%** 成本降低 | langasync 批处理 API |
| **80%** 查询时间缩短 | Klarna 案例 |

### 常见误区

| 误区 | 正确理解 |
|------|----------|
| LCEL 比 Chain 慢 | LCEL 通过并行优化更快 |
| Runnable 只能串行 | 支持并行、条件路由 |
| 必须全部用 LCEL | 可混用，选择合适工具 |

---

## 总结

### 面试成功的关键

1. **多层次理解**：接口、架构、生态
2. **实战经验**：真实项目案例
3. **技术深度**：底层机制和优化
4. **最新动态**：2025-2026 年发展
5. **对比分析**：与其他技术对比

### 推荐学习路径

1. 理解 Runnable 协议设计
2. 掌握 LCEL 组合模式
3. 实践 invoke/batch/stream
4. 学习生产环境最佳实践
5. 关注最新发展和案例

---

**下一步**: 阅读 [09_化骨绵掌.md](./09_化骨绵掌.md) 巩固所有知识点
