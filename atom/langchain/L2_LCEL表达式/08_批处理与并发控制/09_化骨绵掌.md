# 化骨绵掌

> 批处理与并发控制的进阶技巧和实战经验

---

## 什么是化骨绵掌？

**化骨绵掌**：看似柔和，实则深入骨髓的高级技巧。在批处理与并发控制中，指那些不起眼但威力巨大的实战经验。

**特点**：
- 不是基础知识，而是实战经验
- 不是理论分析，而是具体技巧
- 不是通用方案，而是场景化解决方案

---

## 技巧 1：动态调整并发数

### 问题场景

固定的 max_concurrency 无法适应不同的负载情况：
- 低峰期：并发数过低，浪费资源
- 高峰期：并发数过高，触发限流

### 解决方案

```python
import time
from collections import deque

class AdaptiveBatchProcessor:
    def __init__(self, chain, initial_concurrency=10):
        self.chain = chain
        self.concurrency = initial_concurrency
        self.success_rate = deque(maxlen=100)  # 最近100次的成功率

    def adjust_concurrency(self):
        """根据成功率动态调整并发数"""
        if len(self.success_rate) < 10:
            return

        recent_success = sum(self.success_rate) / len(self.success_rate)

        if recent_success > 0.95:
            # 成功率高，增加并发
            self.concurrency = min(self.concurrency + 2, 50)
        elif recent_success < 0.80:
            # 成功率低，降低并发
            self.concurrency = max(self.concurrency - 2, 5)

    def batch(self, inputs):
        """自适应批处理"""
        results = []
        for i in range(0, len(inputs), self.concurrency):
            batch = inputs[i:i + self.concurrency]

            try:
                batch_results = self.chain.batch(
                    batch,
                    config={"max_concurrency": self.concurrency}
                )
                results.extend(batch_results)
                self.success_rate.extend([1] * len(batch))
            except Exception as e:
                print(f"批处理失败: {e}")
                self.success_rate.extend([0] * len(batch))
                # 降级处理：逐个重试
                for inp in batch:
                    try:
                        result = self.chain.invoke(inp)
                        results.append(result)
                    except:
                        results.append(None)

            self.adjust_concurrency()
            print(f"当前并发数: {self.concurrency}")

        return results
```

**核心价值**：
- 自动适应负载变化
- 避免手动调整
- 提高系统稳定性

**参考来源**：[LangChain 最佳实践](https://www.swarnendu.de/blog/langchain-best-practices/)

---

## 技巧 2：智能重试与降级

### 问题场景

批处理中部分任务失败，需要智能重试：
- 速率限制：需要等待后重试
- 网络错误：可以立即重试
- 输入错误：不应该重试

### 解决方案

```python
from openai import RateLimitError, APIError
import time

class SmartRetryBatch:
    def __init__(self, chain, max_retries=3):
        self.chain = chain
        self.max_retries = max_retries

    def classify_error(self, error):
        """分类错误类型"""
        if isinstance(error, RateLimitError):
            return "rate_limit", 60  # 等待60秒
        elif isinstance(error, APIError):
            return "api_error", 1  # 等待1秒
        else:
            return "unknown", 0  # 不重试

    def batch_with_smart_retry(self, inputs):
        """智能重试的批处理"""
        results = [None] * len(inputs)
        pending = list(enumerate(inputs))

        for attempt in range(self.max_retries):
            if not pending:
                break

            print(f"尝试 {attempt + 1}/{self.max_retries}, 待处理: {len(pending)}")

            # 批量处理待处理的任务
            indices, batch_inputs = zip(*pending)

            try:
                batch_results = self.chain.batch(
                    batch_inputs,
                    config={"max_concurrency": 10}
                )

                # 成功的任务
                for idx, result in zip(indices, batch_results):
                    results[idx] = result
                pending = []

            except Exception as e:
                error_type, wait_time = self.classify_error(e)

                if error_type == "rate_limit":
                    print(f"速率限制，等待 {wait_time} 秒")
                    time.sleep(wait_time)
                elif error_type == "api_error":
                    print(f"API 错误，等待 {wait_time} 秒")
                    time.sleep(wait_time)
                else:
                    print(f"未知错误，不重试: {e}")
                    pending = []

        return results
```

**核心价值**：
- 区分错误类型
- 智能等待时间
- 避免无效重试

---

## 技巧 3：流式批处理

### 问题场景

批处理需要实时反馈，但 batch_as_completed() 不支持流式输出。

### 解决方案

```python
import asyncio
from typing import AsyncIterator

class StreamingBatchProcessor:
    def __init__(self, chain):
        self.chain = chain

    async def stream_batch(
        self,
        inputs: list,
        max_concurrency: int = 10
    ) -> AsyncIterator[tuple[int, str]]:
        """流式批处理，实时返回结果"""
        semaphore = asyncio.Semaphore(max_concurrency)

        async def process_one(idx, inp):
            async with semaphore:
                # 流式处理单个任务
                async for chunk in self.chain.astream(inp):
                    yield idx, chunk.content

        # 并发处理所有任务
        tasks = [process_one(i, inp) for i, inp in enumerate(inputs)]

        # 合并所有流
        async for idx, chunk in self._merge_streams(tasks):
            yield idx, chunk

    async def _merge_streams(self, streams):
        """合并多个异步流"""
        pending = {
            asyncio.create_task(stream.__anext__()): stream
            for stream in streams
        }

        while pending:
            done, _ = await asyncio.wait(
                pending.keys(),
                return_when=asyncio.FIRST_COMPLETED
            )

            for task in done:
                stream = pending.pop(task)
                try:
                    result = task.result()
                    yield result
                    # 继续从该流读取
                    pending[asyncio.create_task(stream.__anext__())] = stream
                except StopAsyncIteration:
                    pass

# 使用示例
async def main():
    processor = StreamingBatchProcessor(chain)

    async for idx, chunk in processor.stream_batch(inputs):
        print(f"[任务 {idx}] {chunk}", end="", flush=True)

asyncio.run(main())
```

**核心价值**：
- 实时流式输出
- 并发控制
- 用户体验提升

---

## 技巧 4：成本监控与预算控制

### 问题场景

批处理成本难以控制，容易超出预算。

### 解决方案

```python
class CostAwareBatchProcessor:
    def __init__(self, chain, budget_per_hour=10.0):
        self.chain = chain
        self.budget_per_hour = budget_per_hour
        self.cost_tracker = {
            "total_cost": 0.0,
            "start_time": time.time(),
            "request_count": 0
        }

    def estimate_cost(self, input_text, output_text):
        """估算单次调用成本"""
        # GPT-4o-mini 价格（示例）
        input_tokens = len(input_text.split()) * 1.3  # 粗略估算
        output_tokens = len(output_text.split()) * 1.3

        input_cost = (input_tokens / 1000) * 0.00015  # $0.15/1M tokens
        output_cost = (output_tokens / 1000) * 0.0006  # $0.60/1M tokens

        return input_cost + output_cost

    def check_budget(self):
        """检查是否超出预算"""
        elapsed_hours = (time.time() - self.cost_tracker["start_time"]) / 3600
        hourly_cost = self.cost_tracker["total_cost"] / max(elapsed_hours, 0.01)

        if hourly_cost > self.budget_per_hour:
            raise Exception(
                f"超出预算！当前每小时成本: ${hourly_cost:.2f}, "
                f"预算: ${self.budget_per_hour:.2f}"
            )

    def batch(self, inputs):
        """带成本控制的批处理"""
        results = []

        for inp in inputs:
            self.check_budget()

            result = self.chain.invoke(inp)
            results.append(result)

            # 更新成本
            cost = self.estimate_cost(str(inp), result.content)
            self.cost_tracker["total_cost"] += cost
            self.cost_tracker["request_count"] += 1

            # 打印成本信息
            if self.cost_tracker["request_count"] % 10 == 0:
                print(f"已处理: {self.cost_tracker['request_count']}, "
                      f"总成本: ${self.cost_tracker['total_cost']:.4f}")

        return results
```

**核心价值**：
- 实时成本监控
- 预算控制
- 避免意外账单

**参考来源**：[LangChain 成本优化](https://python.plainenglish.io/how-to-reduce-chatgpt-api-costs-in-python-projects-02b7ebdce6f6)

---

## 技巧 5：批处理缓存

### 问题场景

批处理中有重复的输入，浪费 API 调用。

### 解决方案

```python
import hashlib
import json
from functools import lru_cache

class CachedBatchProcessor:
    def __init__(self, chain, cache_size=1000):
        self.chain = chain
        self.cache = {}
        self.cache_size = cache_size
        self.cache_hits = 0
        self.cache_misses = 0

    def _hash_input(self, inp):
        """生成输入的哈希值"""
        inp_str = json.dumps(inp, sort_keys=True)
        return hashlib.md5(inp_str.encode()).hexdigest()

    def batch(self, inputs):
        """带缓存的批处理"""
        results = []
        uncached_inputs = []
        uncached_indices = []

        # 检查缓存
        for i, inp in enumerate(inputs):
            cache_key = self._hash_input(inp)
            if cache_key in self.cache:
                results.append(self.cache[cache_key])
                self.cache_hits += 1
            else:
                results.append(None)
                uncached_inputs.append(inp)
                uncached_indices.append(i)
                self.cache_misses += 1

        # 批处理未缓存的输入
        if uncached_inputs:
            uncached_results = self.chain.batch(
                uncached_inputs,
                config={"max_concurrency": 10}
            )

            # 更新结果和缓存
            for idx, result in zip(uncached_indices, uncached_results):
                results[idx] = result
                cache_key = self._hash_input(inputs[idx])
                self.cache[cache_key] = result

                # 限制缓存大小
                if len(self.cache) > self.cache_size:
                    self.cache.pop(next(iter(self.cache)))

        # 打印缓存统计
        total = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total if total > 0 else 0
        print(f"缓存命中率: {hit_rate:.2%} ({self.cache_hits}/{total})")

        return results
```

**核心价值**：
- 避免重复调用
- 降低成本
- 提升性能

---

## 技巧 6：批处理分片

### 问题场景

大批量任务（10000+）一次性处理容易失败。

### 解决方案

```python
class ChunkedBatchProcessor:
    def __init__(self, chain, chunk_size=100):
        self.chain = chain
        self.chunk_size = chunk_size

    def batch(self, inputs, max_concurrency=10):
        """分片批处理"""
        all_results = []
        total_chunks = (len(inputs) + self.chunk_size - 1) // self.chunk_size

        for chunk_idx in range(total_chunks):
            start = chunk_idx * self.chunk_size
            end = min(start + self.chunk_size, len(inputs))
            chunk = inputs[start:end]

            print(f"处理分片 {chunk_idx + 1}/{total_chunks} "
                  f"({len(chunk)} 个任务)")

            try:
                chunk_results = self.chain.batch(
                    chunk,
                    config={"max_concurrency": max_concurrency}
                )
                all_results.extend(chunk_results)

            except Exception as e:
                print(f"分片 {chunk_idx + 1} 失败: {e}")
                # 降级：逐个处理
                for inp in chunk:
                    try:
                        result = self.chain.invoke(inp)
                        all_results.append(result)
                    except:
                        all_results.append(None)

            # 分片间休息，避免限流
            if chunk_idx < total_chunks - 1:
                time.sleep(1)

        return all_results
```

**核心价值**：
- 降低失败风险
- 提供进度反馈
- 避免限流

---

## 技巧 7：优先级队列

### 问题场景

批处理中有些任务更重要，需要优先处理。

### 解决方案

```python
import heapq
from dataclasses import dataclass, field
from typing import Any

@dataclass(order=True)
class PriorityTask:
    priority: int
    index: int = field(compare=False)
    input: Any = field(compare=False)

class PriorityBatchProcessor:
    def __init__(self, chain):
        self.chain = chain

    def batch(self, inputs, priorities=None, max_concurrency=10):
        """优先级批处理"""
        if priorities is None:
            priorities = [0] * len(inputs)

        # 创建优先级队列（最小堆，所以用负数）
        queue = [
            PriorityTask(-priority, i, inp)
            for i, (inp, priority) in enumerate(zip(inputs, priorities))
        ]
        heapq.heapify(queue)

        results = [None] * len(inputs)

        while queue:
            # 取出最高优先级的任务
            batch = []
            batch_tasks = []

            for _ in range(min(max_concurrency, len(queue))):
                if queue:
                    task = heapq.heappop(queue)
                    batch.append(task.input)
                    batch_tasks.append(task)

            # 批处理
            try:
                batch_results = self.chain.batch(
                    batch,
                    config={"max_concurrency": max_concurrency}
                )

                for task, result in zip(batch_tasks, batch_results):
                    results[task.index] = result

            except Exception as e:
                print(f"批处理失败: {e}")
                # 重新加入队列
                for task in batch_tasks:
                    heapq.heappush(queue, task)

        return results

# 使用示例
processor = PriorityBatchProcessor(chain)
results = processor.batch(
    inputs,
    priorities=[1, 5, 3, 2, 4],  # 5 最高优先级
    max_concurrency=10
)
```

**核心价值**：
- 优先处理重要任务
- 提升用户体验
- 资源优化分配

---

## 技巧 8：批处理断点续传

### 问题场景

长时间批处理中断后，需要从断点继续。

### 解决方案

```python
import pickle
import os

class ResumableBatchProcessor:
    def __init__(self, chain, checkpoint_file="batch_checkpoint.pkl"):
        self.chain = chain
        self.checkpoint_file = checkpoint_file

    def save_checkpoint(self, results, processed_count):
        """保存检查点"""
        with open(self.checkpoint_file, "wb") as f:
            pickle.dump({
                "results": results,
                "processed_count": processed_count
            }, f)

    def load_checkpoint(self):
        """加载检查点"""
        if os.path.exists(self.checkpoint_file):
            with open(self.checkpoint_file, "rb") as f:
                return pickle.load(f)
        return None

    def batch(self, inputs, max_concurrency=10, checkpoint_interval=10):
        """支持断点续传的批处理"""
        # 尝试加载检查点
        checkpoint = self.load_checkpoint()
        if checkpoint:
            results = checkpoint["results"]
            start_idx = checkpoint["processed_count"]
            print(f"从检查点恢复，已处理: {start_idx}/{len(inputs)}")
        else:
            results = []
            start_idx = 0

        # 继续处理
        for i in range(start_idx, len(inputs), max_concurrency):
            batch = inputs[i:i + max_concurrency]

            try:
                batch_results = self.chain.batch(
                    batch,
                    config={"max_concurrency": max_concurrency}
                )
                results.extend(batch_results)

                # 定期保存检查点
                if (i + len(batch)) % checkpoint_interval == 0:
                    self.save_checkpoint(results, i + len(batch))
                    print(f"检查点已保存: {i + len(batch)}/{len(inputs)}")

            except Exception as e:
                print(f"批处理失败: {e}")
                self.save_checkpoint(results, i)
                raise

        # 清理检查点
        if os.path.exists(self.checkpoint_file):
            os.remove(self.checkpoint_file)

        return results
```

**核心价值**：
- 避免重复处理
- 提高可靠性
- 节省成本

---

## 技巧 9：批处理性能分析

### 问题场景

批处理性能不佳，需要找出瓶颈。

### 解决方案

```python
import time
from collections import defaultdict

class ProfilingBatchProcessor:
    def __init__(self, chain):
        self.chain = chain
        self.metrics = defaultdict(list)

    def batch(self, inputs, max_concurrency=10):
        """带性能分析的批处理"""
        overall_start = time.time()

        results = []
        for i in range(0, len(inputs), max_concurrency):
            batch = inputs[i:i + max_concurrency]

            # 测量批处理时间
            batch_start = time.time()
            batch_results = self.chain.batch(
                batch,
                config={"max_concurrency": max_concurrency}
            )
            batch_duration = time.time() - batch_start

            results.extend(batch_results)

            # 记录指标
            self.metrics["batch_size"].append(len(batch))
            self.metrics["batch_duration"].append(batch_duration)
            self.metrics["throughput"].append(len(batch) / batch_duration)

        overall_duration = time.time() - overall_start

        # 打印性能报告
        self.print_report(len(inputs), overall_duration)

        return results

    def print_report(self, total_inputs, total_duration):
        """打印性能报告"""
        print("\n=== 性能分析报告 ===")
        print(f"总任务数: {total_inputs}")
        print(f"总时间: {total_duration:.2f}秒")
        print(f"平均吞吐量: {total_inputs / total_duration:.2f} 请求/秒")

        avg_batch_duration = sum(self.metrics["batch_duration"]) / len(self.metrics["batch_duration"])
        print(f"平均批处理时间: {avg_batch_duration:.2f}秒")

        avg_throughput = sum(self.metrics["throughput"]) / len(self.metrics["throughput"])
        print(f"平均批处理吞吐量: {avg_throughput:.2f} 请求/秒")
```

**核心价值**：
- 识别性能瓶颈
- 优化并发参数
- 量化改进效果

---

## 技巧 10：批处理与 langasync 集成

### 问题场景

需要在生产环境中使用 langasync 降低成本。

### 解决方案

```python
class HybridBatchProcessor:
    def __init__(self, chain):
        self.chain = chain
        self.langasync_available = False

        try:
            from langasync import wrap_chain
            self.async_chain = wrap_chain(chain)
            self.langasync_available = True
        except ImportError:
            print("langasync 未安装，使用普通批处理")

    def batch(self, inputs, use_batch_api=False, max_concurrency=10):
        """混合批处理：支持普通和批处理 API"""
        if use_batch_api and self.langasync_available:
            # 使用批处理 API（成本降低 50%）
            print("使用批处理 API（成本降低 50%）")
            job = self.async_chain.submit_batch(inputs)
            print(f"任务 ID: {job.id}")
            print("等待完成（24小时内）...")
            return job.wait()
        else:
            # 使用普通批处理（实时）
            print("使用普通批处理（实时）")
            return self.chain.batch(
                inputs,
                config={"max_concurrency": max_concurrency}
            )

# 使用示例
processor = HybridBatchProcessor(chain)

# 实时任务
results = processor.batch(inputs, use_batch_api=False)

# 离线任务（成本优化）
results = processor.batch(inputs, use_batch_api=True)
```

**核心价值**：
- 灵活选择模式
- 成本优化
- 生产环境友好

**参考来源**：[langasync GitHub](https://github.com/langasync/langasync)

---

## 总结

批处理与并发控制的10个化骨绵掌技巧：

1. **动态调整并发数** - 自适应负载变化
2. **智能重试与降级** - 区分错误类型，智能等待
3. **流式批处理** - 实时流式输出
4. **成本监控与预算控制** - 避免意外账单
5. **批处理缓存** - 避免重复调用
6. **批处理分片** - 降低失败风险
7. **优先级队列** - 优先处理重要任务
8. **批处理断点续传** - 避免重复处理
9. **批处理性能分析** - 识别性能瓶颈
10. **批处理与 langasync 集成** - 灵活选择模式

**核心原则**：
- 根据场景选择技巧
- 组合使用多个技巧
- 持续监控和优化
- 平衡性能、成本和可靠性

---

## 参考来源

1. [LangChain 最佳实践](https://www.swarnendu.de/blog/langchain-best-practices/) - 并发与性能优化
2. [LangChain 成本优化](https://python.plainenglish.io/how-to-reduce-chatgpt-api-costs-in-python-projects-02b7ebdce6f6) - 成本控制策略
3. [langasync GitHub](https://github.com/langasync/langasync) - 成本优化工具
4. [LangChain 生产实践](https://medium.com/@kasimoluwasegun/langchain-in-production-beyond-the-tutorials-e7b7f2506572) - 生产环境优化

---

**下一步**：阅读核心概念文件，深入理解 batch 方法、并发控制机制和批量优化策略
