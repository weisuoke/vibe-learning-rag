# 实战代码3：跨集群数据迁移

> 完整的跨集群数据迁移实现

---

## 场景概述

本场景演示如何在不同 Milvus 集群间迁移数据：
- 从单机版迁移到分布式集群
- 跨地域数据迁移
- 开发/测试/生产环境数据同步
- 零停机迁移方案

**适用场景：**
- 集群扩容和升级
- 多地域部署
- 环境间数据同步
- 灾难恢复

---

## 完整示例代码

### 示例1：基础跨集群迁移

```python
#!/usr/bin/env python3
"""
跨集群数据迁移基础示例
"""

from pymilvus import Collection, connections, utility
import pandas as pd
import time
from typing import Dict, List
import os

class CrossClusterMigrator:
    """跨集群迁移工具"""

    def __init__(
        self,
        source_host: str,
        source_port: int,
        target_host: str,
        target_port: int
    ):
        """初始化"""
        self.source_host = source_host
        self.source_port = source_port
        self.target_host = target_host
        self.target_port = target_port

    def migrate_collection(
        self,
        collection_name: str,
        batch_size: int = 10000
    ):
        """迁移 Collection"""
        print(f"=== 跨集群迁移: {collection_name} ===")
        print(f"源: {self.source_host}:{self.source_port}")
        print(f"目标: {self.target_host}:{self.target_port}")

        # 1. 连接源和目标集群
        print("\n[1/6] 连接集群...")
        connections.connect(
            alias="source",
            host=self.source_host,
            port=self.source_port
        )
        connections.connect(
            alias="target",
            host=self.target_host,
            port=self.target_port
        )

        # 2. 获取源 Collection
        source_collection = Collection(collection_name, using="source")
        source_collection.load()

        # 3. 在目标创建 Collection
        print("\n[2/6] 创建目标 Collection...")
        self._create_target_collection(source_collection, collection_name)

        # 4. 迁移数据
        print("\n[3/6] 迁移数据...")
        self._migrate_data(source_collection, collection_name, batch_size)

        # 5. 创建索引
        print("\n[4/6] 创建索引...")
        self._create_indexes(source_collection, collection_name)

        # 6. 验证数据
        print("\n[5/6] 验证数据...")
        if not self._verify_migration(collection_name):
            raise Exception("数据验证失败")

        # 7. 加载到内存
        print("\n[6/6] 加载 Collection...")
        target_collection = Collection(collection_name, using="target")
        target_collection.load()

        print(f"\n✅ 迁移完成")
        print(f"  源数据量: {source_collection.num_entities}")
        print(f"  目标数据量: {target_collection.num_entities}")

    def _create_target_collection(
        self,
        source_collection: Collection,
        collection_name: str
    ):
        """在目标集群创建 Collection"""
        # 检查是否已存在
        if utility.has_collection(collection_name, using="target"):
            print(f"  目标 Collection 已存在，跳过创建")
            return

        # 复制 Schema
        schema = source_collection.schema
        target_collection = Collection(
            collection_name,
            schema,
            using="target"
        )

        print(f"  ✅ 目标 Collection 已创建")

    def _migrate_data(
        self,
        source_collection: Collection,
        collection_name: str,
        batch_size: int
    ):
        """迁移数据"""
        total = source_collection.num_entities
        target_collection = Collection(collection_name, using="target")

        print(f"  迁移 {total} 条数据...")

        for offset in range(0, total, batch_size):
            # 从源读取
            batch = source_collection.query(
                expr="id >= 0",
                limit=batch_size,
                offset=offset,
                output_fields=["*"]
            )

            # 写入目标
            if batch:
                target_collection.insert(batch)

            progress = min(offset + batch_size, total)
            print(f"    进度: {progress}/{total} ({progress*100//total}%)")

        # 刷新
        target_collection.flush()
        print(f"  ✅ 数据迁移完成")

    def _create_indexes(
        self,
        source_collection: Collection,
        collection_name: str
    ):
        """创建索引"""
        target_collection = Collection(collection_name, using="target")

        # 复制所有索引
        for field in source_collection.schema.fields:
            if field.dtype in [DataType.FLOAT_VECTOR, DataType.BINARY_VECTOR]:
                try:
                    source_index = source_collection.index(field.name)
                    if source_index:
                        print(f"  创建索引: {field.name}")
                        target_collection.create_index(
                            field.name,
                            source_index.params
                        )
                except:
                    pass

        print(f"  ✅ 索引创建完成")

    def _verify_migration(self, collection_name: str) -> bool:
        """验证迁移"""
        source = Collection(collection_name, using="source")
        target = Collection(collection_name, using="target")

        # 验证数据量
        if source.num_entities != target.num_entities:
            print(f"  ❌ 数据量不一致: {source.num_entities} vs {target.num_entities}")
            return False

        # 抽样验证
        sample_size = min(100, source.num_entities)
        sample_ids = list(range(sample_size))

        source_data = source.query(
            expr=f"id in {sample_ids}",
            output_fields=["*"]
        )
        target_data = target.query(
            expr=f"id in {sample_ids}",
            output_fields=["*"]
        )

        if len(source_data) != len(target_data):
            print(f"  ❌ 抽样数据不一致")
            return False

        print(f"  ✅ 数据验证通过")
        return True


def main():
    """主函数"""
    migrator = CrossClusterMigrator(
        source_host="source-milvus.internal",
        source_port=19530,
        target_host="target-milvus.internal",
        target_port=19530
    )

    # 迁移 Collection
    migrator.migrate_collection(
        collection_name="my_collection",
        batch_size=10000
    )


if __name__ == "__main__":
    main()
```

**运行示例：**

```bash
python cross_cluster_migration.py
```

**输出：**

```
=== 跨集群迁移: my_collection ===
源: source-milvus.internal:19530
目标: target-milvus.internal:19530

[1/6] 连接集群...

[2/6] 创建目标 Collection...
  ✅ 目标 Collection 已创建

[3/6] 迁移数据...
  迁移 1000000 条数据...
    进度: 10000/1000000 (1%)
    进度: 20000/1000000 (2%)
    ...
    进度: 1000000/1000000 (100%)
  ✅ 数据迁移完成

[4/6] 创建索引...
  创建索引: vector
  ✅ 索引创建完成

[5/6] 验证数据...
  ✅ 数据验证通过

[6/6] 加载 Collection...

✅ 迁移完成
  源数据量: 1000000
  目标数据量: 1000000
```

---

### 示例2：零停机迁移（双写方案）

```python
#!/usr/bin/env python3
"""
零停机迁移 - 双写方案
"""

import threading
import queue
from typing import Dict, List, Any
import time

class ZeroDowntimeMigrator:
    """零停机迁移工具"""

    def __init__(
        self,
        source_host: str,
        target_host: str
    ):
        """初始化"""
        self.source_host = source_host
        self.target_host = target_host
        self.dual_write_enabled = False
        self.write_queue = queue.Queue()
        self.failed_writes = []

    def migrate_with_zero_downtime(
        self,
        collection_name: str
    ):
        """零停机迁移"""
        print(f"=== 零停机迁移: {collection_name} ===")

        # 1. 全量迁移历史数据
        print("\n[1/6] 全量迁移历史数据...")
        start_time = time.time()
        self._full_migration(collection_name)

        # 2. 启动双写
        print("\n[2/6] 启动双写...")
        self.enable_dual_write()

        # 3. 同步增量数据
        print("\n[3/6] 同步增量数据...")
        self._sync_incremental(collection_name, start_time)

        # 4. 验证数据一致性
        print("\n[4/6] 验证数据一致性...")
        self._verify_consistency(collection_name)

        # 5. 灰度切换读流量
        print("\n[5/6] 灰度切换读流量...")
        self._gradual_switch_reads()

        # 6. 停止双写
        print("\n[6/6] 停止双写...")
        self.disable_dual_write()

        print(f"\n✅ 零停机迁移完成")

    def _full_migration(self, collection_name: str):
        """全量迁移"""
        migrator = CrossClusterMigrator(
            source_host=self.source_host,
            source_port=19530,
            target_host=self.target_host,
            target_port=19530
        )

        migrator.migrate_collection(collection_name)

    def enable_dual_write(self):
        """启动双写"""
        self.dual_write_enabled = True

        # 启动后台线程处理写入队列
        self.write_thread = threading.Thread(
            target=self._process_write_queue,
            daemon=True
        )
        self.write_thread.start()

        print("  ✅ 双写已启动")

    def disable_dual_write(self):
        """停止双写"""
        self.dual_write_enabled = False

        # 等待队列清空
        self.write_queue.join()

        print("  ✅ 双写已停止")

    def insert_with_dual_write(
        self,
        collection_name: str,
        data: List[Dict[str, Any]]
    ):
        """双写插入"""
        if not self.dual_write_enabled:
            # 只写入源
            self._insert_to_source(collection_name, data)
        else:
            # 加入写入队列
            self.write_queue.put({
                "collection": collection_name,
                "data": data,
                "timestamp": time.time()
            })

    def _process_write_queue(self):
        """处理写入队列"""
        while True:
            try:
                item = self.write_queue.get(timeout=1)

                # 同时写入源和目标
                try:
                    self._insert_to_source(
                        item["collection"],
                        item["data"]
                    )
                    self._insert_to_target(
                        item["collection"],
                        item["data"]
                    )
                except Exception as e:
                    print(f"  ⚠️ 双写失败: {e}")
                    self.failed_writes.append(item)

                self.write_queue.task_done()

            except queue.Empty:
                if not self.dual_write_enabled:
                    break

    def _insert_to_source(self, collection_name: str, data: List[Dict]):
        """写入源集群"""
        connections.connect(alias="source", host=self.source_host)
        collection = Collection(collection_name, using="source")
        collection.insert(data)

    def _insert_to_target(self, collection_name: str, data: List[Dict]):
        """写入目标集群"""
        connections.connect(alias="target", host=self.target_host)
        collection = Collection(collection_name, using="target")
        collection.insert(data)

    def _sync_incremental(
        self,
        collection_name: str,
        since_time: float
    ):
        """同步增量数据"""
        connections.connect(alias="source", host=self.source_host)
        connections.connect(alias="target", host=self.target_host)

        source = Collection(collection_name, using="source")
        target = Collection(collection_name, using="target")

        # 查询增量数据
        incremental_data = source.query(
            expr=f"timestamp > {since_time}",
            output_fields=["*"]
        )

        if incremental_data:
            print(f"  同步 {len(incremental_data)} 条增量数据")
            target.insert(incremental_data)
            target.flush()

    def _verify_consistency(self, collection_name: str):
        """验证数据一致性"""
        connections.connect(alias="source", host=self.source_host)
        connections.connect(alias="target", host=self.target_host)

        source = Collection(collection_name, using="source")
        target = Collection(collection_name, using="target")

        # 多次验证
        for i in range(5):
            if source.num_entities == target.num_entities:
                print(f"  ✅ 数据一致 ({source.num_entities} 条)")
                return
            else:
                print(f"  ⚠️ 数据量不一致: {source.num_entities} vs {target.num_entities}")
                time.sleep(2)

        raise Exception("数据一致性验证失败")

    def _gradual_switch_reads(self):
        """灰度切换读流量"""
        # 10% → 50% → 100%
        for ratio in [0.1, 0.5, 1.0]:
            print(f"  切换 {ratio*100:.0f}% 读流量到目标...")

            # 这里需要配合负载均衡器实现
            # 示例：更新 Nginx 配置
            self._update_load_balancer(ratio)

            # 等待观察
            time.sleep(300)  # 5分钟

            # 检查错误率
            error_rate = self._get_error_rate()
            if error_rate > 0.01:  # 错误率 > 1%
                print(f"  ❌ 错误率过高: {error_rate*100:.2f}%")
                self._update_load_balancer(0)  # 回滚
                raise Exception("灰度切换失败")

        print(f"  ✅ 读流量已完全切换")

    def _update_load_balancer(self, target_ratio: float):
        """更新负载均衡器配置"""
        # 示例：更新 Nginx upstream 权重
        # upstream milvus {
        #     server source-milvus.internal:19530 weight=90;
        #     server target-milvus.internal:19530 weight=10;
        # }
        pass

    def _get_error_rate(self) -> float:
        """获取错误率"""
        # 从监控系统获取错误率
        return 0.001  # 示例值


def main():
    """主函数"""
    migrator = ZeroDowntimeMigrator(
        source_host="source-milvus.internal",
        target_host="target-milvus.internal"
    )

    # 零停机迁移
    migrator.migrate_with_zero_downtime(
        collection_name="my_collection"
    )


if __name__ == "__main__":
    main()
```

---

### 示例3：并行迁移多个 Collection

```python
#!/usr/bin/env python3
"""
并行迁移多个 Collection
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List
import time

class ParallelMigrator:
    """并行迁移工具"""

    def __init__(
        self,
        source_host: str,
        target_host: str,
        max_workers: int = 4
    ):
        """初始化"""
        self.source_host = source_host
        self.target_host = target_host
        self.max_workers = max_workers

    def migrate_multiple_collections(
        self,
        collections: List[str]
    ):
        """并行迁移多个 Collection"""
        print(f"=== 并行迁移 {len(collections)} 个 Collection ===")
        print(f"并发数: {self.max_workers}")

        start_time = time.time()
        results = {}

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # 提交所有任务
            futures = {
                executor.submit(
                    self._migrate_single_collection,
                    collection
                ): collection
                for collection in collections
            }

            # 等待完成
            for future in as_completed(futures):
                collection = futures[future]
                try:
                    result = future.result()
                    results[collection] = {
                        "status": "success",
                        "entities": result["entities"],
                        "duration": result["duration"]
                    }
                    print(f"✅ {collection}: {result['entities']} 条数据, {result['duration']:.1f}秒")
                except Exception as e:
                    results[collection] = {
                        "status": "failed",
                        "error": str(e)
                    }
                    print(f"❌ {collection}: {e}")

        total_time = time.time() - start_time

        # 统计
        success_count = sum(1 for r in results.values() if r["status"] == "success")
        failed_count = len(results) - success_count

        print(f"\n=== 迁移完成 ===")
        print(f"总耗时: {total_time:.1f} 秒")
        print(f"成功: {success_count} 个")
        print(f"失败: {failed_count} 个")

        return results

    def _migrate_single_collection(self, collection_name: str) -> Dict:
        """迁移单个 Collection"""
        start_time = time.time()

        migrator = CrossClusterMigrator(
            source_host=self.source_host,
            source_port=19530,
            target_host=self.target_host,
            target_port=19530
        )

        migrator.migrate_collection(collection_name)

        # 获取数据量
        connections.connect(alias="target", host=self.target_host)
        collection = Collection(collection_name, using="target")
        entities = collection.num_entities

        duration = time.time() - start_time

        return {
            "entities": entities,
            "duration": duration
        }


def main():
    """主函数"""
    migrator = ParallelMigrator(
        source_host="source-milvus.internal",
        target_host="target-milvus.internal",
        max_workers=4
    )

    # 并行迁移多个 Collection
    collections = [
        "collection1",
        "collection2",
        "collection3",
        "collection4",
        "collection5"
    ]

    results = migrator.migrate_multiple_collections(collections)


if __name__ == "__main__":
    main()
```

**输出：**

```
=== 并行迁移 5 个 Collection ===
并发数: 4
✅ collection1: 1000000 条数据, 120.5秒
✅ collection2: 500000 条数据, 65.3秒
✅ collection3: 2000000 条数据, 240.1秒
✅ collection4: 800000 条数据, 95.7秒
✅ collection5: 1500000 条数据, 180.2秒

=== 迁移完成 ===
总耗时: 245.8 秒
成功: 5 个
失败: 0 个
```

---

### 示例4：跨地域迁移（带压缩传输）

```python
#!/usr/bin/env python3
"""
跨地域迁移 - 带压缩传输
"""

import gzip
import json
import subprocess
from pathlib import Path

class CrossRegionMigrator:
    """跨地域迁移工具"""

    def __init__(
        self,
        source_host: str,
        target_host: str,
        temp_dir: str = "/tmp/migration"
    ):
        """初始化"""
        self.source_host = source_host
        self.target_host = target_host
        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(parents=True, exist_ok=True)

    def migrate_cross_region(
        self,
        collection_name: str
    ):
        """跨地域迁移"""
        print(f"=== 跨地域迁移: {collection_name} ===")

        # 1. 导出数据
        print("\n[1/5] 导出数据...")
        export_dir = self._export_collection(collection_name)

        # 2. 压缩数据
        print("\n[2/5] 压缩数据...")
        archive_file = self._compress_data(export_dir)

        # 3. 传输到目标地域
        print("\n[3/5] 传输数据...")
        self._transfer_to_target(archive_file)

        # 4. 在目标地域解压
        print("\n[4/5] 解压数据...")
        self._decompress_on_target(archive_file)

        # 5. 导入数据
        print("\n[5/5] 导入数据...")
        self._import_on_target(collection_name)

        print(f"\n✅ 跨地域迁移完成")

    def _export_collection(self, collection_name: str) -> Path:
        """导出 Collection"""
        export_dir = self.temp_dir / collection_name

        # 使用 CollectionMigrator 导出
        from collection_migration import CollectionMigrator

        migrator = CollectionMigrator(source_host=self.source_host)
        migrator.export_collection(
            collection_name=collection_name,
            output_dir=str(export_dir),
            format="parquet"
        )

        return export_dir

    def _compress_data(self, export_dir: Path) -> Path:
        """压缩数据"""
        archive_file = export_dir.with_suffix(".tar.gz")

        # 使用 tar + gzip 压缩
        subprocess.run([
            "tar", "-czf",
            str(archive_file),
            "-C", str(export_dir.parent),
            export_dir.name
        ], check=True)

        # 显示压缩率
        original_size = sum(
            f.stat().st_size
            for f in export_dir.rglob("*")
            if f.is_file()
        )
        compressed_size = archive_file.stat().st_size
        ratio = (1 - compressed_size / original_size) * 100

        print(f"  原始大小: {original_size / 1024 / 1024:.2f} MB")
        print(f"  压缩后: {compressed_size / 1024 / 1024:.2f} MB")
        print(f"  压缩率: {ratio:.1f}%")

        return archive_file

    def _transfer_to_target(self, archive_file: Path):
        """传输到目标地域"""
        # 使用 rsync 传输（支持断点续传）
        subprocess.run([
            "rsync",
            "-avz",
            "--progress",
            str(archive_file),
            f"{self.target_host}:/tmp/"
        ], check=True)

        print(f"  ✅ 传输完成")

    def _decompress_on_target(self, archive_file: Path):
        """在目标地域解压"""
        remote_file = f"/tmp/{archive_file.name}"

        subprocess.run([
            "ssh", self.target_host,
            f"tar -xzf {remote_file} -C /tmp/"
        ], check=True)

        print(f"  ✅ 解压完成")

    def _import_on_target(self, collection_name: str):
        """在目标地域导入"""
        # 在目标服务器上执行导入脚本
        subprocess.run([
            "ssh", self.target_host,
            f"python3 /opt/scripts/import_collection.py {collection_name}"
        ], check=True)

        print(f"  ✅ 导入完成")


def main():
    """主函数"""
    migrator = CrossRegionMigrator(
        source_host="us-west-milvus.internal",
        target_host="eu-central-milvus.internal"
    )

    # 跨地域迁移
    migrator.migrate_cross_region(
        collection_name="my_collection"
    )


if __name__ == "__main__":
    main()
```

---

## 总结

### 核心要点

1. **基础迁移**：适合小规模、可停机的场景
2. **零停机迁移**：使用双写方案，适合生产环境
3. **并行迁移**：提高多 Collection 迁移效率
4. **跨地域迁移**：使用压缩传输，节省带宽

### 适用场景

- ✅ 集群扩容和升级
- ✅ 多地域部署
- ✅ 环境间数据同步
- ✅ 灾难恢复

### 下一步

- 学习 [自动化备份系统](./07_实战代码_04_自动化备份系统.md)
- 回顾 [核心概念](./03_核心概念_01_Milvus_Backup工具.md)
