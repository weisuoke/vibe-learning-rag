# 核心概念 1：哈希函数

## 什么是哈希函数？

**哈希函数是将任意大小的数据映射到固定大小值的函数。**

```
hash: 任意数据 → 固定大小的整数
```

### 直觉理解

**哈希函数就像身份证号码生成规则：**
- 输入：姓名、出生日期、地址等信息（任意长度）
- 输出：18位身份证号（固定长度）
- 特点：同一个人总是得到相同的号码

---

## 三个核心特性

### 特性 1：确定性（Deterministic）

**同一输入必须总是产生同一输出。**

```python
# 正确：确定性
def good_hash(s: str) -> int:
    h = 0
    for char in s:
        h = (h * 31 + ord(char))
    return h

print(good_hash("hello"))  # 99162322
print(good_hash("hello"))  # 99162322  <- 总是相同

# 错误：非确定性
import random
def bad_hash(s: str) -> int:
    return random.randint(0, 1000000)

print(bad_hash("hello"))  # 456789
print(bad_hash("hello"))  # 123456  <- 每次不同！
```

**为什么重要？**
- 插入时计算哈希值存储
- 查找时必须能重新计算相同哈希值
- 否则永远找不到数据

---

### 特性 2：均匀分布（Uniform Distribution）

**不同输入应该均匀分布在输出空间。**

```python
# 坏的哈希函数：只看第一个字符
def bad_hash(s: str, m: int) -> int:
    if not s:
        return 0
    return ord(s[0]) % m

# 测试分布
words = ["apple", "ant", "amazing", "banana", "bear", "cat", "car"]
buckets = [0] * 10

for word in words:
    index = bad_hash(word, 10)
    buckets[index] += 1

print("分布:", buckets)
# 输出：[0, 0, 0, 0, 0, 0, 0, 7, 2, 0]  <- 极不均匀！

# 好的哈希函数：考虑所有字符
def good_hash(s: str, m: int) -> int:
    h = 0
    for char in s:
        h = (h * 31 + ord(char)) % m
    return h

buckets = [0] * 10
for word in words:
    index = good_hash(word, 10)
    buckets[index] += 1

print("分布:", buckets)
# 输出：[1, 1, 1, 1, 1, 1, 0, 0, 1, 0]  <- 更均匀
```

**为什么重要？**
- 不均匀分布导致冲突集中
- 冲突多 → 性能退化
- 均匀分布 → 保持 O(1) 性能

---

### 特性 3：雪崩效应（Avalanche Effect）

**输入的微小变化应该导致输出的巨大变化。**

```python
def avalanche_hash(s: str) -> int:
    h = 0
    for char in s:
        h = (h * 31 + ord(char))
    return h

# 测试雪崩效应
print(f"hello:  {avalanche_hash('hello'):020d}")
print(f"hallo:  {avalanche_hash('hallo'):020d}")
print(f"相差:   {abs(avalanche_hash('hello') - avalanche_hash('hallo')):020d}")

# 输出：
# hello:  00000000099162322
# hallo:  00000000099159042
# 相差:   00000000000003280  <- 只改一个字母，哈希值差异很大
```

**为什么重要？**
- 相似的键应该分散到不同位置
- 避免局部聚集
- 提高缓存效率

---

## 常见哈希函数

### 1. 简单求和（不推荐）

```python
def sum_hash(s: str, m: int) -> int:
    """最简单但最差的哈希函数"""
    return sum(ord(c) for c in s) % m

# 问题：字母顺序不影响结果
print(sum_hash("abc", 100))  # 294 % 100 = 94
print(sum_hash("bca", 100))  # 294 % 100 = 94  <- 冲突！
print(sum_hash("cab", 100))  # 294 % 100 = 94  <- 冲突！
```

**缺点：**
- 不考虑字符顺序
- 容易产生冲突
- 分布不均匀

---

### 2. 多项式滚动哈希（推荐）

```python
def polynomial_hash(s: str, m: int, p: int = 31) -> int:
    """
    多项式滚动哈希
    h = (s[0] * p^(n-1) + s[1] * p^(n-2) + ... + s[n-1]) % m
    """
    h = 0
    for char in s:
        h = (h * p + ord(char)) % m
    return h

# 测试
print(polynomial_hash("abc", 100))  # 96
print(polynomial_hash("bca", 100))  # 67  <- 不同
print(polynomial_hash("cab", 100))  # 42  <- 不同
```

**优点：**
- 考虑字符顺序
- 分布均匀
- 计算高效

**为什么选择 31？**
- 质数，减少冲突
- 2^5 - 1，编译器可优化为位运算
- Java String.hashCode() 使用 31

---

### 3. Python 内置 hash()

```python
# Python 使用 SipHash（安全哈希）
print(hash("hello"))  # 每次运行可能不同（加盐）
print(hash("world"))

# 对于字符串，Python 3.3+ 使用随机种子
# 防止哈希碰撞攻击
import sys
print(f"Hash seed: {sys.hash_info.hash_bits}")
```

**特点：**
- 安全性高（防止 DoS 攻击）
- 每次运行结果可能不同（随机种子）
- 不适合持久化存储

---

### 4. 加密哈希（MD5, SHA-256）

```python
import hashlib

def crypto_hash(s: str) -> str:
    """加密哈希：不可逆，安全性高"""
    return hashlib.sha256(s.encode()).hexdigest()

print(crypto_hash("hello"))
# 输出：2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824

print(crypto_hash("hallo"))
# 输出：d3751d33f9cd5049c4af2b462735457e4d3baf130bcbb87f389e349fbaeb20b9
```

**特点：**
- 不可逆（无法从哈希值还原原文）
- 雪崩效应强
- 计算开销大
- 适合密码存储、数字签名

**何时使用：**
- ✅ 密码存储
- ✅ 文件完整性校验
- ✅ 数字签名
- ❌ 哈希表（太慢）

---

## 哈希函数的设计原则

### 原则 1：快速计算

```python
import time

# 快速哈希
def fast_hash(s: str, m: int) -> int:
    h = 0
    for char in s:
        h = (h * 31 + ord(char)) % m
    return h

# 慢速哈希（加密哈希）
def slow_hash(s: str, m: int) -> int:
    import hashlib
    return int(hashlib.sha256(s.encode()).hexdigest(), 16) % m

# 性能测试
text = "hello world" * 100
iterations = 10000

start = time.time()
for _ in range(iterations):
    fast_hash(text, 1000)
fast_time = time.time() - start

start = time.time()
for _ in range(iterations):
    slow_hash(text, 1000)
slow_time = time.time() - start

print(f"快速哈希: {fast_time:.4f}s")
print(f"慢速哈希: {slow_time:.4f}s")
print(f"速度差异: {slow_time / fast_time:.1f}x")

# 典型输出：
# 快速哈希: 0.0234s
# 慢速哈希: 0.8765s
# 速度差异: 37.5x
```

---

### 原则 2：低冲突率

```python
def collision_rate(hash_func, data, m):
    """计算冲突率"""
    hashes = [hash_func(item, m) for item in data]
    unique_hashes = len(set(hashes))
    collision_rate = 1 - (unique_hashes / len(data))
    return collision_rate

# 测试数据
words = [f"word_{i}" for i in range(1000)]

# 坏的哈希函数
bad_rate = collision_rate(lambda s, m: ord(s[0]) % m, words, 100)
print(f"坏哈希冲突率: {bad_rate:.2%}")  # ~90%

# 好的哈希函数
good_rate = collision_rate(polynomial_hash, words, 100)
print(f"好哈希冲突率: {good_rate:.2%}")  # ~10%
```

---

### 原则 3：适应数据特征

```python
# 针对整数的哈希
def int_hash(n: int, m: int) -> int:
    """整数哈希：简单取模"""
    return n % m

# 针对字符串的哈希
def str_hash(s: str, m: int) -> int:
    """字符串哈希：多项式滚动"""
    h = 0
    for char in s:
        h = (h * 31 + ord(char)) % m
    return h

# 针对元组的哈希
def tuple_hash(t: tuple, m: int) -> int:
    """元组哈希：组合各元素哈希"""
    h = 0
    for item in t:
        if isinstance(item, int):
            h = (h * 31 + item) % m
        elif isinstance(item, str):
            h = (h * 31 + str_hash(item, m)) % m
    return h

# 测试
print(int_hash(12345, 100))  # 45
print(str_hash("hello", 100))  # 22
print(tuple_hash((123, "abc"), 100))  # 67
```

---

## 2026 AI Agent 应用

### 应用 1：Token 映射

```python
class TokenMapper:
    """LLM Token 映射系统"""
    def __init__(self, vocab_size=50000):
        self.vocab_size = vocab_size
        self.token_to_id = {}
        self.id_to_token = {}
        self.next_id = 0

    def _hash(self, token: str) -> int:
        """哈希函数：Token → 初始位置"""
        h = 0
        for char in token:
            h = (h * 31 + ord(char)) % self.vocab_size
        return h

    def add_token(self, token: str) -> int:
        """添加 Token，返回 ID"""
        if token in self.token_to_id:
            return self.token_to_id[token]

        token_id = self.next_id
        self.token_to_id[token] = token_id
        self.id_to_token[token_id] = token
        self.next_id += 1
        return token_id

    def encode(self, text: str) -> list[int]:
        """文本 → Token IDs"""
        tokens = text.split()  # 简化：按空格分词
        return [self.add_token(token) for token in tokens]

    def decode(self, ids: list[int]) -> str:
        """Token IDs → 文本"""
        tokens = [self.id_to_token[id] for id in ids]
        return " ".join(tokens)

# 使用
tokenizer = TokenMapper()
text = "Hello world from AI Agent"
ids = tokenizer.encode(text)
print(f"Token IDs: {ids}")  # [0, 1, 2, 3, 4]
print(f"Decoded: {tokenizer.decode(ids)}")  # "Hello world from AI Agent"
```

---

### 应用 2：语义哈希（LSH）

```python
import numpy as np

class LocalitySensitiveHash:
    """局部敏感哈希：相似输入 → 相似哈希"""
    def __init__(self, num_hashes=10, dim=128):
        self.num_hashes = num_hashes
        self.dim = dim
        # 随机投影向量
        self.projections = [
            np.random.randn(dim) for _ in range(num_hashes)
        ]

    def hash(self, vector: np.ndarray) -> tuple:
        """向量 → 哈希签名"""
        signature = []
        for proj in self.projections:
            # 投影到随机向量，取符号
            bit = 1 if np.dot(vector, proj) > 0 else 0
            signature.append(bit)
        return tuple(signature)

    def similarity(self, hash1: tuple, hash2: tuple) -> float:
        """计算哈希相似度"""
        matches = sum(b1 == b2 for b1, b2 in zip(hash1, hash2))
        return matches / len(hash1)

# 使用
lsh = LocalitySensitiveHash(num_hashes=10, dim=128)

# 相似向量
v1 = np.random.randn(128)
v2 = v1 + np.random.randn(128) * 0.1  # 加小噪声
v3 = np.random.randn(128)  # 完全不同

h1 = lsh.hash(v1)
h2 = lsh.hash(v2)
h3 = lsh.hash(v3)

print(f"v1 vs v2 相似度: {lsh.similarity(h1, h2):.2f}")  # ~0.8
print(f"v1 vs v3 相似度: {lsh.similarity(h1, h3):.2f}")  # ~0.5
```

**2026 应用场景：**
- **语义缓存**：相似查询命中缓存
- **文档去重**：快速检测相似文档
- **向量检索**：加速最近邻搜索

---

### 应用 3：Spotlight Attention 非线性哈希

```python
class SpotlightHash:
    """
    Spotlight Attention (NeurIPS 2025)
    非线性哈希用于 KV Cache 检索
    """
    def __init__(self, hash_bits=32):
        self.hash_bits = hash_bits

    def nonlinear_hash(self, key_vector: np.ndarray) -> int:
        """
        非线性哈希：5x 更短的哈希码
        使用 tanh 激活函数增强非线性
        """
        # 简化版：使用 tanh 压缩
        compressed = np.tanh(key_vector)
        # 量化到整数
        quantized = (compressed * 1000).astype(int)
        # 组合为哈希码
        h = 0
        for val in quantized[:self.hash_bits]:
            h = (h * 31 + int(val)) % (2**32)
        return h

    def store_kv(self, key: np.ndarray, value: np.ndarray, cache: dict):
        """存储 KV 对"""
        h = self.nonlinear_hash(key)
        cache[h] = (key, value)

    def retrieve_kv(self, query: np.ndarray, cache: dict):
        """检索 KV 对"""
        h = self.nonlinear_hash(query)
        return cache.get(h)

# 使用
spotlight = SpotlightHash(hash_bits=32)
kv_cache = {}

# 存储
key = np.random.randn(128)
value = np.random.randn(128)
spotlight.store_kv(key, value, kv_cache)

# 检索
retrieved = spotlight.retrieve_kv(key, kv_cache)
print(f"检索成功: {retrieved is not None}")  # True
```

---

## 实战练习

### 练习 1：实现自己的哈希函数

```python
def my_hash(s: str, m: int) -> int:
    """
    要求：
    1. 确定性
    2. 均匀分布
    3. 考虑字符顺序
    """
    # TODO: 实现你的哈希函数
    pass

# 测试
test_words = ["apple", "banana", "cherry", "date", "elderberry"]
hashes = [my_hash(word, 100) for word in test_words]
print(f"哈希值: {hashes}")
print(f"唯一值: {len(set(hashes))} / {len(test_words)}")
```

### 练习 2：比较不同哈希函数

```python
def compare_hash_functions():
    """比较不同哈希函数的性能"""
    import time

    # 测试数据
    words = [f"word_{i}" for i in range(10000)]
    m = 1000

    # 哈希函数列表
    hash_funcs = {
        "sum": lambda s, m: sum(ord(c) for c in s) % m,
        "polynomial": polynomial_hash,
        "python_builtin": lambda s, m: hash(s) % m,
    }

    for name, func in hash_funcs.items():
        # 计算时间
        start = time.time()
        hashes = [func(word, m) for word in words]
        elapsed = time.time() - start

        # 计算冲突率
        unique = len(set(hashes))
        collision_rate = 1 - (unique / len(words))

        print(f"{name:15s}: 时间={elapsed:.4f}s, 冲突率={collision_rate:.2%}")

compare_hash_functions()
```

---

## 核心要点总结

### 三个必须记住的特性

1. **确定性**：同一输入 → 同一输出
2. **均匀分布**：不同输入 → 均匀分散
3. **雪崩效应**：微小变化 → 巨大差异

### 设计原则

- 快速计算（避免加密哈希）
- 低冲突率（考虑字符顺序）
- 适应数据特征（整数、字符串、向量）

### 2026 AI Agent 核心应用

- **Token 映射**：文本 → Token ID（O(1) 查找）
- **语义哈希**：相似查询 → 相似哈希（缓存命中）
- **非线性哈希**：KV Cache 检索（5x 更短哈希码）

**记住：哈希函数是哈希表的灵魂，决定了性能的上限。**
