# 面试必问

如果被问到 ReRank 相关问题，怎么答出彩。

---

## 问题1："什么是 ReRank？为什么 RAG 系统需要它？"

### 普通回答（❌ 不出彩）

"ReRank 就是对检索结果重新排序，让更相关的文档排在前面。"

### 出彩回答（✅ 推荐）

> **ReRank 是 RAG 系统中的精排环节，用 Cross-Encoder 对向量检索的候选结果进行二次排序。**
>
> **为什么需要 ReRank？这要从向量检索的局限性说起：**
>
> 1. **Bi-Encoder 的先天缺陷**：向量检索使用 Bi-Encoder，Query 和 Document 分别编码，没有交互。这就像两个人各自写了一份简历，只能通过简历比较，无法当面交流。
>
> 2. **向量是压缩表示**：一段 500 字的文本压缩成 768 维向量，必然丢失细节。语义相似不等于问题相关。
>
> 3. **ANN 索引是近似搜索**：HNSW、IVF 等索引为了速度牺牲精度，排序不够精确。
>
> **ReRank 如何解决这些问题？**
>
> Cross-Encoder 把 Query 和 Document 拼接后一起编码，每个 Token 都能相互 Attention，实现深度语义交互。就像让两个人当面对话，而不是只看简历。
>
> **实际效果**：在我们的项目中，添加 ReRank 后，检索准确率从 85% 提升到 93%，Top-1 命中率提升了 15%。
>
> **权衡**：ReRank 更准但更慢，所以采用两阶段架构——先用向量检索快速召回 50-100 个候选，再用 Cross-Encoder 精排取 Top 5-10。

### 为什么这个回答出彩？

1. ✅ **有结构**：先定义，再解释原因，最后说效果
2. ✅ **有深度**：从 Bi-Encoder vs Cross-Encoder 的原理层面解释
3. ✅ **有类比**：用"简历 vs 当面交流"帮助理解
4. ✅ **有数据**：给出具体的效果提升数字
5. ✅ **有权衡**：说明了速度和精度的 trade-off

---

## 问题2："Bi-Encoder 和 Cross-Encoder 有什么区别？"

### 普通回答（❌ 不出彩）

"Bi-Encoder 分别编码，Cross-Encoder 一起编码。"

### 出彩回答（✅ 推荐）

> **核心区别在于 Query 和 Document 是否有交互：**
>
> | 对比项 | Bi-Encoder | Cross-Encoder |
> |--------|------------|---------------|
> | 编码方式 | 分别编码 | 拼接后一起编码 |
> | 交互深度 | 无交互 | Token 级 Attention |
> | 输出 | 向量 | 相关性分数 |
> | 速度 | 快（可预计算） | 慢（每次重算） |
> | 精度 | 较高 | 更高 |
>
> **为什么 Bi-Encoder 快？**
>
> Document 的向量可以离线预计算并建立索引。查询时只需要编码 Query，然后做向量检索，复杂度是 O(log N)。
>
> **为什么 Cross-Encoder 准？**
>
> 因为 Query 和 Document 的每个 Token 都能相互 Attention。比如：
> - Query: "Python 读取 JSON"
> - Document: "使用 json.load() 函数"
>
> Cross-Encoder 能理解 "读取" 和 "load" 是同一个意思，"JSON" 在两边指的是同一个东西。这种细粒度的语义匹配，Bi-Encoder 做不到。
>
> **实际应用中如何选择？**
>
> 不是二选一，而是配合使用：
> - Bi-Encoder 负责大规模召回（100 万 → 100）
> - Cross-Encoder 负责小规模精排（100 → 5）
>
> 这就是两阶段检索架构，兼顾速度和精度。

### 为什么这个回答出彩？

1. ✅ **有对比表格**：清晰展示差异
2. ✅ **解释了"为什么"**：不只说是什么，还说为什么
3. ✅ **有具体例子**：用实际的 Query-Document 对说明
4. ✅ **有实践建议**：说明了实际应用中的选择策略

---

## 问题3："ReRank 的候选数量怎么选？"

### 普通回答（❌ 不出彩）

"一般选 50-100 个。"

### 出彩回答（✅ 推荐）

> **候选数量是召回率、精度和延迟的三方权衡。**
>
> **太少的问题（如 Top 20）：**
> - 可能漏掉相关文档
> - 召回率不足，ReRank 再准也没用
> - 因为 ReRank 只能重排，不能召回
>
> **太多的问题（如 Top 500）：**
> - Cross-Encoder 是 O(N) 复杂度，延迟线性增加
> - 噪声增加，低质量文档混入
> - 边际收益递减：Top 100 → Top 500，召回率可能只提升 2-3%
>
> **我的选择策略：**
>
> ```
> 实时问答场景：Top 30-50（延迟敏感）
> 文档搜索场景：Top 50-100（平衡选择）
> 离线分析场景：Top 100-200（精度优先）
> ```
>
> **实际调优方法：**
>
> 1. 先用 Top 100 作为基准
> 2. 测量召回率和延迟
> 3. 如果召回率不足，增加候选数量
> 4. 如果延迟太高，减少候选数量或优化模型
>
> **一个经验公式：**
>
> Top-N : Top-K ≈ 10:1 到 20:1
>
> 比如最终需要 5 个结果，召回 50-100 个候选是合理的。

### 为什么这个回答出彩？

1. ✅ **说明了权衡**：不是简单给数字，而是解释背后的 trade-off
2. ✅ **分场景讨论**：不同场景有不同选择
3. ✅ **有调优方法**：给出了实际可操作的调优步骤
4. ✅ **有经验公式**：提供了可参考的比例

---

## 面试加分技巧

### 1. 主动提及实际经验

```
"在我们的项目中，添加 ReRank 后..."
"我们测试过不同的候选数量..."
"我们对比过几个 ReRank 模型..."
```

### 2. 展示对 trade-off 的理解

```
"这是一个速度和精度的权衡..."
"需要根据具体场景选择..."
"没有银弹，要看业务需求..."
```

### 3. 提及相关技术

```
"除了 Cross-Encoder，还可以用 LLM 做 ReRank..."
"可以结合 RRF（Reciprocal Rank Fusion）..."
"Cohere 和 Jina 都提供了 ReRank API..."
```

---

**下一步：** [09_化骨绵掌](./09_化骨绵掌.md) - 10 个 2 分钟知识卡片，碎片化学习 ReRank
