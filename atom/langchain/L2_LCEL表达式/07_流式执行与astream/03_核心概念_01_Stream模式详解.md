# æ ¸å¿ƒæ¦‚å¿µ 01ï¼šStream æ¨¡å¼è¯¦è§£

## æ¦‚è¿°

æ·±å…¥ç†è§£ LangChain æµå¼æ‰§è¡Œçš„ä¸‰ç§æ ¸å¿ƒæ¨¡å¼ï¼šupdatesã€messagesã€customï¼ŒæŒæ¡æ¯ç§æ¨¡å¼çš„åŸç†ã€ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µã€‚

---

## æ¨¡å¼ 1ï¼š`stream_mode="updates"` - Agent è¿›åº¦æµå¼

### æ ¸å¿ƒåŸç†

**updates æ¨¡å¼è¿½è¸ª Agent æ‰§è¡Œå›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€æ›´æ–°ã€‚**

```python
# æ‰§è¡Œæµç¨‹
Agent è¿›å…¥èŠ‚ç‚¹ A
â†“
æ‰§è¡ŒèŠ‚ç‚¹ A çš„é€»è¾‘
â†“
æ›´æ–°çŠ¶æ€ï¼ˆstate updateï¼‰
â†“
è§¦å‘æµå¼å›è°ƒï¼šyield {node_name: state_update}
â†“
ç”¨æˆ·æ¥æ”¶åˆ°æ›´æ–°
â†“
é‡å¤ç›´åˆ°æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œå®Œæ¯•
```

---

### è¿”å›æ•°æ®ç»“æ„

```python
# å•ä¸ª chunk çš„ç»“æ„
{
    "node_name": {
        "messages": [AIMessage(...), ToolMessage(...), ...],
        "other_state_keys": ...
    }
}
```

**å…³é”®å­—æ®µ**ï¼š
- `node_name`: æ‰§è¡Œçš„èŠ‚ç‚¹åç§°ï¼ˆå¦‚ "model", "tools", "custom_node"ï¼‰
- `messages`: è¯¥èŠ‚ç‚¹äº§ç”Ÿçš„æ¶ˆæ¯åˆ—è¡¨
- å…¶ä»–çŠ¶æ€é”®ï¼šæ ¹æ® Agent çš„çŠ¶æ€å®šä¹‰

---

### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”"""
    return f"{city}çš„å¤©æ°”æ˜¯æ™´å¤©"

agent = create_agent(
    model="gpt-4o-mini",
    tools=[get_weather]
)

# æµå¼è¿½è¸ª Agent æ‰§è¡Œ
for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]},
    stream_mode="updates"
):
    for node_name, data in chunk.items():
        print(f"\n[èŠ‚ç‚¹: {node_name}]")
        if "messages" in data:
            last_msg = data["messages"][-1]
            print(f"æ¶ˆæ¯ç±»å‹: {type(last_msg).__name__}")
            if hasattr(last_msg, 'content'):
                print(f"å†…å®¹: {last_msg.content[:100]}")
            if hasattr(last_msg, 'tool_calls'):
                print(f"å·¥å…·è°ƒç”¨: {last_msg.tool_calls}")
```

**è¾“å‡º**ï¼š
```
[èŠ‚ç‚¹: model]
æ¶ˆæ¯ç±»å‹: AIMessage
å·¥å…·è°ƒç”¨: [{'name': 'get_weather', 'args': {'city': 'åŒ—äº¬'}, 'id': 'call_123'}]

[èŠ‚ç‚¹: tools]
æ¶ˆæ¯ç±»å‹: ToolMessage
å†…å®¹: åŒ—äº¬çš„å¤©æ°”æ˜¯æ™´å¤©

[èŠ‚ç‚¹: model]
æ¶ˆæ¯ç±»å‹: AIMessage
å†…å®¹: åŒ—äº¬ä»Šå¤©æ˜¯æ™´å¤©
```

---

### é€‚ç”¨åœºæ™¯

1. **å¤šæ­¥æ¨ç†ç›‘æ§**
```python
# ç›‘æ§ Agent çš„æ¨ç†æ­¥éª¤
for chunk in agent.stream(input, stream_mode="updates"):
    for node, data in chunk.items():
        if node == "model":
            print("ğŸ¤– LLM æ­£åœ¨æ€è€ƒ...")
        elif node == "tools":
            print("ğŸ”§ å·¥å…·æ­£åœ¨æ‰§è¡Œ...")
```

2. **å·¥ä½œæµè°ƒè¯•**
```python
# è°ƒè¯•å¤æ‚çš„ Agent å·¥ä½œæµ
execution_log = []
for chunk in agent.stream(input, stream_mode="updates"):
    execution_log.append(chunk)
    print(f"æ­¥éª¤ {len(execution_log)}: {list(chunk.keys())}")

# åˆ†ææ‰§è¡Œè·¯å¾„
print(f"æ€»å…±æ‰§è¡Œäº† {len(execution_log)} ä¸ªæ­¥éª¤")
```

3. **è¿›åº¦è¿½è¸ª**
```python
# æ˜¾ç¤ºæ‰§è¡Œè¿›åº¦
total_steps = 5  # é¢„ä¼°æ­¥éª¤æ•°
current_step = 0

for chunk in agent.stream(input, stream_mode="updates"):
    current_step += 1
    progress = (current_step / total_steps) * 100
    print(f"è¿›åº¦: {progress:.0f}%")
```

---

### é«˜çº§ç‰¹æ€§ï¼šSubgraph æµå¼

**å¯ç”¨å­å›¾æµå¼**ï¼š

```python
# è¿½è¸ªåµŒå¥— Agent çš„æ‰§è¡Œ
for namespace, mode, data in agent.stream(
    input,
    stream_mode="updates",
    subgraphs=True  # å¯ç”¨å­å›¾æµå¼
):
    level = len(namespace)
    indent = "  " * level
    print(f"{indent}[Level {level}] {list(data.keys())}")
```

**å‘½åç©ºé—´æ ¼å¼**ï¼š
```python
# é¡¶å±‚ Agent
namespace = ()

# ä¸€å±‚åµŒå¥—
namespace = ('tools:call_abc123',)

# ä¸¤å±‚åµŒå¥—
namespace = ('tools:call_abc123', 'tools:call_def456')
```

---

## æ¨¡å¼ 2ï¼š`stream_mode="messages"` - LLM ä»¤ç‰Œæµå¼

### æ ¸å¿ƒåŸç†

**messages æ¨¡å¼å®æ—¶è¿”å› LLM ç”Ÿæˆçš„æ¯ä¸ª token åŠå…¶å…ƒæ•°æ®ã€‚**

```python
# æ‰§è¡Œæµç¨‹
LLM å¼€å§‹ç”Ÿæˆ
â†“
ç”Ÿæˆç¬¬ä¸€ä¸ª token
â†“
è§¦å‘å›è°ƒï¼šon_llm_new_token(token)
â†“
åˆ›å»º AIMessageChunk(content=token)
â†“
yield (chunk, metadata)
â†“
é‡å¤ç›´åˆ°ç”Ÿæˆå®Œæˆ
```

---

### è¿”å›æ•°æ®ç»“æ„

```python
# è¿”å›å…ƒç»„ï¼š(token, metadata)
(
    AIMessageChunk(content="Hello"),  # token
    {
        "langgraph_node": "model",
        "langgraph_step": 1,
        "lc_agent_name": "main_agent"  # 2026 æ–°å¢
    }  # metadata
)
```

**Token ç±»å‹**ï¼š
- `AIMessageChunk`: LLM ç”Ÿæˆçš„æ–‡æœ¬æˆ–å·¥å…·è°ƒç”¨
- `ToolMessage`: å·¥å…·æ‰§è¡Œç»“æœ
- å…¶ä»–æ¶ˆæ¯ç±»å‹

---

### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.agents import create_agent

agent = create_agent(model="gpt-4o-mini", tools=[...])

# æµå¼è¾“å‡º LLM tokens
for token, metadata in agent.stream(
    {"messages": [{"role": "user", "content": "è®²ä¸ªç¬‘è¯"}]},
    stream_mode="messages"
):
    # è¿‡æ»¤ï¼šåªè¾“å‡ºæ–‡æœ¬å†…å®¹
    if hasattr(token, 'content') and token.content:
        print(token.content, end="", flush=True)

    # è¿‡æ»¤ï¼šåªè¾“å‡ºæ¥è‡ªç‰¹å®šèŠ‚ç‚¹çš„ token
    if metadata.get('langgraph_node') == 'model':
        # åªå¤„ç†æ¨¡å‹èŠ‚ç‚¹çš„è¾“å‡º
        pass

    # è¿‡æ»¤ï¼šåªè¾“å‡ºç‰¹å®š Agent çš„ tokenï¼ˆ2026 æ–°å¢ï¼‰
    if metadata.get('lc_agent_name') == 'main_agent':
        # åªå¤„ç†ä¸» Agent çš„è¾“å‡º
        pass
```

---

### Token ç±»å‹è¯¦è§£

#### 1. æ–‡æœ¬ Token

```python
AIMessageChunk(
    content="Hello",  # æ–‡æœ¬å†…å®¹
    chunk_position="first"  # ä½ç½®æ ‡è®°ï¼šfirst, middle, last
)
```

#### 2. å·¥å…·è°ƒç”¨ Tokenï¼ˆéƒ¨åˆ† JSONï¼‰

```python
AIMessageChunk(
    content="",
    tool_call_chunks=[
        {
            'name': 'get_weather',
            'args': '{"ci',  # éƒ¨åˆ† JSON
            'id': 'call_123',
            'index': 0,
            'type': 'tool_call_chunk'
        }
    ]
)
```

#### 3. å·¥å…·ç»“æœ

```python
ToolMessage(
    content="åŒ—äº¬çš„å¤©æ°”æ˜¯æ™´å¤©",
    name="get_weather",
    tool_call_id="call_123"
)
```

---

### é€‚ç”¨åœºæ™¯

1. **ChatGPT å¼å¯¹è¯**
```python
# å®æ—¶æ˜¾ç¤º LLM è¾“å‡º
async for token, metadata in agent.astream(input, stream_mode="messages"):
    if hasattr(token, 'content'):
        print(token.content, end="", flush=True)
print()  # æ¢è¡Œ
```

2. **å·¥å…·è°ƒç”¨è¿½è¸ª**
```python
# è¿½è¸ªå·¥å…·è°ƒç”¨çš„å®Œæ•´è¿‡ç¨‹
tool_call_buffer = ""

for token, metadata in agent.stream(input, stream_mode="messages"):
    if hasattr(token, 'tool_call_chunks') and token.tool_call_chunks:
        # ç´¯ç§¯å·¥å…·è°ƒç”¨çš„ JSON
        tool_call_buffer += token.tool_call_chunks[0]['args']
        print(f"å·¥å…·è°ƒç”¨è¿›åº¦: {tool_call_buffer}")
```

3. **å¤š Agent åœºæ™¯**
```python
# åŒºåˆ†ä¸åŒ Agent çš„è¾“å‡ºï¼ˆ2026 æ–°å¢ï¼‰
current_agent = None

for token, metadata in agent.stream(input, stream_mode="messages"):
    agent_name = metadata.get('lc_agent_name')
    if agent_name != current_agent:
        print(f"\n[{agent_name}]:")
        current_agent = agent_name

    if hasattr(token, 'content'):
        print(token.content, end="")
```

---

### é«˜çº§ç‰¹æ€§ï¼šæ¶ˆæ¯èšåˆ

**èšåˆ Token ä¸ºå®Œæ•´æ¶ˆæ¯**ï¼š

```python
from langchain_core.messages import AIMessageChunk

full_message = None

for token, metadata in agent.stream(input, stream_mode="messages"):
    if isinstance(token, AIMessageChunk):
        if full_message is None:
            full_message = token
        else:
            full_message = full_message + token  # ç´¯åŠ 

        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ª chunk
        if token.chunk_position == "last":
            print(f"\nå®Œæ•´æ¶ˆæ¯: {full_message}")
            if full_message.tool_calls:
                print(f"å·¥å…·è°ƒç”¨: {full_message.tool_calls}")
            full_message = None
```

---

## æ¨¡å¼ 3ï¼š`stream_mode="custom"` - è‡ªå®šä¹‰æ•°æ®æµå¼

### æ ¸å¿ƒåŸç†

**custom æ¨¡å¼å…è®¸åœ¨å·¥å…·å‡½æ•°ä¸­å‘é€ä»»æ„è‡ªå®šä¹‰æ•°æ®ã€‚**

```python
# æ‰§è¡Œæµç¨‹
å·¥å…·å‡½æ•°è¢«è°ƒç”¨
â†“
è°ƒç”¨ get_stream_writer()
â†“
writer(custom_data)
â†“
æ•°æ®æ”¾å…¥æµå¼é˜Ÿåˆ—
â†“
yield custom_data
â†“
ç”¨æˆ·æ¥æ”¶åˆ°è‡ªå®šä¹‰æ•°æ®
```

---

### å®ç°æœºåˆ¶

```python
# langgraph/config.pyï¼ˆç®€åŒ–ï¼‰
from contextvars import ContextVar

_stream_writer: ContextVar[Optional[StreamWriter]] = ContextVar(
    "_stream_writer", default=None
)

def get_stream_writer(config: Optional[RunnableConfig] = None) -> StreamWriter:
    """ä»ä¸Šä¸‹æ–‡è·å– writer"""
    writer = _stream_writer.get()
    if writer is None:
        raise RuntimeError("No stream writer in context")
    return writer

class StreamWriter:
    def __init__(self, queue: asyncio.Queue):
        self.queue = queue

    def __call__(self, data: Any) -> None:
        """å†™å…¥æ•°æ®åˆ°æµ"""
        self.queue.put_nowait(data)
```

---

### ä½¿ç”¨ç¤ºä¾‹

```python
from langchain.agents import create_agent
from langgraph.config import get_stream_writer

def process_data(items: list[str]) -> str:
    """å¤„ç†æ•°æ®å¹¶å‘é€è¿›åº¦"""
    writer = get_stream_writer()

    writer(f"å¼€å§‹å¤„ç† {len(items)} ä¸ªé¡¹ç›®")

    for i, item in enumerate(items):
        # å¤„ç†é€»è¾‘
        result = process_item(item)

        # å‘é€è¿›åº¦
        progress = (i + 1) / len(items) * 100
        writer(f"è¿›åº¦: {progress:.1f}% - å·²å¤„ç† {item}")

    writer("å¤„ç†å®Œæˆ")
    return "Done"

agent = create_agent(model="gpt-4o-mini", tools=[process_data])

# æµå¼æ¥æ”¶è‡ªå®šä¹‰æ•°æ®
for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "å¤„ç†æ•°æ®"}]},
    stream_mode="custom"
):
    print(chunk)
```

**è¾“å‡º**ï¼š
```
å¼€å§‹å¤„ç† 10 ä¸ªé¡¹ç›®
è¿›åº¦: 10.0% - å·²å¤„ç† item1
è¿›åº¦: 20.0% - å·²å¤„ç† item2
...
è¿›åº¦: 100.0% - å·²å¤„ç† item10
å¤„ç†å®Œæˆ
```

---

### é€‚ç”¨åœºæ™¯

1. **æ•°æ®å¤„ç†è¿›åº¦**
```python
def batch_process(files: list) -> str:
    writer = get_stream_writer()

    for i, file in enumerate(files):
        writer({"type": "progress", "current": i+1, "total": len(files)})
        process_file(file)

    return "Done"
```

2. **ä¸šåŠ¡çŠ¶æ€æ›´æ–°**
```python
def order_workflow(order_id: str) -> str:
    writer = get_stream_writer()

    writer({"status": "éªŒè¯è®¢å•", "order_id": order_id})
    validate_order(order_id)

    writer({"status": "å¤„ç†æ”¯ä»˜", "order_id": order_id})
    process_payment(order_id)

    writer({"status": "å‘è´§", "order_id": order_id})
    ship_order(order_id)

    return "è®¢å•å®Œæˆ"
```

3. **è°ƒè¯•ä¿¡æ¯**
```python
def complex_calculation(data: dict) -> str:
    writer = get_stream_writer()

    writer(f"è¾“å…¥æ•°æ®: {data}")

    intermediate = step1(data)
    writer(f"æ­¥éª¤ 1 ç»“æœ: {intermediate}")

    result = step2(intermediate)
    writer(f"æ­¥éª¤ 2 ç»“æœ: {result}")

    return result
```

---

### Python < 3.11 å…¼å®¹æ€§

```python
from langchain_core.runnables import RunnableConfig

# Python < 3.11 éœ€è¦æ‰‹åŠ¨ä¼ é€’ config
def my_tool(query: str, config: RunnableConfig) -> str:
    writer = get_stream_writer(config)  # ä¼ é€’ config
    writer("Processing...")
    return "Done"

# Python 3.11+ è‡ªåŠ¨ä¼ æ’­ä¸Šä¸‹æ–‡
def my_tool(query: str) -> str:
    writer = get_stream_writer()  # æ— éœ€ä¼ é€’ config
    writer("Processing...")
    return "Done"
```

---

## å¤šæ¨¡å¼ç»„åˆ

### ç»„åˆä½¿ç”¨

```python
# åŒæ—¶ä½¿ç”¨å¤šç§æ¨¡å¼
for mode, data in agent.stream(
    input,
    stream_mode=["updates", "messages", "custom"]
):
    if mode == "updates":
        # å¤„ç† Agent è¿›åº¦
        for node, state in data.items():
            print(f"\n[æ­¥éª¤] {node} æ‰§è¡Œå®Œæˆ")

    elif mode == "messages":
        # å¤„ç† LLM tokens
        token, metadata = data
        if hasattr(token, 'content') and token.content:
            print(token.content, end="", flush=True)

    elif mode == "custom":
        # å¤„ç†è‡ªå®šä¹‰æ•°æ®
        print(f"\n[è¿›åº¦] {data}")
```

---

### æ•°æ®é¡ºåºé—®é¢˜

**é‡è¦**ï¼šå¤šæ¨¡å¼æµå¼çš„æ•°æ®é¡ºåºä¸ä¿è¯ï¼Œä¸åŒæ¨¡å¼çš„æ•°æ®å¯èƒ½äº¤é”™è¿”å›ã€‚

```python
# å¯èƒ½çš„è¾“å‡ºé¡ºåº
[messages] token1
[messages] token2
[custom] "Processing..."
[updates] {'model': {...}}
[messages] token3
[custom] "Done"
[updates] {'tools': {...}}
```

**æ­£ç¡®å¤„ç†æ–¹å¼**ï¼š

```python
# ä½¿ç”¨ç¼“å†²åŒºæ”¶é›†æ•°æ®
buffers = {
    "updates": [],
    "messages": [],
    "custom": []
}

for mode, data in agent.stream(input, stream_mode=["updates", "messages", "custom"]):
    buffers[mode].append(data)

# å¤„ç†å®Œæ‰€æœ‰æ•°æ®åå†ä½¿ç”¨
print(f"æ€»å…± {len(buffers['updates'])} ä¸ªæ­¥éª¤")
print(f"æ€»å…± {len(buffers['messages'])} ä¸ª token")
print(f"æ€»å…± {len(buffers['custom'])} ä¸ªè‡ªå®šä¹‰æ¶ˆæ¯")
```

---

## æ¨¡å¼é€‰æ‹©å†³ç­–

### å†³ç­–æ ‘

```
éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
â”œâ”€ Agent æ‰§è¡Œæ­¥éª¤
â”‚   â””â”€ stream_mode="updates"
â”‚       â”œâ”€ éœ€è¦å­å›¾ä¿¡æ¯ï¼Ÿ
â”‚       â”‚   â””â”€ subgraphs=True
â”‚       â””â”€ åªéœ€è¦é¡¶å±‚ï¼Ÿ
â”‚           â””â”€ subgraphs=Falseï¼ˆé»˜è®¤ï¼‰
â”‚
â”œâ”€ LLM å®æ—¶è¾“å‡º
â”‚   â””â”€ stream_mode="messages"
â”‚       â”œâ”€ éœ€è¦åŒºåˆ† Agentï¼Ÿ
â”‚       â”‚   â””â”€ ä½¿ç”¨ metadata['lc_agent_name']
â”‚       â””â”€ éœ€è¦è¿‡æ»¤èŠ‚ç‚¹ï¼Ÿ
â”‚           â””â”€ ä½¿ç”¨ metadata['langgraph_node']
â”‚
â”œâ”€ è‡ªå®šä¹‰è¿›åº¦ä¿¡å·
â”‚   â””â”€ stream_mode="custom"
â”‚       â””â”€ åœ¨å·¥å…·ä¸­ä½¿ç”¨ get_stream_writer()
â”‚
â””â”€ éœ€è¦å¤šç§ä¿¡æ¯
    â””â”€ stream_mode=["updates", "messages", "custom"]
        â””â”€ ä½¿ç”¨ç¼“å†²åŒºæ”¶é›†æ•°æ®
```

---

### åœºæ™¯å¯¹ç…§è¡¨

| åœºæ™¯ | æ¨èæ¨¡å¼ | åŸå›  |
|------|----------|------|
| ChatGPT å¼å¯¹è¯ | `messages` | éœ€è¦å®æ—¶æ˜¾ç¤º LLM è¾“å‡º |
| å¤šæ­¥ Agent ç›‘æ§ | `updates` | éœ€è¦è¿½è¸ªæ¯ä¸ªæ­¥éª¤ |
| æ•°æ®å¤„ç†è¿›åº¦ | `custom` | éœ€è¦è‡ªå®šä¹‰è¿›åº¦ä¿¡å· |
| å·¥å…·è°ƒç”¨è¿½è¸ª | `messages` + `updates` | éœ€è¦ token å’Œæ­¥éª¤ä¿¡æ¯ |
| åµŒå¥— Agent | `updates` + `subgraphs=True` | éœ€è¦è¿½è¸ªå­å›¾ |
| å…¨é¢ç›‘æ§ | ä¸‰ç§æ¨¡å¼ç»„åˆ | éœ€è¦æ‰€æœ‰ä¿¡æ¯ |

---

## æ€§èƒ½è€ƒè™‘

### æ¨¡å¼å¼€é”€å¯¹æ¯”

```python
# å•æ¨¡å¼å¼€é”€
updates: ~2%
messages: ~5%
custom: ~1%

# å¤šæ¨¡å¼å¼€é”€ï¼ˆéçº¿æ€§ç´¯åŠ ï¼‰
updates + messages: ~6%
updates + messages + custom: ~8%
```

### ä¼˜åŒ–å»ºè®®

1. **åªå¯ç”¨éœ€è¦çš„æ¨¡å¼**
```python
# âŒ ä¸æ¨èï¼šå¯ç”¨æ‰€æœ‰æ¨¡å¼
stream_mode=["updates", "messages", "custom"]

# âœ… æ¨èï¼šåªå¯ç”¨éœ€è¦çš„
stream_mode="messages"  # åªéœ€è¦ LLM è¾“å‡º
```

2. **åˆç†ä½¿ç”¨ subgraphs**
```python
# âŒ ä¸æ¨èï¼šæ€»æ˜¯å¯ç”¨
subgraphs=True

# âœ… æ¨èï¼šåªåœ¨éœ€è¦æ—¶å¯ç”¨
subgraphs=False  # é»˜è®¤ï¼Œæ€§èƒ½æ›´å¥½
```

3. **è¿‡æ»¤ä¸éœ€è¦çš„æ•°æ®**
```python
# åªå¤„ç†ç‰¹å®šèŠ‚ç‚¹çš„æ•°æ®
for token, metadata in agent.stream(input, stream_mode="messages"):
    if metadata.get('langgraph_node') == 'model':
        # åªå¤„ç†æ¨¡å‹èŠ‚ç‚¹
        print(token.content, end="")
```

---

## æ€»ç»“

### ä¸‰ç§æ¨¡å¼å¯¹æ¯”

| ç‰¹æ€§ | updates | messages | custom |
|------|---------|----------|--------|
| **ç”¨é€”** | Agent æ­¥éª¤è¿½è¸ª | LLM å®æ—¶è¾“å‡º | è‡ªå®šä¹‰è¿›åº¦ |
| **è¿”å›æ•°æ®** | `{node: state}` | `(token, metadata)` | ä»»æ„æ•°æ® |
| **å¼€é”€** | ~2% | ~5% | ~1% |
| **é€‚ç”¨åœºæ™¯** | ç›‘æ§ã€è°ƒè¯• | å¯¹è¯ã€é•¿æ–‡æœ¬ | ä¸šåŠ¡è¿›åº¦ |
| **é«˜çº§ç‰¹æ€§** | Subgraph æµå¼ | æ¶ˆæ¯èšåˆ | ä¸Šä¸‹æ–‡ä¾èµ– |

### æ ¸å¿ƒè¦ç‚¹

1. **updates**: è¿½è¸ª Agent æ‰§è¡Œå›¾çš„æ¯ä¸ªèŠ‚ç‚¹
2. **messages**: å®æ—¶è¿”å› LLM ç”Ÿæˆçš„ token
3. **custom**: åœ¨å·¥å…·ä¸­å‘é€è‡ªå®šä¹‰æ•°æ®
4. **å¤šæ¨¡å¼**: å¯ä»¥ç»„åˆä½¿ç”¨ï¼Œä½†æ•°æ®é¡ºåºä¸ä¿è¯
5. **æ€§èƒ½**: åªå¯ç”¨éœ€è¦çš„æ¨¡å¼ï¼Œé¿å…ä¸å¿…è¦çš„å¼€é”€

---

## å‚è€ƒèµ„æº

- **å®˜æ–¹æ–‡æ¡£**: https://docs.langchain.com/oss/python/langchain/streaming/overview
- **æºç ä½ç½®**:
  - `langchain_core/tracers/event_stream.py` - äº‹ä»¶æµå®ç°
  - `langgraph/config.py` - get_stream_writer å®ç°
- **ç›¸å…³çŸ¥è¯†ç‚¹**:
  - 02_ç¬¬ä¸€æ€§åŸç† - æ·±å…¥ç†è§£è®¾è®¡åŸç†
  - 04_æœ€å°å¯ç”¨ - æœ€å° API é›†
  - 07_å®æˆ˜ä»£ç  - å®Œæ•´ä»£ç ç¤ºä¾‹

---

**ç‰ˆæœ¬**: LangChain 0.3.x (2025-2026)
**æœ€åæ›´æ–°**: 2026-02-21
