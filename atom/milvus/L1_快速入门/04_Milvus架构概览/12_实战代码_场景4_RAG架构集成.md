# 实战代码 - 场景 4：RAG 架构集成

> **场景定位**：将 Milvus 集群集成到完整的 RAG 系统中
> **难度等级**：综合实战
> **预计时间**：45 分钟

---

## 1. 场景描述

### 1.1 业务场景

构建一个生产级的 RAG 系统，需要将 Milvus 与其他组件无缝集成:
- 文档处理：PDF、Word、Markdown 等格式
- 向量化：使用 OpenAI Embeddings
- 向量存储：Milvus 分布式集群
- 检索增强：混合检索 + ReRank
- 生成回答：OpenAI GPT-4

### 1.2 技术目标

1. 设计完整的 RAG 架构
2. 集成 Milvus 作为向量存储
3. 实现文档处理 Pipeline
4. 实现检索增强生成
5. 添加监控和日志

### 1.3 系统架构

```
┌─────────────────────────────────────────────────────────┐
│                    RAG System Architecture               │
└─────────────────────────────────────────────────────────┘

    ┌──────────┐
    │ Documents│
    └────┬─────┘
         │
         ↓
    ┌──────────────┐
    │ Document     │
    │ Processor    │
    └────┬─────────┘
         │
         ↓
    ┌──────────────┐
    │ Text Chunker │
    └────┬─────────┘
         │
         ↓
    ┌──────────────┐
    │ Embedding    │
    │ Generator    │
    └────┬─────────┘
         │
         ↓
    ┌──────────────┐
    │   Milvus     │
    │   Cluster    │
    └────┬─────────┘
         │
         ↓
    ┌──────────────┐
    │   Retriever  │
    └────┬─────────┘
         │
         ↓
    ┌──────────────┐
    │   ReRanker   │
    └────┬─────────┘
         │
         ↓
    ┌──────────────┐
    │   Generator  │
    │   (GPT-4)    │
    └──────────────┘
```

---

## 2. 环境准备

### 2.1 安装依赖

```bash
# 核心依赖
uv add pymilvus openai langchain langchain-openai

# 文档处理
uv add pypdf python-docx markdown

# 工具库
uv add python-dotenv pydantic fastapi uvicorn
```

### 2.2 配置环境变量

```bash
# .env
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL=https://api.openai.com/v1

MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION=rag_documents
```

---

## 3. 核心代码实现

### 3.1 配置管理

```python
"""
配置管理模块
"""

from pydantic import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """系统配置"""

    # OpenAI 配置
    openai_api_key: str
    openai_base_url: str = "https://api.openai.com/v1"
    embedding_model: str = "text-embedding-3-small"
    chat_model: str = "gpt-4"

    # Milvus 配置
    milvus_host: str = "localhost"
    milvus_port: int = 19530
    milvus_collection: str = "rag_documents"

    # 检索配置
    chunk_size: int = 500
    chunk_overlap: int = 50
    top_k: int = 5
    rerank_top_k: int = 3

    # 索引配置
    index_type: str = "HNSW"
    metric_type: str = "L2"
    index_params: dict = {
        "M": 16,
        "efConstruction": 200
    }

    class Config:
        env_file = ".env"


# 全局配置实例
settings = Settings()
```

### 3.2 文档处理模块

```python
"""
文档处理模块
"""

from typing import List, Dict
from pathlib import Path
import pypdf
from docx import Document as DocxDocument
import markdown

class DocumentProcessor:
    """文档处理器"""

    def load_pdf(self, file_path: str) -> str:
        """加载 PDF 文件"""
        text = ""
        with open(file_path, "rb") as f:
            pdf_reader = pypdf.PdfReader(f)
            for page in pdf_reader.pages:
                text += page.extract_text()
        return text

    def load_docx(self, file_path: str) -> str:
        """加载 Word 文件"""
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        return text

    def load_markdown(self, file_path: str) -> str:
        """加载 Markdown 文件"""
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
        return text

    def load_document(self, file_path: str) -> Dict:
        """加载文档（自动识别格式）"""
        path = Path(file_path)
        suffix = path.suffix.lower()

        if suffix == ".pdf":
            text = self.load_pdf(file_path)
        elif suffix in [".docx", ".doc"]:
            text = self.load_docx(file_path)
        elif suffix in [".md", ".markdown"]:
            text = self.load_markdown(file_path)
        elif suffix == ".txt":
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
        else:
            raise ValueError(f"Unsupported file format: {suffix}")

        return {
            "text": text,
            "metadata": {
                "source": file_path,
                "filename": path.name,
                "format": suffix
            }
        }


class TextChunker:
    """文本分块器"""

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def chunk_text(self, text: str, metadata: Dict = None) -> List[Dict]:
        """分块文本"""
        chunks = []
        start = 0

        while start < len(text):
            end = start + self.chunk_size
            chunk_text = text[start:end]

            chunk = {
                "text": chunk_text,
                "metadata": metadata or {},
                "start_index": start,
                "end_index": end
            }
            chunks.append(chunk)

            start += self.chunk_size - self.chunk_overlap

        return chunks
```

### 3.3 Milvus 集成模块

```python
"""
Milvus 集成模块
"""

from pymilvus import (
    connections, Collection, CollectionSchema, FieldSchema, DataType,
    utility
)
from typing import List, Dict
import numpy as np

class MilvusClient:
    """Milvus 客户端"""

    def __init__(self, host: str, port: int, collection_name: str):
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.collection = None

    def connect(self):
        """连接到 Milvus"""
        connections.connect(host=self.host, port=str(self.port))
        print(f"Connected to Milvus at {self.host}:{self.port}")

    def create_collection(self, dim: int, index_params: Dict = None):
        """创建 Collection"""
        # 检查是否已存在
        if utility.has_collection(self.collection_name):
            print(f"Collection {self.collection_name} already exists")
            self.collection = Collection(self.collection_name)
            return

        # 定义 Schema
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=512),
            FieldSchema(name="chunk_index", dtype=DataType.INT64)
        ]
        schema = CollectionSchema(fields, description="RAG document collection")

        # 创建 Collection
        self.collection = Collection(self.collection_name, schema)
        print(f"Created collection: {self.collection_name}")

        # 创建索引
        if index_params:
            self.collection.create_index(
                field_name="embedding",
                index_params=index_params
            )
            print("Index created")

    def insert_documents(self, embeddings: List[List[float]],
                        texts: List[str], sources: List[str],
                        chunk_indices: List[int]):
        """插入文档"""
        data = [
            embeddings,
            texts,
            sources,
            chunk_indices
        ]

        self.collection.insert(data)
        self.collection.flush()
        print(f"Inserted {len(texts)} documents")

    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:
        """搜索相似文档"""
        self.collection.load()

        search_params = {"metric_type": "L2", "params": {"ef": 100}}
        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            output_fields=["text", "source", "chunk_index"]
        )

        documents = []
        for hits in results:
            for hit in hits:
                documents.append({
                    "id": hit.id,
                    "distance": hit.distance,
                    "text": hit.entity.get("text"),
                    "source": hit.entity.get("source"),
                    "chunk_index": hit.entity.get("chunk_index")
                })

        return documents

    def disconnect(self):
        """断开连接"""
        connections.disconnect()
```

### 3.4 RAG Pipeline

```python
"""
RAG Pipeline 主模块
"""

from openai import OpenAI
from typing import List, Dict
import time

class RAGPipeline:
    """RAG Pipeline"""

    def __init__(self, settings: Settings):
        self.settings = settings

        # 初始化组件
        self.doc_processor = DocumentProcessor()
        self.chunker = TextChunker(
            chunk_size=settings.chunk_size,
            chunk_overlap=settings.chunk_overlap
        )
        self.milvus_client = MilvusClient(
            host=settings.milvus_host,
            port=settings.milvus_port,
            collection_name=settings.milvus_collection
        )
        self.openai_client = OpenAI(
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url
        )

    def initialize(self):
        """初始化 Pipeline"""
        # 连接 Milvus
        self.milvus_client.connect()

        # 创建 Collection
        index_params = {
            "index_type": self.settings.index_type,
            "metric_type": self.settings.metric_type,
            "params": self.settings.index_params
        }
        self.milvus_client.create_collection(dim=1536, index_params=index_params)

        print("RAG Pipeline initialized")

    def generate_embedding(self, text: str) -> List[float]:
        """生成 Embedding"""
        response = self.openai_client.embeddings.create(
            model=self.settings.embedding_model,
            input=text
        )
        return response.data[0].embedding

    def ingest_document(self, file_path: str):
        """摄入文档"""
        print(f"\nIngesting document: {file_path}")

        # 1. 加载文档
        doc = self.doc_processor.load_document(file_path)
        print(f"Loaded document: {len(doc['text'])} characters")

        # 2. 分块
        chunks = self.chunker.chunk_text(doc["text"], doc["metadata"])
        print(f"Created {len(chunks)} chunks")

        # 3. 生成 Embeddings
        print("Generating embeddings...")
        embeddings = []
        texts = []
        sources = []
        chunk_indices = []

        for i, chunk in enumerate(chunks):
            embedding = self.generate_embedding(chunk["text"])
            embeddings.append(embedding)
            texts.append(chunk["text"])
            sources.append(chunk["metadata"]["source"])
            chunk_indices.append(i)

            if (i + 1) % 10 == 0:
                print(f"  Processed {i + 1}/{len(chunks)} chunks")

        # 4. 插入 Milvus
        self.milvus_client.insert_documents(
            embeddings=embeddings,
            texts=texts,
            sources=sources,
            chunk_indices=chunk_indices
        )

        print(f"Document ingested successfully")

    def retrieve(self, query: str, top_k: int = None) -> List[Dict]:
        """检索相关文档"""
        top_k = top_k or self.settings.top_k

        # 生成查询 Embedding
        query_embedding = self.generate_embedding(query)

        # 检索
        documents = self.milvus_client.search(query_embedding, top_k=top_k)

        return documents

    def rerank(self, query: str, documents: List[Dict], top_k: int = None) -> List[Dict]:
        """重排序（简化版：基于距离）"""
        top_k = top_k or self.settings.rerank_top_k

        # 按距离排序
        sorted_docs = sorted(documents, key=lambda x: x["distance"])

        return sorted_docs[:top_k]

    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """生成回答"""
        # 构建上下文
        context = "\n\n".join([
            f"[Document {i+1}]\n{doc['text']}"
            for i, doc in enumerate(context_docs)
        ])

        # 构建 Prompt
        prompt = f"""Based on the following context, answer the question.

Context:
{context}

Question: {query}

Answer:"""

        # 调用 LLM
        response = self.openai_client.chat.completions.create(
            model=self.settings.chat_model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that answers questions based on the provided context."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )

        return response.choices[0].message.content

    def query(self, question: str) -> Dict:
        """完整的 RAG 查询流程"""
        start_time = time.time()

        # 1. 检索
        print(f"\nQuery: {question}")
        print("Retrieving documents...")
        retrieved_docs = self.retrieve(question)
        print(f"Retrieved {len(retrieved_docs)} documents")

        # 2. 重排序
        print("Reranking documents...")
        reranked_docs = self.rerank(question, retrieved_docs)
        print(f"Reranked to top {len(reranked_docs)} documents")

        # 3. 生成回答
        print("Generating answer...")
        answer = self.generate_answer(question, reranked_docs)

        elapsed_time = time.time() - start_time

        return {
            "question": question,
            "answer": answer,
            "sources": [doc["source"] for doc in reranked_docs],
            "retrieved_docs": len(retrieved_docs),
            "reranked_docs": len(reranked_docs),
            "elapsed_time": elapsed_time
        }

    def shutdown(self):
        """关闭 Pipeline"""
        self.milvus_client.disconnect()
        print("RAG Pipeline shutdown")
```

### 3.5 完整示例

```python
"""
完整的 RAG 系统示例
"""

from dotenv import load_dotenv

def main():
    """主函数"""
    # 加载环境变量
    load_dotenv()

    # 创建 Pipeline
    pipeline = RAGPipeline(settings)

    try:
        # 初始化
        pipeline.initialize()

        # 摄入文档
        documents = [
            "docs/milvus_architecture.md",
            "docs/rag_best_practices.md",
            "docs/vector_search_guide.md"
        ]

        for doc_path in documents:
            pipeline.ingest_document(doc_path)

        # 查询示例
        questions = [
            "What is the architecture of Milvus?",
            "How to optimize RAG performance?",
            "What are the best practices for vector search?"
        ]

        for question in questions:
            result = pipeline.query(question)

            print("\n" + "="*70)
            print(f"Question: {result['question']}")
            print(f"\nAnswer: {result['answer']}")
            print(f"\nSources:")
            for source in result['sources']:
                print(f"  - {source}")
            print(f"\nStats:")
            print(f"  Retrieved: {result['retrieved_docs']} docs")
            print(f"  Reranked: {result['reranked_docs']} docs")
            print(f"  Time: {result['elapsed_time']:.2f}s")
            print("="*70)

    finally:
        # 关闭
        pipeline.shutdown()


if __name__ == "__main__":
    main()
```

---

## 4. FastAPI 服务化

### 4.1 API 服务

```python
"""
FastAPI 服务
"""

from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel
from typing import List, Optional
import tempfile
import os

app = FastAPI(title="RAG API", version="1.0.0")

# 全局 Pipeline
pipeline = None

class QueryRequest(BaseModel):
    """查询请求"""
    question: str
    top_k: Optional[int] = None

class QueryResponse(BaseModel):
    """查询响应"""
    question: str
    answer: str
    sources: List[str]
    elapsed_time: float

@app.on_event("startup")
async def startup_event():
    """启动事件"""
    global pipeline
    load_dotenv()
    pipeline = RAGPipeline(settings)
    pipeline.initialize()
    print("RAG API started")

@app.on_event("shutdown")
async def shutdown_event():
    """关闭事件"""
    global pipeline
    if pipeline:
        pipeline.shutdown()
    print("RAG API shutdown")

@app.post("/ingest")
async def ingest_document(file: UploadFile = File(...)):
    """摄入文档"""
    try:
        # 保存临时文件
        with tempfile.NamedTemporaryFile(delete=False, suffix=file.filename) as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name

        # 摄入文档
        pipeline.ingest_document(tmp_path)

        # 删除临时文件
        os.unlink(tmp_path)

        return {"message": "Document ingested successfully", "filename": file.filename}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """查询"""
    try:
        result = pipeline.query(request.question)
        return QueryResponse(
            question=result["question"],
            answer=result["answer"],
            sources=result["sources"],
            elapsed_time=result["elapsed_time"]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """健康检查"""
    return {"status": "healthy"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 4.2 客户端示例

```python
"""
API 客户端示例
"""

import requests

class RAGClient:
    """RAG API 客户端"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url

    def ingest_document(self, file_path: str):
        """摄入文档"""
        with open(file_path, "rb") as f:
            files = {"file": f}
            response = requests.post(f"{self.base_url}/ingest", files=files)
            response.raise_for_status()
            return response.json()

    def query(self, question: str, top_k: int = None):
        """查询"""
        data = {"question": question}
        if top_k:
            data["top_k"] = top_k

        response = requests.post(f"{self.base_url}/query", json=data)
        response.raise_for_status()
        return response.json()

    def health_check(self):
        """健康检查"""
        response = requests.get(f"{self.base_url}/health")
        response.raise_for_status()
        return response.json()


# 使用示例
if __name__ == "__main__":
    client = RAGClient()

    # 健康检查
    print(client.health_check())

    # 摄入文档
    result = client.ingest_document("docs/example.pdf")
    print(result)

    # 查询
    result = client.query("What is Milvus?")
    print(f"Answer: {result['answer']}")
    print(f"Sources: {result['sources']}")
```

---

## 5. 监控和日志

### 5.1 日志配置

```python
"""
日志配置
"""

import logging
from datetime import datetime

def setup_logging():
    """配置日志"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'logs/rag_{datetime.now().strftime("%Y%m%d")}.log'),
            logging.StreamHandler()
        ]
    )

logger = logging.getLogger(__name__)
```

### 5.2 性能监控

```python
"""
性能监控
"""

import time
from functools import wraps

def monitor_performance(func):
    """性能监控装饰器"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start

        logger.info(f"{func.__name__} took {elapsed:.2f}s")
        return result

    return wrapper

# 使用示例
class MonitoredRAGPipeline(RAGPipeline):
    @monitor_performance
    def retrieve(self, query: str, top_k: int = None):
        return super().retrieve(query, top_k)

    @monitor_performance
    def generate_answer(self, query: str, context_docs: List[Dict]):
        return super().generate_answer(query, context_docs)
```

---

## 6. 最佳实践

### 6.1 错误处理

```python
class RAGError(Exception):
    """RAG 系统错误基类"""
    pass

class DocumentProcessingError(RAGError):
    """文档处理错误"""
    pass

class RetrievalError(RAGError):
    """检索错误"""
    pass

class GenerationError(RAGError):
    """生成错误"""
    pass

# 使用示例
def safe_ingest_document(self, file_path: str):
    """安全的文档摄入"""
    try:
        self.ingest_document(file_path)
    except Exception as e:
        logger.error(f"Failed to ingest document: {e}")
        raise DocumentProcessingError(f"Document processing failed: {e}")
```

### 6.2 批量处理

```python
def batch_ingest_documents(self, file_paths: List[str], batch_size: int = 10):
    """批量摄入文档"""
    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i+batch_size]
        for file_path in batch:
            try:
                self.ingest_document(file_path)
            except Exception as e:
                logger.error(f"Failed to ingest {file_path}: {e}")
```

### 6.3 缓存优化

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_generate_embedding(self, text: str) -> tuple:
    """缓存 Embedding 生成"""
    embedding = self.generate_embedding(text)
    return tuple(embedding)  # 转换为 tuple 以支持缓存
```

---

## 7. 总结

### 7.1 核心要点

1. **模块化设计**：文档处理、向量化、检索、生成各模块独立
2. **Milvus 集成**：作为向量存储的核心组件
3. **完整 Pipeline**：从文档摄入到答案生成的端到端流程
4. **服务化部署**：FastAPI 提供 RESTful API
5. **监控日志**：完善的日志和性能监控

### 7.2 架构优势

1. **高性能**：Milvus 分布式集群支持大规模向量检索
2. **可扩展**：模块化设计便于扩展和维护
3. **生产就绪**：包含错误处理、日志、监控等生产特性
4. **易于集成**：RESTful API 便于与其他系统集成

### 7.3 下一步优化

1. **混合检索**：结合关键词检索和向量检索
2. **ReRank 优化**：使用专门的 ReRank 模型
3. **缓存策略**：添加查询结果缓存
4. **异步处理**：使用异步 I/O 提升性能
5. **分布式部署**：多实例部署提升吞吐量

---

**完整代码**：`examples/rag_milvus_integration.py`
