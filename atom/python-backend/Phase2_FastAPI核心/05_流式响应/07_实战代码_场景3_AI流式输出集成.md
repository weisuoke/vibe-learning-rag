# 实战代码 - 场景3：AI 流式输出集成

> 完整可运行的 AI 流式对话系统示例

---

## 场景描述

集成 OpenAI 和 Anthropic 的流式 API，实现完整的 AI 对话系统，包括流式输出、错误处理、重试机制等。

---

## 完整代码

```python
"""
场景3：AI 流式输出集成
演示：OpenAI/Anthropic 流式 API 的完整集成
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, AsyncGenerator, Literal
import asyncio
import json
import logging
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="AI 流式输出集成示例")

# 添加 CORS 支持
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 初始化客户端
openai_client = AsyncOpenAI()
claude_client = AsyncAnthropic()

# ===== 数据模型 =====

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    model: str = "gpt-4"
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    stream: bool = True

# ===== 1. OpenAI 基础流式输出 =====
print("=== 场景1：OpenAI 基础流式输出 ===")

async def openai_basic_stream(prompt: str) -> AsyncGenerator[str, None]:
    """OpenAI 基础流式输出"""
    try:
        stream = await openai_client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    except Exception as e:
        logger.error(f"OpenAI 流式输出错误: {e}")
        yield f"\n\n[错误: {e}]"

@app.post("/openai/basic")
async def openai_basic_endpoint(prompt: str):
    """OpenAI 基础流式输出端点"""
    return StreamingResponse(
        openai_basic_stream(prompt),
        media_type="text/plain"
    )

# ===== 2. OpenAI SSE 格式流式输出 =====
print("=== 场景2：OpenAI SSE 格式流式输出 ===")

async def openai_sse_stream(request: ChatRequest) -> AsyncGenerator[str, None]:
    """OpenAI SSE 格式流式输出"""
    try:
        # 发送开始事件
        yield "event: start\n"
        yield f"data: {json.dumps({'model': request.model})}\n\n"

        # 调用 OpenAI API
        stream = await openai_client.chat.completions.create(
            model=request.model,
            messages=[msg.dict() for msg in request.messages],
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            stream=True
        )

        message_id = 0
        full_content = ""

        async for chunk in stream:
            choice = chunk.choices[0]

            # 内容块
            if choice.delta.content:
                content = choice.delta.content
                full_content += content
                message_id += 1

                yield f"id: {message_id}\n"
                yield "event: content\n"
                yield f"data: {json.dumps({'content': content})}\n\n"

            # 结束原因
            if choice.finish_reason:
                yield "event: complete\n"
                yield f"data: {json.dumps({
                    'full_content': full_content,
                    'finish_reason': choice.finish_reason,
                    'token_count': message_id
                })}\n\n"

    except Exception as e:
        logger.error(f"OpenAI SSE 流式输出错误: {e}")
        yield "event: error\n"
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.post("/openai/sse")
async def openai_sse_endpoint(request: ChatRequest):
    """OpenAI SSE 格式流式输出端点"""
    return StreamingResponse(
        openai_sse_stream(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )

# ===== 3. Claude 基础流式输出 =====
print("=== 场景3：Claude 基础流式输出 ===")

async def claude_basic_stream(prompt: str) -> AsyncGenerator[str, None]:
    """Claude 基础流式输出"""
    try:
        async with claude_client.messages.stream(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        ) as stream:
            async for text in stream.text_stream:
                yield text

    except Exception as e:
        logger.error(f"Claude 流式输出错误: {e}")
        yield f"\n\n[错误: {e}]"

@app.post("/claude/basic")
async def claude_basic_endpoint(prompt: str):
    """Claude 基础流式输出端点"""
    return StreamingResponse(
        claude_basic_stream(prompt),
        media_type="text/plain"
    )

# ===== 4. Claude SSE 格式流式输出 =====
print("=== 场景4：Claude SSE 格式流式输出 ===")

async def claude_sse_stream(request: ChatRequest) -> AsyncGenerator[str, None]:
    """Claude SSE 格式流式输出"""
    try:
        yield "event: start\n"
        yield f"data: {json.dumps({'model': request.model})}\n\n"

        async with claude_client.messages.stream(
            model="claude-3-5-sonnet-20241022",
            max_tokens=request.max_tokens or 1024,
            messages=[msg.dict() for msg in request.messages]
        ) as stream:
            message_id = 0

            async for text in stream.text_stream:
                message_id += 1

                yield f"id: {message_id}\n"
                yield "event: content\n"
                yield f"data: {json.dumps({'content': text})}\n\n"

            # 获取最终消息
            final_message = await stream.get_final_message()

            yield "event: complete\n"
            yield f"data: {json.dumps({
                'full_content': final_message.content[0].text,
                'usage': {
                    'input_tokens': final_message.usage.input_tokens,
                    'output_tokens': final_message.usage.output_tokens
                }
            })}\n\n"

    except Exception as e:
        logger.error(f"Claude SSE 流式输出错误: {e}")
        yield "event: error\n"
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.post("/claude/sse")
async def claude_sse_endpoint(request: ChatRequest):
    """Claude SSE 格式流式输出端点"""
    return StreamingResponse(
        claude_sse_stream(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )

# ===== 5. 统一的 AI 流式输出接口 =====
print("=== 场景5：统一的 AI 流式输出接口 ===")

class UnifiedChatRequest(BaseModel):
    messages: List[Message]
    provider: Literal["openai", "claude"] = "openai"
    model: Optional[str] = None
    temperature: float = 0.7
    max_tokens: Optional[int] = None

async def unified_ai_stream(request: UnifiedChatRequest) -> AsyncGenerator[str, None]:
    """统一的 AI 流式输出接口"""
    try:
        yield "event: start\n"
        yield f"data: {json.dumps({'provider': request.provider})}\n\n"

        if request.provider == "openai":
            # OpenAI 流式输出
            model = request.model or "gpt-4"
            stream = await openai_client.chat.completions.create(
                model=model,
                messages=[msg.dict() for msg in request.messages],
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=True
            )

            message_id = 0
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    message_id += 1

                    yield f"id: {message_id}\n"
                    yield "event: content\n"
                    yield f"data: {json.dumps({'content': content})}\n\n"

        elif request.provider == "claude":
            # Claude 流式输出
            model = request.model or "claude-3-5-sonnet-20241022"
            async with claude_client.messages.stream(
                model=model,
                max_tokens=request.max_tokens or 1024,
                messages=[msg.dict() for msg in request.messages]
            ) as stream:
                message_id = 0

                async for text in stream.text_stream:
                    message_id += 1

                    yield f"id: {message_id}\n"
                    yield "event: content\n"
                    yield f"data: {json.dumps({'content': text})}\n\n"

        yield "event: complete\n"
        yield "data: {\"message\": \"生成完成\"}\n\n"

    except Exception as e:
        logger.error(f"统一 AI 流式输出错误: {e}")
        yield "event: error\n"
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.post("/ai/unified")
async def unified_ai_endpoint(request: UnifiedChatRequest):
    """统一的 AI 流式输出端点"""
    return StreamingResponse(
        unified_ai_stream(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )

# ===== 6. 带重试机制的流式输出 =====
print("=== 场景6：带重试机制的流式输出 ===")

async def ai_stream_with_retry(
    prompt: str,
    max_retries: int = 3
) -> AsyncGenerator[str, None]:
    """带重试机制的 AI 流式输出"""
    retry_count = 0

    while retry_count < max_retries:
        try:
            if retry_count == 0:
                yield "event: start\n"
                yield "data: {\"message\": \"开始生成\"}\n\n"
            else:
                yield "event: retry\n"
                yield f"data: {{\"attempt\": {retry_count + 1}}}\n\n"

            stream = await openai_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                timeout=30.0
            )

            message_id = 0
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    message_id += 1

                    yield f"id: {message_id}\n"
                    yield "event: content\n"
                    yield f"data: {json.dumps({'content': content})}\n\n"

            yield "event: complete\n"
            yield "data: {\"message\": \"生成完成\"}\n\n"
            return

        except asyncio.TimeoutError:
            logger.error(f"超时，重试 {retry_count + 1}/{max_retries}")
            retry_count += 1
            if retry_count >= max_retries:
                yield "event: error\n"
                yield "data: {\"error\": \"请求超时，已达最大重试次数\"}\n\n"
                return
            await asyncio.sleep(1)

        except Exception as e:
            logger.error(f"错误: {e}")
            yield "event: error\n"
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
            return

@app.post("/ai/retry")
async def ai_retry_endpoint(prompt: str):
    """带重试机制的 AI 流式输出端点"""
    return StreamingResponse(
        ai_stream_with_retry(prompt),
        media_type="text/event-stream"
    )

# ===== 7. 多轮对话流式输出 =====
print("=== 场景7：多轮对话流式输出 ===")

# 简单的内存存储（生产环境应该用数据库）
conversation_history = {}

class ConversationRequest(BaseModel):
    session_id: str
    message: str
    provider: Literal["openai", "claude"] = "openai"

async def conversation_stream(request: ConversationRequest) -> AsyncGenerator[str, None]:
    """多轮对话流式输出"""
    try:
        # 获取或创建对话历史
        if request.session_id not in conversation_history:
            conversation_history[request.session_id] = []

        # 添加用户消息
        conversation_history[request.session_id].append({
            "role": "user",
            "content": request.message
        })

        yield "event: start\n"
        yield f"data: {json.dumps({'session_id': request.session_id})}\n\n"

        # 调用 AI API
        if request.provider == "openai":
            stream = await openai_client.chat.completions.create(
                model="gpt-4",
                messages=conversation_history[request.session_id],
                stream=True
            )

            assistant_message = ""
            message_id = 0

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    assistant_message += content
                    message_id += 1

                    yield f"id: {message_id}\n"
                    yield "event: content\n"
                    yield f"data: {json.dumps({'content': content})}\n\n"

            # 保存助手消息
            conversation_history[request.session_id].append({
                "role": "assistant",
                "content": assistant_message
            })

        yield "event: complete\n"
        yield f"data: {json.dumps({
            'message_count': len(conversation_history[request.session_id])
        })}\n\n"

    except Exception as e:
        logger.error(f"多轮对话错误: {e}")
        yield "event: error\n"
        yield f"data: {json.dumps({'error': str(e)})}\n\n"

@app.post("/ai/conversation")
async def conversation_endpoint(request: ConversationRequest):
    """多轮对话流式输出端点"""
    return StreamingResponse(
        conversation_stream(request),
        media_type="text/event-stream"
    )

@app.delete("/ai/conversation/{session_id}")
async def clear_conversation(session_id: str):
    """清除对话历史"""
    if session_id in conversation_history:
        del conversation_history[session_id]
        return {"message": "对话历史已清除"}
    return {"message": "会话不存在"}

# ===== 运行服务器 =====
if __name__ == "__main__":
    import uvicorn

    print("\n" + "="*50)
    print("启动 FastAPI 服务器")
    print("="*50)
    print("\n可用端点：")
    print("  POST http://localhost:8000/openai/basic")
    print("  POST http://localhost:8000/openai/sse")
    print("  POST http://localhost:8000/claude/basic")
    print("  POST http://localhost:8000/claude/sse")
    print("  POST http://localhost:8000/ai/unified")
    print("  POST http://localhost:8000/ai/retry")
    print("  POST http://localhost:8000/ai/conversation")
    print("\n环境变量配置：")
    print("  export OPENAI_API_KEY=your_key")
    print("  export ANTHROPIC_API_KEY=your_key")
    print("\n按 Ctrl+C 停止服务器")
    print("="*50 + "\n")

    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 测试示例

### 使用 curl 测试

```bash
# 1. OpenAI 基础流式输出
curl -X POST http://localhost:8000/openai/basic \
  -H "Content-Type: application/json" \
  -d '{"prompt": "写一首关于春天的诗"}'

# 2. OpenAI SSE 格式
curl -X POST http://localhost:8000/openai/sse \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "解释什么是流式响应"}],
    "model": "gpt-4"
  }'

# 3. Claude 基础流式输出
curl -X POST http://localhost:8000/claude/basic \
  -H "Content-Type: application/json" \
  -d '{"prompt": "写一个Python函数计算斐波那契数列"}'

# 4. 统一接口（OpenAI）
curl -X POST http://localhost:8000/ai/unified \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "你好"}],
    "provider": "openai"
  }'

# 5. 统一接口（Claude）
curl -X POST http://localhost:8000/ai/unified \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "你好"}],
    "provider": "claude"
  }'

# 6. 多轮对话
curl -X POST http://localhost:8000/ai/conversation \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "user123",
    "message": "你好，我是小明"
  }'

curl -X POST http://localhost:8000/ai/conversation \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "user123",
    "message": "我刚才说我叫什么？"
  }'
```

### 使用 Python httpx 测试

```python
import httpx
import json

async def test_openai_stream():
    """测试 OpenAI 流式输出"""
    async with httpx.AsyncClient() as client:
        async with client.stream(
            'POST',
            'http://localhost:8000/openai/basic',
            json={"prompt": "写一首诗"}
        ) as response:
            async for chunk in response.aiter_text():
                print(chunk, end='', flush=True)

async def test_sse_stream():
    """测试 SSE 格式流式输出"""
    async with httpx.AsyncClient() as client:
        async with client.stream(
            'POST',
            'http://localhost:8000/openai/sse',
            json={
                "messages": [{"role": "user", "content": "你好"}],
                "model": "gpt-4"
            }
        ) as response:
            async for line in response.aiter_lines():
                if line.startswith('data: '):
                    data = json.loads(line[6:])
                    if 'content' in data:
                        print(data['content'], end='', flush=True)

# 运行测试
import asyncio
asyncio.run(test_openai_stream())
asyncio.run(test_sse_stream())
```

---

## 前端完整示例（React）

```javascript
import { useState } from 'react';

function AIChat() {
    const [messages, setMessages] = useState([]);
    const [input, setInput] = useState('');
    const [streaming, setStreaming] = useState(false);
    const [provider, setProvider] = useState('openai');

    const sendMessage = async () => {
        if (!input.trim() || streaming) return;

        const userMessage = { role: 'user', content: input };
        setMessages(prev => [...prev, userMessage]);
        setInput('');
        setStreaming(true);

        try {
            const response = await fetch('http://localhost:8000/ai/unified', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    messages: [...messages, userMessage],
                    provider: provider
                })
            });

            const reader = response.body.getReader();
            const decoder = new TextDecoder();
            let assistantMessage = { role: 'assistant', content: '' };
            setMessages(prev => [...prev, assistantMessage]);

            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                const chunk = decoder.decode(value);
                const lines = chunk.split('\n');

                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        try {
                            const data = JSON.parse(line.slice(6));
                            if (data.content) {
                                assistantMessage.content += data.content;
                                setMessages(prev => [
                                    ...prev.slice(0, -1),
                                    { ...assistantMessage }
                                ]);
                            }
                        } catch (e) {
                            // 忽略解析错误
                        }
                    }
                }
            }
        } catch (error) {
            console.error('Error:', error);
            setMessages(prev => [
                ...prev,
                { role: 'error', content: `错误: ${error.message}` }
            ]);
        } finally {
            setStreaming(false);
        }
    };

    return (
        <div className="chat-container">
            <div className="provider-selector">
                <label>
                    <input
                        type="radio"
                        value="openai"
                        checked={provider === 'openai'}
                        onChange={(e) => setProvider(e.target.value)}
                    />
                    OpenAI
                </label>
                <label>
                    <input
                        type="radio"
                        value="claude"
                        checked={provider === 'claude'}
                        onChange={(e) => setProvider(e.target.value)}
                    />
                    Claude
                </label>
            </div>

            <div className="messages">
                {messages.map((msg, i) => (
                    <div key={i} className={`message ${msg.role}`}>
                        <strong>{msg.role}:</strong> {msg.content}
                    </div>
                ))}
            </div>

            <div className="input-area">
                <input
                    value={input}
                    onChange={(e) => setInput(e.target.value)}
                    onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
                    disabled={streaming}
                    placeholder="输入消息..."
                />
                <button onClick={sendMessage} disabled={streaming}>
                    {streaming ? '生成中...' : '发送'}
                </button>
            </div>
        </div>
    );
}

export default AIChat;
```

---

## 关键知识点

### 1. OpenAI 流式 API

```python
stream = await client.chat.completions.create(
    model="gpt-4",
    messages=[...],
    stream=True  # 开启流式模式
)

async for chunk in stream:
    if chunk.choices[0].delta.content:
        yield chunk.choices[0].delta.content
```

### 2. Claude 流式 API

```python
async with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[...]
) as stream:
    async for text in stream.text_stream:
        yield text
```

### 3. SSE 事件格式

```python
yield "event: content\n"
yield f"data: {json.dumps({'content': text})}\n\n"
```

### 4. 错误处理

```python
try:
    async for chunk in stream:
        yield chunk
except Exception as e:
    yield "event: error\n"
    yield f"data: {json.dumps({'error': str(e)})}\n\n"
```

---

## 下一步

完成本场景后，继续学习：
- **场景4**：大文件流式传输
- **场景5**：生产级流式响应系统
