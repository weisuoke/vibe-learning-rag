# 实战代码 05：上下文窗口优化

> Token 计数 + 语义压缩 + 2025-2026 最佳实践

---

## 1. 基于 Token 计数的窗口管理

```python
from collections import deque
import tiktoken
from typing import List, Dict

class TokenWindowManager:
    """基于 Token 计数的窗口管理器"""
    
    def __init__(self, max_tokens: int = 4096, model: str = "gpt-4"):
        self.max_tokens = max_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)
    
    def count_tokens(self, text: str) -> int:
        """计算文本的 token 数量"""
        return len(self.encoding.encode(text))
    
    def get_total_tokens(self) -> int:
        """计算当前所有消息的 token 总数"""
        total = 0
        for msg in self.messages:
            # 消息格式开销：role + content
            total += 4  # 每条消息的格式开销
            total += self.count_tokens(msg["content"])
        return total
    
    def add_message(self, role: str, content: str):
        """添加消息，自动淘汰超出窗口的消息"""
        self.messages.append({"role": role, "content": content})
        
        # 移除旧消息直到满足 token 限制
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.popleft()
    
    def get_context(self) -> List[Dict]:
        return list(self.messages)

# 使用示例
manager = TokenWindowManager(max_tokens=100)

manager.add_message("user", "这是一个很长的消息" * 10)
manager.add_message("assistant", "这是回复" * 5)
manager.add_message("user", "继续对话")

print(f"当前 token 数: {manager.get_total_tokens()}")
print(f"消息数量: {len(manager.get_context())}")
```

---

## 2. 带系统提示的 Token 窗口管理

```python
from collections import deque
import tiktoken
from typing import List, Dict

class TokenWindowWithSystem:
    """带系统提示的 Token 窗口管理器"""
    
    def __init__(
        self,
        system_prompt: str,
        max_tokens: int = 4096,
        model: str = "gpt-4"
    ):
        self.system_prompt = system_prompt
        self.max_tokens = max_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)
        
        # 计算系统提示的 token 数
        self.system_tokens = self.count_tokens(system_prompt) + 4
    
    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))
    
    def get_total_tokens(self) -> int:
        """计算总 token 数（包括系统提示）"""
        total = self.system_tokens
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total
    
    def add_message(self, role: str, content: str):
        """添加消息"""
        self.messages.append({"role": role, "content": content})
        
        # 移除旧消息直到满足限制
        available_tokens = self.max_tokens - self.system_tokens
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.popleft()
    
    def get_context(self) -> List[Dict]:
        """获取上下文（包括系统提示）"""
        return [
            {"role": "system", "content": self.system_prompt},
            *list(self.messages)
        ]

# 使用示例
manager = TokenWindowWithSystem(
    system_prompt="你是一个友好的 AI 助手。",
    max_tokens=200
)

manager.add_message("user", "你好")
manager.add_message("assistant", "你好！有什么可以帮你的？")

print(f"总 token 数: {manager.get_total_tokens()}")
```

---

## 3. 语义压缩：摘要策略

```python
from collections import deque
import tiktoken
from typing import List, Dict

class SummarizingWindowManager:
    """带摘要功能的窗口管理器"""
    
    def __init__(
        self,
        max_tokens: int = 4096,
        summary_threshold: int = 2000,
        model: str = "gpt-4"
    ):
        self.max_tokens = max_tokens
        self.summary_threshold = summary_threshold
        self.messages = deque()
        self.summaries = []  # 存储历史摘要
        self.encoding = tiktoken.encoding_for_model(model)
    
    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))
    
    def get_total_tokens(self) -> int:
        total = 0
        # 摘要的 tokens
        for summary in self.summaries:
            total += self.count_tokens(summary)
        # 当前消息的 tokens
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total
    
    def add_message(self, role: str, content: str):
        """添加消息"""
        self.messages.append({"role": role, "content": content})
        
        # 如果超过摘要阈值，触发摘要
        if self.get_total_tokens() > self.summary_threshold:
            self._summarize_old_messages()
        
        # 如果仍然超过限制，移除最旧的消息
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.popleft()
    
    def _summarize_old_messages(self):
        """摘要旧消息"""
        if len(self.messages) < 4:
            return
        
        # 取前一半消息进行摘要
        num_to_summarize = len(self.messages) // 2
        messages_to_summarize = [self.messages.popleft() for _ in range(num_to_summarize)]
        
        # 生成摘要（实际应该调用 LLM）
        summary = f"[摘要] 讨论了 {num_to_summarize} 条消息的内容"
        self.summaries.append(summary)
    
    def get_context(self) -> List[Dict]:
        """获取上下文（摘要 + 最近消息）"""
        context = []
        
        # 添加摘要
        for summary in self.summaries:
            context.append({"role": "system", "content": summary})
        
        # 添加最近消息
        context.extend(list(self.messages))
        
        return context

# 使用示例
manager = SummarizingWindowManager(
    max_tokens=500,
    summary_threshold=300
)

for i in range(20):
    manager.add_message("user", f"问题 {i}")
    manager.add_message("assistant", f"回答 {i}")

print(f"摘要数量: {len(manager.summaries)}")
print(f"当前消息数: {len(manager.messages)}")
print(f"总 token 数: {manager.get_total_tokens()}")
```

---

## 4. 缓存 Token 计数优化

```python
from collections import deque
import tiktoken
from functools import lru_cache
from typing import List, Dict

class CachedTokenWindowManager:
    """带缓存的 Token 窗口管理器"""
    
    def __init__(self, max_tokens: int = 4096, model: str = "gpt-4"):
        self.max_tokens = max_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)
        self.token_cache = {}  # 缓存消息的 token 数
    
    @lru_cache(maxsize=1000)
    def count_tokens(self, text: str) -> int:
        """缓存 token 计数"""
        return len(self.encoding.encode(text))
    
    def get_total_tokens(self) -> int:
        total = 0
        for i, msg in enumerate(self.messages):
            # 使用缓存
            if i not in self.token_cache:
                self.token_cache[i] = 4 + self.count_tokens(msg["content"])
            total += self.token_cache[i]
        return total
    
    def add_message(self, role: str, content: str):
        """添加消息"""
        self.messages.append({"role": role, "content": content})
        
        # 更新缓存
        msg_index = len(self.messages) - 1
        self.token_cache[msg_index] = 4 + self.count_tokens(content)
        
        # 移除旧消息
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.popleft()
            # 清理缓存
            if 0 in self.token_cache:
                del self.token_cache[0]
            # 重新索引缓存
            self.token_cache = {k-1: v for k, v in self.token_cache.items() if k > 0}
    
    def get_context(self) -> List[Dict]:
        return list(self.messages)
```

---

## 5. 预留响应空间

```python
from collections import deque
import tiktoken
from typing import List, Dict

class OneUptimeStyleManager:
    """OneUptime 2026 风格的上下文管理"""
    
    def __init__(
        self,
        max_tokens: int = 4096,
        reserve_tokens: int = 500,
        model: str = "gpt-4"
    ):
        self.max_tokens = max_tokens
        self.reserve_tokens = reserve_tokens
        self.available_tokens = max_tokens - reserve_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)
    
    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))
    
    def get_total_tokens(self) -> int:
        total = 0
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total
    
    def add_message(self, role: str, content: str):
        """添加消息"""
        self.messages.append({"role": role, "content": content})
        
        # 确保不超过可用 token 数
        while self.get_total_tokens() > self.available_tokens and len(self.messages) > 1:
            self.messages.popleft()
    
    def get_context(self) -> List[Dict]:
        return list(self.messages)
    
    def get_available_response_tokens(self) -> int:
        """获取可用于响应的 token 数"""
        used_tokens = self.get_total_tokens()
        return self.max_tokens - used_tokens

# 使用示例
manager = OneUptimeStyleManager(
    max_tokens=4096,
    reserve_tokens=1000  # 为响应预留 1000 tokens
)

manager.add_message("user", "问题")
manager.add_message("assistant", "回答")

print(f"可用响应 tokens: {manager.get_available_response_tokens()}")
```

---

## 6. 完整的生产级实现

```python
from collections import deque
import tiktoken
from typing import List, Dict, Optional
import time

class ProductionTokenManager:
    """生产级 Token 窗口管理器"""
    
    def __init__(
        self,
        max_tokens: int = 4096,
        reserve_tokens: int = 500,
        system_prompt: Optional[str] = None,
        model: str = "gpt-4"
    ):
        self.max_tokens = max_tokens
        self.reserve_tokens = reserve_tokens
        self.system_prompt = system_prompt
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)
        
        # 计算系统提示的 token 数
        self.system_tokens = 0
        if system_prompt:
            self.system_tokens = self.count_tokens(system_prompt) + 4
        
        # 可用 token 数
        self.available_tokens = max_tokens - reserve_tokens - self.system_tokens
        
        # 统计信息
        self.stats = {
            "total_messages": 0,
            "tokens_used": 0,
            "messages_evicted": 0
        }
    
    def count_tokens(self, text: str) -> int:
        """计算 token 数量"""
        return len(self.encoding.encode(text))
    
    def get_total_tokens(self) -> int:
        """计算当前总 token 数"""
        total = self.system_tokens
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total
    
    def add_message(self, role: str, content: str):
        """添加消息"""
        # 检查单条消息是否超过可用空间
        msg_tokens = 4 + self.count_tokens(content)
        if msg_tokens > self.available_tokens:
            # 截断消息
            max_content_tokens = self.available_tokens - 4 - 10  # 留出余量
            tokens = self.encoding.encode(content)[:max_content_tokens]
            content = self.encoding.decode(tokens) + "..."
        
        self.messages.append({"role": role, "content": content})
        self.stats["total_messages"] += 1
        
        # 移除旧消息直到满足限制
        while self.get_total_tokens() > self.max_tokens - self.reserve_tokens and len(self.messages) > 1:
            self.messages.popleft()
            self.stats["messages_evicted"] += 1
        
        self.stats["tokens_used"] = self.get_total_tokens()
    
    def get_context(self) -> List[Dict]:
        """获取上下文"""
        context = []
        
        if self.system_prompt:
            context.append({"role": "system", "content": self.system_prompt})
        
        context.extend(list(self.messages))
        
        return context
    
    def get_stats(self) -> Dict:
        """获取统计信息"""
        return {
            **self.stats,
            "current_messages": len(self.messages),
            "available_response_tokens": self.max_tokens - self.get_total_tokens(),
            "utilization": f"{self.get_total_tokens() / self.max_tokens:.2%}"
        }

# 使用示例
manager = ProductionTokenManager(
    max_tokens=4096,
    reserve_tokens=1000,
    system_prompt="你是一个友好的 AI 助手。"
)

for i in range(50):
    manager.add_message("user", f"这是问题 {i}" * 10)
    manager.add_message("assistant", f"这是回答 {i}" * 10)

print("统计信息:")
import json
print(json.dumps(manager.get_stats(), indent=2))
```

---

## 7. 性能基准测试

```python
import time
import tiktoken
from collections import deque

def benchmark_token_counting():
    """Token 计数性能测试"""
    encoding = tiktoken.encoding_for_model("gpt-4")
    
    # 测试文本
    texts = [f"这是测试文本 {i}" * 10 for i in range(1000)]
    
    # 方法1：每次都计算
    start = time.time()
    for text in texts:
        tokens = len(encoding.encode(text))
    no_cache_time = time.time() - start
    
    # 方法2：使用缓存
    from functools import lru_cache
    
    @lru_cache(maxsize=1000)
    def count_tokens_cached(text: str) -> int:
        return len(encoding.encode(text))
    
    start = time.time()
    for text in texts:
        tokens = count_tokens_cached(text)
    cache_time = time.time() - start
    
    print(f"无缓存: {no_cache_time:.4f}s")
    print(f"有缓存: {cache_time:.4f}s")
    print(f"性能提升: {no_cache_time / cache_time:.0f} 倍")

benchmark_token_counting()
```

---

**版本**: v1.0
**最后更新**: 2026-02-13
