# 核心概念 05：上下文构建

## 概述

上下文构建是从 JSONL 状态文件中提取对话历史，转换为 LLM API 所需的消息格式，并进行 Token 优化的过程。这是连接状态管理和 LLM 调用的桥梁。

**核心流程：加载 JSONL → 回溯分支 → 转换格式 → Token 优化 → 传递给 LLM**

---

## 为什么需要上下文构建？

### 问题1：LLM 需要完整的对话历史

```typescript
// LLM API 需要的格式
const messages = [
  { role: 'user', content: 'Hello' },
  { role: 'assistant', content: 'Hi there!' },
  { role: 'user', content: 'How are you?' }
];

// 但我们的状态是 JSONL 格式
{"id":"1","parentId":null,"type":"user","content":"Hello"}
{"id":"2","parentId":"1","type":"assistant","content":"Hi there!"}
{"id":"3","parentId":"2","type":"user","content":"How are you?"}

// 需要转换
```

### 问题2：分支选择

```typescript
// JSONL 中有多个分支
1 (root)
├── 2
│   ├── 3 (分支 A)
│   └── 4 (分支 B)

// 需要选择一个分支构建上下文
// 分支 A: [1, 2, 3]
// 分支 B: [1, 2, 4]
```

### 问题3：Token 限制

```typescript
// LLM 有 Context Window 限制
// Claude 3.5 Sonnet: 200K tokens
// GPT-4 Turbo: 128K tokens

// 如果对话历史超过限制，需要压缩或截断
```

---

## 上下文构建的四个步骤

### 步骤1：加载 JSONL 文件

```typescript
import { readFile } from 'fs/promises';

interface SessionEntry {
  id: string;
  parentId?: string;
  timestamp: number;
  type: 'user' | 'assistant' | 'tool_call' | 'tool_result';
  content: string;
  metadata?: Record<string, unknown>;
}

async function loadSession(path: string): Promise<SessionEntry[]> {
  const content = await readFile(path, 'utf-8');

  return content
    .split('\n')
    .filter(line => line.trim())
    .map(line => JSON.parse(line));
}
```

---

### 步骤2：回溯分支历史

```typescript
function buildBranchHistory(
  entries: SessionEntry[],
  branchId: string
): SessionEntry[] {
  const history: SessionEntry[] = [];
  let currentId: string | undefined = branchId;

  // 从叶子节点回溯到根节点
  while (currentId) {
    const entry = entries.find(e => e.id === currentId);
    if (!entry) break;

    history.unshift(entry);  // 插入到开头
    currentId = entry.parentId;
  }

  return history;
}
```

**示例：**
```typescript
// JSONL 内容
const entries = [
  { id: "1", parentId: null, type: "user", content: "Hello" },
  { id: "2", parentId: "1", type: "assistant", content: "Hi!" },
  { id: "3", parentId: "2", type: "user", content: "How are you?" },
  { id: "4", parentId: "3", type: "assistant", content: "Good!" }
];

// 回溯分支 4
const history = buildBranchHistory(entries, "4");
// 返回: [1, 2, 3, 4]
```

---

### 步骤3：转换为 LLM 消息格式

```typescript
interface LLMMessage {
  role: 'user' | 'assistant';
  content: string | Array<{ type: string; [key: string]: any }>;
}

function convertToLLMMessages(entries: SessionEntry[]): LLMMessage[] {
  const messages: LLMMessage[] = [];

  for (const entry of entries) {
    if (entry.type === 'user') {
      messages.push({
        role: 'user',
        content: entry.content
      });
    } else if (entry.type === 'assistant') {
      messages.push({
        role: 'assistant',
        content: entry.content
      });
    } else if (entry.type === 'tool_call') {
      // 工具调用需要特殊处理
      const toolCall = JSON.parse(entry.content);
      messages.push({
        role: 'assistant',
        content: [
          {
            type: 'tool_use',
            id: entry.id,
            name: toolCall.name,
            input: toolCall.input
          }
        ]
      });
    } else if (entry.type === 'tool_result') {
      // 工具结果作为 user 消息
      messages.push({
        role: 'user',
        content: [
          {
            type: 'tool_result',
            tool_use_id: entry.parentId,
            content: entry.content
          }
        ]
      });
    }
  }

  return messages;
}
```

---

### 步骤4：Token 优化

```typescript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

async function countTokens(messages: LLMMessage[]): Promise<number> {
  const response = await client.messages.countTokens({
    model: 'claude-3-5-sonnet-20241022',
    messages: messages
  });

  return response.input_tokens;
}

async function optimizeContext(
  messages: LLMMessage[],
  maxTokens: number = 180000  // 留 20K 给输出
): Promise<LLMMessage[]> {
  const tokens = await countTokens(messages);

  if (tokens <= maxTokens) {
    return messages;  // 不需要优化
  }

  // 策略1：截断旧消息
  return truncateOldMessages(messages, maxTokens);

  // 策略2：压缩中间消息
  // return compressMiddleMessages(messages, maxTokens);

  // 策略3：智能摘要
  // return summarizeMessages(messages, maxTokens);
}
```

---

## Token 优化策略

### 策略1：截断旧消息（最简单）

```typescript
function truncateOldMessages(
  messages: LLMMessage[],
  maxTokens: number
): LLMMessage[] {
  // 保留系统消息和最近的消息
  const systemMessages = messages.filter(m => m.role === 'system');
  const recentMessages = messages.slice(-50);  // 保留最近 50 条

  let result = [...systemMessages, ...recentMessages];
  let tokens = estimateTokens(result);

  // 如果还是太多，继续截断
  while (tokens > maxTokens && result.length > systemMessages.length + 10) {
    result.splice(systemMessages.length, 1);  // 删除最旧的非系统消息
    tokens = estimateTokens(result);
  }

  return result;
}

// 快速估算 Token 数（1 token ≈ 4 字符）
function estimateTokens(messages: LLMMessage[]): number {
  const text = JSON.stringify(messages);
  return Math.ceil(text.length / 4);
}
```

**优点：**
- 实现简单
- 性能好（不需要调用 LLM）

**缺点：**
- 丢失了中间的重要信息
- 可能导致上下文不连贯

---

### 策略2：压缩中间消息（平衡）

```typescript
async function compressMiddleMessages(
  messages: LLMMessage[],
  maxTokens: number
): Promise<LLMMessage[]> {
  const systemMessages = messages.filter(m => m.role === 'system');
  const recentMessages = messages.slice(-20);  // 保留最近 20 条
  const middleMessages = messages.slice(
    systemMessages.length,
    messages.length - 20
  );

  // 压缩中间消息
  const compressed = await summarizeMessages(middleMessages);

  return [
    ...systemMessages,
    {
      role: 'user',
      content: `[Previous conversation summary]\n${compressed}`
    },
    ...recentMessages
  ];
}

async function summarizeMessages(messages: LLMMessage[]): Promise<string> {
  const client = new Anthropic();

  const response = await client.messages.create({
    model: 'claude-3-5-haiku-20241022',  // 使用更便宜的模型
    max_tokens: 1000,
    messages: [
      {
        role: 'user',
        content: `Summarize the following conversation in 500 words or less:\n\n${JSON.stringify(messages)}`
      }
    ]
  });

  return response.content[0].text;
}
```

**优点：**
- 保留了重要信息
- 上下文更连贯

**缺点：**
- 需要额外的 LLM 调用（成本和延迟）
- 摘要可能丢失细节

---

### 策略3：智能摘要（最复杂）

```typescript
async function smartSummarize(
  messages: LLMMessage[],
  maxTokens: number
): Promise<LLMMessage[]> {
  // 1. 识别重要消息（包含工具调用、错误、关键决策）
  const importantMessages = messages.filter(m => {
    const content = JSON.stringify(m.content);
    return (
      content.includes('tool_use') ||
      content.includes('error') ||
      content.includes('Error') ||
      content.length > 1000  // 长消息通常更重要
    );
  });

  // 2. 对不重要的消息进行摘要
  const unimportantMessages = messages.filter(m =>
    !importantMessages.includes(m)
  );

  const summary = await summarizeMessages(unimportantMessages);

  // 3. 合并重要消息和摘要
  return [
    {
      role: 'user',
      content: `[Conversation summary]\n${summary}`
    },
    ...importantMessages
  ];
}
```

**优点：**
- 保留了最重要的信息
- 压缩率高

**缺点：**
- 实现复杂
- 需要额外的 LLM 调用

---

## Pi-mono 的上下文构建实现

### 完整的上下文构建器

```typescript
import { readFile } from 'fs/promises';
import Anthropic from '@anthropic-ai/sdk';

interface ContextBuilderOptions {
  maxTokens?: number;
  compressionStrategy?: 'truncate' | 'compress' | 'smart';
  keepRecentCount?: number;
}

class ContextBuilder {
  private client = new Anthropic();

  constructor(private options: ContextBuilderOptions = {}) {
    this.options.maxTokens = options.maxTokens || 180000;
    this.options.compressionStrategy = options.compressionStrategy || 'truncate';
    this.options.keepRecentCount = options.keepRecentCount || 50;
  }

  // 主入口：构建上下文
  async buildContext(
    sessionPath: string,
    branchId: string
  ): Promise<LLMMessage[]> {
    // 1. 加载 JSONL
    const entries = await this.loadSession(sessionPath);

    // 2. 回溯分支
    const history = this.buildBranchHistory(entries, branchId);

    // 3. 转换格式
    const messages = this.convertToLLMMessages(history);

    // 4. Token 优化
    const optimized = await this.optimizeContext(messages);

    return optimized;
  }

  private async loadSession(path: string): Promise<SessionEntry[]> {
    const content = await readFile(path, 'utf-8');
    return content
      .split('\n')
      .filter(line => line.trim())
      .map(line => JSON.parse(line));
  }

  private buildBranchHistory(
    entries: SessionEntry[],
    branchId: string
  ): SessionEntry[] {
    const history: SessionEntry[] = [];
    let currentId: string | undefined = branchId;

    while (currentId) {
      const entry = entries.find(e => e.id === currentId);
      if (!entry) break;

      history.unshift(entry);
      currentId = entry.parentId;
    }

    return history;
  }

  private convertToLLMMessages(entries: SessionEntry[]): LLMMessage[] {
    const messages: LLMMessage[] = [];

    for (const entry of entries) {
      if (entry.type === 'user') {
        messages.push({
          role: 'user',
          content: entry.content
        });
      } else if (entry.type === 'assistant') {
        messages.push({
          role: 'assistant',
          content: entry.content
        });
      } else if (entry.type === 'tool_call') {
        const toolCall = JSON.parse(entry.content);
        messages.push({
          role: 'assistant',
          content: [
            {
              type: 'tool_use',
              id: entry.id,
              name: toolCall.name,
              input: toolCall.input
            }
          ]
        });
      } else if (entry.type === 'tool_result') {
        messages.push({
          role: 'user',
          content: [
            {
              type: 'tool_result',
              tool_use_id: entry.parentId!,
              content: entry.content
            }
          ]
        });
      }
    }

    return messages;
  }

  private async optimizeContext(messages: LLMMessage[]): Promise<LLMMessage[]> {
    const tokens = await this.countTokens(messages);

    if (tokens <= this.options.maxTokens!) {
      return messages;
    }

    console.log(`Context too large (${tokens} tokens), optimizing...`);

    switch (this.options.compressionStrategy) {
      case 'truncate':
        return this.truncateOldMessages(messages);
      case 'compress':
        return await this.compressMiddleMessages(messages);
      case 'smart':
        return await this.smartSummarize(messages);
      default:
        return this.truncateOldMessages(messages);
    }
  }

  private async countTokens(messages: LLMMessage[]): Promise<number> {
    try {
      const response = await this.client.messages.countTokens({
        model: 'claude-3-5-sonnet-20241022',
        messages: messages as any
      });
      return response.input_tokens;
    } catch (error) {
      // 如果 API 调用失败，使用估算
      return this.estimateTokens(messages);
    }
  }

  private estimateTokens(messages: LLMMessage[]): number {
    const text = JSON.stringify(messages);
    return Math.ceil(text.length / 4);
  }

  private truncateOldMessages(messages: LLMMessage[]): LLMMessage[] {
    const keepCount = this.options.keepRecentCount!;
    if (messages.length <= keepCount) {
      return messages;
    }

    return messages.slice(-keepCount);
  }

  private async compressMiddleMessages(messages: LLMMessage[]): Promise<LLMMessage[]> {
    const keepCount = this.options.keepRecentCount!;
    const recentMessages = messages.slice(-keepCount);
    const oldMessages = messages.slice(0, -keepCount);

    if (oldMessages.length === 0) {
      return messages;
    }

    const summary = await this.summarizeMessages(oldMessages);

    return [
      {
        role: 'user',
        content: `[Previous conversation summary]\n${summary}`
      },
      ...recentMessages
    ];
  }

  private async summarizeMessages(messages: LLMMessage[]): Promise<string> {
    const response = await this.client.messages.create({
      model: 'claude-3-5-haiku-20241022',
      max_tokens: 1000,
      messages: [
        {
          role: 'user',
          content: `Summarize the following conversation concisely:\n\n${JSON.stringify(messages)}`
        }
      ]
    });

    return response.content[0].text;
  }

  private async smartSummarize(messages: LLMMessage[]): Promise<LLMMessage[]> {
    // 识别重要消息
    const importantMessages = messages.filter(m => {
      const content = JSON.stringify(m.content);
      return (
        content.includes('tool_use') ||
        content.includes('error') ||
        content.includes('Error') ||
        content.length > 1000
      );
    });

    const unimportantMessages = messages.filter(m =>
      !importantMessages.includes(m)
    );

    if (unimportantMessages.length === 0) {
      return messages;
    }

    const summary = await this.summarizeMessages(unimportantMessages);

    return [
      {
        role: 'user',
        content: `[Conversation summary]\n${summary}`
      },
      ...importantMessages
    ];
  }
}
```

---

## 使用示例

### 基本使用

```typescript
const builder = new ContextBuilder({
  maxTokens: 180000,
  compressionStrategy: 'truncate',
  keepRecentCount: 50
});

const messages = await builder.buildContext(
  './session.jsonl',
  'branch-id-123'
);

// 传递给 LLM
const response = await client.messages.create({
  model: 'claude-3-5-sonnet-20241022',
  max_tokens: 4096,
  messages: messages
});
```

### 不同压缩策略对比

```typescript
// 策略1：截断（最快）
const builder1 = new ContextBuilder({
  compressionStrategy: 'truncate',
  keepRecentCount: 50
});

// 策略2：压缩（平衡）
const builder2 = new ContextBuilder({
  compressionStrategy: 'compress',
  keepRecentCount: 20
});

// 策略3：智能摘要（最优）
const builder3 = new ContextBuilder({
  compressionStrategy: 'smart'
});

// 性能对比
console.time('truncate');
const messages1 = await builder1.buildContext(sessionPath, branchId);
console.timeEnd('truncate');  // ~10ms

console.time('compress');
const messages2 = await builder2.buildContext(sessionPath, branchId);
console.timeEnd('compress');  // ~500ms（需要 LLM 调用）

console.time('smart');
const messages3 = await builder3.buildContext(sessionPath, branchId);
console.timeEnd('smart');  // ~800ms（需要 LLM 调用）
```

---

## 高级特性

### 1. 缓存上下文

```typescript
class CachedContextBuilder extends ContextBuilder {
  private cache = new Map<string, { messages: LLMMessage[]; timestamp: number }>();
  private cacheTTL = 60000;  // 1 分钟

  async buildContext(sessionPath: string, branchId: string): Promise<LLMMessage[]> {
    const cacheKey = `${sessionPath}:${branchId}`;
    const cached = this.cache.get(cacheKey);

    if (cached && Date.now() - cached.timestamp < this.cacheTTL) {
      console.log('✓ Using cached context');
      return cached.messages;
    }

    const messages = await super.buildContext(sessionPath, branchId);

    this.cache.set(cacheKey, {
      messages,
      timestamp: Date.now()
    });

    return messages;
  }
}
```

### 2. 增量更新

```typescript
class IncrementalContextBuilder extends ContextBuilder {
  private lastBranchId?: string;
  private lastMessages: LLMMessage[] = [];

  async buildContext(sessionPath: string, branchId: string): Promise<LLMMessage[]> {
    // 如果是同一个分支，只需要追加新消息
    if (branchId === this.lastBranchId) {
      const entries = await this.loadSession(sessionPath);
      const newEntries = entries.filter(e =>
        e.timestamp > this.getLastTimestamp()
      );

      if (newEntries.length === 0) {
        return this.lastMessages;
      }

      const newMessages = this.convertToLLMMessages(newEntries);
      this.lastMessages = [...this.lastMessages, ...newMessages];

      return this.lastMessages;
    }

    // 不同分支，重新构建
    this.lastBranchId = branchId;
    this.lastMessages = await super.buildContext(sessionPath, branchId);

    return this.lastMessages;
  }

  private getLastTimestamp(): number {
    // 从最后一条消息的元数据中获取时间戳
    return 0;  // 简化实现
  }
}
```

### 3. 多分支合并

```typescript
async function mergeBranches(
  sessionPath: string,
  branchIds: string[]
): Promise<LLMMessage[]> {
  const builder = new ContextBuilder();
  const allMessages: LLMMessage[] = [];

  for (const branchId of branchIds) {
    const messages = await builder.buildContext(sessionPath, branchId);
    allMessages.push(...messages);
  }

  // 去重（根据内容）
  const unique = allMessages.filter((m, i, arr) =>
    arr.findIndex(m2 => JSON.stringify(m2) === JSON.stringify(m)) === i
  );

  return unique;
}
```

---

## 性能优化

### 1. 并行加载

```typescript
async function buildMultipleContexts(
  sessionPaths: string[],
  branchIds: string[]
): Promise<LLMMessage[][]> {
  const builder = new ContextBuilder();

  // 并行构建多个上下文
  return await Promise.all(
    sessionPaths.map((path, i) =>
      builder.buildContext(path, branchIds[i])
    )
  );
}
```

### 2. 流式加载

```typescript
import { createReadStream } from 'fs';
import { createInterface } from 'readline';

async function* streamSession(path: string): AsyncGenerator<SessionEntry> {
  const stream = createReadStream(path);
  const rl = createInterface({ input: stream });

  for await (const line of rl) {
    if (line.trim()) {
      yield JSON.parse(line);
    }
  }
}

// 使用
for await (const entry of streamSession('./session.jsonl')) {
  console.log(entry);
}
```

---

## 总结

**上下文构建的四个关键步骤：**

1. **加载 JSONL**：读取状态文件，解析为对象数组
2. **回溯分支**：从叶子节点回溯到根节点，重建对话历史
3. **转换格式**：将内部格式转换为 LLM API 所需格式
4. **Token 优化**：根据 Context Window 限制进行压缩或截断

**三种压缩策略对比：**

| 策略 | 性能 | 质量 | 适用场景 |
|------|------|------|---------|
| 截断 | 最快（~10ms） | 一般 | 短对话、实时性要求高 |
| 压缩 | 中等（~500ms） | 较好 | 中等长度对话 |
| 智能摘要 | 最慢（~800ms） | 最好 | 长对话、质量要求高 |

**Pi-mono 的设计优势：**
- 灵活的压缩策略（可配置）
- 增量更新（性能优化）
- 缓存机制（减少重复计算）
- 支持多分支合并

---

**参考文献：**
- [6] Google ADK Context Management: https://github.com/google/adk-python/discussions/826
- [8] AI Agent Guidebook: https://github.com/ai-infra-curriculum/ai-agent-guidebook
