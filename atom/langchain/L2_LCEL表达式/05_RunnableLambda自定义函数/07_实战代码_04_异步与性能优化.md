# 07_实战代码_04_异步与性能优化

> **本文档提供 RunnableLambda 异步与性能优化的完整实战代码。**

---

## 场景 1：异步批处理优化

### 需求

批量调用外部 API，需要高效的异步处理。

### 完整实现

```python
import asyncio
import aiohttp
from langchain_core.runnables import RunnableLambda
import time

# 同步版本（性能差）
def sync_call_api(query: str) -> dict:
    """同步调用 API"""
    import requests
    response = requests.get(f"https://api.example.com/search?q={query}")
    return response.json()

# 异步版本（性能好）
async def async_call_api(query: str) -> dict:
    """异步调用 API"""
    async with aiohttp.ClientSession() as session:
        async with session.get(f"https://api.example.com/search?q={query}") as response:
            return await response.json()

# 创建 Runnable（提供 afunc）
api_caller = RunnableLambda(sync_call_api, afunc=async_call_api)

# 性能对比
async def performance_test():
    queries = [f"query_{i}" for i in range(50)]

    # 方式1: 默认委托（慢）
    start = time.time()
    runnable_slow = RunnableLambda(sync_call_api)
    results1 = await runnable_slow.abatch(queries)
    time1 = time.time() - start
    print(f"默认委托: {time1:.2f}秒")

    # 方式2: 提供 afunc（快）
    start = time.time()
    runnable_fast = RunnableLambda(sync_call_api, afunc=async_call_api)
    results2 = await runnable_fast.abatch(queries)
    time2 = time.time() - start
    print(f"提供 afunc: {time2:.2f}秒")

    print(f"性能提升: {time1 / time2:.1f}x")

# 运行测试
# asyncio.run(performance_test())
```

---

## 场景 2：并发控制与限流

### 需求

控制并发数量，避免触发 API 限流。

### 完整实现

```python
import asyncio
from langchain_core.runnables import RunnableLambda

# 创建信号量（限制并发数）
semaphore = asyncio.Semaphore(10)

async def rate_limited_api_call(query: str) -> dict:
    """带限流的 API 调用"""
    async with semaphore:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"https://api.example.com/search?q={query}") as response:
                return await response.json()

# 创建 Runnable
rate_limited_caller = RunnableLambda(
    lambda x: x,  # 同步版本（占位）
    afunc=rate_limited_api_call
)

# 批量调用（自动限流）
async def batch_with_rate_limit():
    queries = [f"query_{i}" for i in range(100)]
    results = await rate_limited_caller.abatch(queries)
    return results

# 高级限流：令牌桶算法
class TokenBucket:
    """令牌桶限流器"""

    def __init__(self, rate: int, capacity: int):
        self.rate = rate  # 每秒生成的令牌数
        self.capacity = capacity  # 桶容量
        self.tokens = capacity
        self.last_update = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """获取令牌"""
        async with self.lock:
            now = time.time()
            # 补充令牌
            elapsed = now - self.last_update
            self.tokens = min(self.capacity, self.tokens + elapsed * self.rate)
            self.last_update = now

            # 等待令牌
            if self.tokens < 1:
                wait_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(wait_time)
                self.tokens = 0
            else:
                self.tokens -= 1

# 使用令牌桶
bucket = TokenBucket(rate=10, capacity=20)  # 每秒10个请求

async def token_bucket_api_call(query: str) -> dict:
    """使用令牌桶的 API 调用"""
    await bucket.acquire()
    async with aiohttp.ClientSession() as session:
        async with session.get(f"https://api.example.com/search?q={query}") as response:
            return await response.json()

token_bucket_caller = RunnableLambda(lambda x: x, afunc=token_bucket_api_call)
```

---

## 场景 3：批处理优化策略

### 需求

优化批处理性能，减少开销。

### 完整实现

```python
from langchain_core.runnables import RunnableLambda
import asyncio

# 策略1: 分批处理
async def batch_in_chunks(items: list, chunk_size: int = 10):
    """分批处理"""
    results = []
    for i in range(0, len(items), chunk_size):
        chunk = items[i:i + chunk_size]
        chunk_results = await process_chunk(chunk)
        results.extend(chunk_results)
    return results

# 策略2: 动态批大小
async def adaptive_batch(items: list):
    """自适应批大小"""
    # 根据系统负载动态调整
    import psutil
    cpu_percent = psutil.cpu_percent()

    if cpu_percent < 50:
        chunk_size = 20  # 低负载：大批次
    elif cpu_percent < 80:
        chunk_size = 10  # 中负载：中批次
    else:
        chunk_size = 5   # 高负载：小批次

    return await batch_in_chunks(items, chunk_size)

# 策略3: 优先级队列
import heapq

class PriorityBatchProcessor:
    """优先级批处理器"""

    def __init__(self, batch_size: int = 10):
        self.batch_size = batch_size
        self.queue = []
        self.processing = False

    async def add(self, item, priority: int = 0):
        """添加任务"""
        heapq.heappush(self.queue, (-priority, item))

        if not self.processing:
            await self.process()

    async def process(self):
        """处理队列"""
        self.processing = True

        while self.queue:
            # 取出一批
            batch = []
            for _ in range(min(self.batch_size, len(self.queue))):
                if self.queue:
                    _, item = heapq.heappop(self.queue)
                    batch.append(item)

            # 处理批次
            await process_batch(batch)

        self.processing = False

processor = PriorityBatchProcessor(batch_size=10)
```

---

## 场景 4：缓存与记忆化

### 需求

缓存计算结果，避免重复计算。

### 完整实现

```python
from langchain_core.runnables import RunnableLambda
import hashlib
import json
from functools import lru_cache

# 方式1: 内存缓存
cache = {}

def cached_process(input: str) -> str:
    """带缓存的处理"""
    # 生成缓存键
    key = hashlib.md5(input.encode()).hexdigest()

    # 检查缓存
    if key in cache:
        print(f"Cache hit: {input[:50]}")
        return cache[key]

    # 执行处理
    result = expensive_operation(input)

    # 存入缓存
    cache[key] = result
    return result

cached_runnable = RunnableLambda(cached_process)

# 方式2: LRU 缓存
@lru_cache(maxsize=128)
def lru_cached_process(input: str) -> str:
    """LRU 缓存处理"""
    return expensive_operation(input)

lru_runnable = RunnableLambda(lru_cached_process)

# 方式3: 异步缓存
class AsyncCache:
    """异步缓存"""

    def __init__(self, ttl: int = 3600):
        self.cache = {}
        self.ttl = ttl

    async def get(self, key: str):
        """获取缓存"""
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
            else:
                del self.cache[key]
        return None

    async def set(self, key: str, value):
        """设置缓存"""
        self.cache[key] = (value, time.time())

async_cache = AsyncCache(ttl=3600)

async def async_cached_process(input: str) -> str:
    """异步缓存处理"""
    key = hashlib.md5(input.encode()).hexdigest()

    # 检查缓存
    cached = await async_cache.get(key)
    if cached:
        return cached

    # 执行处理
    result = await async_expensive_operation(input)

    # 存入缓存
    await async_cache.set(key, result)
    return result

async_cached_runnable = RunnableLambda(lambda x: x, afunc=async_cached_process)
```

---

## 场景 5：性能监控与分析

### 需求

监控性能指标，识别瓶颈。

### 完整实现

```python
from langchain_core.runnables import RunnableLambda
import time
from collections import defaultdict

class PerformanceMonitor:
    """性能监控器"""

    def __init__(self):
        self.metrics = defaultdict(list)

    def record(self, name: str, duration: float):
        """记录性能指标"""
        self.metrics[name].append(duration)

    def report(self):
        """生成报告"""
        report = {}
        for name, durations in self.metrics.items():
            report[name] = {
                "count": len(durations),
                "total": sum(durations),
                "avg": sum(durations) / len(durations),
                "min": min(durations),
                "max": max(durations)
            }
        return report

monitor = PerformanceMonitor()

def monitored_process(input: str) -> str:
    """带监控的处理"""
    start = time.time()
    result = expensive_operation(input)
    duration = time.time() - start

    monitor.record("process", duration)
    return result

monitored_runnable = RunnableLambda(monitored_process)

# 使用
results = monitored_runnable.batch(["a", "b", "c"])
print(monitor.report())
# {
#     "process": {
#         "count": 3,
#         "total": 0.3,
#         "avg": 0.1,
#         "min": 0.09,
#         "max": 0.11
#     }
# }
```

---

## 完整示例：高性能 RAG 系统

### 需求

构建一个高性能的 RAG 系统，支持大规模并发。

### 完整实现

```python
import asyncio
import aiohttp
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI
import time

# 1. 异步检索
semaphore = asyncio.Semaphore(20)
retrieval_cache = AsyncCache(ttl=3600)

async def async_retrieve(query: str) -> list:
    """异步检索（带缓存和限流）"""
    # 检查缓存
    cached = await retrieval_cache.get(query)
    if cached:
        return cached

    # 限流
    async with semaphore:
        # 执行检索
        docs = await vectorstore.asimilarity_search(query, k=5)

        # 存入缓存
        await retrieval_cache.set(query, docs)
        return docs

retrieve_runnable = RunnableLambda(lambda x: x, afunc=async_retrieve)

# 2. 批量重排序
rerank_semaphore = asyncio.Semaphore(10)

async def async_rerank(docs: list) -> list:
    """异步重排序"""
    async with rerank_semaphore:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "https://api.rerank.com/rerank",
                json={"docs": [d.page_content for d in docs]}
            ) as response:
                ranked = await response.json()
                return ranked["docs"]

rerank_runnable = RunnableLambda(lambda x: x, afunc=async_rerank)

# 3. 异步生成
generation_cache = AsyncCache(ttl=1800)

async def async_generate(context: dict) -> str:
    """异步生成（带缓存）"""
    cache_key = f"{context['query']}:{hash(context['context'])}"

    # 检查缓存
    cached = await generation_cache.get(cache_key)
    if cached:
        return cached

    # 执行生成
    model = ChatOpenAI(model="gpt-4")
    result = await model.ainvoke(context["prompt"])

    # 存入缓存
    await generation_cache.set(cache_key, result.content)
    return result.content

generate_runnable = RunnableLambda(lambda x: x, afunc=async_generate)

# 4. 性能监控
perf_monitor = PerformanceMonitor()

async def monitored_step(name: str, func, input):
    """监控步骤"""
    start = time.time()
    result = await func(input)
    duration = time.time() - start
    perf_monitor.record(name, duration)
    return result

# 5. 完整 RAG 链
async def high_performance_rag(query: str) -> dict:
    """高性能 RAG"""
    start = time.time()

    # 步骤1: 检索
    docs = await monitored_step("retrieve", async_retrieve, query)

    # 步骤2: 重排序
    ranked_docs = await monitored_step("rerank", async_rerank, docs)

    # 步骤3: 生成
    context = {
        "query": query,
        "context": "\n".join(ranked_docs),
        "prompt": f"Context: {ranked_docs}\n\nQuestion: {query}"
    }
    answer = await monitored_step("generate", async_generate, context)

    total_time = time.time() - start

    return {
        "answer": answer,
        "total_time": total_time,
        "metrics": perf_monitor.report()
    }

# 批量处理
async def batch_rag(queries: list[str]) -> list[dict]:
    """批量 RAG"""
    tasks = [high_performance_rag(q) for q in queries]
    return await asyncio.gather(*tasks)

# 测试
async def test_performance():
    queries = [f"query_{i}" for i in range(100)]

    start = time.time()
    results = await batch_rag(queries)
    total_time = time.time() - start

    print(f"处理 {len(queries)} 个查询")
    print(f"总时间: {total_time:.2f}秒")
    print(f"平均时间: {total_time / len(queries):.3f}秒/查询")
    print(f"吞吐量: {len(queries) / total_time:.1f} 查询/秒")

# asyncio.run(test_performance())
```

---

## 性能优化检查清单

### 异步优化
- [ ] 为 I/O 操作提供 afunc
- [ ] 使用 abatch 而非循环 ainvoke
- [ ] 验证性能提升（10-20x）

### 并发控制
- [ ] 使用信号量限制并发数
- [ ] 实现令牌桶限流
- [ ] 监控 API 限流错误

### 缓存策略
- [ ] 缓存检索结果
- [ ] 缓存生成结果
- [ ] 设置合理的 TTL

### 性能监控
- [ ] 记录每个步骤的耗时
- [ ] 生成性能报告
- [ ] 识别瓶颈

---

## 总结

### 核心技巧

1. **提供 afunc**: 性能提升 10-20x
2. **并发控制**: 信号量 + 令牌桶
3. **缓存优化**: 内存缓存 + TTL
4. **批处理**: 分批 + 自适应
5. **性能监控**: 记录 + 分析

### 最佳实践

- 高并发场景必须提供 afunc
- 使用信号量控制并发数
- 实现多层缓存策略
- 持续监控性能指标

---

## 引用来源

- [LangChain Async Guide](https://python.langchain.com/docs/expression_language/how_to/async)
- [MUFG Bank Case Study](https://blog.langchain.dev/customers-mufgbank) - 10倍效率提升

---

**版本**: v1.0
**最后更新**: 2026-02-20
