# 实战代码 - 场景1：学术论文问答

> 使用分层索引实现学术论文的高效问答系统

---

## 场景描述

**需求**：构建一个学术论文问答系统，支持对长篇论文（10K-50K tokens）进行精准问答。

**挑战**：
- 论文结构复杂（摘要、引言、方法、实验、结论）
- 用户查询通常针对特定章节
- 全文检索噪音大，准确率低

**解决方案**：使用分层索引（Hierarchical Indexing）

---

## 完整实现

### 步骤1：环境准备

```python
# 安装依赖
# uv add langchain langchain-openai chromadb pypdf python-dotenv

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import pypdf
from dotenv import load_dotenv
from typing import List, Dict
import os

# 加载环境变量
load_dotenv()
```

### 步骤2：论文加载与预处理

```python
class PaperLoader:
    """学术论文加载器"""

    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path

    def load(self) -> str:
        """加载PDF论文"""
        reader = pypdf.PdfReader(self.pdf_path)
        text = "\n\n".join([
            page.extract_text()
            for page in reader.pages
        ])
        return text

    def extract_sections(self, text: str) -> Dict[str, str]:
        """提取论文章节（简化版）"""
        # 实际应用中可以使用更复杂的章节识别算法
        sections = {
            "abstract": "",
            "introduction": "",
            "methods": "",
            "results": "",
            "conclusion": ""
        }

        # 简单的章节分割（基于关键词）
        lines = text.split("\n")
        current_section = "abstract"

        for line in lines:
            line_lower = line.lower()

            if "abstract" in line_lower:
                current_section = "abstract"
            elif "introduction" in line_lower:
                current_section = "introduction"
            elif "method" in line_lower:
                current_section = "methods"
            elif "result" in line_lower:
                current_section = "results"
            elif "conclusion" in line_lower:
                current_section = "conclusion"

            sections[current_section] += line + "\n"

        return sections
```

### 步骤3：分层索引构建

```python
class HierarchicalPaperIndexer:
    """分层论文索引器"""

    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.section_store = None  # 章节级索引
        self.paragraph_store = None  # 段落级索引

    def build_index(self, text: str, sections: Dict[str, str]) -> None:
        """构建两层索引"""

        # 第一层：章节级索引
        section_docs = []
        for section_name, section_text in sections.items():
            if section_text.strip():
                section_docs.append({
                    "text": section_text,
                    "metadata": {
                        "section": section_name,
                        "level": "section"
                    }
                })

        self.section_store = Chroma.from_texts(
            texts=[doc["text"] for doc in section_docs],
            metadatas=[doc["metadata"] for doc in section_docs],
            embedding=self.embeddings,
            collection_name="paper_sections"
        )

        # 第二层：段落级索引
        paragraph_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", " ", ""]
        )

        paragraph_docs = []
        for section_name, section_text in sections.items():
            if section_text.strip():
                paragraphs = paragraph_splitter.split_text(section_text)

                for i, paragraph in enumerate(paragraphs):
                    paragraph_docs.append({
                        "text": paragraph,
                        "metadata": {
                            "section": section_name,
                            "paragraph_index": i,
                            "level": "paragraph"
                        }
                    })

        self.paragraph_store = Chroma.from_texts(
            texts=[doc["text"] for doc in paragraph_docs],
            metadatas=[doc["metadata"] for doc in paragraph_docs],
            embedding=self.embeddings,
            collection_name="paper_paragraphs"
        )

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """两阶段分层检索"""

        # 第一阶段：章节级粗检索
        relevant_sections = self.section_store.similarity_search(
            query, k=2
        )

        # 提取相关章节名称
        section_names = [
            doc.metadata['section']
            for doc in relevant_sections
        ]

        print(f"相关章节: {section_names}")

        # 第二阶段：段落级精检索
        # 只在相关章节内检索
        all_paragraphs = self.paragraph_store.similarity_search(
            query, k=50
        )

        # 过滤：只保留相关章节内的段落
        relevant_paragraphs = [
            doc for doc in all_paragraphs
            if doc.metadata['section'] in section_names
        ][:top_k]

        return relevant_paragraphs
```

### 步骤4：问答系统构建

```python
class PaperQASystem:
    """学术论文问答系统"""

    def __init__(self, indexer: HierarchicalPaperIndexer):
        self.indexer = indexer
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    def answer(self, question: str) -> Dict:
        """回答问题"""

        # 1. 检索相关段落
        relevant_docs = self.indexer.search(question, top_k=5)

        # 2. 构建上下文
        context = "\n\n".join([
            f"[{doc.metadata['section']}] {doc.page_content}"
            for doc in relevant_docs
        ])

        # 3. 生成答案
        prompt = PromptTemplate(
            template="""
            基于以下论文内容回答问题：

            论文内容：
            {context}

            问题：{question}

            请提供详细的答案，并引用具体的章节。

            答案：
            """,
            input_variables=["context", "question"]
        )

        answer = self.llm.predict(
            prompt.format(context=context, question=question)
        )

        return {
            "question": question,
            "answer": answer,
            "sources": [
                {
                    "section": doc.metadata['section'],
                    "text": doc.page_content[:200] + "..."
                }
                for doc in relevant_docs
            ]
        }
```

### 步骤5：完整示例

```python
def main():
    """完整示例"""

    # 1. 加载论文
    print("加载论文...")
    loader = PaperLoader("paper.pdf")
    text = loader.load()
    sections = loader.extract_sections(text)

    print(f"论文总长度: {len(text)} 字符")
    print(f"识别章节: {list(sections.keys())}")

    # 2. 构建分层索引
    print("\n构建分层索引...")
    indexer = HierarchicalPaperIndexer()
    indexer.build_index(text, sections)

    # 3. 创建问答系统
    print("\n创建问答系统...")
    qa_system = PaperQASystem(indexer)

    # 4. 测试问答
    questions = [
        "What is the main contribution of this paper?",
        "What methodology was used in the experiments?",
        "What are the key findings?",
        "What are the limitations of this work?"
    ]

    for question in questions:
        print(f"\n问题: {question}")
        result = qa_system.answer(question)

        print(f"答案: {result['answer']}")
        print(f"\n引用章节:")
        for source in result['sources']:
            print(f"  - [{source['section']}] {source['text']}")

if __name__ == "__main__":
    main()
```

---

## 2026年生产级优化

### 优化1：使用LATTICE框架

```python
from lattice import LATTICERetriever

class AdvancedPaperQASystem:
    """使用LATTICE的高级论文问答系统"""

    def __init__(self, pdf_path: str):
        self.retriever = LATTICERetriever(
            embedding_model="text-embedding-3-large",
            num_levels=3,  # 三层索引：章节、段落、句子
            adaptive=True  # 自适应层级调整
        )

        # 加载论文
        loader = PaperLoader(pdf_path)
        text = loader.load()

        # 构建索引
        self.retriever.build_index([text])

    def answer(self, question: str) -> Dict:
        """回答问题"""

        # LATTICE自动决定使用哪些层级
        results = self.retriever.retrieve(
            query=question,
            top_k=5
        )

        # 生成答案
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        context = "\n\n".join([doc.page_content for doc in results])

        prompt = f"""
        基于以下论文内容回答问题：

        论文内容：
        {context}

        问题：{question}

        答案：
        """

        answer = llm.predict(prompt)

        return {
            "question": question,
            "answer": answer,
            "sources": results
        }
```

### 优化2：缓存优化

```python
from functools import lru_cache
import hashlib

class CachedPaperQASystem(PaperQASystem):
    """带缓存的论文问答系统"""

    @lru_cache(maxsize=100)
    def _cached_search(self, query_hash: str, top_k: int):
        """缓存检索结果"""
        # 实际检索逻辑
        return self.indexer.search(query_hash, top_k)

    def answer(self, question: str) -> Dict:
        """回答问题（带缓存）"""

        # 计算问题哈希
        query_hash = hashlib.md5(question.encode()).hexdigest()

        # 使用缓存检索
        relevant_docs = self._cached_search(query_hash, top_k=5)

        # 生成答案（与原实现相同）
        context = "\n\n".join([
            f"[{doc.metadata['section']}] {doc.page_content}"
            for doc in relevant_docs
        ])

        prompt = PromptTemplate(
            template="""
            基于以下论文内容回答问题：

            论文内容：
            {context}

            问题：{question}

            答案：
            """,
            input_variables=["context", "question"]
        )

        answer = self.llm.predict(
            prompt.format(context=context, question=question)
        )

        return {
            "question": question,
            "answer": answer,
            "sources": [
                {
                    "section": doc.metadata['section'],
                    "text": doc.page_content[:200] + "..."
                }
                for doc in relevant_docs
            ]
        }
```

### 优化3：批量问答

```python
from concurrent.futures import ThreadPoolExecutor

class BatchPaperQASystem(PaperQASystem):
    """批量论文问答系统"""

    def batch_answer(
        self,
        questions: List[str],
        max_workers: int = 5
    ) -> List[Dict]:
        """批量回答问题"""

        def process_question(question):
            return self.answer(question)

        # 并行处理
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(process_question, questions))

        return results
```

---

## 性能评估

### 评估代码

```python
import time

def evaluate_paper_qa(qa_system, test_questions, ground_truth):
    """评估论文问答系统"""

    metrics = {
        "accuracy": 0.0,
        "avg_retrieval_time": 0.0,
        "avg_answer_time": 0.0,
        "section_accuracy": 0.0  # 章节定位准确率
    }

    total_retrieval_time = 0
    total_answer_time = 0
    correct_sections = 0

    for question, truth in zip(test_questions, ground_truth):
        # 检索时间
        start_time = time.time()
        relevant_docs = qa_system.indexer.search(question, top_k=5)
        retrieval_time = time.time() - start_time
        total_retrieval_time += retrieval_time

        # 答案生成时间
        start_time = time.time()
        result = qa_system.answer(question)
        answer_time = time.time() - start_time
        total_answer_time += answer_time

        # 章节准确率
        retrieved_sections = set([
            doc.metadata['section']
            for doc in relevant_docs
        ])
        if truth['section'] in retrieved_sections:
            correct_sections += 1

    metrics["avg_retrieval_time"] = total_retrieval_time / len(test_questions)
    metrics["avg_answer_time"] = total_answer_time / len(test_questions)
    metrics["section_accuracy"] = correct_sections / len(test_questions)

    return metrics
```

---

## 常见问题

### Q1: 如何处理没有明确章节结构的论文？

**A**: 使用自动章节识别

```python
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering

def auto_extract_sections(text: str, num_sections: int = 5) -> Dict[str, str]:
    """自动提取章节"""

    # 分段
    paragraphs = text.split("\n\n")

    # 计算段落embedding
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(paragraphs)

    # 聚类
    clustering = AgglomerativeClustering(n_clusters=num_sections)
    labels = clustering.fit_predict(embeddings)

    # 构建章节
    sections = {}
    for i in range(num_sections):
        section_paragraphs = [
            p for p, label in zip(paragraphs, labels) if label == i
        ]
        sections[f"section_{i}"] = "\n\n".join(section_paragraphs)

    return sections
```

### Q2: 如何提升检索精度？

**A**: 使用加权融合

```python
def weighted_search(
    query: str,
    section_store,
    paragraph_store,
    section_weight: float = 0.3,
    paragraph_weight: float = 0.7,
    top_k: int = 5
) -> List[Dict]:
    """加权融合检索"""

    # 章节级检索
    section_results = section_store.similarity_search_with_score(
        query, k=3
    )

    # 段落级检索
    paragraph_results = paragraph_store.similarity_search_with_score(
        query, k=20
    )

    # 加权融合
    scored_paragraphs = []
    for para_doc, para_score in paragraph_results:
        section_name = para_doc.metadata['section']

        # 找到对应章节的分数
        section_score = 0.0
        for sect_doc, sect_score in section_results:
            if sect_doc.metadata['section'] == section_name:
                section_score = sect_score
                break

        # 加权融合
        final_score = (
            section_weight * section_score +
            paragraph_weight * para_score
        )

        scored_paragraphs.append((para_doc, final_score))

    # 排序
    scored_paragraphs.sort(key=lambda x: x[1], reverse=True)

    return [doc for doc, score in scored_paragraphs[:top_k]]
```

---

## 核心记忆

### 关键点

1. **分层索引**：章节级（粗）+ 段落级（细）
2. **两阶段检索**：先定位章节，再精确检索段落
3. **章节识别**：自动或手动提取论文章节结构
4. **性能优化**：缓存、批量处理、并行检索

### 2026年技术

1. **LATTICE框架**：自适应层级调整，准确率提升23%
2. **加权融合**：章节级和段落级分数融合
3. **自动章节识别**：基于聚类的章节提取

### 生产级配置

- 章节级chunk size: 2000 tokens
- 段落级chunk size: 500 tokens
- Top-K: 章节级2-3，段落级5-10
- 缓存: LRU缓存，maxsize=100

---

**版本**: v1.0 (2025-2026 Research Edition)
**最后更新**: 2026-02-17
**代码行数**: ~200行
