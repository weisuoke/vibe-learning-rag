# RunnableParallel并行执行 - 最小可用知识

> 掌握20%的核心知识，解决80%的并行执行问题

---

## 学习目标

完成本文档学习后，你将能够：
- ✅ 编写基础的并行执行代码
- ✅ 识别适合并行的场景
- ✅ 理解并行执行的性能优势
- ✅ 避免常见的并行陷阱

**预计学习时间**: 10-15分钟

---

## 核心知识1：基础语法

### 概念说明

**RunnableParallel 将多个 Runnable 包装成一个并行执行单元**

```python
from langchain_core.runnables import RunnableParallel

# 基础语法
parallel = RunnableParallel(
    key1=runnable1,
    key2=runnable2,
    key3=runnable3
)

# 执行
result = parallel.invoke(input)
# 返回: {"key1": result1, "key2": result2, "key3": result3}
```

**关键点**：
1. 使用**字典形式**定义任务（键名 = 任务名）
2. 所有任务**同时执行**
3. 返回结果是**字典**（键名对应结果）

---

### 代码示例

```python
"""
最小可用示例：多维度文本分析
"""
from langchain_core.runnables import RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1. 定义模型
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 2. 定义三个分析任务
sentiment_chain = (
    ChatPromptTemplate.from_template("分析以下文本的情感（积极/消极/中性）：\n{text}")
    | llm
)

topic_chain = (
    ChatPromptTemplate.from_template("提取以下文本的主题（一句话）：\n{text}")
    | llm
)

summary_chain = (
    ChatPromptTemplate.from_template("用一句话总结以下文本：\n{text}")
    | llm
)

# 3. 创建并行执行器
parallel = RunnableParallel(
    sentiment=sentiment_chain,
    topic=topic_chain,
    summary=summary_chain
)

# 4. 执行
text = "LangChain 是一个强大的 AI 应用开发框架，它简化了 LLM 应用的构建过程。"
result = parallel.invoke({"text": text})

# 5. 查看结果
print("情感分析:", result["sentiment"].content)
print("主题提取:", result["topic"].content)
print("文本总结:", result["summary"].content)
```

**运行输出**：
```
情感分析: 积极
主题提取: LangChain AI应用开发框架
文本总结: LangChain简化了LLM应用开发
```

**性能对比**：
- 串行执行：4.5秒（1.5s × 3）
- 并行执行：1.5秒（max(1.5s, 1.5s, 1.5s)）
- **提升：3倍**

---

### 实际应用场景

**场景1：内容审核系统**
```python
# 同时检测多种违规内容
moderation = RunnableParallel(
    violence=violence_detector,
    nsfw=nsfw_detector,
    hate_speech=hate_detector
)
result = moderation.invoke({"content": user_content})
```

**场景2：多语言翻译**
```python
# 同时翻译成多种语言
translation = RunnableParallel(
    chinese=chinese_translator,
    japanese=japanese_translator,
    korean=korean_translator
)
result = translation.invoke({"text": english_text})
```

**场景3：RAG 多路检索**
```python
# 同时从多个数据源检索
retrieval = RunnableParallel(
    vector_db=vector_retriever,
    knowledge_graph=kg_retriever,
    web_search=search_retriever
)
context = retrieval.invoke({"query": user_question})
```

---

## 核心知识2：与LCEL管道组合

### 概念说明

**RunnableParallel 可以与其他 Runnable 无缝组合**

```python
# 并行执行 → 后续处理
chain = parallel | post_processor

# 前置处理 → 并行执行
chain = pre_processor | parallel

# 完整管道
chain = pre_processor | parallel | post_processor
```

**关键点**：
1. 并行执行的**输出是字典**
2. 后续处理需要**接收字典输入**
3. 可以使用 `RunnableLambda` 进行格式转换

---

### 代码示例

```python
"""
LCEL组合示例：内容生成 + 质量检查
"""
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o-mini")

# 1. 内容生成链
generator = (
    ChatPromptTemplate.from_template("为以下主题写一篇100字的文章：\n{topic}")
    | llm
)

# 2. 质量检查链
quality_checker = (
    ChatPromptTemplate.from_template("评估以下文章的质量（1-10分）：\n{content}")
    | llm
)

# 3. 并行执行：生成内容的同时进行预检查
parallel = RunnableParallel(
    content=generator,
    # 使用相同的输入进行预检查（基于主题）
    topic_check=ChatPromptTemplate.from_template("主题'{topic}'是否合适？") | llm
)

# 4. 后处理：合并结果
def merge_results(data):
    return {
        "article": data["content"].content,
        "topic_ok": data["topic_check"].content,
        "status": "ready_for_review"
    }

# 5. 完整管道
chain = parallel | RunnableLambda(merge_results)

# 6. 执行
result = chain.invoke({"topic": "人工智能的未来"})
print(result)
```

**运行输出**：
```python
{
    "article": "人工智能正在改变世界...",
    "topic_ok": "是，该主题合适",
    "status": "ready_for_review"
}
```

---

### 实际应用场景

**场景1：博客文章生成系统**
```python
# 生成文章 + 生成配图提示 + 生成SEO标签
blog_pipeline = RunnableParallel(
    article=article_generator,
    image_prompt=image_prompt_generator,
    seo_tags=seo_tag_generator
) | format_blog_post
```

**场景2：智能客服系统**
```python
# 理解意图 + 检索知识 + 生成回复
customer_service = (
    intent_classifier
    | RunnableParallel(
        knowledge=knowledge_retriever,
        sentiment=sentiment_analyzer
    )
    | response_generator
)
```

**场景3：代码审查系统**
```python
# 静态分析 + 安全检查 + 风格检查
code_review = RunnableParallel(
    static_analysis=static_analyzer,
    security_check=security_scanner,
    style_check=style_checker
) | generate_review_report
```

---

## 核心知识3：异步执行

### 概念说明

**RunnableParallel 支持异步执行，性能更好**

```python
# 同步执行（使用线程池）
result = parallel.invoke(input)

# 异步执行（使用 asyncio）
result = await parallel.ainvoke(input)
```

**关键点**：
1. 异步执行**性能更好**（内存开销更小）
2. 需要在 `async` 函数中使用
3. 适合**高并发**场景（100+并发）

---

### 代码示例

```python
"""
异步执行示例：批量处理
"""
import asyncio
from langchain_core.runnables import RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o-mini")

# 1. 定义分析任务
sentiment_chain = (
    ChatPromptTemplate.from_template("分析情感: {text}")
    | llm
)

topic_chain = (
    ChatPromptTemplate.from_template("提取主题: {text}")
    | llm
)

# 2. 创建并行执行器
parallel = RunnableParallel(
    sentiment=sentiment_chain,
    topic=topic_chain
)

# 3. 异步批量处理
async def process_batch(texts):
    """批量处理多个文本"""
    tasks = [parallel.ainvoke({"text": text}) for text in texts]
    results = await asyncio.gather(*tasks)
    return results

# 4. 执行
texts = [
    "LangChain 很棒！",
    "我不喜欢这个产品。",
    "天气不错。"
]

# 运行异步函数
results = asyncio.run(process_batch(texts))

# 5. 查看结果
for i, result in enumerate(results):
    print(f"\n文本{i+1}: {texts[i]}")
    print(f"情感: {result['sentiment'].content}")
    print(f"主题: {result['topic'].content}")
```

**运行输出**：
```
文本1: LangChain 很棒！
情感: 积极
主题: LangChain评价

文本2: 我不喜欢这个产品。
情感: 消极
主题: 产品评价

文本3: 天气不错。
情感: 中性
主题: 天气
```

**性能对比**：
- 同步批处理：4.5秒（3个文本 × 1.5秒）
- 异步批处理：1.5秒（并行处理）
- **提升：3倍**

---

### 实际应用场景

**场景1：批量内容审核**
```python
async def moderate_batch(contents):
    """批量审核内容"""
    moderation = RunnableParallel(
        violence=violence_detector,
        nsfw=nsfw_detector
    )
    tasks = [moderation.ainvoke({"content": c}) for c in contents]
    return await asyncio.gather(*tasks)
```

**场景2：实时数据分析**
```python
async def analyze_stream(data_stream):
    """实时分析数据流"""
    analyzer = RunnableParallel(
        stats=stats_calculator,
        anomaly=anomaly_detector
    )
    async for data in data_stream:
        result = await analyzer.ainvoke({"data": data})
        yield result
```

**场景3：高并发API服务**
```python
# FastAPI 集成
from fastapi import FastAPI

app = FastAPI()

@app.post("/analyze")
async def analyze(text: str):
    parallel = RunnableParallel(
        sentiment=sentiment_chain,
        topic=topic_chain
    )
    result = await parallel.ainvoke({"text": text})
    return result
```

---

## 核心知识4：错误处理

### 概念说明

**使用 `with_fallback` 处理并行执行中的错误**

```python
# 为每个任务添加备用方案
parallel = RunnableParallel(
    task1=chain1.with_fallback([backup1]),
    task2=chain2.with_fallback([backup2])
)
```

**关键点**：
1. 默认情况下，**一个任务失败会导致整体失败**
2. 使用 `with_fallback` 提供**备用方案**
3. 生产环境**必须**添加错误处理

---

### 代码示例

```python
"""
错误处理示例：多模型容错
"""
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1. 定义主模型和备用模型
primary_llm = ChatOpenAI(model="gpt-4o", temperature=0)
backup_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 2. 创建带容错的链
sentiment_chain = (
    ChatPromptTemplate.from_template("分析情感: {text}")
    | primary_llm
).with_fallback([
    ChatPromptTemplate.from_template("分析情感: {text}") | backup_llm
])

topic_chain = (
    ChatPromptTemplate.from_template("提取主题: {text}")
    | primary_llm
).with_fallback([
    ChatPromptTemplate.from_template("提取主题: {text}") | backup_llm
])

# 3. 创建并行执行器
parallel = RunnableParallel(
    sentiment=sentiment_chain,
    topic=topic_chain
)

# 4. 执行（即使主模型失败，也会使用备用模型）
try:
    result = parallel.invoke({"text": "LangChain 很棒！"})
    print("成功:", result)
except Exception as e:
    print("失败:", e)
```

**容错策略**：
```
主模型失败 → 自动切换到备用模型 → 返回结果
```

---

### 实际应用场景

**场景1：多模型集成（高可用）**
```python
# 主模型 + 备用模型
analysis = RunnableParallel(
    gpt4=gpt4_chain.with_fallback([gpt4_mini_chain]),
    claude=claude_chain.with_fallback([default_response])
)
```

**场景2：多数据源检索（部分失败容错）**
```python
# 向量库 + 知识图谱 + 搜索引擎
retrieval = RunnableParallel(
    vector=vector_retriever.with_fallback([empty_list]),
    kg=kg_retriever.with_fallback([empty_list]),
    search=search_retriever.with_fallback([empty_list])
)
# 即使某个数据源失败，仍能返回其他结果
```

**场景3：外部API调用（超时处理）**
```python
# 设置超时 + 备用方案
external_api = RunnableParallel(
    weather=weather_api.with_config({"timeout": 5}).with_fallback([default_weather]),
    news=news_api.with_config({"timeout": 5}).with_fallback([default_news])
)
```

---

## 核心知识5：性能优化

### 概念说明

**控制并发数量，避免资源耗尽**

```python
# LangGraph 中设置并发限制
graph.add_node(
    "parallel_tasks",
    parallel_node,
    max_concurrency=5  # 最多5个并发
)
```

**关键点**：
1. 并发不是越多越好
2. 需要考虑 **API 限流**（如 OpenAI: 500 RPM）
3. 需要考虑 **内存和成本**

---

### 代码示例

```python
"""
性能优化示例：批处理 + 并发控制
"""
import asyncio
from langchain_core.runnables import RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o-mini")

# 1. 定义分析任务
analyzer = RunnableParallel(
    sentiment=ChatPromptTemplate.from_template("分析情感: {text}") | llm,
    topic=ChatPromptTemplate.from_template("提取主题: {text}") | llm
)

# 2. 批处理函数（控制并发）
async def process_with_limit(texts, max_concurrent=5):
    """
    批处理文本，控制并发数量

    Args:
        texts: 文本列表
        max_concurrent: 最大并发数
    """
    semaphore = asyncio.Semaphore(max_concurrent)

    async def limited_process(text):
        async with semaphore:
            return await analyzer.ainvoke({"text": text})

    tasks = [limited_process(text) for text in texts]
    results = await asyncio.gather(*tasks)
    return results

# 3. 执行
texts = [f"文本{i}" for i in range(20)]  # 20个文本

# 不限制并发（可能触发限流）
# results = await asyncio.gather(*[analyzer.ainvoke({"text": t}) for t in texts])

# 限制并发（推荐）
results = asyncio.run(process_with_limit(texts, max_concurrent=5))

print(f"处理了 {len(results)} 个文本")
```

**性能对比**：
```
无限制并发（20个）：
- 可能触发 API 限流
- 内存占用高
- 成本高（同时消耗）

限制并发（5个）：
- 稳定，无限流
- 内存占用可控
- 成本分散
```

---

### 实际应用场景

**场景1：大规模批处理**
```python
# 处理10000个文档，每次5个并发
async def process_large_batch(documents):
    for i in range(0, len(documents), 5):
        batch = documents[i:i+5]
        results = await process_batch(batch)
        yield results
```

**场景2：成本优化（langasync）**
```python
# 使用批处理API降低成本50%
from langasync import wrap_chain

async_parallel = wrap_chain(parallel, batch_size=10)
results = await async_parallel.abatch(inputs)
```

**场景3：实时监控**
```python
# 使用 LangSmith 监控性能
from langsmith import trace

@trace
def parallel_analysis(text):
    return parallel.invoke({"text": text})
```

---

## 快速参考卡

### 基础语法
```python
from langchain_core.runnables import RunnableParallel

parallel = RunnableParallel(
    task1=chain1,
    task2=chain2
)
result = parallel.invoke(input)
# 返回: {"task1": result1, "task2": result2}
```

### 异步执行
```python
result = await parallel.ainvoke(input)
```

### 错误处理
```python
parallel = RunnableParallel(
    task1=chain1.with_fallback([backup1]),
    task2=chain2.with_fallback([backup2])
)
```

### LCEL组合
```python
chain = pre_processor | parallel | post_processor
```

### 并发控制
```python
# LangGraph
graph.add_node("tasks", node, max_concurrency=5)

# 手动控制
semaphore = asyncio.Semaphore(5)
async with semaphore:
    result = await parallel.ainvoke(input)
```

---

## 学习检查清单

完成以下检查，确保掌握最小可用知识：

- [ ] 能够编写基础的 RunnableParallel 代码
- [ ] 理解并行执行返回字典格式
- [ ] 知道如何与 LCEL 管道组合
- [ ] 能够使用异步执行（ainvoke）
- [ ] 知道如何添加错误处理（with_fallback）
- [ ] 理解并发控制的重要性
- [ ] 能够识别适合并行的场景

---

## 这些知识足以

掌握以上5个核心知识点后，你已经可以：

✅ **基础应用**：
- 编写多维度文本分析
- 实现多模型并行调用
- 构建多数据源检索系统

✅ **生产应用**：
- 处理批量数据
- 实现容错机制
- 控制并发数量

✅ **性能优化**：
- 使用异步执行提升性能
- 避免 API 限流
- 降低成本

---

## 下一步学习

### 如果你想深入理解原理
→ 阅读 `02_第一性原理.md`
→ 阅读 `03_核心概念_01_并行执行机制.md`

### 如果你想看更多实战代码
→ 阅读 `07_实战代码_01_基础并行场景.md`
→ 阅读 `07_实战代码_03_错误处理场景.md`

### 如果你想准备面试
→ 阅读 `08_面试必问.md`
→ 阅读 `09_化骨绵掌.md`

---

## 参考资料

[来源: Using RunnableParallel in LangChain LCEL for Concurrent LLM Workflows - https://www.jellyfishtechnologies.com/using-runnableparallel-langchain-lcel-concurrent-llm-workflows, 访问日期: 2026-02-18]

[来源: Building Production-Ready AI Pipelines with LangChain Runnables - https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557, 访问日期: 2026-02-18]

---

**版本**: v1.0
**最后更新**: 2026-02-18
**核心知识点**: 5个（20%核心）
