# å®æˆ˜ä»£ç  04 - æµå¼è¾“å‡º

> å®æ—¶è¿”å› LLM ç”Ÿæˆå†…å®¹ï¼Œæå‡ç”¨æˆ·ä½“éªŒ

---

## åœºæ™¯1ï¼šåŸºç¡€æµå¼è¾“å‡º

### éœ€æ±‚

å®ç°é€å­—æ˜¾ç¤º LLM ç”Ÿæˆå†…å®¹ï¼Œåƒ ChatGPT ä¸€æ ·ã€‚

---

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯1ï¼šåŸºç¡€æµå¼è¾“å‡º
æ¼”ç¤ºï¼šastream() é€å—è¿”å›å†…å®¹
"""

import os
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ===== 1. å®šä¹‰é“¾ =====
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„æ•…äº‹")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
parser = StrOutputParser()

chain = prompt | llm | parser

# ===== 2. æ™®é€šè¾“å‡ºï¼ˆç­‰å¾…å®Œæ•´ç»“æœï¼‰ =====
print("=== æ™®é€šè¾“å‡º ===")

import time
start = time.time()
result = chain.invoke({"topic": "ç¨‹åºå‘˜"})
print(f"è€—æ—¶: {time.time() - start:.1f}ç§’")
print(f"ç»“æœ: {result}")

# ===== 3. æµå¼è¾“å‡ºï¼ˆé€å—æ˜¾ç¤ºï¼‰ =====
print("\n\n=== æµå¼è¾“å‡º ===")

async def stream_example():
    start = time.time()
    async for chunk in chain.astream({"topic": "ç¨‹åºå‘˜"}):
        print(chunk, end="", flush=True)
    print(f"\n\nè€—æ—¶: {time.time() - start:.1f}ç§’")

asyncio.run(stream_example())
```

**è¿è¡Œè¾“å‡ºç¤ºä¾‹**ï¼š
```
=== æ™®é€šè¾“å‡º ===
è€—æ—¶: 3.2ç§’
ç»“æœ: ä»å‰æœ‰ä¸€ä¸ªç¨‹åºå‘˜ï¼Œä»–æ¯å¤©éƒ½åœ¨å†™ä»£ç ...ï¼ˆå®Œæ•´æ•…äº‹ï¼‰

=== æµå¼è¾“å‡º ===
ä»å‰æœ‰ä¸€ä¸ªç¨‹åºå‘˜ï¼Œä»–æ¯å¤©éƒ½åœ¨å†™ä»£ç ...ï¼ˆé€å­—æ˜¾ç¤ºï¼‰

è€—æ—¶: 3.2ç§’
```

**å…³é”®ç‚¹**ï¼š
- æ€»è€—æ—¶ç›¸åŒï¼Œä½†ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿæ›´çŸ­
- æµå¼è¾“å‡ºç«‹å³å¼€å§‹æ˜¾ç¤ºï¼Œæ™®é€šè¾“å‡ºéœ€è¦ç­‰å¾…å®Œæ•´ç»“æœ

---

## åœºæ™¯2ï¼šFastAPI æµå¼ç«¯ç‚¹

### éœ€æ±‚

åœ¨ FastAPI ä¸­å®ç°æµå¼å“åº”ï¼Œå‰ç«¯å®æ—¶æ˜¾ç¤ºã€‚

---

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯2ï¼šFastAPI æµå¼ç«¯ç‚¹
æ¼”ç¤ºï¼šStreamingResponse + Server-Sent Events
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

app = FastAPI()

# å®šä¹‰é“¾
prompt = ChatPromptTemplate.from_template("å›ç­”ï¼š{question}")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
parser = StrOutputParser()
chain = prompt | llm | parser

class Question(BaseModel):
    text: str

@app.post("/chat/stream")
async def chat_stream(question: Question):
    """æµå¼é—®ç­”ç«¯ç‚¹"""
    async def generate():
        async for chunk in chain.astream({"question": question.text}):
            # SSE æ ¼å¼ï¼šdata: {content}\n\n
            yield f"data: {chunk}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

@app.post("/chat")
async def chat(question: Question):
    """æ™®é€šé—®ç­”ç«¯ç‚¹ï¼ˆå¯¹æ¯”ï¼‰"""
    answer = await chain.ainvoke({"question": question.text})
    return {"answer": answer}

# è¿è¡Œï¼šuvicorn main:app --reload
```

**å‰ç«¯æ¥æ”¶ä»£ç **ï¼š

```javascript
// æ–¹å¼1ï¼šä½¿ç”¨ EventSourceï¼ˆæ¨èï¼‰
const eventSource = new EventSource('/chat/stream', {
    method: 'POST',
    body: JSON.stringify({ text: 'ä»€ä¹ˆæ˜¯ LCELï¼Ÿ' })
});

eventSource.onmessage = (event) => {
    console.log(event.data);  // é€å—æ˜¾ç¤º
};

// æ–¹å¼2ï¼šä½¿ç”¨ fetch + ReadableStream
async function streamChat(question) {
    const response = await fetch('/chat/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: question })
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        console.log(chunk);  // é€å—æ˜¾ç¤º
    }
}
```

---

## åœºæ™¯3ï¼šRAG æµå¼è¾“å‡º

### éœ€æ±‚

åœ¨ RAG åœºæ™¯ä¸­å®ç°æµå¼è¾“å‡ºã€‚

---

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯3ï¼šRAG æµå¼è¾“å‡º
æ¼”ç¤ºï¼šæ£€ç´¢ + æµå¼ç”Ÿæˆ
"""

import os
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.vectorstores import Chroma

load_dotenv()

# ===== 1. å‡†å¤‡å‘é‡å­˜å‚¨ =====
documents = [
    "LCEL æ˜¯ LangChain Expression Language çš„ç¼©å†™ã€‚",
    "Runnable æ˜¯ LCEL çš„æ ¸å¿ƒæ¥å£ã€‚",
    "ç®¡é“æ“ä½œç¬¦ | ç”¨äºè¿æ¥ç»„ä»¶ã€‚"
]

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(texts=documents, embedding=embeddings)
retriever = vectorstore.as_retriever()

# ===== 2. å®šä¹‰æ ¼å¼åŒ–å‡½æ•° =====
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# ===== 3. æ„å»º RAG é“¾ =====
prompt = ChatPromptTemplate.from_template("""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
""")

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
parser = StrOutputParser()

rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | parser
)

# ===== 4. æµå¼è¾“å‡º =====
async def rag_stream():
    print("=== RAG æµå¼è¾“å‡º ===\n")

    question = "ä»€ä¹ˆæ˜¯ LCELï¼Ÿ"
    print(f"é—®é¢˜: {question}\n")
    print("å›ç­”: ", end="", flush=True)

    async for chunk in rag_chain.astream(question):
        print(chunk, end="", flush=True)

    print("\n")

asyncio.run(rag_stream())
```

---

## åœºæ™¯4ï¼šå¸¦è¿›åº¦æç¤ºçš„æµå¼è¾“å‡º

### éœ€æ±‚

åœ¨æµå¼è¾“å‡ºä¸­æ˜¾ç¤ºå¤„ç†è¿›åº¦ï¼ˆæ£€ç´¢ä¸­ã€ç”Ÿæˆä¸­ç­‰ï¼‰ã€‚

---

### å®Œæ•´ä»£ç 

```python
"""
åœºæ™¯4ï¼šå¸¦è¿›åº¦æç¤ºçš„æµå¼è¾“å‡º
æ¼”ç¤ºï¼šè‡ªå®šä¹‰æµå¼è¾“å‡ºæ ¼å¼
"""

import os
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.vectorstores import Chroma

load_dotenv()

# ===== 1. å‡†å¤‡æ•°æ® =====
documents = ["LCEL æ˜¯è¡¨è¾¾å¼è¯­è¨€ã€‚", "Runnable æ˜¯æ ¸å¿ƒæ¥å£ã€‚"]
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(texts=documents, embedding=embeddings)
retriever = vectorstore.as_retriever()

def format_docs(docs):
    return "\n".join(doc.page_content for doc in docs)

# ===== 2. æ„å»ºé“¾ =====
prompt = ChatPromptTemplate.from_template("åŸºäº {context} å›ç­” {question}")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
parser = StrOutputParser()

rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | parser
)

# ===== 3. å¸¦è¿›åº¦çš„æµå¼è¾“å‡º =====
async def stream_with_progress():
    question = "ä»€ä¹ˆæ˜¯ LCELï¼Ÿ"

    # é˜¶æ®µ1ï¼šæ£€ç´¢
    print("ğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
    docs = retriever.invoke(question)
    print(f"âœ… æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£\n")

    # é˜¶æ®µ2ï¼šç”Ÿæˆ
    print("ğŸ’­ ç”Ÿæˆå›ç­”...\n")

    async for chunk in rag_chain.astream(question):
        print(chunk, end="", flush=True)

    print("\n\nâœ… å›ç­”å®Œæˆ")

asyncio.run(stream_with_progress())
```

**è¿è¡Œè¾“å‡ºç¤ºä¾‹**ï¼š
```
ğŸ” æ£€ç´¢ç›¸å…³æ–‡æ¡£...
âœ… æ‰¾åˆ° 2 ä¸ªç›¸å…³æ–‡æ¡£

ğŸ’­ ç”Ÿæˆå›ç­”...

LCEL æ˜¯ LangChain Expression Language çš„ç¼©å†™ï¼Œæ˜¯ä¸€ç§è¡¨è¾¾å¼è¯­è¨€...

âœ… å›ç­”å®Œæˆ
```

---

## åœºæ™¯5ï¼šFastAPI å®Œæ•´ç¤ºä¾‹

### éœ€æ±‚

å®Œæ•´çš„ FastAPI æµå¼é—®ç­”æœåŠ¡ï¼ŒåŒ…å«å‰åç«¯ä»£ç ã€‚

---

### åç«¯ä»£ç 

```python
"""
FastAPI æµå¼é—®ç­”æœåŠ¡
æ–‡ä»¶ï¼šmain.py
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

app = FastAPI()

# å…è®¸è·¨åŸŸ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# å®šä¹‰é“¾
prompt = ChatPromptTemplate.from_template("å›ç­”ï¼š{question}")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
parser = StrOutputParser()
chain = prompt | llm | parser

class Question(BaseModel):
    text: str

@app.post("/chat/stream")
async def chat_stream(question: Question):
    """æµå¼é—®ç­”"""
    async def generate():
        try:
            async for chunk in chain.astream({"question": question.text}):
                yield f"data: {chunk}\n\n"
        except Exception as e:
            yield f"data: [ERROR] {str(e)}\n\n"
        finally:
            yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### å‰ç«¯ä»£ç 

```html
<!-- æ–‡ä»¶ï¼šindex.html -->
<!DOCTYPE html>
<html>
<head>
    <title>æµå¼é—®ç­”</title>
    <style>
        body { font-family: Arial; max-width: 800px; margin: 50px auto; }
        #question { width: 100%; padding: 10px; font-size: 16px; }
        #answer { margin-top: 20px; padding: 20px; background: #f5f5f5; min-height: 100px; }
        button { padding: 10px 20px; font-size: 16px; cursor: pointer; }
    </style>
</head>
<body>
    <h1>æµå¼é—®ç­”</h1>

    <input type="text" id="question" placeholder="è¾“å…¥é—®é¢˜..." />
    <button onclick="ask()">æé—®</button>

    <div id="answer"></div>

    <script>
        async function ask() {
            const question = document.getElementById('question').value;
            const answerDiv = document.getElementById('answer');
            answerDiv.textContent = '';

            const response = await fetch('http://localhost:8000/chat/stream', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ text: question })
            });

            const reader = response.body.getReader();
            const decoder = new TextDecoder();

            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                const chunk = decoder.decode(value);
                const lines = chunk.split('\n');

                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        const data = line.slice(6);
                        if (data === '[DONE]') break;
                        if (data.startsWith('[ERROR]')) {
                            answerDiv.textContent += '\né”™è¯¯: ' + data;
                            break;
                        }
                        answerDiv.textContent += data;
                    }
                }
            }
        }
    </script>
</body>
</html>
```

---

## æ€»ç»“

### äº”ä¸ªåœºæ™¯å¯¹æ¯”

| åœºæ™¯ | æ ¸å¿ƒç‰¹æ€§ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| **åœºæ™¯1** | åŸºç¡€æµå¼ | å‘½ä»¤è¡Œå·¥å…· |
| **åœºæ™¯2** | FastAPI é›†æˆ | Web æœåŠ¡ |
| **åœºæ™¯3** | RAG æµå¼ | æ–‡æ¡£é—®ç­” |
| **åœºæ™¯4** | è¿›åº¦æç¤º | å¤æ‚æµç¨‹ |
| **åœºæ™¯5** | å®Œæ•´ç¤ºä¾‹ | ç”Ÿäº§ç¯å¢ƒ |

---

### å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ä½¿ç”¨ astream() å®ç°æµå¼è¾“å‡º
- [ ] åœ¨ FastAPI ä¸­ä½¿ç”¨ StreamingResponse
- [ ] ç†è§£ Server-Sent Events (SSE) æ ¼å¼
- [ ] å‰ç«¯æ¥æ”¶æµå¼æ•°æ®ï¼ˆEventSource / fetchï¼‰
- [ ] åœ¨ RAG åœºæ™¯ä¸­å®ç°æµå¼è¾“å‡º
- [ ] æ·»åŠ è¿›åº¦æç¤ºæå‡ç”¨æˆ·ä½“éªŒ
- [ ] æ„å»ºå®Œæ•´çš„æµå¼é—®ç­”æœåŠ¡

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-12
