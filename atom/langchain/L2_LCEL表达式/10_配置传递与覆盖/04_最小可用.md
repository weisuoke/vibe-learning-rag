# 最小可用示例

> **目标**: 用最少的代码理解配置传递与覆盖的核心概念

---

## 一、最简单的配置传递

### 1.1 基础示例（10行）

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 创建链
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
llm = ChatOpenAI()
chain = prompt | llm

# 使用配置
config = {"tags": ["test"]}
result = chain.invoke({"topic": "AI"}, config=config)
print(result.content)
```

**核心要点**:
- 配置通过`config`参数传递
- 配置自动传播到链中的所有组件
- 最简单的配置只需要一个字段

---

## 二、配置合并示例

### 2.1 合并两个配置（15行）

```python
from langchain_core.runnables.config import merge_configs

# 基础配置
base_config = {
    "tags": ["base"],
    "max_concurrency": 10
}

# 自定义配置
custom_config = {
    "tags": ["custom"],
    "max_concurrency": 5
}

# 合并
merged = merge_configs(base_config, custom_config)
print(merged)
# {"tags": ["base", "custom"], "max_concurrency": 5}
```

**核心要点**:
- `merge_configs()`合并多个配置
- tags连接：`["base", "custom"]`
- max_concurrency覆盖：使用`5`

---

## 三、with_config绑定

### 3.1 绑定配置（12行）

```python
from langchain_openai import ChatOpenAI

# 创建LLM
llm = ChatOpenAI()

# 绑定配置
llm_with_config = llm.with_config({
    "tags": ["production"],
    "max_concurrency": 3
})

# 使用绑定的配置
result = llm_with_config.invoke("Hello")
```

**核心要点**:
- `with_config()`创建绑定了配置的新Runnable
- 不修改原始Runnable
- 配置在每次invoke时自动应用

---

## 四、可配置字段

### 4.1 动态配置temperature（15行）

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField

# 定义可配置字段
llm = ChatOpenAI(temperature=0.5).configurable_fields(
    temperature=ConfigurableField(id="temp")
)

# 运行时配置
result_creative = llm.invoke(
    "Hello",
    config={"configurable": {"temp": 0.9}}
)

result_precise = llm.invoke(
    "Hello",
    config={"configurable": {"temp": 0.1}}
)
```

**核心要点**:
- `configurable_fields()`使字段可配置
- 通过`config.configurable`运行时覆盖
- 无需创建多个LLM实例

---

## 五、可配置替代方案

### 5.1 多模型切换（18行）

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField

# 定义替代方案
llm = ChatOpenAI(model="gpt-4").configurable_alternatives(
    ConfigurableField(id="model"),
    default_key="gpt4",
    gpt35=ChatOpenAI(model="gpt-3.5-turbo")
)

# 使用默认（gpt-4）
result_gpt4 = llm.invoke("Hello")

# 切换到gpt-3.5-turbo
result_gpt35 = llm.invoke(
    "Hello",
    config={"configurable": {"model": "gpt35"}}
)
```

**核心要点**:
- `configurable_alternatives()`定义替代方案
- 运行时切换整个组件
- 用于多模型切换、成本优化

---

## 六、Callbacks追踪

### 6.1 添加回调（20行）

```python
from langchain_core.callbacks import BaseCallbackHandler

class SimpleCallback(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"LLM开始: {prompts[0][:50]}...")

    def on_llm_end(self, response, **kwargs):
        print(f"LLM结束")

# 使用回调
config = {
    "callbacks": [SimpleCallback()],
    "tags": ["with-callback"]
}

chain = prompt | llm
result = chain.invoke({"topic": "AI"}, config=config)
# 输出:
# LLM开始: Tell me about AI...
# LLM结束
```

**核心要点**:
- 自定义回调继承`BaseCallbackHandler`
- 通过`config.callbacks`传递
- 用于追踪、日志、监控

---

## 七、并发控制

### 7.1 批处理with并发限制（15行）

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

# 批处理输入
inputs = [f"Tell me about topic {i}" for i in range(100)]

# 限制并发数
config = {"max_concurrency": 5}

# 批处理
results = llm.batch(inputs, config=config)
# 最多同时执行5个请求
print(f"完成 {len(results)} 个请求")
```

**核心要点**:
- `max_concurrency`限制并发数
- 防止资源耗尽
- 用于批处理优化

---

## 八、递归控制

### 8.1 限制Agent递归深度（12行）

```python
from langchain.agents import create_openai_functions_agent

# 创建Agent
agent = create_openai_functions_agent(llm, tools, prompt)

# 限制递归深度
config = {
    "recursion_limit": 10,
    "tags": ["agent-task"]
}

result = agent.invoke("复杂任务", config=config)
# 最多递归10层
```

**核心要点**:
- `recursion_limit`限制递归深度
- 防止Agent无限循环
- 默认值为25

---

## 九、完整最小RAG示例

### 9.1 带配置的RAG（30行）

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# 创建组件
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)
retriever = vectorstore.as_retriever()

prompt = ChatPromptTemplate.from_template(
    "基于以下上下文回答问题:\n{context}\n\n问题: {question}"
)
llm = ChatOpenAI()

# 创建RAG链
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

# 使用配置
config = {
    "tags": ["rag", "production"],
    "metadata": {"user_id": "user-123"},
    "max_concurrency": 3
}

result = rag_chain.invoke("What is AI?", config=config)
print(result.content)
```

**核心要点**:
- 配置传播到retriever、prompt、llm
- 使用tags和metadata追踪
- 使用max_concurrency控制并发

---

## 十、配置工厂模式

### 10.1 可重用配置（20行）

```python
def create_config(env: str, user_id: str):
    """配置工厂函数"""
    if env == "production":
        return {
            "tags": ["production"],
            "metadata": {"user_id": user_id, "env": "prod"},
            "max_concurrency": 3,
            "recursion_limit": 10
        }
    else:
        return {
            "tags": ["development"],
            "metadata": {"user_id": user_id, "env": "dev"},
            "max_concurrency": 10,
            "recursion_limit": 50
        }

# 使用
prod_config = create_config("production", "user-123")
result = chain.invoke(input, config=prod_config)
```

**核心要点**:
- 使用工厂函数创建配置
- 根据环境返回不同配置
- 提高配置可重用性

---

## 十一、快速参考

### 11.1 8个配置字段

```python
config = {
    "tags": ["tag1", "tag2"],              # 标签列表
    "metadata": {"key": "value"},          # 元数据字典
    "callbacks": [callback1, callback2],   # 回调列表
    "run_name": "my_run",                  # 运行名称
    "max_concurrency": 5,                  # 最大并发数
    "recursion_limit": 25,                 # 递归限制
    "configurable": {"field": "value"},    # 可配置字段
    "run_id": "uuid-1234"                  # 运行ID
}
```

### 11.2 3种传递方式

```python
# 方式1：直接传递
result = chain.invoke(input, config=config)

# 方式2：绑定配置
chain_with_config = chain.with_config(config)
result = chain_with_config.invoke(input)

# 方式3：合并配置
merged = merge_configs(base_config, custom_config)
result = chain.invoke(input, config=merged)
```

### 11.3 合并规则

```python
# 简单字段：后来覆盖
# run_name, max_concurrency, recursion_limit, run_id

# 列表字段：连接
# callbacks, tags

# 字典字段：浅合并
# metadata, configurable
```

---

## 十二、运行环境

### 12.1 环境设置

```bash
# 安装依赖
uv sync

# 激活环境
source .venv/bin/activate

# 设置API密钥
export OPENAI_API_KEY="your-key"
```

### 12.2 测试代码

```python
# 保存为test_config.py
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
config = {"tags": ["test"]}
result = llm.invoke("Hello", config=config)
print(result.content)
```

```bash
# 运行
python test_config.py
```

---

## 十三、下一步

### 13.1 深入学习

- [02_第一性原理](./02_第一性原理.md) - 理解为什么需要配置传递
- [03_核心概念_01_RunnableConfig结构与字段](./03_核心概念_01_RunnableConfig结构与字段.md) - 详细了解8个字段
- [03_核心概念_02_配置合并与覆盖策略](./03_核心概念_02_配置合并与覆盖策略.md) - 掌握合并规则

### 13.2 实战练习

- [07_实战代码_01_基础配置传递](./07_实战代码_01_基础配置传递.md) - 完整可运行示例
- [07_实战代码_02_配置合并与覆盖](./07_实战代码_02_配置合并与覆盖.md) - 合并实战
- [07_实战代码_08_完整RAG应用配置](./07_实战代码_08_完整RAG应用配置.md) - 生产就绪示例

---

## 十四、关键要点总结

1. **配置传递**: 通过`config`参数传递，自动传播到所有组件
2. **配置合并**: 使用`merge_configs()`，不同字段有不同规则
3. **配置绑定**: 使用`with_config()`创建绑定配置的Runnable
4. **动态配置**: 使用`configurable_fields()`和`configurable_alternatives()`
5. **Callbacks**: 用于追踪、日志、监控
6. **并发控制**: 使用`max_concurrency`限制并发数
7. **递归控制**: 使用`recursion_limit`防止无限循环
8. **配置工厂**: 使用工厂模式创建可重用配置

---

**记住**: 配置传递与覆盖是LangChain中统一管理运行时参数的核心机制，掌握它是构建生产级应用的基础。
