# 实战代码：生产级后台任务

> 完整可运行的生产级后台任务示例，包含错误处理、监控、重试等最佳实践

---

## 示例1：带错误处理的后台任务

### 场景描述
生产环境中的后台任务必须有完善的错误处理和日志记录。

### 完整代码

```python
"""
示例1：带错误处理的后台任务
演示：生产级错误处理和日志记录
"""

from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel, EmailStr
import structlog
from functools import wraps
import asyncio
import time
from typing import Any, Callable

# ===== 1. 配置结构化日志 =====
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.stdlib.LoggerFactory(),
)

logger = structlog.get_logger()

app = FastAPI()

# ===== 2. 错误处理装饰器 =====
def safe_background_task(func: Callable) -> Callable:
    """为后台任务添加错误处理"""
    @wraps(func)
    async def async_wrapper(*args: Any, **kwargs: Any) -> Any:
        start_time = time.time()
        task_name = func.__name__

        logger.info(
            "任务开始",
            task=task_name,
            args=args,
            kwargs=kwargs
        )

        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time

            logger.info(
                "任务成功",
                task=task_name,
                duration=duration,
                result=result
            )

            return result

        except Exception as e:
            duration = time.time() - start_time

            logger.error(
                "任务失败",
                task=task_name,
                duration=duration,
                error=str(e),
                error_type=type(e).__name__,
                exc_info=True
            )

            # 可选：记录失败任务到数据库
            # await save_failed_task(task_name, args, kwargs, str(e))

    @wraps(func)
    def sync_wrapper(*args: Any, **kwargs: Any) -> Any:
        start_time = time.time()
        task_name = func.__name__

        logger.info(
            "任务开始",
            task=task_name,
            args=args,
            kwargs=kwargs
        )

        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time

            logger.info(
                "任务成功",
                task=task_name,
                duration=duration,
                result=result
            )

            return result

        except Exception as e:
            duration = time.time() - start_time

            logger.error(
                "任务失败",
                task=task_name,
                duration=duration,
                error=str(e),
                error_type=type(e).__name__,
                exc_info=True
            )

    if asyncio.iscoroutinefunction(func):
        return async_wrapper
    else:
        return sync_wrapper

# ===== 3. 后台任务示例 =====
@safe_background_task
async def send_email_with_retry(
    email: str,
    subject: str,
    body: str,
    max_retries: int = 3
):
    """发送邮件（带重试）"""
    for attempt in range(max_retries):
        try:
            logger.info(
                "尝试发送邮件",
                email=email,
                attempt=attempt + 1,
                max_retries=max_retries
            )

            # 模拟发送邮件
            await asyncio.sleep(1)

            # 模拟随机失败
            import random
            if random.random() < 0.3 and attempt < max_retries - 1:
                raise Exception("邮件服务器连接失败")

            logger.info("邮件发送成功", email=email)
            return {"status": "success", "email": email}

        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # 指数退避
                logger.warning(
                    "邮件发送失败，准备重试",
                    email=email,
                    attempt=attempt + 1,
                    wait_time=wait_time,
                    error=str(e)
                )
                await asyncio.sleep(wait_time)
            else:
                logger.error(
                    "邮件发送失败，已达最大重试次数",
                    email=email,
                    max_retries=max_retries
                )
                raise

# ===== 4. API 端点 =====
class EmailRequest(BaseModel):
    email: EmailStr
    subject: str
    body: str

@app.post("/send-email")
async def send_email_endpoint(
    request: EmailRequest,
    background_tasks: BackgroundTasks
):
    """发送邮件"""
    background_tasks.add_task(
        send_email_with_retry,
        request.email,
        request.subject,
        request.body
    )

    return {
        "message": "邮件发送任务已提交",
        "email": request.email
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 运行方式

```bash
# 1. 安装依赖
uv add structlog

# 2. 运行服务
python examples/production/01_error_handling.py

# 3. 测试 API
curl -X POST "http://localhost:8000/send-email" \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "subject": "测试",
    "body": "这是测试邮件"
  }'
```

### 预期输出（JSON 格式日志）

```json
{
  "event": "任务开始",
  "task": "send_email_with_retry",
  "args": ["user@example.com", "测试", "这是测试邮件"],
  "timestamp": "2026-02-11T10:30:00.123456Z",
  "level": "info"
}
{
  "event": "尝试发送邮件",
  "email": "user@example.com",
  "attempt": 1,
  "max_retries": 3,
  "timestamp": "2026-02-11T10:30:00.234567Z",
  "level": "info"
}
{
  "event": "邮件发送成功",
  "email": "user@example.com",
  "timestamp": "2026-02-11T10:30:01.345678Z",
  "level": "info"
}
{
  "event": "任务成功",
  "task": "send_email_with_retry",
  "duration": 1.234,
  "timestamp": "2026-02-11T10:30:01.456789Z",
  "level": "info"
}
```

---

## 示例2：任务监控和指标收集

### 场景描述
收集任务执行指标，用于监控和告警。

### 完整代码

```python
"""
示例2：任务监控和指标收集
演示：收集任务执行指标
"""

from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from collections import defaultdict
from datetime import datetime
import time
import asyncio
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ===== 1. 指标收集器 =====
class TaskMetrics:
    """任务指标收集器"""
    def __init__(self):
        self.success_count = defaultdict(int)
        self.failure_count = defaultdict(int)
        self.total_duration = defaultdict(float)
        self.last_execution = defaultdict(datetime)

    def record_success(self, task_name: str, duration: float):
        """记录成功任务"""
        self.success_count[task_name] += 1
        self.total_duration[task_name] += duration
        self.last_execution[task_name] = datetime.now()

    def record_failure(self, task_name: str, duration: float):
        """记录失败任务"""
        self.failure_count[task_name] += 1
        self.total_duration[task_name] += duration
        self.last_execution[task_name] = datetime.now()

    def get_stats(self, task_name: str):
        """获取任务统计"""
        total = self.success_count[task_name] + self.failure_count[task_name]
        if total == 0:
            return None

        return {
            "task": task_name,
            "total_executions": total,
            "success_count": self.success_count[task_name],
            "failure_count": self.failure_count[task_name],
            "success_rate": self.success_count[task_name] / total,
            "avg_duration": self.total_duration[task_name] / total,
            "last_execution": self.last_execution[task_name].isoformat()
        }

    def get_all_stats(self):
        """获取所有任务统计"""
        all_tasks = set(
            list(self.success_count.keys()) +
            list(self.failure_count.keys())
        )
        return {
            task: self.get_stats(task)
            for task in all_tasks
        }

# 全局指标收集器
metrics = TaskMetrics()

# ===== 2. 监控装饰器 =====
def monitored_task(func):
    """监控任务执行的装饰器"""
    async def async_wrapper(*args, **kwargs):
        start_time = time.time()
        task_name = func.__name__

        try:
            result = await func(*args, **kwargs)
            duration = time.time() - start_time
            metrics.record_success(task_name, duration)

            logger.info(
                f"任务成功: {task_name}, 耗时: {duration:.2f}秒"
            )

            return result

        except Exception as e:
            duration = time.time() - start_time
            metrics.record_failure(task_name, duration)

            logger.error(
                f"任务失败: {task_name}, 耗时: {duration:.2f}秒, "
                f"错误: {e}"
            )

            raise

    def sync_wrapper(*args, **kwargs):
        start_time = time.time()
        task_name = func.__name__

        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            metrics.record_success(task_name, duration)

            logger.info(
                f"任务成功: {task_name}, 耗时: {duration:.2f}秒"
            )

            return result

        except Exception as e:
            duration = time.time() - start_time
            metrics.record_failure(task_name, duration)

            logger.error(
                f"任务失败: {task_name}, 耗时: {duration:.2f}秒, "
                f"错误: {e}"
            )

            raise

    if asyncio.iscoroutinefunction(func):
        return async_wrapper
    else:
        return sync_wrapper

# ===== 3. 后台任务示例 =====
@monitored_task
async def process_data(data_id: int):
    """处理数据"""
    logger.info(f"开始处理数据: {data_id}")

    # 模拟处理
    await asyncio.sleep(2)

    # 模拟随机失败
    import random
    if random.random() < 0.2:
        raise Exception("数据处理失败")

    logger.info(f"数据处理完成: {data_id}")
    return {"status": "success", "data_id": data_id}

@monitored_task
async def send_notification(user_id: int, message: str):
    """发送通知"""
    logger.info(f"发送通知: user_id={user_id}")

    # 模拟发送
    await asyncio.sleep(1)

    logger.info(f"通知发送完成: user_id={user_id}")
    return {"status": "success", "user_id": user_id}

# ===== 4. API 端点 =====
class DataRequest(BaseModel):
    data_id: int

class NotificationRequest(BaseModel):
    user_id: int
    message: str

@app.post("/process")
async def process_endpoint(
    request: DataRequest,
    background_tasks: BackgroundTasks
):
    """处理数据"""
    background_tasks.add_task(process_data, request.data_id)
    return {"message": "数据处理任务已提交"}

@app.post("/notify")
async def notify_endpoint(
    request: NotificationRequest,
    background_tasks: BackgroundTasks
):
    """发送通知"""
    background_tasks.add_task(
        send_notification,
        request.user_id,
        request.message
    )
    return {"message": "通知任务已提交"}

# ===== 5. 监控端点 =====
@app.get("/metrics")
async def get_metrics():
    """获取所有任务指标"""
    return metrics.get_all_stats()

@app.get("/metrics/{task_name}")
async def get_task_metrics(task_name: str):
    """获取特定任务指标"""
    stats = metrics.get_stats(task_name)
    if stats is None:
        return {"error": "任务不存在"}
    return stats

@app.get("/health")
async def health_check():
    """健康检查"""
    all_stats = metrics.get_all_stats()

    total_tasks = sum(
        stats["total_executions"]
        for stats in all_stats.values()
    )

    total_failures = sum(
        stats["failure_count"]
        for stats in all_stats.values()
    )

    return {
        "status": "healthy",
        "total_tasks": total_tasks,
        "total_failures": total_failures,
        "failure_rate": total_failures / total_tasks if total_tasks > 0 else 0
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 运行方式

```bash
# 1. 运行服务
python examples/production/02_monitoring.py

# 2. 提交任务
for i in {1..10}; do
  curl -X POST "http://localhost:8000/process" \
    -H "Content-Type: application/json" \
    -d "{\"data_id\": $i}"
  sleep 0.5
done

# 3. 查看指标
curl "http://localhost:8000/metrics"

# 4. 查看特定任务指标
curl "http://localhost:8000/metrics/process_data"

# 5. 健康检查
curl "http://localhost:8000/health"
```

### 预期输出

```json
{
  "process_data": {
    "task": "process_data",
    "total_executions": 10,
    "success_count": 8,
    "failure_count": 2,
    "success_rate": 0.8,
    "avg_duration": 2.05,
    "last_execution": "2026-02-11T10:35:00.123456"
  },
  "send_notification": {
    "task": "send_notification",
    "total_executions": 5,
    "success_count": 5,
    "failure_count": 0,
    "success_rate": 1.0,
    "avg_duration": 1.02,
    "last_execution": "2026-02-11T10:36:00.234567"
  }
}
```

---

## 示例3：资源管理和清理

### 场景描述
确保后台任务正确管理和清理资源（文件、数据库连接等）。

### 完整代码

```python
"""
示例3：资源管理和清理
演示：正确管理资源生命周期
"""

from fastapi import FastAPI, BackgroundTasks, UploadFile, File
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from contextlib import asynccontextmanager
import os
import tempfile
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# ===== 1. 数据库配置 =====
DATABASE_URL = "sqlite:///./test.db"
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(bind=engine)
Base = declarative_base()

class Document(Base):
    __tablename__ = "documents"
    id = Column(Integer, primary_key=True, index=True)
    filename = Column(String)
    content = Column(String)

Base.metadata.create_all(bind=engine)

# ===== 2. 资源管理上下文管理器 =====
@asynccontextmanager
async def temp_file_context(file_path: str):
    """临时文件上下文管理器"""
    try:
        logger.info(f"创建临时文件: {file_path}")
        yield file_path
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"临时文件已删除: {file_path}")

def get_db_session():
    """获取数据库会话（上下文管理器）"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
        logger.info("数据库连接已关闭")

# ===== 3. 后台任务：正确的资源管理 =====
async def process_uploaded_file(file_path: str, filename: str):
    """处理上传的文件（带资源清理）"""
    try:
        logger.info(f"开始处理文件: {filename}")

        # 读取文件内容
        with open(file_path, 'r') as f:
            content = f.read()

        logger.info(f"文件内容长度: {len(content)}")

        # 保存到数据库（使用独立的数据库连接）
        db = SessionLocal()
        try:
            document = Document(
                filename=filename,
                content=content
            )
            db.add(document)
            db.commit()
            db.refresh(document)

            logger.info(f"文档已保存到数据库: id={document.id}")

        finally:
            db.close()
            logger.info("数据库连接已关闭")

    finally:
        # 确保删除临时文件
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"临时文件已删除: {file_path}")

# ===== 4. API 端点 =====
@app.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks = None
):
    """上传文件"""
    # 保存到临时文件
    with tempfile.NamedTemporaryFile(delete=False, suffix=".txt") as temp_file:
        content = await file.read()
        temp_file.write(content)
        temp_file_path = temp_file.name

    logger.info(f"文件已保存到临时位置: {temp_file_path}")

    # 添加后台任务
    background_tasks.add_task(
        process_uploaded_file,
        temp_file_path,
        file.filename
    )

    return {
        "message": "文件上传成功，正在后台处理",
        "filename": file.filename
    }

@app.get("/documents")
async def list_documents():
    """列出所有文档"""
    db = SessionLocal()
    try:
        documents = db.query(Document).all()
        return {
            "documents": [
                {
                    "id": doc.id,
                    "filename": doc.filename,
                    "content_length": len(doc.content)
                }
                for doc in documents
            ]
        }
    finally:
        db.close()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 运行方式

```bash
# 1. 安装依赖
uv add sqlalchemy

# 2. 运行服务
python examples/production/03_resource_management.py

# 3. 上传文件
echo "测试内容" > test.txt
curl -X POST "http://localhost:8000/upload" \
  -F "file=@test.txt"

# 4. 查看文档列表
curl "http://localhost:8000/documents"
```

---

## 示例4：完整的生产级后台任务系统

### 场景描述
集成所有最佳实践的完整示例。

### 完整代码

```python
"""
示例4：完整的生产级后台任务系统
演示：集成所有最佳实践
"""

from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
import structlog
from functools import wraps
import asyncio
import time
from collections import defaultdict
from datetime import datetime
from typing import Any, Callable

# ===== 1. 配置 =====
structlog.configure(
    processors=[
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.stdlib.LoggerFactory(),
)

logger = structlog.get_logger()
app = FastAPI()

# ===== 2. 指标收集 =====
class Metrics:
    def __init__(self):
        self.success = defaultdict(int)
        self.failure = defaultdict(int)
        self.duration = defaultdict(list)

    def record(self, task: str, success: bool, duration: float):
        if success:
            self.success[task] += 1
        else:
            self.failure[task] += 1
        self.duration[task].append(duration)

    def get_stats(self, task: str):
        total = self.success[task] + self.failure[task]
        if total == 0:
            return None

        durations = self.duration[task]
        return {
            "total": total,
            "success": self.success[task],
            "failure": self.failure[task],
            "success_rate": self.success[task] / total,
            "avg_duration": sum(durations) / len(durations),
            "min_duration": min(durations),
            "max_duration": max(durations)
        }

metrics = Metrics()

# ===== 3. 生产级装饰器 =====
def production_task(
    max_retries: int = 3,
    timeout: float = 300.0
):
    """生产级后台任务装饰器"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args: Any, **kwargs: Any) -> Any:
            start_time = time.time()
            task_name = func.__name__

            logger.info(
                "任务开始",
                task=task_name,
                args=args,
                kwargs=kwargs
            )

            for attempt in range(max_retries):
                try:
                    # 执行任务（带超时）
                    result = await asyncio.wait_for(
                        func(*args, **kwargs),
                        timeout=timeout
                    )

                    duration = time.time() - start_time

                    logger.info(
                        "任务成功",
                        task=task_name,
                        duration=duration,
                        attempt=attempt + 1
                    )

                    metrics.record(task_name, True, duration)
                    return result

                except asyncio.TimeoutError:
                    duration = time.time() - start_time

                    logger.error(
                        "任务超时",
                        task=task_name,
                        duration=duration,
                        timeout=timeout
                    )

                    metrics.record(task_name, False, duration)
                    raise

                except Exception as e:
                    duration = time.time() - start_time

                    if attempt < max_retries - 1:
                        wait_time = 2 ** attempt
                        logger.warning(
                            "任务失败，准备重试",
                            task=task_name,
                            attempt=attempt + 1,
                            max_retries=max_retries,
                            wait_time=wait_time,
                            error=str(e)
                        )
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(
                            "任务失败，已达最大重试次数",
                            task=task_name,
                            max_retries=max_retries,
                            error=str(e),
                            exc_info=True
                        )
                        metrics.record(task_name, False, duration)
                        raise

        return wrapper
    return decorator

# ===== 4. 后台任务示例 =====
@production_task(max_retries=3, timeout=30.0)
async def process_order(order_id: int):
    """处理订单"""
    logger.info("开始处理订单", order_id=order_id)

    # 模拟处理
    await asyncio.sleep(2)

    # 模拟随机失败
    import random
    if random.random() < 0.2:
        raise Exception("订单处理失败")

    logger.info("订单处理完成", order_id=order_id)
    return {"status": "success", "order_id": order_id}

# ===== 5. API 端点 =====
class OrderRequest(BaseModel):
    order_id: int

@app.post("/order")
async def create_order(
    request: OrderRequest,
    background_tasks: BackgroundTasks
):
    """创建订单"""
    background_tasks.add_task(process_order, request.order_id)
    return {"message": "订单已创建", "order_id": request.order_id}

@app.get("/metrics")
async def get_metrics():
    """获取指标"""
    all_tasks = set(
        list(metrics.success.keys()) +
        list(metrics.failure.keys())
    )
    return {
        task: metrics.get_stats(task)
        for task in all_tasks
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 总结

### 生产级后台任务的关键要素

1. **错误处理**
   - try-except 捕获异常
   - 结构化日志记录
   - 失败任务记录

2. **重试机制**
   - 指数退避
   - 最大重试次数
   - 重试日志

3. **超时控制**
   - asyncio.wait_for
   - 合理的超时时间
   - 超时日志

4. **资源管理**
   - 上下文管理器
   - 确保资源清理
   - 独立的资源实例

5. **监控指标**
   - 成功率
   - 执行时间
   - 失败原因

6. **日志记录**
   - 结构化日志
   - 关键信息记录
   - 便于分析

### 部署检查清单

- [ ] 所有任务都有错误处理
- [ ] 使用结构化日志
- [ ] 实现重试机制
- [ ] 设置超时时间
- [ ] 正确管理资源
- [ ] 收集监控指标
- [ ] 配置告警机制
- [ ] 定期检查失败任务

---

**版本：** v1.0
**最后更新：** 2026-02-11
