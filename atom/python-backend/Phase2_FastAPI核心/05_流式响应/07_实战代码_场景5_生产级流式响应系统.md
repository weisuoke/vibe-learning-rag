# 实战代码 - 场景5：生产级流式响应系统

> 完整可运行的生产级流式响应系统，包含错误处理、监控、日志、限流等

---

## 场景描述

构建一个生产级的流式响应系统，集成错误处理、重试机制、监控指标、结构化日志、限流保护等生产环境必备功能。

---

## 完整代码

```python
"""
场景5：生产级流式响应系统
演示：完整的生产级流式响应实现
"""

from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, AsyncGenerator, Dict, Any
import asyncio
import json
import time
import logging
from datetime import datetime
from contextlib import asynccontextmanager
import structlog
from openai import AsyncOpenAI
from collections import defaultdict

# ===== 结构化日志配置 =====
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# ===== 监控指标 =====
class Metrics:
    """监控指标收集器"""

    def __init__(self):
        self.request_count = defaultdict(int)
        self.error_count = defaultdict(int)
        self.response_times = defaultdict(list)
        self.active_streams = 0

    def record_request(self, endpoint: str):
        """记录请求"""
        self.request_count[endpoint] += 1

    def record_error(self, endpoint: str, error_type: str):
        """记录错误"""
        self.error_count[f"{endpoint}:{error_type}"] += 1

    def record_response_time(self, endpoint: str, duration: float):
        """记录响应时间"""
        self.response_times[endpoint].append(duration)

    def increment_active_streams(self):
        """增加活跃流数量"""
        self.active_streams += 1

    def decrement_active_streams(self):
        """减少活跃流数量"""
        self.active_streams -= 1

    def get_stats(self) -> Dict[str, Any]:
        """获取统计信息"""
        return {
            "request_count": dict(self.request_count),
            "error_count": dict(self.error_count),
            "active_streams": self.active_streams,
            "avg_response_times": {
                endpoint: sum(times) / len(times) if times else 0
                for endpoint, times in self.response_times.items()
            }
        }

metrics = Metrics()

# ===== 限流器 =====
class RateLimiter:
    """简单的令牌桶限流器"""

    def __init__(self, rate: int, capacity: int):
        self.rate = rate  # 每秒生成的令牌数
        self.capacity = capacity  # 桶容量
        self.tokens = capacity
        self.last_update = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self) -> bool:
        """获取令牌"""
        async with self.lock:
            now = time.time()
            elapsed = now - self.last_update

            # 补充令牌
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now

            # 尝试消费令牌
            if self.tokens >= 1:
                self.tokens -= 1
                return True
            return False

# 全局限流器（每秒10个请求）
rate_limiter = RateLimiter(rate=10, capacity=20)

# ===== FastAPI 应用 =====
@asynccontextmanager
async def lifespan(app: FastAPI):
    """应用生命周期管理"""
    logger.info("application_startup", message="应用启动")
    yield
    logger.info("application_shutdown", message="应用关闭")

app = FastAPI(
    title="生产级流式响应系统",
    lifespan=lifespan
)

# CORS 配置
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI 客户端
openai_client = AsyncOpenAI()

# ===== 数据模型 =====
class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    model: str = "gpt-4"
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    stream: bool = True

# ===== 中间件：请求日志 =====
@app.middleware("http")
async def log_requests(request: Request, call_next):
    """记录所有请求"""
    request_id = f"{int(time.time() * 1000)}"
    start_time = time.time()

    logger.info(
        "request_started",
        request_id=request_id,
        method=request.method,
        path=request.url.path,
        client=request.client.host if request.client else None
    )

    try:
        response = await call_next(request)
        duration = time.time() - start_time

        logger.info(
            "request_completed",
            request_id=request_id,
            status_code=response.status_code,
            duration=duration
        )

        metrics.record_response_time(request.url.path, duration)
        return response

    except Exception as e:
        duration = time.time() - start_time

        logger.error(
            "request_failed",
            request_id=request_id,
            error=str(e),
            duration=duration
        )

        metrics.record_error(request.url.path, type(e).__name__)
        raise

# ===== 依赖：限流检查 =====
async def check_rate_limit():
    """检查限流"""
    if not await rate_limiter.acquire():
        logger.warning("rate_limit_exceeded")
        raise HTTPException(
            status_code=429,
            detail="请求过于频繁，请稍后再试"
        )

# ===== 生产级流式响应生成器 =====
async def production_stream(
    request: ChatRequest,
    request_id: str
) -> AsyncGenerator[str, None]:
    """生产级流式响应生成器"""
    start_time = time.time()
    metrics.increment_active_streams()

    try:
        # 发送开始事件
        yield "event: start\n"
        yield f"data: {json.dumps({
            'request_id': request_id,
            'model': request.model,
            'timestamp': datetime.now().isoformat()
        })}\n\n"

        logger.info(
            "stream_started",
            request_id=request_id,
            model=request.model,
            message_count=len(request.messages)
        )

        # 重试配置
        max_retries = 3
        retry_count = 0
        backoff_base = 1

        while retry_count < max_retries:
            try:
                # 调用 OpenAI API
                stream = await openai_client.chat.completions.create(
                    model=request.model,
                    messages=[msg.dict() for msg in request.messages],
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    stream=True,
                    timeout=30.0
                )

                message_id = 0
                full_content = ""
                token_count = 0

                # 流式返回内容
                async for chunk in stream:
                    choice = chunk.choices[0]

                    if choice.delta.content:
                        content = choice.delta.content
                        full_content += content
                        message_id += 1
                        token_count += 1

                        yield f"id: {message_id}\n"
                        yield "event: content\n"
                        yield f"data: {json.dumps({'content': content})}\n\n"

                    if choice.finish_reason:
                        # 发送完成事件
                        duration = time.time() - start_time

                        yield "event: complete\n"
                        yield f"data: {json.dumps({
                            'finish_reason': choice.finish_reason,
                            'token_count': token_count,
                            'duration': duration,
                            'full_content_length': len(full_content)
                        })}\n\n"

                        logger.info(
                            "stream_completed",
                            request_id=request_id,
                            token_count=token_count,
                            duration=duration,
                            finish_reason=choice.finish_reason
                        )

                # 成功完成，退出重试循环
                break

            except asyncio.TimeoutError:
                retry_count += 1
                logger.warning(
                    "stream_timeout",
                    request_id=request_id,
                    retry_count=retry_count,
                    max_retries=max_retries
                )

                if retry_count >= max_retries:
                    yield "event: error\n"
                    yield f"data: {json.dumps({
                        'error': '请求超时',
                        'retry_count': retry_count
                    })}\n\n"

                    metrics.record_error("stream", "timeout")
                    break

                # 指数退避
                backoff = backoff_base * (2 ** (retry_count - 1))
                yield "event: retry\n"
                yield f"data: {json.dumps({
                    'attempt': retry_count,
                    'backoff': backoff
                })}\n\n"

                await asyncio.sleep(backoff)

            except Exception as e:
                logger.error(
                    "stream_error",
                    request_id=request_id,
                    error=str(e),
                    error_type=type(e).__name__
                )

                yield "event: error\n"
                yield f"data: {json.dumps({
                    'error': str(e),
                    'error_type': type(e).__name__
                })}\n\n"

                metrics.record_error("stream", type(e).__name__)
                break

    except GeneratorExit:
        # 客户端断开连接
        duration = time.time() - start_time
        logger.info(
            "stream_disconnected",
            request_id=request_id,
            duration=duration
        )

    except Exception as e:
        logger.error(
            "stream_fatal_error",
            request_id=request_id,
            error=str(e)
        )

        yield "event: error\n"
        yield f"data: {json.dumps({'error': 'Internal server error'})}\n\n"

    finally:
        metrics.decrement_active_streams()
        duration = time.time() - start_time

        logger.info(
            "stream_ended",
            request_id=request_id,
            total_duration=duration
        )

# ===== API 端点 =====
@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatRequest,
    _: None = Depends(check_rate_limit)
):
    """生产级流式对话端点"""
    request_id = f"req_{int(time.time() * 1000)}"

    metrics.record_request("/v1/chat/completions")

    return StreamingResponse(
        production_stream(request, request_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "X-Request-ID": request_id
        }
    )

# ===== 健康检查端点 =====
@app.get("/health")
async def health_check():
    """健康检查"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "active_streams": metrics.active_streams
    }

@app.get("/health/ready")
async def readiness_check():
    """就绪检查"""
    # 检查依赖服务是否可用
    try:
        # 这里可以添加数据库、Redis 等依赖检查
        return {
            "status": "ready",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error("readiness_check_failed", error=str(e))
        raise HTTPException(status_code=503, detail="Service not ready")

# ===== 监控端点 =====
@app.get("/metrics")
async def get_metrics():
    """获取监控指标"""
    return metrics.get_stats()

@app.get("/metrics/prometheus")
async def prometheus_metrics():
    """Prometheus 格式的监控指标"""
    stats = metrics.get_stats()

    lines = []

    # 请求计数
    for endpoint, count in stats["request_count"].items():
        lines.append(f'http_requests_total{{endpoint="{endpoint}"}} {count}')

    # 错误计数
    for key, count in stats["error_count"].items():
        endpoint, error_type = key.split(":", 1)
        lines.append(
            f'http_errors_total{{endpoint="{endpoint}",type="{error_type}"}} {count}'
        )

    # 活跃流数量
    lines.append(f'active_streams {stats["active_streams"]}')

    # 平均响应时间
    for endpoint, avg_time in stats["avg_response_times"].items():
        lines.append(
            f'http_response_time_seconds{{endpoint="{endpoint}"}} {avg_time}'
        )

    return "\n".join(lines)

# ===== 管理端点 =====
@app.post("/admin/reset-metrics")
async def reset_metrics():
    """重置监控指标"""
    global metrics
    metrics = Metrics()
    logger.info("metrics_reset")
    return {"message": "监控指标已重置"}

@app.get("/admin/config")
async def get_config():
    """获取配置信息"""
    return {
        "rate_limit": {
            "rate": rate_limiter.rate,
            "capacity": rate_limiter.capacity
        },
        "openai": {
            "default_model": "gpt-4",
            "timeout": 30.0
        }
    }

# ===== 运行服务器 =====
if __name__ == "__main__":
    import uvicorn

    print("\n" + "="*50)
    print("启动生产级流式响应系统")
    print("="*50)
    print("\n核心功能：")
    print("  ✅ 结构化日志（JSON 格式）")
    print("  ✅ 监控指标收集")
    print("  ✅ 限流保护（10 req/s）")
    print("  ✅ 错误处理和重试")
    print("  ✅ 健康检查端点")
    print("  ✅ Prometheus 监控")
    print("\n可用端点：")
    print("  POST http://localhost:8000/v1/chat/completions")
    print("  GET  http://localhost:8000/health")
    print("  GET  http://localhost:8000/health/ready")
    print("  GET  http://localhost:8000/metrics")
    print("  GET  http://localhost:8000/metrics/prometheus")
    print("\n环境变量配置：")
    print("  export OPENAI_API_KEY=your_key")
    print("\n按 Ctrl+C 停止服务器")
    print("="*50 + "\n")

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_config={
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
                }
            },
            "handlers": {
                "default": {
                    "formatter": "default",
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stdout"
                }
            },
            "root": {
                "level": "INFO",
                "handlers": ["default"]
            }
        }
    )
```

---

## Docker 部署

### Dockerfile

```dockerfile
FROM python:3.13-slim

WORKDIR /app

# 安装依赖
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制代码
COPY . .

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# 暴露端口
EXPOSE 8000

# 运行应用
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### docker-compose.yml

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:
```

### prometheus.yml

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'streaming-api'
    static_configs:
      - targets: ['api:8000']
    metrics_path: '/metrics/prometheus'
```

---

## Kubernetes 部署

### deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: streaming-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: streaming-api
  template:
    metadata:
      labels:
        app: streaming-api
    spec:
      containers:
      - name: api
        image: streaming-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-api-key
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: streaming-api
spec:
  selector:
    app: streaming-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

---

## 监控和告警

### Grafana Dashboard JSON

```json
{
  "dashboard": {
    "title": "Streaming API Metrics",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])"
          }
        ]
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "rate(http_errors_total[5m])"
          }
        ]
      },
      {
        "title": "Active Streams",
        "targets": [
          {
            "expr": "active_streams"
          }
        ]
      },
      {
        "title": "Response Time",
        "targets": [
          {
            "expr": "http_response_time_seconds"
          }
        ]
      }
    ]
  }
}
```

---

## 测试脚本

### 负载测试

```python
import asyncio
import httpx
import time
from typing import List

async def send_request(client: httpx.AsyncClient, request_id: int):
    """发送单个请求"""
    start_time = time.time()

    try:
        async with client.stream(
            'POST',
            'http://localhost:8000/v1/chat/completions',
            json={
                "messages": [{"role": "user", "content": "你好"}],
                "model": "gpt-4"
            },
            timeout=60.0
        ) as response:
            token_count = 0
            async for line in response.aiter_lines():
                if line.startswith('data: '):
                    token_count += 1

            duration = time.time() - start_time
            return {
                "request_id": request_id,
                "status": "success",
                "duration": duration,
                "token_count": token_count
            }

    except Exception as e:
        duration = time.time() - start_time
        return {
            "request_id": request_id,
            "status": "error",
            "duration": duration,
            "error": str(e)
        }

async def load_test(concurrent_requests: int, total_requests: int):
    """负载测试"""
    async with httpx.AsyncClient() as client:
        results = []

        for batch_start in range(0, total_requests, concurrent_requests):
            batch_end = min(batch_start + concurrent_requests, total_requests)
            batch_size = batch_end - batch_start

            print(f"发送请求 {batch_start + 1}-{batch_end}...")

            tasks = [
                send_request(client, i)
                for i in range(batch_start, batch_end)
            ]

            batch_results = await asyncio.gather(*tasks)
            results.extend(batch_results)

        # 统计结果
        success_count = sum(1 for r in results if r["status"] == "success")
        error_count = sum(1 for r in results if r["status"] == "error")
        avg_duration = sum(r["duration"] for r in results) / len(results)

        print(f"\n负载测试结果：")
        print(f"  总请求数: {total_requests}")
        print(f"  成功: {success_count}")
        print(f"  失败: {error_count}")
        print(f"  平均响应时间: {avg_duration:.2f}秒")

# 运行负载测试
asyncio.run(load_test(concurrent_requests=10, total_requests=100))
```

---

## 关键知识点

### 1. 结构化日志

```python
logger.info(
    "stream_completed",
    request_id=request_id,
    token_count=token_count,
    duration=duration
)
```

### 2. 监控指标

```python
metrics.record_request(endpoint)
metrics.record_error(endpoint, error_type)
metrics.record_response_time(endpoint, duration)
```

### 3. 限流保护

```python
if not await rate_limiter.acquire():
    raise HTTPException(status_code=429)
```

### 4. 健康检查

```python
@app.get("/health")
async def health_check():
    return {"status": "healthy"}
```

### 5. 错误重试

```python
max_retries = 3
retry_count = 0
while retry_count < max_retries:
    try:
        # 执行操作
        break
    except Exception:
        retry_count += 1
        await asyncio.sleep(backoff)
```

---

## 生产环境清单

- [x] 结构化日志
- [x] 监控指标收集
- [x] 限流保护
- [x] 错误处理和重试
- [x] 健康检查端点
- [x] Prometheus 集成
- [x] Docker 容器化
- [x] Kubernetes 部署配置
- [x] 负载测试脚本
- [x] Grafana 监控面板

---

## 总结

这个生产级流式响应系统包含了所有必要的生产环境功能，可以直接部署到生产环境使用。
