# 实战代码 - 场景 2：组件监控

> **场景定位**：深入监控 Milvus 各组件的运行状态和性能指标
> **难度等级**：进阶级
> **预计时间**：20 分钟

---

## 1. 场景描述

### 1.1 业务场景

RAG 系统在生产环境中运行，需要实时监控 Milvus 各组件的状态：
- Proxy 的请求处理能力和响应时间
- QueryNode 的查询性能和内存使用
- DataNode 的数据写入速度和存储状态
- IndexNode 的索引构建进度

### 1.2 技术目标

1. 监控 Proxy 组件的请求统计
2. 监控 QueryNode 的查询性能
3. 监控 DataNode 的数据持久化状态
4. 监控 IndexNode 的索引构建进度
5. 实现自动化监控和告警

### 1.3 在 RAG 中的应用

- **性能优化**：识别性能瓶颈，优化查询和写入
- **容量规划**：根据负载趋势规划扩容
- **故障预警**：提前发现异常，避免服务中断
- **成本控制**：优化资源使用，降低运营成本

---

## 2. 环境准备

### 2.1 安装依赖

```bash
uv add pymilvus prometheus-client psutil tabulate
```

### 2.2 启动 Milvus

```bash
docker-compose up -d
```

---

## 3. 核心代码实现

### 3.1 Proxy 组件监控

```python
"""
Proxy 组件监控：请求统计和性能分析
"""

from pymilvus import connections, utility, Collection
from typing import Dict, List
import time
from tabulate import tabulate

class ProxyMonitor:
    """Proxy 组件监控器"""

    def __init__(self, host: str = "localhost", port: str = "19530"):
        self.host = host
        self.port = port
        self.alias = "default"

    def connect(self):
        """连接到 Milvus"""
        connections.connect(alias=self.alias, host=self.host, port=self.port)

    def disconnect(self):
        """断开连接"""
        connections.disconnect(alias=self.alias)

    def measure_connection_latency(self, iterations: int = 10) -> Dict:
        """测量连接延迟"""
        latencies = []

        for _ in range(iterations):
            start = time.time()
            connections.connect(alias="test", host=self.host, port=self.port)
            latency = (time.time() - start) * 1000  # ms
            latencies.append(latency)
            connections.disconnect(alias="test")
            time.sleep(0.1)

        return {
            "avg_latency_ms": sum(latencies) / len(latencies),
            "min_latency_ms": min(latencies),
            "max_latency_ms": max(latencies)
        }

    def measure_query_latency(self, collection_name: str, iterations: int = 10) -> Dict:
        """测量查询延迟"""
        collection = Collection(collection_name)
        collection.load()

        # 准备查询向量
        import random
        dim = collection.schema.fields[-1].params['dim']
        query_vector = [[random.random() for _ in range(dim)]]

        latencies = []
        for _ in range(iterations):
            start = time.time()
            collection.search(
                data=query_vector,
                anns_field="embedding",
                param={"metric_type": "L2", "params": {"nprobe": 10}},
                limit=10
            )
            latency = (time.time() - start) * 1000
            latencies.append(latency)

        return {
            "avg_latency_ms": sum(latencies) / len(latencies),
            "min_latency_ms": min(latencies),
            "max_latency_ms": max(latencies),
            "p95_latency_ms": sorted(latencies)[int(len(latencies) * 0.95)]
        }

    def print_proxy_summary(self, collection_name: str = None):
        """打印 Proxy 监控摘要"""
        print("\n" + "="*60)
        print("Proxy Component Monitoring")
        print("="*60)

        # 连接延迟
        print("\n[Connection Latency]")
        conn_stats = self.measure_connection_latency()
        print(f"  Average: {conn_stats['avg_latency_ms']:.2f} ms")
        print(f"  Min: {conn_stats['min_latency_ms']:.2f} ms")
        print(f"  Max: {conn_stats['max_latency_ms']:.2f} ms")

        # 查询延迟
        if collection_name:
            print("\n[Query Latency]")
            query_stats = self.measure_query_latency(collection_name)
            print(f"  Average: {query_stats['avg_latency_ms']:.2f} ms")
            print(f"  P95: {query_stats['p95_latency_ms']:.2f} ms")

        print("\n" + "="*60)


def main_proxy():
    """Proxy 监控主函数"""
    monitor = ProxyMonitor()

    try:
        monitor.connect()
        monitor.print_proxy_summary()
    finally:
        monitor.disconnect()


if __name__ == "__main__":
    main_proxy()
```

---

### 3.2 QueryNode 组件监控

```python
"""
QueryNode 组件监控：查询性能和资源使用
"""

from pymilvus import Collection, utility
import psutil
from typing import Dict

class QueryNodeMonitor:
    """QueryNode 组件监控器"""

    def __init__(self):
        pass

    def get_segment_info(self, collection_name: str) -> List[Dict]:
        """获取 Segment 信息"""
        segments = utility.get_query_segment_info(collection_name)

        segment_info = []
        for seg in segments:
            segment_info.append({
                "segment_id": seg.segmentID,
                "node_id": seg.nodeID,
                "state": seg.state,
                "num_rows": seg.num_rows,
                "index_name": seg.index_name
            })

        return segment_info

    def get_memory_usage(self) -> Dict:
        """获取内存使用情况"""
        mem = psutil.virtual_memory()
        return {
            "total_gb": mem.total / (1024**3),
            "used_gb": mem.used / (1024**3),
            "available_gb": mem.available / (1024**3),
            "percent": mem.percent
        }

    def analyze_segment_distribution(self, collection_name: str) -> Dict:
        """分析 Segment 分布"""
        segments = self.get_segment_info(collection_name)

        node_distribution = {}
        for seg in segments:
            node_id = seg["node_id"]
            if node_id not in node_distribution:
                node_distribution[node_id] = {
                    "segment_count": 0,
                    "total_rows": 0
                }
            node_distribution[node_id]["segment_count"] += 1
            node_distribution[node_id]["total_rows"] += seg["num_rows"]

        return node_distribution

    def print_querynode_summary(self, collection_name: str):
        """打印 QueryNode 监控摘要"""
        print("\n" + "="*60)
        print("QueryNode Component Monitoring")
        print("="*60)

        # Segment 信息
        segments = self.get_segment_info(collection_name)
        print(f"\n[Segment Info] ({len(segments)} segments)")

        table_data = [
            [seg["segment_id"], seg["node_id"], seg["state"],
             seg["num_rows"], seg["index_name"]]
            for seg in segments[:5]  # 只显示前5个
        ]
        print(tabulate(
            table_data,
            headers=["Segment ID", "Node ID", "State", "Rows", "Index"],
            tablefmt="grid"
        ))

        # Segment 分布
        distribution = self.analyze_segment_distribution(collection_name)
        print(f"\n[Segment Distribution]")
        for node_id, stats in distribution.items():
            print(f"  Node {node_id}: {stats['segment_count']} segments, "
                  f"{stats['total_rows']:,} rows")

        # 内存使用
        memory = self.get_memory_usage()
        print(f"\n[Memory Usage]")
        print(f"  Total: {memory['total_gb']:.2f} GB")
        print(f"  Used: {memory['used_gb']:.2f} GB ({memory['percent']:.1f}%)")
        print(f"  Available: {memory['available_gb']:.2f} GB")

        print("\n" + "="*60)


def main_querynode():
    """QueryNode 监控主函数"""
    monitor = QueryNodeMonitor()
    monitor.print_querynode_summary("your_collection_name")


if __name__ == "__main__":
    main_querynode()
```

---

### 3.3 DataNode 组件监控

```python
"""
DataNode 组件监控：数据持久化和存储状态
"""

from pymilvus import Collection, utility
import time

class DataNodeMonitor:
    """DataNode 组件监控器"""

    def __init__(self):
        pass

    def get_collection_stats(self, collection_name: str) -> Dict:
        """获取 Collection 统计信息"""
        collection = Collection(collection_name)
        stats = collection.get_stats()

        return {
            "row_count": int(stats.get("row_count", 0)),
            "data_size": int(stats.get("data_size", 0))
        }

    def measure_insert_throughput(self, collection_name: str,
                                  batch_size: int = 1000) -> Dict:
        """测量插入吞吐量"""
        collection = Collection(collection_name)

        # 准备测试数据
        import random
        dim = collection.schema.fields[-1].params['dim']

        data = [
            list(range(batch_size)),  # IDs
            [[random.random() for _ in range(dim)] for _ in range(batch_size)]  # Vectors
        ]

        # 测量插入时间
        start = time.time()
        collection.insert(data)
        collection.flush()
        duration = time.time() - start

        return {
            "batch_size": batch_size,
            "duration_s": duration,
            "throughput_rows_per_s": batch_size / duration
        }

    def monitor_flush_status(self, collection_name: str) -> Dict:
        """监控 Flush 状态"""
        collection = Collection(collection_name)

        # 触发 Flush
        start = time.time()
        collection.flush()
        flush_duration = time.time() - start

        # 获取 Segment 信息
        segments = utility.get_query_segment_info(collection_name)
        sealed_count = sum(1 for seg in segments if seg.state == "Sealed")

        return {
            "flush_duration_s": flush_duration,
            "total_segments": len(segments),
            "sealed_segments": sealed_count
        }

    def print_datanode_summary(self, collection_name: str):
        """打印 DataNode 监控摘要"""
        print("\n" + "="*60)
        print("DataNode Component Monitoring")
        print("="*60)

        # Collection 统计
        stats = self.get_collection_stats(collection_name)
        print(f"\n[Collection Stats]")
        print(f"  Row Count: {stats['row_count']:,}")
        print(f"  Data Size: {stats['data_size'] / (1024**2):.2f} MB")

        # 插入吞吐量
        print(f"\n[Insert Throughput]")
        throughput = self.measure_insert_throughput(collection_name)
        print(f"  Batch Size: {throughput['batch_size']}")
        print(f"  Duration: {throughput['duration_s']:.2f} s")
        print(f"  Throughput: {throughput['throughput_rows_per_s']:.0f} rows/s")

        # Flush 状态
        print(f"\n[Flush Status]")
        flush_stats = self.monitor_flush_status(collection_name)
        print(f"  Flush Duration: {flush_stats['flush_duration_s']:.2f} s")
        print(f"  Total Segments: {flush_stats['total_segments']}")
        print(f"  Sealed Segments: {flush_stats['sealed_segments']}")

        print("\n" + "="*60)


def main_datanode():
    """DataNode 监控主函数"""
    monitor = DataNodeMonitor()
    monitor.print_datanode_summary("your_collection_name")


if __name__ == "__main__":
    main_datanode()
```

---

### 3.4 IndexNode 组件监控

```python
"""
IndexNode 组件监控：索引构建进度和状态
"""

from pymilvus import Collection, utility
import time

class IndexNodeMonitor:
    """IndexNode 组件监控器"""

    def __init__(self):
        pass

    def get_index_info(self, collection_name: str) -> List[Dict]:
        """获取索引信息"""
        collection = Collection(collection_name)
        indexes = collection.indexes

        index_info = []
        for index in indexes:
            index_info.append({
                "field_name": index.field_name,
                "index_type": index.params.get("index_type"),
                "metric_type": index.params.get("metric_type"),
                "params": index.params.get("params", {})
            })

        return index_info

    def monitor_index_building(self, collection_name: str,
                              field_name: str = "embedding") -> Dict:
        """监控索引构建进度"""
        collection = Collection(collection_name)

        # 检查索引构建状态
        start = time.time()
        collection.wait_for_index_building_complete(field_name)
        build_duration = time.time() - start

        # 获取索引信息
        index = collection.index(field_name)

        return {
            "field_name": field_name,
            "build_duration_s": build_duration,
            "index_type": index.params.get("index_type"),
            "status": "completed"
        }

    def measure_index_build_time(self, collection_name: str,
                                 index_params: Dict) -> Dict:
        """测量索引构建时间"""
        collection = Collection(collection_name)

        # 删除现有索引
        try:
            collection.drop_index()
        except:
            pass

        # 构建新索引
        start = time.time()
        collection.create_index(
            field_name="embedding",
            index_params=index_params
        )
        collection.wait_for_index_building_complete()
        build_duration = time.time() - start

        # 获取 Collection 统计
        stats = collection.get_stats()
        row_count = int(stats.get("row_count", 0))

        return {
            "index_type": index_params["index_type"],
            "row_count": row_count,
            "build_duration_s": build_duration,
            "rows_per_second": row_count / build_duration if build_duration > 0 else 0
        }

    def print_indexnode_summary(self, collection_name: str):
        """打印 IndexNode 监控摘要"""
        print("\n" + "="*60)
        print("IndexNode Component Monitoring")
        print("="*60)

        # 索引信息
        indexes = self.get_index_info(collection_name)
        print(f"\n[Index Info] ({len(indexes)} indexes)")
        for idx in indexes:
            print(f"  Field: {idx['field_name']}")
            print(f"  Type: {idx['index_type']}")
            print(f"  Metric: {idx['metric_type']}")
            print(f"  Params: {idx['params']}")

        # 索引构建状态
        print(f"\n[Index Building Status]")
        build_stats = self.monitor_index_building(collection_name)
        print(f"  Field: {build_stats['field_name']}")
        print(f"  Status: {build_stats['status']}")
        print(f"  Duration: {build_stats['build_duration_s']:.2f} s")

        print("\n" + "="*60)


def main_indexnode():
    """IndexNode 监控主函数"""
    monitor = IndexNodeMonitor()
    monitor.print_indexnode_summary("your_collection_name")


if __name__ == "__main__":
    main_indexnode()
```

---

### 3.5 完整监控系统

```python
"""
完整的 Milvus 组件监控系统
"""

from pymilvus import connections
import time
from datetime import datetime

class MilvusComponentMonitor:
    """完整的 Milvus 组件监控系统"""

    def __init__(self, host: str = "localhost", port: str = "19530"):
        self.host = host
        self.port = port
        self.proxy_monitor = ProxyMonitor(host, port)
        self.querynode_monitor = QueryNodeMonitor()
        self.datanode_monitor = DataNodeMonitor()
        self.indexnode_monitor = IndexNodeMonitor()

    def connect(self):
        """连接到 Milvus"""
        connections.connect(host=self.host, port=self.port)

    def disconnect(self):
        """断开连接"""
        connections.disconnect()

    def run_full_monitoring(self, collection_name: str):
        """运行完整监控"""
        print("\n" + "="*70)
        print(f"Milvus Component Monitoring Report - {datetime.now()}")
        print("="*70)

        # 1. Proxy 监控
        self.proxy_monitor.print_proxy_summary(collection_name)

        # 2. QueryNode 监控
        self.querynode_monitor.print_querynode_summary(collection_name)

        # 3. DataNode 监控
        self.datanode_monitor.print_datanode_summary(collection_name)

        # 4. IndexNode 监控
        self.indexnode_monitor.print_indexnode_summary(collection_name)

    def continuous_monitoring(self, collection_name: str, interval: int = 60):
        """持续监控"""
        while True:
            try:
                self.run_full_monitoring(collection_name)
                time.sleep(interval)
            except KeyboardInterrupt:
                print("\nMonitoring stopped by user")
                break
            except Exception as e:
                print(f"Error during monitoring: {e}")
                time.sleep(interval)


def main_complete():
    """完整监控主函数"""
    monitor = MilvusComponentMonitor()

    try:
        monitor.connect()
        monitor.run_full_monitoring("your_collection_name")
        # monitor.continuous_monitoring("your_collection_name", interval=60)
    finally:
        monitor.disconnect()


if __name__ == "__main__":
    main_complete()
```

---

## 4. 在 RAG 中的应用

### 4.1 性能优化

```python
def optimize_based_on_monitoring(monitor: MilvusComponentMonitor,
                                collection_name: str):
    """根据监控结果优化性能"""

    # 检查查询延迟
    query_stats = monitor.proxy_monitor.measure_query_latency(collection_name)

    if query_stats['avg_latency_ms'] > 100:
        print("Warning: High query latency detected")
        print("Recommendations:")
        print("- Consider using HNSW index for faster queries")
        print("- Increase nprobe parameter")
        print("- Add more QueryNodes")

    # 检查内存使用
    memory = monitor.querynode_monitor.get_memory_usage()

    if memory['percent'] > 80:
        print("Warning: High memory usage detected")
        print("Recommendations:")
        print("- Release unused collections")
        print("- Use IVF_PQ index to reduce memory")
        print("- Add more memory or QueryNodes")
```

### 4.2 告警系统

```python
def check_alerts(monitor: MilvusComponentMonitor, collection_name: str):
    """检查告警条件"""

    alerts = []

    # 检查查询延迟
    query_stats = monitor.proxy_monitor.measure_query_latency(collection_name)
    if query_stats['p95_latency_ms'] > 200:
        alerts.append({
            "level": "warning",
            "component": "Proxy",
            "message": f"P95 query latency: {query_stats['p95_latency_ms']:.2f} ms"
        })

    # 检查内存使用
    memory = monitor.querynode_monitor.get_memory_usage()
    if memory['percent'] > 90:
        alerts.append({
            "level": "critical",
            "component": "QueryNode",
            "message": f"Memory usage: {memory['percent']:.1f}%"
        })

    # 打印告警
    if alerts:
        print("\nALERTS:")
        for alert in alerts:
            print(f"[{alert['level'].upper()}] {alert['component']}: {alert['message']}")

    return alerts
```

---

## 5. 最佳实践

### 5.1 监控指标选择

**关键指标**：
- Proxy: 连接延迟、查询延迟、P95/P99 延迟
- QueryNode: Segment 分布、内存使用、查询 QPS
- DataNode: 插入吞吐量、Flush 时间、存储大小
- IndexNode: 索引构建时间、索引类型、构建状态

### 5.2 告警阈值设置

```python
ALERT_THRESHOLDS = {
    "query_latency_p95_ms": 200,
    "memory_usage_percent": 85,
    "insert_throughput_rows_per_s": 1000,
    "index_build_duration_s": 300
}
```

### 5.3 监控频率

- **实时监控**：每 10 秒（生产环境）
- **定期监控**：每 1 分钟（开发环境）
- **深度分析**：每 1 小时（性能优化）

---

## 6. 总结

### 6.1 核心要点

1. **多维度监控**：Proxy、QueryNode、DataNode、IndexNode
2. **性能指标**：延迟、吞吐量、资源使用
3. **自动化告警**：设置阈值，及时发现问题
4. **持续优化**：根据监控结果调整配置

### 6.2 下一步

- **场景 3**：分布式部署监控
- **场景 4**：RAG 架构集成监控

---

**完整代码**：`examples/milvus_component_monitoring.py`
