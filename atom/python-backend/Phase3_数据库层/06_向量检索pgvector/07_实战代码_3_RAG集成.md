# 实战代码场景3：RAG 文档问答集成

本示例演示如何使用 pgvector 构建完整的 RAG（Retrieval-Augmented Generation）文档问答系统。

## 完整代码示例

```python
"""
RAG 文档问答系统完整实现
演示：文档入库、向量检索、LLM 生成、FastAPI 集成
"""

import os
import psycopg2
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict
from dataclasses import dataclass

# 加载环境变量
load_dotenv()

# 初始化
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL")
)

# ===== 1. 数据模型 =====
print("=== 1. 数据模型 ===")

@dataclass
class Document:
    """文档模型"""
    id: int
    content: str
    metadata: Dict
    distance: float = None

@dataclass
class RAGResponse:
    """RAG 响应模型"""
    answer: str
    sources: List[Document]
    query: str

# ===== 2. 数据库初始化 =====
print("\n=== 2. 数据库初始化 ===")

class VectorDB:
    """向量数据库封装"""

    def __init__(self, connection_string: str):
        self.conn = psycopg2.connect(connection_string)
        self.cursor = self.conn.cursor()
        self._init_db()

    def _init_db(self):
        """初始化数据库"""
        # 启用 pgvector 扩展
        self.cursor.execute("CREATE EXTENSION IF NOT EXISTS vector")

        # 创建文档表
        self.cursor.execute("""
            DROP TABLE IF EXISTS documents CASCADE
        """)

        self.cursor.execute("""
            CREATE TABLE documents (
                id SERIAL PRIMARY KEY,
                content TEXT NOT NULL,
                embedding vector(1536),
                metadata JSONB,
                created_at TIMESTAMP DEFAULT NOW()
            )
        """)

        self.conn.commit()
        print("✅ 数据库初始化完成")

    def insert_document(self, content: str, embedding: List[float], metadata: Dict) -> int:
        """插入单个文档"""
        self.cursor.execute(
            """
            INSERT INTO documents (content, embedding, metadata)
            VALUES (%s, %s, %s)
            RETURNING id
            """,
            (content, embedding, metadata)
        )
        doc_id = self.cursor.fetchone()[0]
        self.conn.commit()
        return doc_id

    def insert_documents_batch(self, documents: List[tuple]) -> int:
        """批量插入文档"""
        self.cursor.executemany(
            "INSERT INTO documents (content, embedding, metadata) VALUES (%s, %s, %s)",
            documents
        )
        self.conn.commit()
        return len(documents)

    def search(self, query_embedding: List[float], top_k: int = 5, filters: Dict = None) -> List[Document]:
        """向量检索"""
        query = """
            SELECT id, content, metadata, embedding <=> %s AS distance
            FROM documents
        """

        params = [query_embedding]

        # 添加过滤条件
        if filters:
            conditions = []
            for key, value in filters.items():
                conditions.append(f"metadata->>'{key}' = %s")
                params.append(value)
            if conditions:
                query += " WHERE " + " AND ".join(conditions)

        query += " ORDER BY distance LIMIT %s"
        params.append(top_k)

        self.cursor.execute(query, params)

        return [
            Document(
                id=row[0],
                content=row[1],
                metadata=row[2],
                distance=row[3]
            )
            for row in self.cursor.fetchall()
        ]

    def close(self):
        """关闭连接"""
        self.cursor.close()
        self.conn.close()

# 初始化数据库
db = VectorDB(os.getenv("DATABASE_URL"))

# ===== 3. 文档入库 =====
print("\n=== 3. 文档入库 ===")

# 准备知识库文档
knowledge_base = [
    {
        "content": "pgvector 是 PostgreSQL 的向量扩展，支持向量存储和相似度检索。它提供了三种距离函数：余弦距离、欧氏距离和内积距离。",
        "metadata": {"category": "pgvector", "topic": "基础"}
    },
    {
        "content": "HNSW（Hierarchical Navigable Small World）是一种高效的向量索引算法，使用多层图结构实现快速的近似最近邻搜索。查询复杂度为 O(log N)。",
        "metadata": {"category": "pgvector", "topic": "索引"}
    },
    {
        "content": "IVFFlat 索引使用聚类和倒排索引的方式加速向量检索。它将向量聚类成多个簇，查询时只在最近的几个簇内搜索。",
        "metadata": {"category": "pgvector", "topic": "索引"}
    },
    {
        "content": "RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。它先从知识库中检索相关文档，然后将文档作为上下文输入给 LLM 生成回答。",
        "metadata": {"category": "rag", "topic": "架构"}
    },
    {
        "content": "在 RAG 系统中，文档分块（Chunking）是关键步骤。常见策略包括固定长度分块、句子分块、段落分块等。分块大小通常在 200-500 tokens 之间。",
        "metadata": {"category": "rag", "topic": "优化"}
    },
    {
        "content": "Embedding 模型将文本转换为向量表示。OpenAI 的 text-embedding-3-small 模型输出 1536 维向量，适合大多数文本检索场景。",
        "metadata": {"category": "embedding", "topic": "模型"}
    },
    {
        "content": "余弦距离适用于已归一化的向量（如文本 Embedding），只关心向量的方向而不关心长度。值域为 [0, 2]，0 表示完全相同。",
        "metadata": {"category": "pgvector", "topic": "距离函数"}
    },
    {
        "content": "在生产环境中，建议使用 HNSW 索引来加速向量检索。对于 50 万文档，HNSW 索引可以将查询延迟从 5 秒降到 30 毫秒。",
        "metadata": {"category": "pgvector", "topic": "性能"}
    }
]

print(f"正在入库 {len(knowledge_base)} 个文档...")

# 批量生成 Embedding
texts = [doc["content"] for doc in knowledge_base]
embeddings_response = client.embeddings.create(
    input=texts,
    model="text-embedding-3-small"
)

# 批量插入
documents = [
    (doc["content"], emb.embedding, doc["metadata"])
    for doc, emb in zip(knowledge_base, embeddings_response.data)
]

count = db.insert_documents_batch(documents)
print(f"✅ 已入库 {count} 个文档")

# ===== 4. RAG 检索器 =====
print("\n=== 4. RAG 检索器 ===")

class RAGRetriever:
    """RAG 检索器"""

    def __init__(self, db: VectorDB, client: OpenAI):
        self.db = db
        self.client = client

    def retrieve(self, query: str, top_k: int = 3, filters: Dict = None) -> List[Document]:
        """检索相关文档"""
        # 生成查询向量
        query_embedding = self.client.embeddings.create(
            input=query,
            model="text-embedding-3-small"
        ).data[0].embedding

        # 向量检索
        documents = self.db.search(query_embedding, top_k=top_k, filters=filters)

        return documents

retriever = RAGRetriever(db, client)

# 测试检索
query = "如何优化向量检索性能？"
docs = retriever.retrieve(query, top_k=3)

print(f"\n查询: {query}")
print(f"检索到 {len(docs)} 个相关文档：")
for i, doc in enumerate(docs, 1):
    print(f"\n文档 {i} (距离: {doc.distance:.4f}):")
    print(f"  {doc.content[:100]}...")
    print(f"  元数据: {doc.metadata}")

# ===== 5. RAG 生成器 =====
print("\n=== 5. RAG 生成器 ===")

class RAGGenerator:
    """RAG 生成器"""

    def __init__(self, retriever: RAGRetriever, client: OpenAI):
        self.retriever = retriever
        self.client = client

    def generate(self, query: str, top_k: int = 3, filters: Dict = None) -> RAGResponse:
        """生成回答"""
        # 1. 检索相关文档
        documents = self.retriever.retrieve(query, top_k=top_k, filters=filters)

        # 2. 构建上下文
        context = "\n\n".join([
            f"文档 {i+1}:\n{doc.content}"
            for i, doc in enumerate(documents)
        ])

        # 3. 构建 Prompt
        system_prompt = """你是一个专业的技术助手。请根据提供的文档回答用户的问题。

要求：
1. 只使用提供的文档中的信息回答
2. 如果文档中没有相关信息，明确告知用户
3. 回答要简洁、准确、易懂
4. 可以引用文档编号（如"根据文档1..."）"""

        user_prompt = f"""文档：
{context}

问题：{query}

请根据上述文档回答问题。"""

        # 4. 调用 LLM 生成回答
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3
        )

        answer = response.choices[0].message.content

        return RAGResponse(
            answer=answer,
            sources=documents,
            query=query
        )

generator = RAGGenerator(retriever, client)

# 测试生成
print("\n=== 测试 RAG 问答 ===")

questions = [
    "什么是 HNSW 索引？",
    "如何优化向量检索性能？",
    "RAG 系统的工作原理是什么？"
]

for question in questions:
    print(f"\n{'='*60}")
    print(f"问题: {question}")
    print(f"{'='*60}")

    response = generator.generate(question, top_k=3)

    print(f"\n回答:\n{response.answer}")

    print(f"\n参考文档:")
    for i, doc in enumerate(response.sources, 1):
        print(f"  {i}. {doc.content[:80]}... (相似度: {1-doc.distance:.4f})")

# ===== 6. FastAPI 集成 =====
print("\n=== 6. FastAPI 集成 ===")

print("""
以下是 FastAPI 集成代码（保存为 app.py）：

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Optional

app = FastAPI(title="RAG 文档问答 API")

# 初始化 RAG 系统
db = VectorDB(os.getenv("DATABASE_URL"))
retriever = RAGRetriever(db, client)
generator = RAGGenerator(retriever, client)

# 请求/响应模型
class QuestionRequest(BaseModel):
    question: str
    top_k: int = 3
    filters: Optional[Dict] = None

class DocumentResponse(BaseModel):
    id: int
    content: str
    metadata: Dict
    similarity: float

class AnswerResponse(BaseModel):
    answer: str
    sources: List[DocumentResponse]
    query: str

# API 端点
@app.post("/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    \"\"\"问答接口\"\"\"
    try:
        # 生成回答
        response = generator.generate(
            query=request.question,
            top_k=request.top_k,
            filters=request.filters
        )

        # 转换响应格式
        return AnswerResponse(
            answer=response.answer,
            sources=[
                DocumentResponse(
                    id=doc.id,
                    content=doc.content,
                    metadata=doc.metadata,
                    similarity=1 - doc.distance
                )
                for doc in response.sources
            ],
            query=response.query
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ingest")
async def ingest_document(content: str, metadata: Dict):
    \"\"\"文档入库接口\"\"\"
    try:
        # 生成 Embedding
        embedding = client.embeddings.create(
            input=content,
            model="text-embedding-3-small"
        ).data[0].embedding

        # 插入数据库
        doc_id = db.insert_document(content, embedding, metadata)

        return {"id": doc_id, "message": "文档入库成功"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    \"\"\"健康检查\"\"\"
    return {"status": "ok"}

# 运行：uvicorn app:app --reload
```

测试 API：

```bash
# 1. 问答
curl -X POST "http://localhost:8000/ask" \\
  -H "Content-Type: application/json" \\
  -d '{
    "question": "什么是 HNSW 索引？",
    "top_k": 3
  }'

# 2. 文档入库
curl -X POST "http://localhost:8000/ingest" \\
  -H "Content-Type: application/json" \\
  -d '{
    "content": "新文档内容",
    "metadata": {"category": "test"}
  }'

# 3. 健康检查
curl "http://localhost:8000/health"
```
""")

# ===== 7. 流式响应 =====
print("\n=== 7. 流式响应 ===")

def generate_stream(query: str, top_k: int = 3):
    """流式生成回答"""
    # 1. 检索文档
    documents = retriever.retrieve(query, top_k=top_k)

    # 2. 构建上下文
    context = "\n\n".join([f"文档 {i+1}:\n{doc.content}" for i, doc in enumerate(documents)])

    # 3. 流式生成
    stream = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "你是一个专业的技术助手。请根据提供的文档回答用户的问题。"},
            {"role": "user", "content": f"文档：\n{context}\n\n问题：{query}"}
        ],
        stream=True
    )

    print(f"\n问题: {query}")
    print(f"回答: ", end="", flush=True)

    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)

    print("\n")

# 测试流式响应
generate_stream("什么是 RAG 系统？", top_k=2)

# ===== 8. 性能统计 =====
print("\n=== 8. 性能统计 ===")

import time

def benchmark_rag(query: str, iterations: int = 5):
    """测试 RAG 性能"""
    times = {
        "retrieval": [],
        "generation": [],
        "total": []
    }

    for _ in range(iterations):
        # 总时间
        start_total = time.time()

        # 检索时间
        start_retrieval = time.time()
        documents = retriever.retrieve(query, top_k=3)
        retrieval_time = (time.time() - start_retrieval) * 1000

        # 生成时间
        context = "\n\n".join([doc.content for doc in documents])
        start_generation = time.time()
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "回答问题"},
                {"role": "user", "content": f"文档：{context}\n\n问题：{query}"}
            ]
        )
        generation_time = (time.time() - start_generation) * 1000

        total_time = (time.time() - start_total) * 1000

        times["retrieval"].append(retrieval_time)
        times["generation"].append(generation_time)
        times["total"].append(total_time)

    return {
        "retrieval_avg": sum(times["retrieval"]) / len(times["retrieval"]),
        "generation_avg": sum(times["generation"]) / len(times["generation"]),
        "total_avg": sum(times["total"]) / len(times["total"])
    }

stats = benchmark_rag("什么是 HNSW 索引？", iterations=5)
print(f"\nRAG 性能统计（5次平均）：")
print(f"  检索时间: {stats['retrieval_avg']:.2f}ms")
print(f"  生成时间: {stats['generation_avg']:.2f}ms")
print(f"  总时间: {stats['total_avg']:.2f}ms")

# ===== 9. 清理资源 =====
print("\n=== 9. 清理资源 ===")

db.close()
print("✅ 数据库连接已关闭")

print("\n=== 示例完成 ===")
```

## 运行输出示例

```
=== 1. 数据模型 ===

=== 2. 数据库初始化 ===
✅ 数据库初始化完成

=== 3. 文档入库 ===
正在入库 8 个文档...
✅ 已入库 8 个文档

=== 4. RAG 检索器 ===

查询: 如何优化向量检索性能？
检索到 3 个相关文档：

文档 1 (距离: 0.1234):
  在生产环境中，建议使用 HNSW 索引来加速向量检索。对于 50 万文档，HNSW 索引可以将查询延迟从 5 秒降到 30 毫秒。...
  元数据: {'category': 'pgvector', 'topic': '性能'}

文档 2 (距离: 0.2345):
  HNSW（Hierarchical Navigable Small World）是一种高效的向量索引算法，使用多层图结构实现快速的近似最近邻搜索...
  元数据: {'category': 'pgvector', 'topic': '索引'}

文档 3 (距离: 0.2678):
  IVFFlat 索引使用聚类和倒排索引的方式加速向量检索。它将向量聚类成多个簇，查询时只在最近的几个簇内搜索。...
  元数据: {'category': 'pgvector', 'topic': '索引'}

=== 测试 RAG 问答 ===

============================================================
问题: 什么是 HNSW 索引？
============================================================

回答:
HNSW（Hierarchical Navigable Small World）是一种高效的向量索引算法。根据文档2，它使用多层图结构实现快速的近似最近邻搜索，查询复杂度为 O(log N)。

在实际应用中，HNSW 索引能够显著提升检索性能。根据文档1，在生产环境中使用 HNSW 索引，对于 50 万文档的场景，可以将查询延迟从 5 秒降到 30 毫秒，性能提升非常明显。

参考文档:
  1. HNSW（Hierarchical Navigable Small World）是一种高效的向量索引算法，使用多层图... (相似度: 0.9234)
  2. 在生产环境中，建议使用 HNSW 索引来加速向量检索。对于 50 万文档，HNSW 索引可以... (相似度: 0.8876)
  3. pgvector 是 PostgreSQL 的向量扩展，支持向量存储和相似度检索。它提供了三种距离... (相似度: 0.7654)

=== 8. 性能统计 ===

RAG 性能统计（5次平均）：
  检索时间: 12.34ms
  生成时间: 1234.56ms
  总时间: 1246.90ms

=== 示例完成 ===
```

## 代码说明

### 1. 数据库封装

```python
class VectorDB:
    def insert_document(self, content, embedding, metadata):
        # 插入单个文档

    def search(self, query_embedding, top_k, filters):
        # 向量检索
```

### 2. RAG 检索器

```python
class RAGRetriever:
    def retrieve(self, query, top_k, filters):
        # 1. 生成查询向量
        # 2. 向量检索
        # 3. 返回文档
```

### 3. RAG 生成器

```python
class RAGGenerator:
    def generate(self, query, top_k, filters):
        # 1. 检索文档
        # 2. 构建上下文
        # 3. LLM 生成回答
```

### 4. FastAPI 集成

```python
@app.post("/ask")
async def ask_question(request: QuestionRequest):
    response = generator.generate(request.question)
    return response
```

## 常见问题

### Q1: 如何处理长文档？

```python
# 文档分块
def chunk_document(content: str, chunk_size: int = 500):
    """将长文档分块"""
    words = content.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

# 入库时分块
chunks = chunk_document(long_document)
for chunk in chunks:
    embedding = client.embeddings.create(input=chunk, model="text-embedding-3-small").data[0].embedding
    db.insert_document(chunk, embedding, metadata)
```

### Q2: 如何提高回答质量？

```python
# 1. 增加检索文档数量
documents = retriever.retrieve(query, top_k=5)  # 从 3 增加到 5

# 2. 使用更好的 Prompt
system_prompt = """你是一个专业的技术助手。
要求：
1. 只使用提供的文档回答
2. 引用具体的文档编号
3. 如果文档不足以回答，明确说明
4. 回答要结构化、易读"""

# 3. 调整 LLM 参数
response = client.chat.completions.create(
    model="gpt-4",
    temperature=0.1,  # 降低随机性
    max_tokens=500    # 限制回答长度
)
```

### Q3: 如何处理多轮对话？

```python
class ConversationalRAG:
    def __init__(self, generator):
        self.generator = generator
        self.history = []

    def chat(self, query: str):
        # 1. 检索文档
        documents = self.generator.retriever.retrieve(query)

        # 2. 构建对话历史
        messages = [
            {"role": "system", "content": "你是一个专业的技术助手"}
        ]
        for h in self.history:
            messages.append({"role": "user", "content": h["query"]})
            messages.append({"role": "assistant", "content": h["answer"]})

        # 3. 添加当前问题
        context = "\n\n".join([doc.content for doc in documents])
        messages.append({"role": "user", "content": f"文档：{context}\n\n问题：{query}"})

        # 4. 生成回答
        response = client.chat.completions.create(model="gpt-4", messages=messages)
        answer = response.choices[0].message.content

        # 5. 保存历史
        self.history.append({"query": query, "answer": answer})

        return answer
```

## 下一步

完成 RAG 集成后，可以继续学习：
- **场景4**：索引优化（创建索引，提升检索性能）
- **化骨绵掌**：深入理解 pgvector 的各个方面
