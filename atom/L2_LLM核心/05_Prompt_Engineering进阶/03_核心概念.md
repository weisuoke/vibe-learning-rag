# 核心概念

进阶 Prompt 有三个核心概念，掌握它们就能显著提升 RAG 系统的输出质量。

---

## 核心概念1：Few-shot（少样本学习）

**Few-shot 是通过在 Prompt 中提供几个示例，让 LLM 学会你想要的输出格式和风格。**

### 为什么 Few-shot 有效？

LLM 在预训练时见过海量的"输入-输出"对，它学会了**模式匹配**。当你给它几个示例，它会识别模式并应用到新问题上。

```
示例1: 输入A → 输出A'
示例2: 输入B → 输出B'
示例3: 输入C → 输出C'
新问题: 输入D → ?

LLM 会推断：输出应该是 D'（符合前面的模式）
```

### Few-shot 的基本结构

```python
def create_few_shot_prompt(examples: list[dict], query: str) -> str:
    """创建 Few-shot Prompt"""

    # 构建示例部分
    examples_text = ""
    for i, ex in enumerate(examples, 1):
        examples_text += f"""
示例 {i}：
输入：{ex['input']}
输出：{ex['output']}
"""

    prompt = f"""
请按照以下示例的格式回答问题。

{examples_text}

现在请回答：
输入：{query}
输出："""

    return prompt


# 使用示例
examples = [
    {"input": "苹果", "output": "水果，红色或绿色，富含维生素C"},
    {"input": "西红柿", "output": "蔬菜，红色，富含番茄红素"},
    {"input": "胡萝卜", "output": "蔬菜，橙色，富含胡萝卜素"}
]

prompt = create_few_shot_prompt(examples, "香蕉")
# LLM 会输出类似：水果，黄色，富含钾元素
```

### Few-shot 的变体

| 变体 | 示例数量 | 适用场景 |
|------|---------|---------|
| **Zero-shot** | 0个 | 简单任务，LLM 已经知道怎么做 |
| **One-shot** | 1个 | 格式简单，一个例子就够 |
| **Few-shot** | 2-5个 | 复杂格式，需要多个例子强化 |

### 在 RAG 中的应用

```python
# RAG Few-shot 示例：统一回答风格
RAG_FEW_SHOT_PROMPT = """
你是一个客服助手，请按照示例的风格回答问题。

## 示例

问题：如何退款？
参考文档：用户可在购买后7天内申请退款，需要提供订单号。
回答：您好！退款流程如下：
1. 确认您的购买在7天内
2. 准备好订单号
3. 联系客服提交退款申请
如有其他问题，随时联系我们！

问题：运费怎么算？
参考文档：订单满99元免运费，不满99元收取10元运费。
回答：您好！运费规则如下：
- 订单满99元：免运费
- 订单不满99元：收取10元运费
祝您购物愉快！

## 现在请回答

问题：{query}
参考文档：{context}
回答："""
```

### Few-shot 最佳实践

| 实践 | 说明 |
|------|------|
| **示例要多样** | 覆盖不同类型的输入，避免 LLM 过拟合 |
| **示例要高质量** | 示例本身要正确、格式规范 |
| **示例要相关** | 与实际任务场景相似 |
| **数量适中** | 通常 2-5 个，太多会占用 Token |

---

## 核心概念2：Chain-of-Thought（思维链）

**Chain-of-Thought（CoT）是让 LLM 在给出最终答案前，先展示推理过程的技术。**

### 为什么 CoT 有效？

LLM 是自回归模型，每个 token 的生成依赖于前面的 token。当 LLM 先输出推理步骤，这些步骤会成为生成最终答案的"上下文"，帮助它做出更准确的判断。

```
没有 CoT：
问题 → 答案（直接跳到结论，容易出错）

有 CoT：
问题 → 步骤1 → 步骤2 → 步骤3 → 答案（逐步推导，更准确）
```

### CoT 的两种形式

#### 形式1：Zero-shot CoT（零样本思维链）

只需要加一句"让我们一步步思考"：

```python
# Zero-shot CoT
prompt = """
问题：一个商店有 23 个苹果，卖出了 15 个，又进货了 8 个，现在有多少个？

让我们一步步思考：
"""

# LLM 输出：
# 1. 初始苹果数：23个
# 2. 卖出后：23 - 15 = 8个
# 3. 进货后：8 + 8 = 16个
# 答案：16个
```

#### 形式2：Manual CoT（手动思维链）

在示例中展示完整的推理过程：

```python
# Manual CoT（Few-shot + CoT）
prompt = """
问题：小明有 10 元，买了一支 3 元的笔，还剩多少钱？
思考过程：
- 初始金额：10元
- 花费：3元
- 剩余：10 - 3 = 7元
答案：7元

问题：小红有 20 元，买了一本 8 元的书和一支 4 元的笔，还剩多少钱？
思考过程：
- 初始金额：20元
- 书的花费：8元
- 笔的花费：4元
- 总花费：8 + 4 = 12元
- 剩余：20 - 12 = 8元
答案：8元

问题：{query}
思考过程：
"""
```

### 在 RAG 中的应用

```python
# RAG + CoT：处理需要多文档综合的复杂问题
RAG_COT_PROMPT = """
你是一个知识库问答助手。请按以下步骤回答问题：

## 参考文档
{context}

## 用户问题
{query}

## 请按以下步骤思考并回答：

### 步骤1：理解问题
这个问题在问什么？需要什么信息？

### 步骤2：定位信息
在哪些文档中找到了相关信息？

### 步骤3：综合分析
这些信息如何组合起来回答问题？

### 步骤4：最终答案
基于以上分析，答案是：
"""
```

### CoT 最佳实践

| 实践 | 说明 |
|------|------|
| **复杂问题才用** | 简单问题用 CoT 反而浪费 Token |
| **步骤要清晰** | 每个步骤解决一个子问题 |
| **可以组合 Few-shot** | Manual CoT = Few-shot + CoT |
| **注意 Token 消耗** | 推理过程会增加输出长度 |

---

## 核心概念3：结构化输出（Structured Output）

**结构化输出是强制 LLM 按照特定格式（如 JSON、XML）输出的技术。**

### 为什么需要结构化输出？

在实际应用中，LLM 的输出通常需要被程序解析：

```python
# ❌ 非结构化输出（难以解析）
output = "这个产品的评价是正面的，用户很满意，评分大概是4.5分左右。"

# ✅ 结构化输出（易于解析）
output = '{"sentiment": "positive", "score": 4.5, "summary": "用户很满意"}'
result = json.loads(output)  # 直接解析使用
```

### 实现结构化输出的方法

#### 方法1：Prompt 约束

```python
# 在 Prompt 中明确要求 JSON 格式
prompt = """
分析以下评论的情感，输出 JSON 格式：

评论：{review}

请严格按以下 JSON 格式输出，不要输出其他内容：
{
    "sentiment": "positive/negative/neutral",
    "confidence": 0.0到1.0之间的数字,
    "keywords": ["关键词1", "关键词2"]
}
"""
```

#### 方法2：JSON Mode（API 级别）

```python
from openai import OpenAI

client = OpenAI()

# 使用 response_format 参数强制 JSON 输出
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "你是一个情感分析助手，只输出 JSON。"},
        {"role": "user", "content": f"分析这条评论的情感：{review}"}
    ],
    response_format={"type": "json_object"}  # 强制 JSON 输出
)

result = json.loads(response.choices[0].message.content)
```

#### 方法3：Function Calling / Tool Use

```python
# 定义输出结构
tools = [
    {
        "type": "function",
        "function": {
            "name": "analyze_sentiment",
            "description": "分析文本情感",
            "parameters": {
                "type": "object",
                "properties": {
                    "sentiment": {
                        "type": "string",
                        "enum": ["positive", "negative", "neutral"]
                    },
                    "confidence": {
                        "type": "number",
                        "minimum": 0,
                        "maximum": 1
                    },
                    "keywords": {
                        "type": "array",
                        "items": {"type": "string"}
                    }
                },
                "required": ["sentiment", "confidence"]
            }
        }
    }
]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": f"分析：{review}"}],
    tools=tools,
    tool_choice={"type": "function", "function": {"name": "analyze_sentiment"}}
)
```

### 在 RAG 中的应用

```python
# RAG 结构化输出示例
RAG_STRUCTURED_PROMPT = """
你是一个知识库问答助手。

## 参考文档
{context}

## 用户问题
{query}

## 输出要求
请严格按以下 JSON 格式输出：
{{
    "answer": "直接回答用户问题",
    "confidence": "high/medium/low",
    "sources": ["引用的文档来源"],
    "related_topics": ["相关主题建议"]
}}

只输出 JSON，不要输出其他内容。
"""


def parse_rag_response(response: str) -> dict:
    """解析 RAG 结构化输出"""
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        # 尝试提取 JSON 部分
        import re
        match = re.search(r'\{.*\}', response, re.DOTALL)
        if match:
            return json.loads(match.group())
        raise ValueError("无法解析响应")
```

### 结构化输出最佳实践

| 实践 | 说明 |
|------|------|
| **提供 Schema 示例** | 在 Prompt 中展示完整的 JSON 结构 |
| **使用 API 级别约束** | JSON Mode 比 Prompt 约束更可靠 |
| **添加解析容错** | 即使有约束，也要处理解析失败的情况 |
| **字段类型明确** | 说明每个字段的类型和取值范围 |

---

## 三个概念的组合使用

在实际 RAG 系统中，这三个技巧通常组合使用：

```python
# 完整示例：Few-shot + CoT + 结构化输出
RAG_ADVANCED_PROMPT = """
你是一个专业的知识库问答助手。

## 回答示例（Few-shot）

问题：产品保修期是多久？
文档：保修政策第3条：本产品保修期为1年，自购买之日起计算。

思考过程（CoT）：
1. 问题询问保修期时长
2. 在文档中找到"保修期为1年"
3. 信息来源明确，可信度高

输出（结构化）：
{{"answer": "产品保修期为1年，自购买之日起计算", "confidence": "high", "source": "保修政策第3条"}}

---

## 现在请回答

问题：{query}
文档：{context}

思考过程：

输出：
"""
```

---

## 三个概念对比总结

| 概念 | 核心作用 | 适用场景 | Token 消耗 |
|------|---------|---------|-----------|
| **Few-shot** | 统一输出格式和风格 | 格式要求严格、风格要求一致 | 中（示例占用） |
| **CoT** | 提高推理准确率 | 复杂问题、多步推理 | 高（推理过程） |
| **结构化输出** | 输出可被程序解析 | 需要后续处理的场景 | 低 |

---

**下一步：** [04_最小可用](./04_最小可用.md) - 掌握20%核心知识解决80%问题
