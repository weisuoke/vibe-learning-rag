# 实战代码 - 场景4：RAG多模态检索

> 在文档问答系统中应用多向量检索

---

## 场景描述

**目标：** 构建一个完整的 RAG 文档问答系统，支持文本和图片的多模态检索

**应用场景：**
- 技术文档问答（文字说明 + 代码截图 + 架构图）
- 产品手册检索（文字描述 + 产品图片）
- 教程系统（文字教程 + 示意图）

---

## 完整代码

```python
"""
RAG 多模态检索 - 完整示例
演示：构建支持文本和图片的文档问答系统
"""

import os
from pymilvus import connections, Collection, AnnSearchRequest, RRFRanker
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# ===== 1. 初始化 =====
print("=== RAG 多模态检索系统 ===\n")

connections.connect(
    alias="default",
    host=os.getenv("MILVUS_HOST", "localhost"),
    port=os.getenv("MILVUS_PORT", "19530")
)

collection = Collection("doc_qa_multi_vector")
collection.load()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

print("✅ 系统初始化完成")
print(f"   知识库文档数：{collection.num_entities}")

# ===== 2. RAG 检索函数 =====

def get_text_embedding(text):
    """生成文本向量"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

def rag_multimodal_search(user_query, top_k=3, use_image=False):
    """
    RAG 多模态检索

    Args:
        user_query: 用户问题
        top_k: 返回文档数量
        use_image: 是否使用图片向量检索

    Returns:
        检索结果列表
    """
    print(f"\n查询：{user_query}")
    print(f"检索模式：{'文本+图片' if use_image else '仅文本'}")

    # 生成查询向量
    text_embedding = get_text_embedding(user_query)

    if use_image:
        # 多模态检索（文本 + 图片）
        text_req = AnnSearchRequest(
            data=[text_embedding],
            anns_field="text_vector",
            param={"metric_type": "COSINE", "params": {"ef": 100}},
            limit=top_k * 3  # 召回更多候选
        )

        # 图片向量也使用文本向量（因为图片有文字描述）
        image_req = AnnSearchRequest(
            data=[text_embedding],
            anns_field="image_vector",
            param={"metric_type": "COSINE", "params": {"ef": 100}},
            limit=top_k * 2
        )

        # 混合检索
        results = collection.hybrid_search(
            reqs=[text_req, image_req],
            rerank=RRFRanker(),
            limit=top_k,
            output_fields=["title", "content", "image_url", "category"]
        )
    else:
        # 仅文本检索
        results = collection.search(
            data=[text_embedding],
            anns_field="text_vector",
            param={"metric_type": "COSINE", "params": {"ef": 100}},
            limit=top_k,
            output_fields=["title", "content", "image_url", "category"]
        )

    return results[0]

# ===== 3. 构建 RAG 上下文 =====

def build_rag_context(search_results):
    """构建 RAG 上下文"""
    context_parts = []

    for i, hit in enumerate(search_results, 1):
        title = hit.entity.get("title")
        content = hit.entity.get("content")
        image_url = hit.entity.get("image_url")
        category = hit.entity.get("category")

        # 添加文档信息
        context_parts.append(f"【文档{i}】{title}")
        context_parts.append(f"分类：{category}")
        context_parts.append(f"内容：{content}")

        # 如果有图片，添加图片信息
        if image_url:
            context_parts.append(f"相关图片：{image_url}")

        context_parts.append("")  # 空行分隔

    return "\n".join(context_parts)

# ===== 4. 调用 LLM 生成答案 =====

def generate_answer(user_query, context):
    """调用 LLM 生成答案"""
    system_prompt = """你是一个技术文档助手。
根据提供的文档内容回答用户问题。

要求：
1. 只基于提供的文档内容回答
2. 如果文档中没有相关信息，明确告知用户
3. 回答要准确、简洁、易懂
4. 如果文档中有图片链接，可以提及"参考图片"
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"问题：{user_query}\n\n参考文档：\n{context}"}
        ],
        temperature=0.3
    )

    return response.choices[0].message.content

# ===== 5. 完整的 RAG 流程 =====

def rag_qa(user_query, use_multimodal=True, top_k=3, verbose=True):
    """
    完整的 RAG 问答流程

    Args:
        user_query: 用户问题
        use_multimodal: 是否使用多模态检索
        top_k: 检索文档数量
        verbose: 是否打印详细信息

    Returns:
        LLM 生成的答案
    """
    # 1. 检索相关文档
    if verbose:
        print("\n" + "="*60)
        print("步骤1：检索相关文档")
        print("="*60)

    search_results = rag_multimodal_search(
        user_query,
        top_k=top_k,
        use_image=use_multimodal
    )

    if verbose:
        print(f"\n检索到 {len(search_results)} 个相关文档：")
        for i, hit in enumerate(search_results, 1):
            print(f"  {i}. {hit.entity.get('title')}")

    # 2. 构建上下文
    if verbose:
        print("\n" + "="*60)
        print("步骤2：构建 RAG 上下文")
        print("="*60)

    context = build_rag_context(search_results)

    if verbose:
        print(f"\n上下文长度：{len(context)} 字符")
        print(f"上下文预览：\n{context[:200]}...\n")

    # 3. 生成答案
    if verbose:
        print("="*60)
        print("步骤3：调用 LLM 生成答案")
        print("="*60)

    answer = generate_answer(user_query, context)

    if verbose:
        print(f"\n答案：\n{answer}\n")

    return answer

# ===== 6. 测试案例 =====

print("\n" + "="*60)
print("测试案例")
print("="*60)

# 测试1：仅文本检索
print("\n【测试1】仅文本检索")
answer1 = rag_qa(
    user_query="如何安装 Python？",
    use_multimodal=False,
    top_k=2,
    verbose=True
)

# 测试2：多模态检索
print("\n【测试2】多模态检索（文本+图片）")
answer2 = rag_qa(
    user_query="如何安装 Python？",
    use_multimodal=True,
    top_k=2,
    verbose=True
)

# ===== 7. 对比不同检索策略 =====

print("\n" + "="*60)
print("对比不同检索策略")
print("="*60)

test_queries = [
    "如何安装 Python？",
    "Java 有什么特点？",
    "Docker 如何使用？",
]

print("\n对比：仅文本 vs 多模态")
for query in test_queries:
    print(f"\n查询：{query}")

    # 仅文本
    results_text = rag_multimodal_search(query, top_k=3, use_image=False)
    print(f"  仅文本 Top 1: {results_text[0].entity.get('title')}")

    # 多模态
    results_multi = rag_multimodal_search(query, top_k=3, use_image=True)
    print(f"  多模态 Top 1: {results_multi[0].entity.get('title')}")

# ===== 8. 批量问答 =====

print("\n" + "="*60)
print("批量问答")
print("="*60)

qa_pairs = [
    "Python 的安装步骤是什么？",
    "Java 和 Python 有什么区别？",
    "如何使用 Docker 部署应用？",
]

print("\n批量处理多个问题：")
for i, question in enumerate(qa_pairs, 1):
    print(f"\n问题{i}：{question}")
    answer = rag_qa(question, use_multimodal=True, top_k=2, verbose=False)
    print(f"答案：{answer[:100]}...")

# ===== 9. 带过滤的检索 =====

print("\n" + "="*60)
print("带过滤的检索")
print("="*60)

def rag_qa_with_filter(user_query, category_filter=None, top_k=3):
    """带分类过滤的 RAG 问答"""
    print(f"\n查询：{user_query}")
    if category_filter:
        print(f"过滤条件：category == '{category_filter}'")

    # 生成查询向量
    text_embedding = get_text_embedding(user_query)

    # 定义检索请求
    text_req = AnnSearchRequest(
        data=[text_embedding],
        anns_field="text_vector",
        param={"metric_type": "COSINE", "params": {"ef": 100}},
        limit=top_k * 2
    )

    image_req = AnnSearchRequest(
        data=[text_embedding],
        anns_field="image_vector",
        param={"metric_type": "COSINE", "params": {"ef": 100}},
        limit=top_k
    )

    # 混合检索 + 过滤
    expr = f'category == "{category_filter}"' if category_filter else None

    results = collection.hybrid_search(
        reqs=[text_req, image_req],
        rerank=RRFRanker(),
        limit=top_k,
        expr=expr,
        output_fields=["title", "content", "category"]
    )

    # 构建上下文并生成答案
    context = build_rag_context(results[0])
    answer = generate_answer(user_query, context)

    print(f"\n检索到的文档：")
    for i, hit in enumerate(results[0], 1):
        print(f"  {i}. {hit.entity.get('title')} ({hit.entity.get('category')})")

    print(f"\n答案：{answer[:150]}...")

    return answer

# 测试带过滤的检索
print("\n测试1：只检索「教程」分类")
rag_qa_with_filter("如何学习编程？", category_filter="教程", top_k=2)

print("\n测试2：只检索「运维」分类")
rag_qa_with_filter("如何部署应用？", category_filter="运维", top_k=2)

# ===== 10. 性能统计 =====

print("\n" + "="*60)
print("性能统计")
print("="*60)

import time

def benchmark_rag(num_queries=10):
    """性能测试"""
    test_query = "如何安装 Python？"

    # 测试仅文本检索
    times_text = []
    for _ in range(num_queries):
        start = time.time()
        rag_multimodal_search(test_query, top_k=3, use_image=False)
        times_text.append((time.time() - start) * 1000)

    # 测试多模态检索
    times_multi = []
    for _ in range(num_queries):
        start = time.time()
        rag_multimodal_search(test_query, top_k=3, use_image=True)
        times_multi.append((time.time() - start) * 1000)

    print(f"\n性能测试结果（{num_queries}次查询）：")
    print(f"\n仅文本检索：")
    print(f"  平均延迟：{sum(times_text)/len(times_text):.2f}ms")
    print(f"  最小延迟：{min(times_text):.2f}ms")
    print(f"  最大延迟：{max(times_text):.2f}ms")

    print(f"\n多模态检索：")
    print(f"  平均延迟：{sum(times_multi)/len(times_multi):.2f}ms")
    print(f"  最小延迟：{min(times_multi):.2f}ms")
    print(f"  最大延迟：{max(times_multi):.2f}ms")

    print(f"\n性能差异：{((sum(times_multi)/len(times_multi)) / (sum(times_text)/len(times_text)) - 1) * 100:.1f}%")

benchmark_rag(num_queries=10)

print("\n" + "="*60)
print("✅ RAG 多模态检索完成！")
print("="*60)
```

---

## 系统架构

```
用户问题
   ↓
生成查询向量（OpenAI Embedding）
   ↓
多模态检索（Milvus）
   ├─ 文本向量检索
   └─ 图片向量检索
   ↓
RRF 融合
   ↓
构建上下文
   ↓
调用 LLM（GPT-4）
   ↓
生成答案
```

---

## 关键优化

### 1. 召回数量优化

```python
# 多模态检索时，文本召回更多候选
text_req = AnnSearchRequest(
    data=[text_embedding],
    anns_field="text_vector",
    limit=top_k * 3  # 文本召回3倍
)

image_req = AnnSearchRequest(
    data=[text_embedding],
    anns_field="image_vector",
    limit=top_k * 2  # 图片召回2倍
)

# 最终返回 top_k 个
results = collection.hybrid_search(
    reqs=[text_req, image_req],
    rerank=RRFRanker(),
    limit=top_k
)
```

### 2. 上下文构建优化

```python
def build_rag_context(search_results):
    """构建结构化上下文"""
    context_parts = []

    for i, hit in enumerate(search_results, 1):
        # 结构化格式
        context_parts.append(f"【文档{i}】{hit.entity.get('title')}")
        context_parts.append(f"分类：{hit.entity.get('category')}")
        context_parts.append(f"内容：{hit.entity.get('content')}")

        # 图片信息
        if hit.entity.get('image_url'):
            context_parts.append(f"相关图片：{hit.entity.get('image_url')}")

        context_parts.append("")  # 分隔

    return "\n".join(context_parts)
```

### 3. LLM Prompt 优化

```python
system_prompt = """你是一个技术文档助手。
根据提供的文档内容回答用户问题。

要求：
1. 只基于提供的文档内容回答
2. 如果文档中没有相关信息，明确告知用户
3. 回答要准确、简洁、易懂
4. 如果文档中有图片链接，可以提及"参考图片"
"""
```

---

## 实际应用场景

### 场景1：技术文档问答

```python
# 用户问题
user_query = "如何配置 Kubernetes 集群？"

# RAG 检索
results = rag_multimodal_search(user_query, top_k=3, use_image=True)

# 检索结果包含：
# - 文字说明：配置步骤、命令行
# - 架构图：集群拓扑图
# - 配置文件截图：YAML 配置示例

# LLM 生成答案
answer = generate_answer(user_query, context)
# 答案会综合文字说明和图片信息
```

### 场景2：产品手册检索

```python
# 用户问题
user_query = "这个按钮在哪里？"

# 多模态检索
# - 文本向量：匹配按钮的文字描述
# - 图片向量：匹配按钮的视觉特征

# 返回结果：
# - 文字说明：按钮位置、功能
# - 产品截图：标注按钮位置
```

### 场景3：教程系统

```python
# 用户问题
user_query = "如何使用这个功能？"

# 多模态检索
# - 文本向量：匹配教程文字
# - 图片向量：匹配示意图、流程图

# 返回结果：
# - 文字教程：步骤说明
# - 示意图：可视化流程
```

---

## 性能优化建议

### 1. 缓存查询向量

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_cached_embedding(text):
    return get_text_embedding(text)
```

### 2. 批量检索

```python
def batch_rag_qa(queries, use_multimodal=True):
    """批量处理多个问题"""
    # 批量生成向量
    embeddings = [get_text_embedding(q) for q in queries]

    # 批量检索
    results_list = []
    for embedding in embeddings:
        results = rag_multimodal_search_with_embedding(
            embedding,
            use_image=use_multimodal
        )
        results_list.append(results)

    return results_list
```

### 3. 异步处理

```python
import asyncio

async def async_rag_qa(user_query):
    """异步 RAG 问答"""
    # 异步检索
    results = await async_search(user_query)

    # 异步生成答案
    answer = await async_generate(user_query, results)

    return answer
```

---

## 总结

**RAG 多模态检索的关键要点：**

1. **多模态检索**：同时检索文本和图片向量
2. **RRF 融合**：融合多个检索结果
3. **上下文构建**：结构化组织检索结果
4. **LLM 生成**：基于上下文生成答案
5. **性能优化**：缓存、批量、异步

**实际应用价值：**
- 提升检索准确率（融合多个维度）
- 支持多模态内容（文本 + 图片）
- 增强用户体验（更精准的答案）

---

**继续学习：** [13_面试必问.md](./13_面试必问.md)
