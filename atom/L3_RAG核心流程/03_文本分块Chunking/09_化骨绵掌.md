# 化骨绵掌

10个2分钟知识卡片，帮你彻底掌握文本分块。

---

## 卡片1：什么是文本分块？

**一句话：** 文本分块就是把长文档切成小片段，让每个片段都能被单独检索和理解。

**举例：**
```
一本500页的书 → 切成1000个小段落
每个段落约250字，可以独立表达一个知识点
```

**类比：** 就像把一整块披萨切成8块，每块都能单独拿起来吃。

**应用：** RAG 系统检索的最小单位就是 Chunk，分块质量直接决定检索效果。

---

## 卡片2：为什么必须分块？

**一句话：** 三个技术约束决定了必须分块：Embedding 限制、Context Window 限制、检索精度需求。

**举例：**
```
约束1：Embedding 模型最多处理 8K tokens
约束2：LLM 上下文窗口有限且按 token 计费
约束3：整本书作为一个向量无法精准匹配问题
```

**类比：** 就像你不能把整个图书馆的书都搬到桌上找答案，只能先定位到相关的几本。

**应用：** 理解这三个约束，就能理解为什么分块参数要这样设置。

---

## 卡片3：chunk_size 怎么选？

**一句话：** chunk_size 是精度和完整性的权衡，通常 300-500 字符适合问答，1000-2000 适合摘要。

**举例：**
```python
# 问答场景：小块更精准
qa_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

# 摘要场景：大块更完整
summary_splitter = RecursiveCharacterTextSplitter(chunk_size=1500)
```

**类比：** 就像切菜，做沙拉要切小块，做炖菜可以切大块。

**应用：** 先用 500 作为起点，根据评估结果调整。

---

## 卡片4：chunk_overlap 的作用

**一句话：** overlap 是相邻块的重叠部分，防止重要信息被切断，通常设为 chunk_size 的 10-20%。

**举例：**
```
没有重叠：
[块1: "Python的GIL是"] [块2: "一个互斥锁"]
→ 信息被切断

有重叠：
[块1: "Python的GIL是一个互斥锁"]
    [块2: "GIL是一个互斥锁，它防止..."]
→ 边界信息在两个块中都有
```

**类比：** 就像接力赛的交接区，确保交接顺畅不掉棒。

**应用：** chunk_size=500 时，overlap=50 是个好起点。

---

## 卡片5：固定大小分块

**一句话：** 最简单的分块方式，按固定字符数切分，不考虑语义边界。

**举例：**
```python
def fixed_chunk(text, size=500, overlap=50):
    chunks = []
    for i in range(0, len(text), size - overlap):
        chunks.append(text[i:i+size])
    return chunks
```

**优点：** 实现简单，块大小可控
**缺点：** 可能切断句子和段落

**应用：** 适合快速原型，不适合生产环境。

---

## 卡片6：递归字符分块

**一句话：** 按分隔符优先级递归切分，优先在段落、句子等自然边界切分，是最推荐的通用策略。

**举例：**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", "。", ".", " ", ""]
)
```

**工作原理：**
1. 先尝试按 `\n\n`（段落）切分
2. 段落太长就按 `\n`（换行）切分
3. 还是太长就按句号切分
4. 最后按字符切分

**应用：** 90% 的场景用这个就够了。

---

## 卡片7：语义分块

**一句话：** 基于语义相似度切分，在语义变化的地方切分，保证每个块主题一致。

**举例：**
```python
from langchain_experimental.text_splitter import SemanticChunker

splitter = SemanticChunker(
    embeddings=OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile",
)
```

**工作原理：**
1. 先按句子切分
2. 计算相邻句子的语义相似度
3. 在相似度低的地方切分

**应用：** 适合主题多变的文档，但成本高、速度慢。

---

## 卡片8：Markdown 标题分块

**一句话：** 按 Markdown 标题层级切分，每个章节作为一个块，保持文档结构。

**举例：**
```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[
        ("#", "h1"),
        ("##", "h2"),
    ]
)
```

**优点：** 保持章节完整，元数据丰富
**缺点：** 只适用于 Markdown 文档

**应用：** 技术文档、知识库文档的首选。

---

## 卡片9：分块对 RAG 效果的影响

**一句话：** 分块策略直接影响检索精度、上下文质量和系统成本，是 RAG 优化的第一站。

**影响链：**
```
分块策略
    ↓
检索粒度（块大小决定检索的最小单位）
    ↓
检索精度（块内容决定向量表示的准确性）
    ↓
上下文质量（检索结果决定送给 LLM 的内容）
    ↓
答案质量（上下文质量决定最终答案）
```

**实际数据：** 优化分块策略可以将答案准确率提升 10-20%。

**应用：** RAG 效果不好时，先检查分块策略。

---

## 卡片10：分块最佳实践

**一句话：** 选择递归分块作为起点，根据文档类型调整，通过评估迭代优化。

**实践清单：**
```
1. 默认使用 RecursiveCharacterTextSplitter
2. chunk_size=500, overlap=50 作为起点
3. 中文文档添加中文标点到分隔符
4. Markdown 文档考虑标题分块
5. 代码文档使用语言感知分块器
6. 用测试问题评估分块效果
7. 根据评估结果调整参数
```

**记住：** 没有万能配置，只有适合场景的配置。

**应用：** 把这个清单作为每次分块设计的检查表。

---

## 知识卡片总结

| 卡片 | 核心知识点 |
|------|------------|
| 1 | 分块 = 长文档切成小片段 |
| 2 | 三个约束决定必须分块 |
| 3 | chunk_size 是精度与完整性的权衡 |
| 4 | overlap 防止边界信息丢失 |
| 5 | 固定大小分块：简单但粗暴 |
| 6 | 递归字符分块：通用推荐 |
| 7 | 语义分块：高质量但高成本 |
| 8 | Markdown 分块：结构化文档首选 |
| 9 | 分块是 RAG 优化第一站 |
| 10 | 最佳实践：起点 + 评估 + 迭代 |

---

**下一步：** [10_一句话总结](./10_一句话总结.md) - 用一句话总结文本分块
