# 化骨绵掌

把 ReRank 拆成 10 个 2 分钟能看完的知识卡片，碎片化学习。

---

## 卡片1：ReRank 是什么？

**一句话：** ReRank 是对检索结果进行二次排序，让最相关的文档排在最前面。

**类比：**
```
搜索引擎返回 100 个结果
  ↓
ReRank 重新打分
  ↓
最相关的 5 个排到最前面
```

**应用：** 在 RAG 系统中，ReRank 确保送给 LLM 的上下文是最相关的，提升回答质量。

---

## 卡片2：为什么需要 ReRank？

**一句话：** 向量检索快但不够精确，ReRank 用更精确的模型弥补这个缺陷。

**问题所在：**
```
向量检索的局限：
- Query 和 Doc 分别编码，没有交互
- 向量是压缩表示，信息有损
- 语义相似 ≠ 问题相关
```

**应用：** 添加 ReRank 后，检索准确率通常能提升 5-15%。

---

## 卡片3：Bi-Encoder vs Cross-Encoder

**一句话：** Bi-Encoder 分别编码求相似度，Cross-Encoder 拼接后一起编码求相关性。

**对比：**
```
Bi-Encoder（向量检索）：
Query → [编码] → 向量A
Doc   → [编码] → 向量B
            ↓
      余弦相似度

Cross-Encoder（ReRank）：
"Query [SEP] Doc" → [编码] → 相关性分数
```

**应用：** Bi-Encoder 用于大规模召回，Cross-Encoder 用于小规模精排。

---

## 卡片4：两阶段检索架构

**一句话：** 先用 Bi-Encoder 快速召回候选，再用 Cross-Encoder 精确排序。

**流程：**
```
100万文档
    ↓ Bi-Encoder（毫秒级）
Top 100 候选
    ↓ Cross-Encoder（百毫秒级）
Top 5 精排结果
    ↓
送给 LLM 生成回答
```

**应用：** 这是 RAG 系统的标准检索架构，兼顾速度和精度。

---

## 卡片5：Cross-Encoder 为什么更准？

**一句话：** 因为 Query 和 Doc 的每个 Token 都能相互 Attention，实现深度语义交互。

**举例：**
```
Query: "Python 读取 JSON"
Doc:   "使用 json.load() 函数"

Cross-Encoder 能理解：
- "读取" ≈ "load"
- "JSON" 在两边指同一个东西
- 这个 Doc 确实在回答这个问题
```

**应用：** 这种细粒度匹配是向量检索做不到的。

---

## 卡片6：相关性分数怎么用？

**一句话：** Cross-Encoder 输出的分数表示 Query 和 Doc 的相关程度，用于排序和过滤。

**使用方式：**
```python
# 方式1：直接排序
ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)

# 方式2：设置阈值过滤
filtered = [(doc, score) for doc, score in zip(docs, scores) if score > 0.5]

# 方式3：取 Top-K
top_k = ranked[:5]
```

**应用：** 分数范围因模型而异，有的是 [-10, 10]，有的是 [0, 1]。

---

## 卡片7：候选数量怎么选？

**一句话：** 通常选 50-100 个候选，太少会漏掉相关文档，太多会增加延迟。

**权衡：**
```
Top 20:  召回率不足，可能漏掉相关文档
Top 50:  适合实时场景，延迟敏感
Top 100: 平衡选择，推荐默认值
Top 200: 精度优先，可接受较高延迟
Top 500: 边际收益递减，不推荐
```

**应用：** 经验公式 Top-N : Top-K ≈ 10:1 到 20:1。

---

## 卡片8：常用 ReRank 模型

**一句话：** 开源用 sentence-transformers 的 Cross-Encoder，商业用 Cohere Rerank API。

**模型选择：**
```
快速入门：cross-encoder/ms-marco-MiniLM-L-6-v2
更高精度：cross-encoder/ms-marco-MiniLM-L-12-v2
中文场景：BAAI/bge-reranker-base
生产环境：Cohere Rerank API
```

**应用：** 开源模型免费但需要 GPU，商业 API 按量付费但更方便。

---

## 卡片9：ReRank 的三个误区

**一句话：** 分数高≠正确、候选多≠更好、有 ReRank≠不用优化向量检索。

**误区解析：**
```
误区1：分数高 = 内容正确
现实：ReRank 只判断相关性，不判断正确性

误区2：候选越多越好
现实：边际收益递减，延迟线性增加

误区3：ReRank 能兜底
现实：ReRank 只能重排，不能召回
```

**应用：** 向量检索的质量决定了 ReRank 的上限。

---

## 卡片10：ReRank 在 RAG 中的完整流程

**一句话：** 向量检索召回 → Cross-Encoder 精排 → 构建上下文 → LLM 生成。

**代码框架：**
```python
def rag_with_rerank(query):
    # 1. 向量检索召回 Top-N
    candidates = vector_store.search(query, k=100)

    # 2. Cross-Encoder 精排
    pairs = [(query, doc) for doc in candidates]
    scores = reranker.predict(pairs)
    top_docs = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)[:5]

    # 3. 构建上下文
    context = "\n".join([doc for doc, _ in top_docs])

    # 4. LLM 生成
    return llm.generate(f"基于以下内容回答：{context}\n问题：{query}")
```

**应用：** 这是生产级 RAG 系统的标准模式。

---

## 知识卡片总结

| 卡片 | 核心知识点 |
|------|-----------|
| 1 | ReRank = 二次排序 |
| 2 | 弥补向量检索的精度不足 |
| 3 | Bi-Encoder vs Cross-Encoder |
| 4 | 两阶段检索架构 |
| 5 | Cross-Encoder 的深度交互 |
| 6 | 相关性分数的使用 |
| 7 | 候选数量选择 |
| 8 | 常用模型推荐 |
| 9 | 三个常见误区 |
| 10 | RAG 完整流程 |

---

**下一步：** [10_一句话总结](./10_一句话总结.md) - 用一句话总结 ReRank 的全部要点
