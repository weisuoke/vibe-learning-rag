# 反直觉点

上下文注入与生成中最常见的三个误区，以及为什么人们容易这样错。

---

## 误区1：上下文越多越好 ❌

### 错误观点

"检索到的文档越多，给 LLM 的信息越全面，答案就越好。"

### 为什么错？

```
┌─────────────────────────────────────────────────────────────┐
│                上下文数量 vs 答案质量                         │
│                                                             │
│  答案质量                                                    │
│     ↑                                                       │
│     │            ╭──────╮                                   │
│     │           ╱        ╲                                  │
│     │          ╱          ╲                                 │
│     │         ╱            ╲                                │
│     │        ╱              ╲                               │
│     │       ╱                ╲                              │
│     │──────╱                  ╲─────────────                │
│     └──────────────────────────────────────→ 上下文数量     │
│            最佳点                                            │
│           (3-5个)                                           │
│                                                             │
│  结论：存在一个"最佳点"，超过后质量反而下降                    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**上下文过多的三个问题：**

1. **Lost in the Middle**：中间内容被忽略
2. **噪声干扰**：无关内容干扰 LLM 判断
3. **Token 浪费**：占用输出空间，答案被截断

### 为什么人们容易这样错？

```
日常经验的误导：

考试时：资料越多，越可能找到答案 ✓
写论文：参考文献越多，越显得专业 ✓
做决策：信息越全面，决策越准确 ✓

但 LLM 不是人：
- 人可以快速跳过无关内容
- LLM 会"认真阅读"每一个 token
- 无关内容会稀释相关内容的权重
```

### 正确理解

```python
# ❌ 错误做法：检索尽可能多的文档
retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

# ✅ 正确做法：检索适量 + 质量过滤
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "k": 5,                    # 数量适中
        "score_threshold": 0.7    # 质量过滤
    }
)
```

**经验法则：**
- 简单问题：3-5 个文档
- 复杂问题：5-10 个文档
- 超过 10 个：考虑 ReRank 或 Map-Reduce

---

## 误区2：检索结果直接拼接就行 ❌

### 错误观点

"把检索到的文档直接拼在一起，LLM 自然能理解。"

### 为什么错？

```python
# ❌ 错误做法：直接拼接
context = doc1.page_content + doc2.page_content + doc3.page_content

# 问题：
# 1. LLM 不知道哪里是分隔
# 2. 无法区分不同来源
# 3. 可能产生语义混淆
```

**实际案例：**

```
直接拼接的上下文：
"员工入职满1年享有5天年假。请假需提前3天申请。病假需要医院证明。"

LLM 可能的误解：
- "5天年假需要医院证明"（错误关联）
- 不知道哪句话来自哪个文档
```

### 为什么人们容易这样错？

```
编程习惯的误导：

字符串拼接是最简单的操作：
result = str1 + str2 + str3

看起来没问题，代码能跑，输出也有内容。

但忽略了：
- LLM 需要结构化信息
- 来源追溯需要标记
- 语义边界需要明确
```

### 正确理解

```python
# ✅ 正确做法：结构化格式化
def format_docs(docs):
    formatted = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get("source", "未知")
        formatted.append(f"""
【文档{i}】来源：{source}
{doc.page_content}
""")
    return "\n---\n".join(formatted)

# 输出示例：
# 【文档1】来源：员工手册.pdf
# 员工入职满1年享有5天年假。
# ---
# 【文档2】来源：请假制度.docx
# 请假需提前3天申请。
```

**格式化的好处：**
- LLM 能区分不同文档
- 答案可以引用具体来源
- 减少语义混淆

---

## 误区3：LLM 会自动忽略无关内容 ❌

### 错误观点

"LLM 很聪明，即使上下文里有无关内容，它也会自动忽略，只用相关的部分。"

### 为什么错？

```
┌─────────────────────────────────────────────────────────────┐
│                LLM 对无关内容的处理                          │
│                                                             │
│  你以为的 LLM：                                              │
│  "这段不相关，跳过... 这段有用，记住..."                      │
│                                                             │
│  实际的 LLM：                                                │
│  "每个 token 都要计算注意力权重..."                          │
│  "无关内容也会影响最终输出..."                               │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**实验证据：**

```
问题："公司的年假政策是什么？"

场景A：只有相关上下文
- 上下文："员工入职满1年享有5天年假..."
- 答案准确率：95%

场景B：混入无关上下文
- 上下文："员工入职满1年享有5天年假... [大量无关的财务报表数据]"
- 答案准确率：78%

场景C：无关内容占主导
- 上下文："[大量无关内容]... 员工入职满1年享有5天年假..."
- 答案准确率：52%
```

### 为什么人们容易这样错？

```
对 LLM "智能"的过度信任：

人类阅读时：
- 快速扫描，跳过无关段落
- 聚焦关键词，忽略噪声
- 有明确的"相关性判断"

LLM 处理时：
- 每个 token 都参与计算
- 注意力是"软"分配，不是"硬"过滤
- 无关内容会"稀释"相关内容的权重

类比：
- 人类：在嘈杂餐厅里能专注听对面人说话
- LLM：把所有声音都录下来，混在一起分析
```

### 正确理解

```python
# ❌ 错误做法：不过滤，全部塞进去
context = "\n".join([doc.page_content for doc in all_docs])

# ✅ 正确做法：严格过滤 + 相关性阈值
def filter_relevant_docs(docs, threshold=0.7):
    """只保留相关性高的文档"""
    return [
        doc for doc in docs
        if doc.metadata.get("score", 0) >= threshold
    ]

# ✅ 更好的做法：使用 ReRank 二次筛选
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

compressor = CohereRerank(top_n=3)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
```

**关键原则：**
- 宁缺毋滥：少而精 > 多而杂
- 主动过滤：不要依赖 LLM 自己过滤
- 质量优先：相关性阈值 + ReRank

---

## 三个误区的共同根源

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│  根源：把 LLM 当成"超级人类"                                 │
│                                                             │
│  误区1：人类信息越多越好 → LLM 也是                          │
│  误区2：人类能自动理解结构 → LLM 也能                        │
│  误区3：人类能忽略噪声 → LLM 也能                            │
│                                                             │
│  正确认知：                                                  │
│  LLM 是"统计模式匹配器"，不是"理解者"                        │
│  - 它处理每个 token，不会"跳过"                              │
│  - 它需要明确的结构，不会"猜测"                              │
│  - 它会被噪声干扰，不会"过滤"                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 正确做法速查表

| 误区 | 错误做法 | 正确做法 |
|------|----------|----------|
| 上下文越多越好 | `k=20` | `k=5` + 质量过滤 |
| 直接拼接 | `str1 + str2` | 结构化格式 + 来源标注 |
| LLM 自动过滤 | 不过滤 | 相关性阈值 + ReRank |

---

**下一步：** [07_实战代码](./07_实战代码.md) - 完整可运行的 RAG 生成示例
