# 实战代码 03：性能基准测试

> 完整的性能测试工具和基准测试代码

---

## 概述

本文提供完整的性能基准测试工具，包括延迟测试、吞吐量测试、流式测试等。

---

## 1. 延迟基准测试

```python
"""
延迟基准测试工具
"""
import time
import statistics
from typing import List, Dict
from langchain_core.runnables import Runnable

def benchmark_latency(
    chain: Runnable,
    inputs: List[dict],
    warmup: int = 3
) -> Dict[str, float]:
    """
    延迟基准测试

    Args:
        chain: 要测试的链
        inputs: 测试输入列表
        warmup: 预热次数

    Returns:
        性能指标字典
    """
    # 预热
    for i in range(min(warmup, len(inputs))):
        chain.invoke(inputs[i])

    # 测试
    latencies = []
    for inp in inputs:
        start = time.time()
        chain.invoke(inp)
        latency = time.time() - start
        latencies.append(latency)

    # 计算指标
    return {
        "p50": statistics.median(latencies),
        "p95": statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies),
        "p99": statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else max(latencies),
        "mean": statistics.mean(latencies),
        "min": min(latencies),
        "max": max(latencies),
        "std": statistics.stdev(latencies) if len(latencies) > 1 else 0
    }

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from dotenv import load_dotenv

    load_dotenv()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")
    chain = prompt | llm | StrOutputParser()

    test_inputs = [{"text": f"测试文本{i}"} for i in range(20)]

    metrics = benchmark_latency(chain, test_inputs)
    print("=== 延迟基准测试结果 ===")
    print(f"P50: {metrics['p50']:.3f}s")
    print(f"P95: {metrics['p95']:.3f}s")
    print(f"P99: {metrics['p99']:.3f}s")
    print(f"平均: {metrics['mean']:.3f}s")
    print(f"最小: {metrics['min']:.3f}s")
    print(f"最大: {metrics['max']:.3f}s")
```

---

## 2. 吞吐量基准测试

```python
"""
吞吐量基准测试工具
"""
import time
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict
from langchain_core.runnables import Runnable

def benchmark_throughput(
    chain: Runnable,
    inputs: List[dict],
    concurrency: int = 10
) -> Dict[str, float]:
    """
    吞吐量基准测试

    Args:
        chain: 要测试的链
        inputs: 测试输入列表
        concurrency: 并发数

    Returns:
        性能指标字典
    """
    def process_one(inp):
        try:
            start = time.time()
            chain.invoke(inp)
            return time.time() - start, None
        except Exception as e:
            return None, str(e)

    start_time = time.time()

    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        results = list(executor.map(process_one, inputs))

    total_time = time.time() - start_time

    latencies = [r[0] for r in results if r[0] is not None]
    errors = [r[1] for r in results if r[1] is not None]

    return {
        "total_requests": len(inputs),
        "successful_requests": len(latencies),
        "failed_requests": len(errors),
        "total_time": total_time,
        "throughput": len(latencies) / total_time,
        "avg_latency": sum(latencies) / len(latencies) if latencies else 0,
        "error_rate": len(errors) / len(inputs)
    }

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from dotenv import load_dotenv

    load_dotenv()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")
    chain = prompt | llm | StrOutputParser()

    test_inputs = [{"text": f"测试文本{i}"} for i in range(100)]

    metrics = benchmark_throughput(chain, test_inputs, concurrency=10)
    print("=== 吞吐量基准测试结果 ===")
    print(f"总请求数: {metrics['total_requests']}")
    print(f"成功请求数: {metrics['successful_requests']}")
    print(f"失败请求数: {metrics['failed_requests']}")
    print(f"总耗时: {metrics['total_time']:.2f}s")
    print(f"吞吐量: {metrics['throughput']:.2f} req/s")
    print(f"平均延迟: {metrics['avg_latency']:.3f}s")
    print(f"错误率: {metrics['error_rate']:.2%}")
```

---

## 3. 流式性能测试

```python
"""
流式性能测试工具
"""
import time
from typing import List, Dict
from langchain_core.runnables import Runnable

def benchmark_streaming(
    chain: Runnable,
    inputs: List[dict]
) -> Dict[str, float]:
    """
    流式性能测试

    Args:
        chain: 要测试的链
        inputs: 测试输入列表

    Returns:
        性能指标字典
    """
    first_token_times = []
    total_times = []
    chunk_counts = []

    for inp in inputs:
        start = time.time()
        first_token_time = None
        chunk_count = 0

        for chunk in chain.stream(inp):
            if first_token_time is None:
                first_token_time = time.time() - start
            chunk_count += 1

        total_time = time.time() - start

        first_token_times.append(first_token_time)
        total_times.append(total_time)
        chunk_counts.append(chunk_count)

    return {
        "avg_first_token_time": sum(first_token_times) / len(first_token_times),
        "avg_total_time": sum(total_times) / len(total_times),
        "avg_chunks": sum(chunk_counts) / len(chunk_counts),
        "min_first_token_time": min(first_token_times),
        "max_first_token_time": max(first_token_times)
    }

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from dotenv import load_dotenv

    load_dotenv()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("写一篇100字的文章：{topic}")
    chain = prompt | llm | StrOutputParser()

    test_inputs = [{"topic": f"主题{i}"} for i in range(10)]

    metrics = benchmark_streaming(chain, test_inputs)
    print("=== 流式性能测试结果 ===")
    print(f"平均首token时间: {metrics['avg_first_token_time']:.3f}s")
    print(f"平均总时间: {metrics['avg_total_time']:.3f}s")
    print(f"平均chunk数: {metrics['avg_chunks']:.1f}")
    print(f"最小首token时间: {metrics['min_first_token_time']:.3f}s")
    print(f"最大首token时间: {metrics['max_first_token_time']:.3f}s")
```

---

## 4. 对比测试工具

```python
"""
LCEL vs 传统 Chain 对比测试
"""
import time
from typing import Dict
from langchain_core.runnables import Runnable

def compare_performance(
    old_chain,
    new_chain: Runnable,
    test_inputs: list,
    old_invoke_method: str = "run"
) -> Dict[str, any]:
    """
    对比测试工具

    Args:
        old_chain: 传统 Chain
        new_chain: LCEL 链
        test_inputs: 测试输入
        old_invoke_method: 旧链的调用方法名

    Returns:
        对比结果
    """
    # 测试旧链
    old_times = []
    for inp in test_inputs:
        start = time.time()
        if old_invoke_method == "run":
            old_chain.run(**inp)
        else:
            old_chain(inp)
        old_times.append(time.time() - start)

    # 测试新链
    new_times = []
    for inp in test_inputs:
        start = time.time()
        new_chain.invoke(inp)
        new_times.append(time.time() - start)

    old_avg = sum(old_times) / len(old_times)
    new_avg = sum(new_times) / len(new_times)

    return {
        "old_avg_time": old_avg,
        "new_avg_time": new_avg,
        "speedup": old_avg / new_avg,
        "improvement_pct": (old_avg - new_avg) / old_avg * 100
    }

# 使用示例
if __name__ == "__main__":
    from langchain.chains import LLMChain
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from dotenv import load_dotenv

    load_dotenv()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")

    # 传统 Chain
    old_chain = LLMChain(llm=llm, prompt=prompt)

    # LCEL
    new_chain = prompt | llm | StrOutputParser()

    test_inputs = [{"text": f"测试{i}"} for i in range(10)]

    results = compare_performance(old_chain, new_chain, test_inputs)
    print("=== 性能对比结果 ===")
    print(f"传统 Chain 平均时间: {results['old_avg_time']:.3f}s")
    print(f"LCEL 平均时间: {results['new_avg_time']:.3f}s")
    print(f"加速比: {results['speedup']:.2f}x")
    print(f"性能提升: {results['improvement_pct']:.1f}%")
```

---

## 5. 完整测试套件

```python
"""
完整性能测试套件
"""
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict
from langchain_core.runnables import Runnable
from dotenv import load_dotenv

load_dotenv()

class PerformanceBenchmark:
    """性能基准测试套件"""

    def __init__(self, chain: Runnable):
        self.chain = chain

    def run_all_tests(self, test_inputs: List[dict]) -> Dict:
        """运行所有测试"""
        print("=" * 60)
        print("性能基准测试套件")
        print("=" * 60)

        results = {}

        # 1. 延迟测试
        print("\n1. 延迟测试")
        print("-" * 60)
        results["latency"] = self.test_latency(test_inputs[:20])
        self.print_latency_results(results["latency"])

        # 2. 吞吐量测试
        print("\n2. 吞吐量测试")
        print("-" * 60)
        results["throughput"] = self.test_throughput(test_inputs)
        self.print_throughput_results(results["throughput"])

        # 3. 流式测试
        print("\n3. 流式测试")
        print("-" * 60)
        results["streaming"] = self.test_streaming(test_inputs[:10])
        self.print_streaming_results(results["streaming"])

        print("\n" + "=" * 60)
        print("所有测试完成！")
        print("=" * 60)

        return results

    def test_latency(self, inputs: List[dict]) -> Dict:
        """延迟测试"""
        latencies = []
        for inp in inputs:
            start = time.time()
            self.chain.invoke(inp)
            latencies.append(time.time() - start)

        return {
            "p50": statistics.median(latencies),
            "p95": statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies),
            "mean": statistics.mean(latencies),
            "min": min(latencies),
            "max": max(latencies)
        }

    def test_throughput(self, inputs: List[dict], concurrency: int = 10) -> Dict:
        """吞吐量测试"""
        def process_one(inp):
            try:
                start = time.time()
                self.chain.invoke(inp)
                return time.time() - start, None
            except Exception as e:
                return None, str(e)

        start_time = time.time()
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            results = list(executor.map(process_one, inputs))
        total_time = time.time() - start_time

        latencies = [r[0] for r in results if r[0] is not None]
        errors = [r[1] for r in results if r[1] is not None]

        return {
            "total_requests": len(inputs),
            "successful": len(latencies),
            "failed": len(errors),
            "total_time": total_time,
            "throughput": len(latencies) / total_time,
            "error_rate": len(errors) / len(inputs)
        }

    def test_streaming(self, inputs: List[dict]) -> Dict:
        """流式测试"""
        first_token_times = []
        total_times = []

        for inp in inputs:
            start = time.time()
            first_token_time = None

            for chunk in self.chain.stream(inp):
                if first_token_time is None:
                    first_token_time = time.time() - start

            total_times.append(time.time() - start)
            first_token_times.append(first_token_time)

        return {
            "avg_first_token": sum(first_token_times) / len(first_token_times),
            "avg_total": sum(total_times) / len(total_times),
            "min_first_token": min(first_token_times)
        }

    def print_latency_results(self, results: Dict):
        """打印延迟测试结果"""
        print(f"P50: {results['p50']:.3f}s")
        print(f"P95: {results['p95']:.3f}s")
        print(f"平均: {results['mean']:.3f}s")
        print(f"最小: {results['min']:.3f}s")
        print(f"最大: {results['max']:.3f}s")

    def print_throughput_results(self, results: Dict):
        """打印吞吐量测试结果"""
        print(f"总请求: {results['total_requests']}")
        print(f"成功: {results['successful']}")
        print(f"失败: {results['failed']}")
        print(f"总耗时: {results['total_time']:.2f}s")
        print(f"吞吐量: {results['throughput']:.2f} req/s")
        print(f"错误率: {results['error_rate']:.2%}")

    def print_streaming_results(self, results: Dict):
        """打印流式测试结果"""
        print(f"平均首token: {results['avg_first_token']:.3f}s")
        print(f"平均总时间: {results['avg_total']:.3f}s")
        print(f"最小首token: {results['min_first_token']:.3f}s")

# 使用示例
if __name__ == "__main__":
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    prompt = ChatPromptTemplate.from_template("翻译成英文：{text}")
    chain = prompt | llm | StrOutputParser()

    test_inputs = [{"text": f"测试文本{i}"} for i in range(100)]

    benchmark = PerformanceBenchmark(chain)
    results = benchmark.run_all_tests(test_inputs)
```

---

## 总结

### 测试工具总结

| 工具 | 用途 | 关键指标 |
|------|------|----------|
| benchmark_latency | 延迟测试 | P50, P95, P99 |
| benchmark_throughput | 吞吐量测试 | req/s, 错误率 |
| benchmark_streaming | 流式测试 | 首token时间 |
| compare_performance | 对比测试 | 加速比 |

### 关键要点

1. **延迟测试**：关注 P95、P99
2. **吞吐量测试**：关注 req/s
3. **流式测试**：关注首 token 时间
4. **对比测试**：验证性能提升

---

**下一步**：阅读 `07_实战代码_04_成本优化实践.md` 学习成本优化
