# 实战代码 06: 批处理场景

> **本文目标**: 掌握批量处理的优化技巧和成本控制方法

---

## 概述

批量处理能显著提高效率、降低成本。本文提供完整的批处理实现示例和优化策略。

---

## 1. 基础批量处理

### 1.1 简单批量调用

```python
"""
示例1: 基础批量处理
演示：使用 batch() 方法批量处理请求
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

load_dotenv()

# ===== 1. 创建模型 =====
model = ChatOpenAI(model="gpt-4o-mini")

# ===== 2. 准备批量输入 =====
questions = [
    "什么是Python？",
    "什么是JavaScript？",
    "什么是Rust？"
]

inputs = [[HumanMessage(content=q)] for q in questions]

# ===== 3. 批量调用 =====
print("=== 批量处理 ===\n")

responses = model.batch(inputs)

for q, r in zip(questions, responses):
    print(f"Q: {q}")
    print(f"A: {r.content[:100]}...\n")
```

### 1.2 与模板组合

```python
"""
示例2: 批量处理与模板组合
演示：在链中使用批量处理
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

# ===== 1. 创建链 =====
template = ChatPromptTemplate.from_messages([
    ("system", "你是{role}，回答要简洁"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

# ===== 2. 准备批量输入 =====
inputs = [
    {"role": "Python专家", "question": "什么是装饰器？"},
    {"role": "JavaScript专家", "question": "什么是闭包？"},
    {"role": "Rust专家", "question": "什么是所有权？"}
]

# ===== 3. 批量调用链 =====
print("=== 链式批量处理 ===\n")

responses = chain.batch(inputs)

for inp, resp in zip(inputs, responses):
    print(f"角色: {inp['role']}")
    print(f"问题: {inp['question']}")
    print(f"回答: {resp.content}\n")
```

---

## 2. 性能对比

### 2.1 循环 vs 批量

```python
"""
示例3: 性能对比
演示：循环调用 vs 批量调用的性能差异
"""
import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

template = ChatPromptTemplate.from_messages([
    ("system", "你是助手，回答要简洁"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

# 准备测试数据
questions = [f"问题{i}" for i in range(10)]
inputs = [{"question": q} for q in questions]

print("=== 性能对比 ===\n")

# ===== 方式1: 循环调用 =====
print("--- 循环调用 ---")
start = time.time()
responses1 = []
for inp in inputs:
    response = chain.invoke(inp)
    responses1.append(response)
time1 = time.time() - start
print(f"耗时: {time1:.2f}秒")
print(f"平均: {time1/len(inputs):.2f}秒/个\n")

# ===== 方式2: 批量调用 =====
print("--- 批量调用 ---")
start = time.time()
responses2 = chain.batch(inputs)
time2 = time.time() - start
print(f"耗时: {time2:.2f}秒")
print(f"平均: {time2/len(inputs):.2f}秒/个\n")

# ===== 性能提升 =====
improvement = (time1 - time2) / time1 * 100
print(f"性能提升: {improvement:.1f}%")
```

---

## 3. 批量评估

### 3.1 模型评估

```python
"""
示例4: 批量评估模型
演示：使用批量处理评估模型性能
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from typing import List, Dict

load_dotenv()

# ===== 1. 准备测试用例 =====
test_cases = [
    {"question": "1+1=?", "expected": "2"},
    {"question": "2+2=?", "expected": "4"},
    {"question": "3+3=?", "expected": "6"},
    {"question": "Python的创始人是谁？", "expected": "Guido van Rossum"},
    {"question": "JavaScript是哪年创建的？", "expected": "1995"}
]

# ===== 2. 创建链 =====
template = ChatPromptTemplate.from_messages([
    ("system", "你是助手，回答要简洁准确"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
chain = template | model

# ===== 3. 批量评估 =====
print("=== 批量评估 ===\n")

inputs = [{"question": tc["question"]} for tc in test_cases]
responses = chain.batch(inputs)

# ===== 4. 评估结果 =====
results = []
for tc, resp in zip(test_cases, responses):
    actual = resp.content.strip()
    expected = tc["expected"]
    passed = expected.lower() in actual.lower()

    results.append({
        "question": tc["question"],
        "expected": expected,
        "actual": actual,
        "passed": passed
    })

# ===== 5. 输出报告 =====
print("评估报告:")
print("-" * 80)
for i, result in enumerate(results, 1):
    status = "✅" if result["passed"] else "❌"
    print(f"{i}. {status} {result['question']}")
    print(f"   期望: {result['expected']}")
    print(f"   实际: {result['actual'][:50]}...")
    print()

passed_count = sum(1 for r in results if r["passed"])
total_count = len(results)
print(f"通过率: {passed_count}/{total_count} ({passed_count/total_count*100:.1f}%)")
```

### 3.2 A/B 测试

```python
"""
示例5: A/B 测试
演示：批量对比不同提示词的效果
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

# ===== 1. 定义两个版本的提示词 =====
template_a = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

template_b = ChatPromptTemplate.from_messages([
    ("system", "你是专业的技术专家，回答要准确、详细"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain_a = template_a | model
chain_b = template_b | model

# ===== 2. 准备测试问题 =====
questions = [
    "什么是Python？",
    "什么是装饰器？",
    "如何优化代码？"
]

inputs = [{"question": q} for q in questions]

# ===== 3. 批量测试两个版本 =====
print("=== A/B 测试 ===\n")

responses_a = chain_a.batch(inputs)
responses_b = chain_b.batch(inputs)

# ===== 4. 对比结果 =====
for i, q in enumerate(questions):
    print(f"问题{i+1}: {q}")
    print(f"\n版本A (简单提示):")
    print(f"{responses_a[i].content[:100]}...")
    print(f"\n版本B (详细提示):")
    print(f"{responses_b[i].content[:100]}...")
    print("\n" + "="*80 + "\n")
```

---

## 4. 批量数据处理

### 4.1 批量分类

```python
"""
示例6: 批量文本分类
演示：批量处理文本分类任务
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

load_dotenv()

# ===== 1. 定义输出结构 =====
class Classification(BaseModel):
    """分类结果"""
    category: str = Field(description="类别：技术/商业/娱乐/体育")
    confidence: float = Field(description="置信度：0-1")

# ===== 2. 创建链 =====
template = ChatPromptTemplate.from_messages([
    ("system", "将文本分类为：技术/商业/娱乐/体育"),
    ("human", "{text}")
])

model = ChatOpenAI(model="gpt-4o-mini")
structured_model = model.with_structured_output(Classification)
chain = template | structured_model

# ===== 3. 准备数据 =====
texts = [
    "Python 3.12 发布了新特性",
    "股市今日大涨3%",
    "新电影票房破10亿",
    "NBA总决赛精彩对决",
    "AI技术突破性进展",
    "公司季度财报超预期",
    "音乐节门票售罄",
    "世界杯预选赛开始"
]

inputs = [{"text": t} for t in texts]

# ===== 4. 批量分类 =====
print("=== 批量分类 ===\n")

results = chain.batch(inputs)

# ===== 5. 统计结果 =====
from collections import Counter

categories = [r.category for r in results]
category_counts = Counter(categories)

print("分类结果:")
for text, result in zip(texts, results):
    print(f"文本: {text}")
    print(f"类别: {result.category} (置信度: {result.confidence:.2f})\n")

print("统计:")
for category, count in category_counts.items():
    print(f"{category}: {count}条")
```

### 4.2 批量信息提取

```python
"""
示例7: 批量信息提取
演示：从多个文本中提取结构化信息
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import Optional

load_dotenv()

# ===== 1. 定义输出结构 =====
class ContactInfo(BaseModel):
    """联系信息"""
    name: str = Field(description="姓名")
    email: Optional[str] = Field(description="邮箱", default=None)
    phone: Optional[str] = Field(description="电话", default=None)
    company: Optional[str] = Field(description="公司", default=None)

# ===== 2. 创建链 =====
template = ChatPromptTemplate.from_messages([
    ("system", "从文本中提取联系信息"),
    ("human", "{text}")
])

model = ChatOpenAI(model="gpt-4o-mini")
structured_model = model.with_structured_output(ContactInfo)
chain = template | structured_model

# ===== 3. 准备数据 =====
texts = [
    "我是张三，邮箱zhangsan@example.com",
    "李四，电话13800138000，ABC公司",
    "王五，wangwu@test.com，15900159000",
    "赵六，XYZ科技，zhaoliu@xyz.com"
]

inputs = [{"text": t} for t in texts]

# ===== 4. 批量提取 =====
print("=== 批量信息提取 ===\n")

results = chain.batch(inputs)

# ===== 5. 输出结果 =====
for text, result in zip(texts, results):
    print(f"原文: {text}")
    print(f"姓名: {result.name}")
    if result.email:
        print(f"邮箱: {result.email}")
    if result.phone:
        print(f"电话: {result.phone}")
    if result.company:
        print(f"公司: {result.company}")
    print()
```

---

## 5. 并发控制

### 5.1 限制并发数

```python
"""
示例8: 控制并发数
演示：使用 max_concurrency 限制并发
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
import time

load_dotenv()

template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

# 准备大量输入
inputs = [{"question": f"问题{i}"} for i in range(20)]

print("=== 并发控制对比 ===\n")

# ===== 测试1: 无限制 =====
print("--- 无并发限制 ---")
start = time.time()
responses1 = chain.batch(inputs)
time1 = time.time() - start
print(f"耗时: {time1:.2f}秒\n")

# ===== 测试2: 限制并发 =====
print("--- 限制并发数=5 ---")
config = RunnableConfig(max_concurrency=5)
start = time.time()
responses2 = chain.batch(inputs, config=config)
time2 = time.time() - start
print(f"耗时: {time2:.2f}秒\n")

print(f"说明: 限制并发可以避免速率限制错误")
```

---

## 6. 错误处理

### 6.1 批量错误处理

```python
"""
示例9: 批量错误处理
演示：处理批量调用中的错误
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from typing import List, Dict, Any

load_dotenv()

def safe_batch_process(chain, inputs: List[Dict]) -> List[Dict[str, Any]]:
    """安全的批量处理"""
    results = []

    try:
        # 尝试批量处理
        responses = chain.batch(inputs)

        for inp, resp in zip(inputs, responses):
            results.append({
                "input": inp,
                "output": resp.content,
                "success": True,
                "error": None
            })

    except Exception as e:
        print(f"批量处理失败: {e}")
        print("切换到逐个处理模式...\n")

        # 逐个处理
        for inp in inputs:
            try:
                resp = chain.invoke(inp)
                results.append({
                    "input": inp,
                    "output": resp.content,
                    "success": True,
                    "error": None
                })
            except Exception as e:
                results.append({
                    "input": inp,
                    "output": None,
                    "success": False,
                    "error": str(e)
                })

    return results

# ===== 使用 =====
template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

inputs = [{"question": f"问题{i}"} for i in range(5)]

print("=== 安全批量处理 ===\n")
results = safe_batch_process(chain, inputs)

# 统计
success_count = sum(1 for r in results if r["success"])
print(f"成功: {success_count}/{len(results)}")

for i, result in enumerate(results, 1):
    status = "✅" if result["success"] else "❌"
    print(f"{i}. {status} {result['input']['question']}")
    if not result["success"]:
        print(f"   错误: {result['error']}")
```

---

## 7. 成本优化

### 7.1 Token 统计

```python
"""
示例10: 批量处理的成本统计
演示：统计批量处理的 token 使用和成本
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.callbacks import get_openai_callback

load_dotenv()

template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

# 准备输入
inputs = [{"question": f"介绍一下编程语言{i}"} for i in range(10)]

print("=== 成本统计 ===\n")

# 使用回调统计
with get_openai_callback() as cb:
    responses = chain.batch(inputs)

    print(f"总请求数: {cb.successful_requests}")
    print(f"总 tokens: {cb.total_tokens}")
    print(f"  - Prompt tokens: {cb.prompt_tokens}")
    print(f"  - Completion tokens: {cb.completion_tokens}")
    print(f"总成本: ${cb.total_cost:.6f}")
    print(f"平均成本: ${cb.total_cost/len(inputs):.6f}/个")
```

### 7.2 批处理 API（2025+）

```python
"""
示例11: 使用批处理 API 降低成本
演示：2025+ 的批处理 API 功能
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

# ===== 1. 启用批处理模式 =====
model = ChatOpenAI(
    model="gpt-4o-mini",
    batch_mode=True  # 启用批处理 API（2025+）
)

template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

chain = template | model

# ===== 2. 批量处理 =====
inputs = [{"question": f"问题{i}"} for i in range(100)]

print("=== 批处理 API ===")
print("注意: 批处理 API 成本降低 50%，但延迟增加\n")

responses = chain.batch(inputs)

print(f"处理完成: {len(responses)} 个请求")
print("成本节省: 约 50%")
```

---

## 8. 完整应用示例

### 8.1 批量内容审核

```python
"""
示例12: 批量内容审核系统
演示：完整的批量处理应用
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from typing import List
import time

load_dotenv()

# ===== 1. 定义输出结构 =====
class ModerationResult(BaseModel):
    """审核结果"""
    is_safe: bool = Field(description="是否安全")
    risk_level: str = Field(description="风险等级：低/中/高")
    issues: List[str] = Field(description="发现的问题列表")
    suggestion: str = Field(description="处理建议")

# ===== 2. 创建审核链 =====
template = ChatPromptTemplate.from_messages([
    ("system", "你是内容审核专家，评估文本是否包含不当内容"),
    ("human", "审核以下内容：{content}")
])

model = ChatOpenAI(model="gpt-4o-mini")
structured_model = model.with_structured_output(ModerationResult)
chain = template | structured_model

# ===== 3. 批量审核函数 =====
class ContentModerator:
    """内容审核器"""

    def __init__(self, chain, batch_size: int = 10):
        self.chain = chain
        self.batch_size = batch_size

    def moderate_batch(self, contents: List[str]) -> List[dict]:
        """批量审核"""
        results = []
        total = len(contents)

        print(f"开始审核 {total} 条内容...")

        # 分批处理
        for i in range(0, total, self.batch_size):
            batch = contents[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (total + self.batch_size - 1) // self.batch_size

            print(f"处理批次 {batch_num}/{total_batches}...")

            inputs = [{"content": c} for c in batch]
            batch_results = self.chain.batch(inputs)

            for content, result in zip(batch, batch_results):
                results.append({
                    "content": content,
                    "is_safe": result.is_safe,
                    "risk_level": result.risk_level,
                    "issues": result.issues,
                    "suggestion": result.suggestion
                })

        return results

# ===== 4. 测试数据 =====
test_contents = [
    "这是一条正常的评论",
    "Python是很好的编程语言",
    "今天天气不错",
    "学习编程很有趣",
    "推荐大家学习AI技术"
] * 4  # 20条内容

# ===== 5. 执行审核 =====
print("=== 批量内容审核 ===\n")

moderator = ContentModerator(chain, batch_size=5)
start = time.time()
results = moderator.moderate_batch(test_contents)
duration = time.time() - start

# ===== 6. 统计报告 =====
print(f"\n审核完成，耗时 {duration:.2f}秒")
print(f"平均速度: {len(results)/duration:.1f} 条/秒\n")

safe_count = sum(1 for r in results if r["is_safe"])
print(f"安全内容: {safe_count}/{len(results)}")

risk_levels = {}
for r in results:
    level = r["risk_level"]
    risk_levels[level] = risk_levels.get(level, 0) + 1

print("\n风险等级分布:")
for level, count in risk_levels.items():
    print(f"  {level}: {count}条")

# 显示部分结果
print("\n示例结果:")
for i, result in enumerate(results[:3], 1):
    print(f"{i}. {result['content'][:30]}...")
    print(f"   安全: {'是' if result['is_safe'] else '否'}")
    print(f"   风险: {result['risk_level']}")
    if result['issues']:
        print(f"   问题: {', '.join(result['issues'])}")
    print()
```

---

## 9. 最佳实践

### 9.1 推荐做法

```python
"""
最佳实践示例
"""

# ✅ 1. 使用批量而非循环
inputs = [{"question": q} for q in questions]
responses = chain.batch(inputs)  # 快

# ✅ 2. 控制并发数
config = RunnableConfig(max_concurrency=10)
responses = chain.batch(inputs, config=config)

# ✅ 3. 分批处理大量数据
BATCH_SIZE = 100
for i in range(0, len(inputs), BATCH_SIZE):
    batch = inputs[i:i + BATCH_SIZE]
    results = chain.batch(batch)

# ✅ 4. 错误处理
try:
    responses = chain.batch(inputs)
except Exception as e:
    # 降级到逐个处理
    responses = [chain.invoke(inp) for inp in inputs]

# ✅ 5. 成本统计
with get_openai_callback() as cb:
    responses = chain.batch(inputs)
    print(f"成本: ${cb.total_cost}")
```

### 9.2 避免的做法

```python
# ❌ 1. 循环调用
for inp in inputs:
    response = chain.invoke(inp)  # 慢

# ❌ 2. 不控制并发
responses = chain.batch(huge_inputs)  # 可能触发速率限制

# ❌ 3. 一次处理过多
responses = chain.batch(inputs_10000)  # 可能超时或内存不足

# ❌ 4. 忽略错误
responses = chain.batch(inputs)  # 一个失败全部失败

# ❌ 5. 不统计成本
responses = chain.batch(inputs)  # 不知道花了多少钱
```

---

## 检查清单

完成本节实战后，你应该能够：

- [ ] 使用 batch() 进行批量处理
- [ ] 在链中使用批量处理
- [ ] 理解循环 vs 批量的性能差异
- [ ] 实现批量评估和 A/B 测试
- [ ] 批量处理分类和信息提取
- [ ] 控制并发数
- [ ] 处理批量错误
- [ ] 统计 token 使用和成本
- [ ] 使用批处理 API（2025+）
- [ ] 构建完整的批量处理应用

---

**下一步**: 阅读 `08_面试必问.md` 准备面试问题
