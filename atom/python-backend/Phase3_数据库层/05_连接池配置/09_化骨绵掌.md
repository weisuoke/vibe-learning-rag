# 化骨绵掌

> 10个2分钟知识卡片，帮助你快速掌握连接池配置的核心知识

---

## 卡片1：连接池的本质

**一句话：** 连接池是预先创建并维护的一组可复用的数据库连接，通过复用避免频繁建立连接的开销。

**举例：**
```python
# 没有连接池：每次都创建新连接
建立连接 (65-160ms) + 查询 (5-20ms) + 关闭连接 (5-10ms) = 75-190ms

# 有连接池：复用已有连接
获取连接 (1-5ms) + 查询 (5-20ms) + 归还连接 (<1ms) = 6-26ms

# 性能提升：10-30倍
```

**应用：** 在 FastAPI + SQLAlchemy 中，通过 `create_engine()` 配置连接池，每个请求自动从连接池获取连接，用完自动归还。

**日常类比：** 共享单车 - 从停车点取车，用完还回去，下一个人继续用。

---

## 卡片2：pool_size（常驻连接数）

**一句话：** pool_size 是连接池中始终保持的连接数，应用启动时预创建，始终保持在池中。

**举例：**
```python
engine = create_engine(
    DATABASE_URL,
    pool_size=10,  # 始终保持10个连接
)

# 应用启动时：
# 1. 预创建10个连接
# 2. 放入连接队列
# 3. 等待请求获取
```

**配置公式：**
```python
pool_size = 并发请求数 × 0.5

# 示例：
# 预期100个并发请求
pool_size = 100 × 0.5 = 50
```

**应用：** 根据实际并发量配置，过小会导致请求排队，过大会浪费资源。

**日常类比：** 餐厅的固定座位数 - 根据平时客流量配置。

---

## 卡片3：max_overflow（溢出连接数）

**一句话：** max_overflow 是在 pool_size 基础上，最多可以创建的临时连接数，用完立即关闭。

**举例：**
```python
engine = create_engine(
    DATABASE_URL,
    pool_size=10,       # 常驻连接：10个
    max_overflow=20,    # 临时连接：最多20个
    # 总连接数：10 + 20 = 30个
)

# 工作流程：
# 前10个请求：使用常驻连接
# 第11-30个请求：创建临时连接
# 第31个请求：等待或超时
```

**配置建议：**
```python
max_overflow = pool_size × 1.5 - 2

# 示例：
pool_size=10, max_overflow=15-20
pool_size=20, max_overflow=30-40
```

**应用：** 应对突发流量，高峰期自动扩容，流量下降后自动缩容。

**日常类比：** 餐厅的临时加座 - 高峰期临时加座，客人少了撤掉。

---

## 卡片4：pool_timeout（获取连接超时）

**一句话：** pool_timeout 是等待获取连接的最长时间（秒），超时后抛出异常，防止请求无限等待。

**举例：**
```python
engine = create_engine(
    DATABASE_URL,
    pool_timeout=30,  # 30秒内无法获取连接，抛出异常
)

# 连接池已满，新请求到达
try:
    conn = pool.get_connection(timeout=30)
except TimeoutError:
    # 30秒后仍无可用连接
    raise HTTPException(503, "数据库连接池已满")
```

**配置建议：**
```python
# Web API：30秒（避免请求长时间挂起）
pool_timeout=30

# 批处理：300秒（可以等待更久）
pool_timeout=300

# 实时系统：5秒（快速失败）
pool_timeout=5
```

**应用：** 防止请求无限等待，提升用户体验。

**日常类比：** 餐厅等位超时 - 等座位超过30分钟，客人就走了。

---

## 卡片5：pool_recycle（连接回收时间）

**一句话：** pool_recycle 是连接使用超过指定时间后，自动关闭并重新创建（秒），防止连接失效。

**举例：**
```python
engine = create_engine(
    DATABASE_URL,
    pool_recycle=3600,  # 1小时后回收连接
)

# 为什么需要回收？
# 1. 数据库主动断开连接（MySQL默认8小时）
# 2. 网络设备超时断开
# 3. 连接状态累积（临时表、会话变量）
```

**配置建议：**
```python
# 查询数据库的连接超时时间
# MySQL:
SHOW VARIABLES LIKE 'wait_timeout';  # 默认28800秒（8小时）

# 配置 pool_recycle
# 建议：小于数据库超时时间的80%
pool_recycle = 28800 * 0.8 = 23040  # 约6.4小时

# 实际配置（更保守）
pool_recycle=3600  # 1小时
```

**应用：** 防止使用失效连接导致的错误。

**日常类比：** 牛奶保质期 - 开封后7天过期，建议5天内喝完。

---

## 卡片6：pool_pre_ping（连接前检测）

**一句话：** pool_pre_ping 是使用连接前先发送一个简单查询（SELECT 1），检测连接是否有效，失效则自动重建。

**举例：**
```python
engine = create_engine(
    DATABASE_URL,
    pool_pre_ping=True,  # 使用前先ping
)

# 工作原理：
# 1. 从连接池获取连接
# 2. 发送 SELECT 1 检测连接
# 3. 如果连接有效，返回连接
# 4. 如果连接失效，重新创建连接
```

**性能开销：**
```python
# SELECT 1 的开销：<1ms
# 如果连接刚被使用过（<1秒），跳过ping
# 所以实际开销很小

# 对比：
pool_pre_ping=True:  1ms + 5-20ms = 6-21ms
pool_pre_ping=False: 70-180ms（如果连接失效）
```

**应用：** 生产环境必须开启，避免使用失效连接。

**日常类比：** 开车前检查 - 每次开车前检查轮胎、油量，防止开到半路抛锚。

---

## 卡片7：FastAPI 依赖注入管理 Session

**一句话：** FastAPI 通过依赖注入（Depends）自动管理 Session 的生命周期，每个请求独立 Session，请求结束后自动关闭。

**举例：**
```python
# 1. 依赖注入函数
def get_db():
    db = SessionLocal()  # 从连接池获取连接
    try:
        yield db
    finally:
        db.close()  # 归还连接到池中

# 2. 在路由中使用
@app.get("/users/{user_id}")
async def get_user(user_id: int, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.id == user_id).first()
    return user
    # 请求结束后，连接自动归还到池中
```

**工作流程：**
```
请求到达 → 调用get_db() → 创建Session → 从连接池获取连接
→ 执行查询 → 请求结束 → 执行finally → 关闭Session → 归还连接
```

**应用：** 无需手动管理连接的获取和释放，FastAPI 自动处理。

**前端类比：** Express 中间件 - 请求开始时获取连接，请求结束时释放连接。

---

## 卡片8：连接池监控指标

**一句话：** 监控连接池的核心指标包括：pool_size、checked_in、checked_out、overflow、pool_usage，及时发现性能瓶颈。

**举例：**
```python
# 获取连接池状态
pool = engine.pool
pool_size = pool.size()           # 连接池大小
checked_in = pool.checkedin()     # 空闲连接数
checked_out = pool.checkedout()   # 正在使用的连接数
overflow = pool.overflow()        # 溢出连接数

# 计算使用率
pool_usage = (checked_out + overflow) / (pool_size + max_overflow)

# 告警阈值
if pool_usage > 0.9:
    logger.error("连接池使用率过高")
if checked_in == 0:
    logger.error("无空闲连接")
```

**监控指标：**

| 指标 | 正常范围 | 告警阈值 |
|------|---------|---------|
| pool_usage | < 80% | > 90% |
| checked_in | > 20% | < 10% |
| wait_time | < 10ms | > 100ms |

**应用：** 定期监控连接池状态，根据使用率调整配置。

**日常类比：** 餐厅座位使用率 - 监控座位使用情况，决定是否需要加座。

---

## 卡片9：读写分离配置

**一句话：** 读写分离是将读操作路由到从库，写操作路由到主库，减轻主库压力，提高系统性能。

**举例：**
```python
# 主库引擎（写操作）
master_engine = create_engine(
    MASTER_URL,
    pool_size=10,  # 写操作较少，连接池较小
)

# 从库引擎（读操作）
slave_engine = create_engine(
    SLAVE_URL,
    pool_size=20,  # 读操作较多，连接池较大
)

# 主库 Session
MasterSessionLocal = sessionmaker(bind=master_engine)

# 从库 Session
SlaveSessionLocal = sessionmaker(bind=slave_engine)

# 依赖注入
def get_master_db():  # 写操作
    db = MasterSessionLocal()
    try:
        yield db
    finally:
        db.close()

def get_slave_db():  # 读操作
    db = SlaveSessionLocal()
    try:
        yield db
    finally:
        db.close()

# 在路由中使用
@app.post("/users")  # 写操作
async def create_user(db: Session = Depends(get_master_db)):
    pass

@app.get("/users")  # 读操作
async def list_users(db: Session = Depends(get_slave_db)):
    pass
```

**优点：**
- 减轻主库压力
- 提高读操作性能
- 提高系统可用性

**注意事项：**
- 主从延迟（通常 < 1秒）
- 写后读可能读到旧数据

**应用：** 适合读多写少的场景（如新闻网站、电商商品列表）。

---

## 卡片10：异步连接池

**一句话：** 异步连接池使用异步 I/O，提供更高的并发能力和更少的线程开销，适合高并发场景。

**举例：**
```python
# 异步引擎
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession

async_engine = create_async_engine(
    "postgresql+asyncpg://user:password@localhost:5432/dbname",
    pool_size=20,
    max_overflow=30,
)

# 异步 SessionLocal
AsyncSessionLocal = async_sessionmaker(
    async_engine,
    class_=AsyncSession,
    expire_on_commit=False,
)

# 异步依赖注入
async def get_async_db():
    async with AsyncSessionLocal() as session:
        yield session

# 在路由中使用
@app.get("/users")
async def list_users(db: AsyncSession = Depends(get_async_db)):
    result = await db.execute(select(User))
    users = result.scalars().all()
    return users
```

**性能对比：**

| 指标 | 同步 | 异步 | 提升 |
|------|------|------|------|
| 并发能力 | 100 req/s | 500 req/s | 5倍 |
| 响应时间 | 50ms | 10ms | 5倍 |
| 内存占用 | 100MB | 50MB | 50% |

**优点：**
- 更高的并发能力（协程级别）
- 更少的线程开销
- 适合 I/O 密集型应用

**缺点：**
- 代码复杂度略高
- 需要异步驱动（asyncpg、aiomysql）

**应用：** 高并发 API 服务、实时数据处理。

---

## 总结：10个知识卡片速查表

| 卡片 | 核心概念 | 关键配置 | 应用场景 |
|------|---------|---------|---------|
| 1 | 连接池本质 | - | 理解连接池的价值 |
| 2 | pool_size | 并发数 × 0.5 | 根据并发量配置 |
| 3 | max_overflow | pool_size × 1.5 | 应对突发流量 |
| 4 | pool_timeout | 30秒 | 防止无限等待 |
| 5 | pool_recycle | 3600秒 | 防止连接失效 |
| 6 | pool_pre_ping | True | 生产环境必须开启 |
| 7 | 依赖注入 | Depends(get_db) | FastAPI 集成 |
| 8 | 监控指标 | pool_usage < 80% | 性能监控 |
| 9 | 读写分离 | 主库/从库 | 读多写少场景 |
| 10 | 异步连接池 | asyncpg | 高并发场景 |

---

## 学习路径建议

### 初学者（第1-3天）
- 卡片1-3：理解连接池的基本概念和核心参数
- 卡片7：学习 FastAPI 依赖注入
- 实战：完成场景1（基础配置）

### 进阶学习（第4-7天）
- 卡片4-6：掌握超时、回收、检测机制
- 卡片8：学习监控和调优
- 实战：完成场景2（监控告警）

### 高级应用（第8-14天）
- 卡片9-10：学习读写分离和异步连接池
- 实战：完成场景3（高级配置）
- 项目：在实际项目中应用连接池

---

## 快速参考

### 生产环境推荐配置

```python
# app/core/database.py
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

DATABASE_URL = os.getenv("DATABASE_URL")
POOL_SIZE = int(os.getenv("POOL_SIZE", "20"))
MAX_OVERFLOW = int(os.getenv("MAX_OVERFLOW", "30"))

engine = create_engine(
    DATABASE_URL,
    pool_size=POOL_SIZE,
    max_overflow=MAX_OVERFLOW,
    pool_timeout=30,
    pool_recycle=3600,
    pool_pre_ping=True,
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

### 常见问题速查

| 问题 | 原因 | 解决方案 |
|------|------|---------|
| 连接池耗尽 | 并发数超过容量 | 增大 pool_size 和 max_overflow |
| 连接泄漏 | Session 没有关闭 | 使用 try-finally 确保关闭 |
| 慢查询 | 查询本身慢 | 添加索引、优化 SQL |
| 连接失效 | 超时被断开 | 开启 pool_pre_ping |

**记住：** 连接池配置是"性能"与"资源"的平衡，需要根据实际负载动态调整。
