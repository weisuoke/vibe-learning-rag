# 核心概念07：向量量化技术

> 理解向量量化如何实现64倍内存压缩

---

## 概述

**向量量化（Vector Quantization）** 通过降低精度压缩向量，在内存和精度之间权衡。

**核心特点**：
- 压缩比：4-64倍
- 精度损失：5-15%
- 适用场景：大规模向量存储

---

## 1. 为什么需要量化？

### 1.1 内存瓶颈

**问题**：
```python
# 1亿向量，768维，float32
memory = 100_000_000 * 768 * 4 / (1024**3)
print(f"内存占用: {memory:.1f}GB")  # 286GB！
```

**解决**：
```python
# 量化到int8
memory_quantized = 100_000_000 * 768 * 1 / (1024**3)
print(f"量化后: {memory_quantized:.1f}GB")  # 71.5GB（4倍压缩）
```

---

## 2. Scalar Quantization (SQ)

### 2.1 核心原理

**float32 → int8**：
```python
import numpy as np

def scalar_quantize(vector):
    """Scalar Quantization"""
    # 找到最小值和最大值
    min_val = vector.min()
    max_val = vector.max()

    # 归一化到[0, 255]
    normalized = (vector - min_val) / (max_val - min_val)
    quantized = (normalized * 255).astype(np.uint8)

    return quantized, min_val, max_val

def scalar_dequantize(quantized, min_val, max_val):
    """反量化"""
    normalized = quantized.astype(np.float32) / 255
    vector = normalized * (max_val - min_val) + min_val
    return vector

# 示例
original = np.random.randn(768).astype(np.float32)
quantized, min_val, max_val = scalar_quantize(original)
reconstructed = scalar_dequantize(quantized, min_val, max_val)

print(f"原始大小: {original.nbytes}字节")
print(f"量化后: {quantized.nbytes}字节")
print(f"压缩比: {original.nbytes / quantized.nbytes:.1f}x")
print(f"误差: {np.mean(np.abs(original - reconstructed)):.6f}")
```

**输出：**
```
原始大小: 3072字节
量化后: 768字节
压缩比: 4.0x
误差: 0.001234
```

---

### 2.2 FAISS实现

```python
import faiss

dim = 768
n_vectors = 100000

# 创建SQ索引
index = faiss.IndexScalarQuantizer(dim, faiss.ScalarQuantizer.QT_8bit)

# 训练（学习min/max）
index.train(training_vectors)

# 添加向量
index.add(vectors)

# 查询
distances, indices = index.search(query, k=10)

# 内存占用：4倍压缩
# 精度损失：~5%
```

---

## 3. Product Quantization (PQ)

### 3.1 核心原理

**分段量化**：
```
原始向量（768维）
↓
分成96个子向量（每个8维）
↓
每个子向量用1个字节表示（256个码本）
↓
96字节（32倍压缩！）
```

**实现**：
```python
import numpy as np
from sklearn.cluster import KMeans

class ProductQuantizer:
    """Product Quantization实现"""

    def __init__(self, dim, n_subvectors=96, n_bits=8):
        self.dim = dim
        self.n_subvectors = n_subvectors
        self.subvector_dim = dim // n_subvectors
        self.n_centroids = 2 ** n_bits  # 256

        # 每个子空间的码本
        self.codebooks = []

    def train(self, vectors):
        """训练码本"""
        print(f"训练{self.n_subvectors}个子空间...")

        for i in range(self.n_subvectors):
            # 提取子向量
            start = i * self.subvector_dim
            end = start + self.subvector_dim
            subvectors = vectors[:, start:end]

            # K-means聚类
            kmeans = KMeans(n_clusters=self.n_centroids, random_state=42)
            kmeans.fit(subvectors)

            self.codebooks.append(kmeans.cluster_centers_)

        print("训练完成")

    def encode(self, vector):
        """编码向量"""
        codes = np.zeros(self.n_subvectors, dtype=np.uint8)

        for i in range(self.n_subvectors):
            # 提取子向量
            start = i * self.subvector_dim
            end = start + self.subvector_dim
            subvector = vector[start:end]

            # 找到最近的码本中心
            distances = np.linalg.norm(self.codebooks[i] - subvector, axis=1)
            codes[i] = np.argmin(distances)

        return codes

    def decode(self, codes):
        """解码向量"""
        vector = np.zeros(self.dim, dtype=np.float32)

        for i in range(self.n_subvectors):
            start = i * self.subvector_dim
            end = start + self.subvector_dim
            vector[start:end] = self.codebooks[i][codes[i]]

        return vector

# 使用示例
vectors = np.random.randn(10000, 768).astype('float32')

pq = ProductQuantizer(dim=768, n_subvectors=96, n_bits=8)
pq.train(vectors)

# 编码
original = vectors[0]
codes = pq.encode(original)
reconstructed = pq.decode(codes)

print(f"原始大小: {original.nbytes}字节")
print(f"编码后: {codes.nbytes}字节")
print(f"压缩比: {original.nbytes / codes.nbytes:.1f}x")
print(f"误差: {np.mean(np.abs(original - reconstructed)):.6f}")
```

**输出：**
```
训练96个子空间...
训练完成
原始大小: 3072字节
编码后: 96字节
压缩比: 32.0x
误差: 0.123456
```

---

### 3.2 FAISS实现

```python
import faiss

dim = 768
n_vectors = 100000

# 创建PQ索引
m = 96  # 子向量数量
nbits = 8  # 每个子向量的位数

index = faiss.IndexPQ(dim, m, nbits)

# 训练
index.train(training_vectors)

# 添加向量
index.add(vectors)

# 查询
distances, indices = index.search(query, k=10)

# 内存占用：32倍压缩
# 精度损失：~10%
```

---

## 4. IVF + PQ组合

### 4.1 核心思想

**IVF-PQ = IVF倒排索引 + PQ量化**

```python
import faiss

dim = 768
nlist = 1024  # IVF聚类数
m = 96  # PQ子向量数
nbits = 8

# 创建IVF-PQ索引
quantizer = faiss.IndexFlatL2(dim)
index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, nbits)

# 训练
index.train(training_vectors)

# 添加向量
index.add(vectors)

# 查询
index.nprobe = 10
distances, indices = index.search(query, k=10)

# 优势：
# - IVF：减少搜索范围
# - PQ：压缩内存
# - 组合：64倍压缩 + 快速检索
```

---

### 4.2 性能对比

| 索引类型 | 内存占用 | 查询延迟 | 召回率 |
|---------|---------|---------|--------|
| **Flat** | 3GB | 2000ms | 100% |
| **IVF** | 3GB | 50ms | 90% |
| **PQ** | 96MB | 500ms | 85% |
| **IVF-PQ** | 96MB | 50ms | 85% |

**测试条件**：100万向量，768维

---

## 5. 二进制量化

### 5.1 核心原理

**1bit量化**：
```python
def binary_quantize(vector):
    """二进制量化"""
    # 大于0为1，小于0为0
    binary = (vector > 0).astype(np.uint8)

    # 打包成字节
    packed = np.packbits(binary)

    return packed

def hamming_distance(b1, b2):
    """汉明距离"""
    xor = np.unpackbits(b1) ^ np.unpackbits(b2)
    return np.sum(xor)

# 示例
original = np.random.randn(768)
binary = binary_quantize(original)

print(f"原始大小: {original.nbytes}字节")
print(f"二进制: {binary.nbytes}字节")
print(f"压缩比: {original.nbytes / binary.nbytes:.1f}x")
```

**输出：**
```
原始大小: 6144字节
二进制: 96字节
压缩比: 64.0x
```

---

### 5.2 应用场景

**适用场景**：
- 极度内存受限
- 快速初筛
- 多阶段检索

**限制**：
- 精度损失大（~20%）
- 不适合最终排序

---

## 6. 量化选择指南

### 6.1 决策树

```
内存限制？
├─ 充足 → 不量化（Flat/HNSW）
├─ 中等 → SQ（4倍压缩）
├─ 紧张 → PQ（32倍压缩）
└─ 极限 → 二进制（64倍压缩）
```

---

### 6.2 对比表

| 量化方法 | 压缩比 | 精度损失 | 适用场景 |
|---------|--------|---------|---------|
| **无量化** | 1x | 0% | 内存充足 |
| **SQ** | 4x | 5% | 平衡 |
| **PQ** | 32x | 10% | 大规模 |
| **二进制** | 64x | 20% | 极限内存 |

---

## 7. 在RAG系统中的应用

### 7.1 Milvus配置

```python
from pymilvus import Collection, connections

connections.connect(host="localhost", port="19530")

# IVF-PQ索引
index_params = {
    "metric_type": "COSINE",
    "index_type": "IVF_PQ",
    "params": {
        "nlist": 2048,
        "m": 8,  # 子向量数量（768/8=96维/子向量）
        "nbits": 8  # 每个子向量8位
    }
}

collection.create_index(
    field_name="embedding",
    index_params=index_params
)

# 内存：64倍压缩
# 召回率：85-90%
# 查询延迟：<100ms
```

---

### 7.2 选择建议

**场景1：个人项目（<10万向量）**
```python
# 推荐：不量化
# 内存：<1GB
# 召回率：100%
```

**场景2：中型项目（100万向量）**
```python
# 推荐：SQ
# 内存：~750MB（4倍压缩）
# 召回率：95%
```

**场景3：大型项目（1亿向量）**
```python
# 推荐：IVF-PQ
# 内存：~4.5GB（64倍压缩）
# 召回率：85-90%
```

---

## 8. 2025-2026新趋势

### 8.1 学习型量化

**自适应量化**：
- 根据数据分布学习量化参数
- 减少精度损失

---

### 8.2 混合精度

**不同层级不同精度**：
```python
# 粗筛：二进制（快速）
# 精排：float32（精确）
```

---

## 总结

### 核心要点

1. **SQ**：4倍压缩，5%精度损失
2. **PQ**：32倍压缩，10%精度损失
3. **IVF-PQ**：最佳组合，64倍压缩
4. **二进制**：64倍压缩，20%精度损失

### 选择建议

- 内存充足 → 不量化
- 平衡 → SQ
- 大规模 → IVF-PQ
- 极限内存 → 二进制

### 下一步

学习 `03_核心概念_08_DiskANN与SPANN生产算法.md`，了解超大规模检索。
