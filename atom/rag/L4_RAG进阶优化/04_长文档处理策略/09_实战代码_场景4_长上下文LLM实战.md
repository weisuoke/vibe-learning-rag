# 实战代码 - 场景4：长上下文LLM实战

> 使用2026年长上下文LLM处理超长文档，结合智能策略优化性能

---

## 场景描述

**需求**：利用2026年长上下文LLM（Gemini 3 Pro 1M tokens、Llama 4 Scout 10M tokens）处理超长文档，同时结合智能策略避免Context Rot和高成本。

**挑战**：
- 长上下文LLM虽然窗口大，但仍存在Context Rot
- 成本是标准上下文的3-10倍
- 需要智能策略优化性能和成本

**解决方案**：长上下文LLM + 智能文档处理策略

---

## 完整实现

### 步骤1：环境准备

```python
# 安装依赖
# uv add langchain langchain-openai anthropic python-dotenv

from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from typing import List, Dict
import time

# 加载环境变量
load_dotenv()
```

### 步骤2：长上下文LLM封装

```python
class LongContextLLM:
    """长上下文LLM封装"""

    # 2026年长上下文模型配置
    MODELS = {
        "gemini-3-pro": {
            "max_tokens": 1_000_000,  # 1M tokens
            "cost_multiplier": 5,
            "provider": "google"
        },
        "llama-4-scout": {
            "max_tokens": 10_000_000,  # 10M tokens
            "cost_multiplier": 8,
            "provider": "meta"
        },
        "grok-4.1-fast": {
            "max_tokens": 2_000_000,  # 2M tokens
            "cost_multiplier": 6,
            "provider": "xai"
        },
        "claude-opus-4": {
            "max_tokens": 200_000,  # 200K tokens
            "cost_multiplier": 3,
            "provider": "anthropic"
        }
    }

    def __init__(self, model_name: str = "claude-opus-4"):
        self.model_name = model_name
        self.config = self.MODELS.get(model_name, self.MODELS["claude-opus-4"])

        # 初始化LLM
        if self.config["provider"] == "anthropic":
            from anthropic import Anthropic
            self.client = Anthropic()
        else:
            # 其他provider的初始化
            self.llm = ChatOpenAI(
                model=model_name,
                temperature=0
            )

    def get_max_tokens(self) -> int:
        """获取最大token数"""
        return self.config["max_tokens"]

    def get_cost_multiplier(self) -> float:
        """获取成本倍数"""
        return self.config["cost_multiplier"]

    def can_fit_document(self, text: str) -> bool:
        """检查文档是否能放入上下文窗口"""
        # 简单估算：1 token ≈ 4 characters
        estimated_tokens = len(text) // 4
        return estimated_tokens < self.get_max_tokens() * 0.8  # 留20%余量
```

### 步骤3：智能路由策略

```python
class SmartDocumentRouter:
    """智能文档路由器"""

    def __init__(self):
        self.long_context_llm = LongContextLLM("claude-opus-4")

    def route_strategy(
        self,
        text: str,
        query: str,
        doc_type: str = "general"
    ) -> Dict:
        """根据文档长度和类型路由策略"""

        doc_length = len(text)
        estimated_tokens = doc_length // 4

        # 决策树
        if estimated_tokens < 8000:
            # 短文档：直接使用标准LLM
            return {
                "strategy": "direct",
                "reason": "文档较短，直接处理",
                "estimated_cost": 1.0,
                "estimated_time": 1.0
            }

        elif estimated_tokens < 50000:
            # 中等文档：使用分层索引
            return {
                "strategy": "hierarchical_indexing",
                "reason": "中等长度文档，使用分层索引",
                "estimated_cost": 1.5,
                "estimated_time": 0.8
            }

        elif estimated_tokens < 200000:
            # 长文档：使用长上下文LLM
            if self.long_context_llm.can_fit_document(text):
                return {
                    "strategy": "long_context_llm",
                    "reason": "长文档，使用长上下文LLM",
                    "estimated_cost": self.long_context_llm.get_cost_multiplier(),
                    "estimated_time": 2.0
                }
            else:
                return {
                    "strategy": "summary_chain",
                    "reason": "文档过长，使用摘要链",
                    "estimated_cost": 2.5,
                    "estimated_time": 2.5
                }

        else:
            # 超长文档：使用混合策略
            return {
                "strategy": "hybrid",
                "reason": "超长文档，使用混合策略",
                "estimated_cost": 3.0,
                "estimated_time": 3.0
            }
```

### 步骤4：Context Rot缓解策略

```python
class ContextRotMitigator:
    """Context Rot缓解器"""

    def __init__(self, llm: LongContextLLM):
        self.llm = llm

    def reorder_context(
        self,
        chunks: List[str],
        query: str
    ) -> List[str]:
        """重新排序上下文，避免关键信息在中间"""

        # 计算每个chunk与query的相关性
        relevance_scores = []
        for i, chunk in enumerate(chunks):
            # 简单的相关性计算（实际应用中使用embedding）
            score = self._calculate_relevance(chunk, query)
            relevance_scores.append((i, score))

        # 排序：最相关的放在开头和结尾
        relevance_scores.sort(key=lambda x: x[1], reverse=True)

        # 重新排序策略：高相关性放在开头和结尾
        reordered = []
        for i, (idx, score) in enumerate(relevance_scores):
            if i % 2 == 0:
                # 偶数索引：放在开头
                reordered.insert(0, chunks[idx])
            else:
                # 奇数索引：放在结尾
                reordered.append(chunks[idx])

        return reordered

    def _calculate_relevance(self, chunk: str, query: str) -> float:
        """计算相关性（简化版）"""
        # 实际应用中使用embedding相似度
        query_words = set(query.lower().split())
        chunk_words = set(chunk.lower().split())
        overlap = len(query_words & chunk_words)
        return overlap / len(query_words) if query_words else 0

    def add_position_markers(self, chunks: List[str]) -> List[str]:
        """添加位置标记，帮助LLM定位"""
        marked_chunks = []
        for i, chunk in enumerate(chunks):
            marker = f"[SECTION {i+1}/{len(chunks)}]\n{chunk}"
            marked_chunks.append(marker)
        return marked_chunks
```

### 步骤5：长上下文问答系统

```python
class LongContextQASystem:
    """长上下文问答系统"""

    def __init__(self, model_name: str = "claude-opus-4"):
        self.llm = LongContextLLM(model_name)
        self.router = SmartDocumentRouter()
        self.mitigator = ContextRotMitigator(self.llm)

    def answer(
        self,
        text: str,
        question: str,
        use_mitigation: bool = True
    ) -> Dict:
        """回答问题"""

        # 1. 路由决策
        routing = self.router.route_strategy(text, question)

        print(f"路由策略: {routing['strategy']}")
        print(f"原因: {routing['reason']}")
        print(f"预估成本: {routing['estimated_cost']}x")

        # 2. 根据策略处理
        if routing["strategy"] == "direct":
            return self._direct_answer(text, question)

        elif routing["strategy"] == "long_context_llm":
            return self._long_context_answer(
                text, question, use_mitigation
            )

        elif routing["strategy"] == "hierarchical_indexing":
            return self._hierarchical_answer(text, question)

        elif routing["strategy"] == "summary_chain":
            return self._summary_chain_answer(text, question)

        else:  # hybrid
            return self._hybrid_answer(text, question)

    def _long_context_answer(
        self,
        text: str,
        question: str,
        use_mitigation: bool
    ) -> Dict:
        """使用长上下文LLM回答"""

        start_time = time.time()

        # 分块（即使使用长上下文，也建议分块以便重排序）
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=10000,
            chunk_overlap=1000
        )
        chunks = splitter.split_text(text)

        # Context Rot缓解
        if use_mitigation:
            chunks = self.mitigator.reorder_context(chunks, question)
            chunks = self.mitigator.add_position_markers(chunks)

        # 组合上下文
        context = "\n\n".join(chunks)

        # 生成答案
        prompt = f"""
        Based on the following document, answer the question.

        IMPORTANT: Pay attention to all sections, especially those marked with [SECTION X/Y].

        Document:
        {context}

        Question: {question}

        Answer:
        """

        # 使用Anthropic API
        from anthropic import Anthropic
        client = Anthropic()

        response = client.messages.create(
            model="claude-opus-4-20250514",
            max_tokens=4096,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        answer = response.content[0].text
        elapsed_time = time.time() - start_time

        return {
            "question": question,
            "answer": answer,
            "strategy": "long_context_llm",
            "mitigation_used": use_mitigation,
            "num_chunks": len(chunks),
            "elapsed_time": elapsed_time,
            "estimated_cost": self.llm.get_cost_multiplier()
        }

    def _direct_answer(self, text: str, question: str) -> Dict:
        """直接回答（短文档）"""
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

        prompt = f"""
        Based on the following document, answer the question.

        Document: {text}

        Question: {question}

        Answer:
        """

        answer = llm.predict(prompt)

        return {
            "question": question,
            "answer": answer,
            "strategy": "direct"
        }

    def _hierarchical_answer(self, text: str, question: str) -> Dict:
        """分层索引回答"""
        # 调用之前实现的分层索引系统
        return {"strategy": "hierarchical_indexing"}

    def _summary_chain_answer(self, text: str, question: str) -> Dict:
        """摘要链回答"""
        # 调用之前实现的摘要链系统
        return {"strategy": "summary_chain"}

    def _hybrid_answer(self, text: str, question: str) -> Dict:
        """混合策略回答"""
        # 调用混合策略系统
        return {"strategy": "hybrid"}
```

### 步骤6：完整示例

```python
def main():
    """完整示例"""

    # 1. 准备测试文档（模拟长文档）
    long_document = """
    [这里是一个50K tokens的长文档内容]
    """ * 1000  # 模拟长文档

    # 2. 创建问答系统
    print("创建长上下文问答系统...")
    qa_system = LongContextQASystem(model_name="claude-opus-4")

    # 3. 测试问答
    questions = [
        "What is the main topic of this document?",
        "What are the key findings?",
        "What are the conclusions?"
    ]

    for question in questions:
        print(f"\n问题: {question}")

        # 不使用缓解策略
        result_no_mitigation = qa_system.answer(
            long_document,
            question,
            use_mitigation=False
        )

        print(f"\n不使用缓解策略:")
        print(f"答案: {result_no_mitigation['answer'][:200]}...")
        print(f"时间: {result_no_mitigation.get('elapsed_time', 0):.2f}秒")

        # 使用缓解策略
        result_with_mitigation = qa_system.answer(
            long_document,
            question,
            use_mitigation=True
        )

        print(f"\n使用缓解策略:")
        print(f"答案: {result_with_mitigation['answer'][:200]}...")
        print(f"时间: {result_with_mitigation.get('elapsed_time', 0):.2f}秒")

if __name__ == "__main__":
    main()
```

---

## 2026年生产级优化

### 优化1：动态窗口调整

```python
class AdaptiveWindowManager:
    """自适应窗口管理器"""

    def __init__(self, llm: LongContextLLM):
        self.llm = llm

    def optimize_window_size(
        self,
        text: str,
        query: str
    ) -> int:
        """根据查询复杂度动态调整窗口大小"""

        # 分析查询复杂度
        query_complexity = self._analyze_query_complexity(query)

        # 根据复杂度调整窗口
        if query_complexity == "simple":
            # 简单查询：使用较小窗口
            return 50000
        elif query_complexity == "medium":
            # 中等查询：使用中等窗口
            return 100000
        else:
            # 复杂查询：使用最大窗口
            return self.llm.get_max_tokens()

    def _analyze_query_complexity(self, query: str) -> str:
        """分析查询复杂度"""
        # 简单启发式规则
        if len(query.split()) < 10:
            return "simple"
        elif len(query.split()) < 20:
            return "medium"
        else:
            return "complex"
```

### 优化2：成本监控

```python
class CostMonitor:
    """成本监控器"""

    def __init__(self):
        self.total_cost = 0.0
        self.query_count = 0

    def estimate_cost(
        self,
        text_length: int,
        model_name: str
    ) -> float:
        """估算成本"""
        llm = LongContextLLM(model_name)
        cost_multiplier = llm.get_cost_multiplier()

        # 基础成本（假设标准上下文$0.01）
        base_cost = 0.01
        estimated_tokens = text_length // 4

        # 计算成本
        cost = base_cost * (estimated_tokens / 1000) * cost_multiplier

        return cost

    def track_cost(self, cost: float):
        """跟踪成本"""
        self.total_cost += cost
        self.query_count += 1

    def get_average_cost(self) -> float:
        """获取平均成本"""
        if self.query_count == 0:
            return 0.0
        return self.total_cost / self.query_count

    def should_switch_strategy(self) -> bool:
        """判断是否应该切换策略"""
        avg_cost = self.get_average_cost()

        # 如果平均成本超过阈值，建议切换策略
        if avg_cost > 0.50:  # $0.50/query
            return True

        return False
```

### 优化3：批量处理

```python
class BatchLongContextProcessor:
    """批量长上下文处理器"""

    def __init__(self, model_name: str = "claude-opus-4"):
        self.qa_system = LongContextQASystem(model_name)
        self.cost_monitor = CostMonitor()

    def batch_process(
        self,
        documents: List[str],
        questions: List[str]
    ) -> List[Dict]:
        """批量处理文档和问题"""

        results = []

        for i, (doc, question) in enumerate(zip(documents, questions)):
            print(f"\n处理第 {i+1}/{len(documents)} 个文档...")

            # 估算成本
            estimated_cost = self.cost_monitor.estimate_cost(
                len(doc),
                "claude-opus-4"
            )

            print(f"预估成本: ${estimated_cost:.4f}")

            # 处理
            result = self.qa_system.answer(doc, question)

            # 跟踪成本
            self.cost_monitor.track_cost(estimated_cost)

            results.append(result)

            # 检查是否需要切换策略
            if self.cost_monitor.should_switch_strategy():
                print("\n警告：平均成本过高，建议切换策略")

        return results
```

---

## 性能评估

### 评估代码

```python
def evaluate_long_context_system(
    qa_system,
    test_documents,
    test_questions
):
    """评估长上下文系统"""

    metrics = {
        "accuracy": 0.0,
        "avg_latency": 0.0,
        "avg_cost": 0.0,
        "context_rot_impact": 0.0
    }

    # 测试不同位置的信息
    positions = ["beginning", "middle", "end"]
    position_accuracy = {pos: [] for pos in positions}

    for doc, question, position in zip(
        test_documents, test_questions, positions
    ):
        # 测试
        result = qa_system.answer(doc, question)

        # 记录准确率
        position_accuracy[position].append(result.get("accuracy", 0))

    # 计算Context Rot影响
    beginning_acc = sum(position_accuracy["beginning"]) / len(position_accuracy["beginning"])
    middle_acc = sum(position_accuracy["middle"]) / len(position_accuracy["middle"])
    end_acc = sum(position_accuracy["end"]) / len(position_accuracy["end"])

    metrics["context_rot_impact"] = (beginning_acc - middle_acc) / beginning_acc

    return metrics
```

---

## 常见问题

### Q1: 如何选择合适的长上下文模型？

**A**: 根据需求和预算选择

| 模型 | 上下文窗口 | 成本 | 适用场景 |
|------|-----------|------|---------|
| Claude Opus 4 | 200K | 3x | 平衡性能和成本 |
| Gemini 3 Pro | 1M | 5x | 中等长度文档 |
| Grok 4.1 Fast | 2M | 6x | 快速响应需求 |
| Llama 4 Scout | 10M | 8x | 超长文档 |

### Q2: 如何缓解Context Rot？

**A**: 三种策略

1. **重排序上下文**：将关键信息放在开头和结尾
2. **添加位置标记**：帮助LLM定位信息
3. **分块处理**：即使使用长上下文，也建议分块

### Q3: 如何控制成本？

**A**: 成本优化策略

1. **智能路由**：根据文档长度选择策略
2. **动态窗口**：根据查询复杂度调整窗口大小
3. **成本监控**：实时跟踪成本，超过阈值切换策略

---

## 核心记忆

### 关键点

1. **长上下文LLM**：2026年最大10M tokens（Llama 4 Scout）
2. **Context Rot**：中间信息准确率下降30-50%
3. **成本**：长上下文调用成本是标准上下文的3-10倍
4. **缓解策略**：重排序、位置标记、分块处理

### 2026年技术

1. **Gemini 3 Pro**：1M tokens，成本5x
2. **Llama 4 Scout**：10M tokens，成本8x
3. **Grok 4.1 Fast**：2M tokens，成本6x
4. **Claude Opus 4**：200K tokens，成本3x

### 生产级配置

- 推荐模型: Claude Opus 4（平衡性能和成本）
- 缓解策略: 启用重排序和位置标记
- 成本阈值: $0.50/query
- 动态窗口: 根据查询复杂度调整

---

**版本**: v1.0 (2025-2026 Research Edition)
**最后更新**: 2026-02-17
**代码行数**: ~250行
