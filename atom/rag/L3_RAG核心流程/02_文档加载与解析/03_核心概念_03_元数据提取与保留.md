# 核心概念3：元数据提取与保留

> 理解元数据在RAG系统中的关键作用及提取策略

---

## 什么是元数据提取与保留？

**元数据提取与保留**是指在文档加载过程中，不仅提取文本内容，还要提取和保留文档的上下文信息（来源、作者、时间、结构等），并在整个RAG流程中传递这些信息。

**核心价值：**
```
文档 → 提取内容 + 提取元数据 → Document(content, metadata)
                                    ↓
                            检索过滤、排序、溯源
```

---

## 1. 元数据的三个层次

### 1.1 文档元数据 (Document Metadata)

**定义：** 描述文档本身的信息

```python
from langchain.schema import Document
from datetime import datetime

# 文档元数据示例
doc = Document(
    page_content="RAG系统通过检索增强生成...",
    metadata={
        # 基础信息
        "source": "reports/rag_guide.pdf",
        "title": "RAG技术指南",
        "author": "张三",
        "created_date": "2025-01-15",
        "modified_date": "2025-02-10",

        # 文件信息
        "file_size": 1024000,  # 字节
        "file_type": "pdf",
        "page_count": 50,

        # 业务信息
        "department": "技术部",
        "category": "技术文档",
        "version": "v2.0",
        "language": "zh"
    }
)
```

**提取方法：**

```python
from pypdf import PdfReader
import os
from datetime import datetime

def extract_document_metadata(file_path: str) -> dict:
    """提取PDF文档元数据"""
    metadata = {}

    # 1. 文件系统元数据
    stat = os.stat(file_path)
    metadata["file_name"] = os.path.basename(file_path)
    metadata["file_size"] = stat.st_size
    metadata["created_date"] = datetime.fromtimestamp(stat.st_ctime).isoformat()
    metadata["modified_date"] = datetime.fromtimestamp(stat.st_mtime).isoformat()

    # 2. PDF内部元数据
    try:
        reader = PdfReader(file_path)
        pdf_meta = reader.metadata

        if pdf_meta:
            metadata["title"] = pdf_meta.get("/Title", "")
            metadata["author"] = pdf_meta.get("/Author", "")
            metadata["subject"] = pdf_meta.get("/Subject", "")
            metadata["creator"] = pdf_meta.get("/Creator", "")
            metadata["producer"] = pdf_meta.get("/Producer", "")

            # 创建日期
            if "/CreationDate" in pdf_meta:
                metadata["pdf_created_date"] = pdf_meta["/CreationDate"]

        metadata["page_count"] = len(reader.pages)
    except Exception as e:
        print(f"提取PDF元数据失败: {e}")

    return metadata

# 使用示例
metadata = extract_document_metadata("report.pdf")
print(metadata)
```

### 1.2 结构元数据 (Structural Metadata)

**定义：** 描述文档内部结构的信息

```python
# 结构元数据示例
doc = Document(
    page_content="3.1 RAG架构设计\n\nRAG系统包含三个核心组件...",
    metadata={
        "source": "rag_guide.pdf",
        "page": 15,

        # 结构信息
        "chapter": "第3章",
        "section": "3.1",
        "heading": "RAG架构设计",
        "heading_level": 2,

        # 位置信息
        "page_position": "top",  # 在页面的位置
        "paragraph_index": 0,    # 段落索引

        # 格式信息
        "is_title": True,
        "font_size": 14,
        "is_bold": True
    }
)
```

**提取方法：**

```python
import pdfplumber
from langchain.schema import Document

def extract_with_structure(file_path: str) -> list[Document]:
    """提取PDF内容并保留结构信息"""
    documents = []

    with pdfplumber.open(file_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            # 提取文本
            text = page.extract_text()

            # 提取表格
            tables = page.extract_tables()

            # 提取文本块（带位置信息）
            words = page.extract_words()

            # 识别标题（基于字体大小）
            headings = []
            for word in words:
                if word['height'] > 12:  # 假设标题字体较大
                    headings.append({
                        'text': word['text'],
                        'size': word['height'],
                        'position': (word['x0'], word['top'])
                    })

            # 创建Document
            doc = Document(
                page_content=text,
                metadata={
                    "source": file_path,
                    "page": page_num,
                    "has_tables": len(tables) > 0,
                    "table_count": len(tables),
                    "headings": [h['text'] for h in headings],
                    "page_width": page.width,
                    "page_height": page.height
                }
            )
            documents.append(doc)

    return documents
```

### 1.3 语义元数据 (Semantic Metadata)

**定义：** 描述文档内容语义的信息

```python
# 语义元数据示例
doc = Document(
    page_content="RAG系统通过检索增强生成...",
    metadata={
        "source": "rag_guide.pdf",

        # 语义信息
        "language": "zh",
        "topics": ["RAG", "检索", "生成", "AI"],
        "keywords": ["向量检索", "Embedding", "LLM"],
        "sentiment": "neutral",
        "complexity": "advanced",

        # 内容统计
        "word_count": 1500,
        "char_count": 4500,
        "sentence_count": 45,

        # 质量指标
        "readability_score": 0.75,
        "technical_density": 0.8
    }
)
```

**提取方法：**

```python
from langchain.schema import Document
import re

def extract_semantic_metadata(text: str) -> dict:
    """提取文本的语义元数据"""
    metadata = {}

    # 1. 语言检测
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    metadata["language"] = "zh" if chinese_chars / len(text) > 0.1 else "en"

    # 2. 内容统计
    metadata["char_count"] = len(text)
    metadata["word_count"] = len(text.split())
    metadata["sentence_count"] = len(re.split(r'[。！？.!?]', text))

    # 3. 关键词提取（简单版本）
    # 实际应用中可以使用jieba、TF-IDF等
    words = text.split()
    word_freq = {}
    for word in words:
        if len(word) > 1:  # 过滤单字
            word_freq[word] = word_freq.get(word, 0) + 1

    # 取频率最高的10个词作为关键词
    keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    metadata["keywords"] = [word for word, freq in keywords]

    # 4. 技术密度（专业术语比例）
    technical_terms = ["RAG", "Embedding", "向量", "检索", "LLM", "Transformer"]
    technical_count = sum(text.count(term) for term in technical_terms)
    metadata["technical_density"] = technical_count / len(words) if words else 0

    return metadata

# 使用示例
text = "RAG系统通过检索增强生成，结合了向量检索和LLM生成..."
semantic_meta = extract_semantic_metadata(text)
print(semantic_meta)
```

---

## 2. 元数据在RAG中的应用

### 2.1 检索过滤 (Retrieval Filtering)

**场景：** 按元数据条件过滤检索结果

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# 构建向量库（包含元数据）
vectorstore = Chroma.from_documents(
    documents,
    OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)

# ===== 1. 按时间过滤 =====
# 只检索2025年的文档
results = vectorstore.similarity_search(
    "RAG技术进展",
    k=5,
    filter={"created_date": {"$gte": "2025-01-01"}}
)

# ===== 2. 按部门过滤 =====
# 只检索技术部的文档
results = vectorstore.similarity_search(
    "系统架构",
    k=5,
    filter={"department": "技术部"}
)

# ===== 3. 按文档类型过滤 =====
# 只检索PDF文档
results = vectorstore.similarity_search(
    "技术报告",
    k=5,
    filter={"file_type": "pdf"}
)

# ===== 4. 组合过滤 =====
# 2025年技术部的PDF文档
results = vectorstore.similarity_search(
    "RAG实践",
    k=5,
    filter={
        "created_date": {"$gte": "2025-01-01"},
        "department": "技术部",
        "file_type": "pdf"
    }
)
```

### 2.2 检索排序 (Retrieval Ranking)

**场景：** 结合元数据和语义相似度进行混合排序

```python
from typing import List
from langchain.schema import Document

def hybrid_ranking(
    query: str,
    documents: List[Document],
    semantic_weight: float = 0.7,
    metadata_weight: float = 0.3
) -> List[Document]:
    """
    混合排序：语义相似度 + 元数据权重
    """
    from datetime import datetime

    scored_docs = []

    for doc in documents:
        # 1. 语义相似度分数（假设已计算）
        semantic_score = doc.metadata.get('similarity_score', 0.5)

        # 2. 元数据分数
        metadata_score = 0.0

        # 时间新鲜度（越新越好）
        if 'created_date' in doc.metadata:
            try:
                created = datetime.fromisoformat(doc.metadata['created_date'])
                days_old = (datetime.now() - created).days
                freshness_score = max(0, 1 - days_old / 365)  # 1年内线性衰减
                metadata_score += freshness_score * 0.4
            except:
                pass

        # 文档权威性（官方文档权重高）
        if doc.metadata.get('category') == '官方文档':
            metadata_score += 0.3

        # 文档完整性（页数多的文档可能更全面）
        page_count = doc.metadata.get('page_count', 0)
        if page_count > 10:
            metadata_score += 0.3

        # 3. 综合分数
        final_score = (
            semantic_score * semantic_weight +
            metadata_score * metadata_weight
        )

        scored_docs.append((doc, final_score))

    # 按分数排序
    scored_docs.sort(key=lambda x: x[1], reverse=True)

    return [doc for doc, score in scored_docs]
```

### 2.3 答案溯源 (Answer Attribution)

**场景：** 在生成答案时引用元数据，提供可追溯性

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# 构建QA链
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# 查询
result = qa_chain({"query": "RAG的核心流程是什么？"})

# 生成带引用的答案
answer = result['result']
sources = result['source_documents']

# 格式化输出
print(f"答案: {answer}\n")
print("参考来源:")
for i, doc in enumerate(sources, 1):
    meta = doc.metadata
    print(f"{i}. {meta.get('title', '未知标题')}")
    print(f"   作者: {meta.get('author', '未知')}")
    print(f"   来源: {meta.get('source', '未知')}")
    print(f"   页码: {meta.get('page', '未知')}")
    print(f"   日期: {meta.get('created_date', '未知')}\n")
```

**输出示例：**
```
答案: RAG的核心流程包括三个阶段：1) 文档加载与索引...

参考来源:
1. RAG技术指南
   作者: 张三
   来源: reports/rag_guide.pdf
   页码: 15
   日期: 2025-01-15

2. RAG实践手册
   作者: 李四
   来源: docs/rag_practice.pdf
   页码: 8
   日期: 2025-02-01
```

---

## 3. 元数据Schema设计

### 3.1 标准Schema

```python
from typing import Optional, List
from datetime import datetime
from pydantic import BaseModel, Field

class StandardMetadata(BaseModel):
    """标准化的元数据Schema"""

    # ===== 必需字段 =====
    source: str = Field(..., description="文件路径或URL")

    # ===== 文档元数据 =====
    title: Optional[str] = Field(None, description="文档标题")
    author: Optional[str] = Field(None, description="作者")
    created_date: Optional[str] = Field(None, description="创建日期(ISO 8601)")
    modified_date: Optional[str] = Field(None, description="修改日期(ISO 8601)")
    language: Optional[str] = Field("zh", description="语言代码")

    # ===== 结构元数据 =====
    page: Optional[int] = Field(None, description="页码")
    page_count: Optional[int] = Field(None, description="总页数")
    section: Optional[str] = Field(None, description="章节")
    heading: Optional[str] = Field(None, description="标题")

    # ===== 文件元数据 =====
    file_name: Optional[str] = Field(None, description="文件名")
    file_size: Optional[int] = Field(None, description="文件大小(字节)")
    file_type: Optional[str] = Field(None, description="文件类型")

    # ===== 语义元数据 =====
    keywords: Optional[List[str]] = Field(None, description="关键词")
    topics: Optional[List[str]] = Field(None, description="主题")
    word_count: Optional[int] = Field(None, description="词数")

    # ===== 业务元数据 =====
    department: Optional[str] = Field(None, description="部门")
    category: Optional[str] = Field(None, description="分类")
    tags: Optional[List[str]] = Field(None, description="标签")
    access_level: Optional[str] = Field("public", description="访问级别")

    # ===== 系统元数据 =====
    loaded_at: Optional[str] = Field(None, description="加载时间(ISO 8601)")
    loader_version: Optional[str] = Field(None, description="加载器版本")

    class Config:
        json_schema_extra = {
            "example": {
                "source": "reports/rag_guide.pdf",
                "title": "RAG技术指南",
                "author": "张三",
                "created_date": "2025-01-15T10:30:00",
                "page": 15,
                "page_count": 50,
                "department": "技术部",
                "category": "技术文档"
            }
        }
```

### 3.2 自定义元数据扩展

```python
from typing import Any, Dict

class ExtendableMetadata(StandardMetadata):
    """可扩展的元数据Schema"""

    # 允许额外字段
    extra_fields: Optional[Dict[str, Any]] = Field(
        None,
        description="自定义扩展字段"
    )

    class Config:
        extra = "allow"  # 允许额外字段

# 使用示例
metadata = ExtendableMetadata(
    source="report.pdf",
    title="技术报告",

    # 标准字段
    department="技术部",

    # 自定义字段
    project_id="PRJ-2025-001",
    review_status="approved",
    confidentiality_level="internal",

    # 或使用extra_fields
    extra_fields={
        "custom_field_1": "value1",
        "custom_field_2": 123
    }
)
```

---

## 4. 元数据提取最佳实践

### 4.1 完整的元数据提取流程

```python
from langchain.document_loaders import PyPDFLoader
from langchain.schema import Document
from typing import List
import os
from datetime import datetime

class MetadataEnhancedLoader:
    """增强的文档加载器：完整的元数据提取"""

    def __init__(self, file_path: str, custom_metadata: dict = None):
        self.file_path = file_path
        self.custom_metadata = custom_metadata or {}

    def load(self) -> List[Document]:
        """加载文档并提取完整元数据"""

        # 1. 基础加载
        loader = PyPDFLoader(self.file_path)
        docs = loader.load()

        # 2. 提取各层元数据
        file_metadata = self._extract_file_metadata()
        document_metadata = self._extract_document_metadata()
        semantic_metadata = self._extract_semantic_metadata(docs)

        # 3. 合并元数据
        for i, doc in enumerate(docs):
            # 合并所有元数据
            doc.metadata.update(file_metadata)
            doc.metadata.update(document_metadata)

            # 添加页面特定的语义元数据
            if i < len(semantic_metadata):
                doc.metadata.update(semantic_metadata[i])

            # 添加自定义元数据
            doc.metadata.update(self.custom_metadata)

            # 添加加载时间戳
            doc.metadata['loaded_at'] = datetime.now().isoformat()

        return docs

    def _extract_file_metadata(self) -> dict:
        """提取文件系统元数据"""
        stat = os.stat(self.file_path)
        return {
            "file_name": os.path.basename(self.file_path),
            "file_path": self.file_path,
            "file_size": stat.st_size,
            "file_type": os.path.splitext(self.file_path)[1][1:],
            "created_date": datetime.fromtimestamp(stat.st_ctime).isoformat(),
            "modified_date": datetime.fromtimestamp(stat.st_mtime).isoformat()
        }

    def _extract_document_metadata(self) -> dict:
        """提取PDF内部元数据"""
        from pypdf import PdfReader

        try:
            reader = PdfReader(self.file_path)
            metadata = reader.metadata or {}

            return {
                "title": metadata.get("/Title", ""),
                "author": metadata.get("/Author", ""),
                "subject": metadata.get("/Subject", ""),
                "creator": metadata.get("/Creator", ""),
                "page_count": len(reader.pages)
            }
        except Exception as e:
            print(f"提取PDF元数据失败: {e}")
            return {}

    def _extract_semantic_metadata(self, docs: List[Document]) -> List[dict]:
        """提取语义元数据"""
        semantic_list = []

        for doc in docs:
            text = doc.page_content

            # 语言检测
            chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
            language = "zh" if chinese_chars / len(text) > 0.1 else "en"

            # 内容统计
            word_count = len(text.split())
            char_count = len(text)

            semantic_list.append({
                "language": language,
                "word_count": word_count,
                "char_count": char_count
            })

        return semantic_list

# 使用示例
loader = MetadataEnhancedLoader(
    "reports/rag_guide.pdf",
    custom_metadata={
        "department": "技术部",
        "category": "技术文档",
        "project_id": "PRJ-2025-001"
    }
)

docs = loader.load()

# 查看完整元数据
print(docs[0].metadata)
```

### 4.2 元数据验证

```python
from pydantic import ValidationError

def validate_metadata(doc: Document) -> bool:
    """验证文档元数据是否符合Schema"""
    try:
        # 使用Pydantic验证
        StandardMetadata(**doc.metadata)
        return True
    except ValidationError as e:
        print(f"元数据验证失败: {e}")
        return False

# 批量验证
def validate_documents(docs: List[Document]) -> tuple:
    """验证文档列表的元数据"""
    valid_docs = []
    invalid_docs = []

    for doc in docs:
        if validate_metadata(doc):
            valid_docs.append(doc)
        else:
            invalid_docs.append(doc)

    return valid_docs, invalid_docs

# 使用示例
valid, invalid = validate_documents(docs)
print(f"有效文档: {len(valid)}, 无效文档: {len(invalid)}")
```

---

## 5. 元数据在RAG全流程中的传递

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# ===== 1. 加载阶段：提取元数据 =====
loader = MetadataEnhancedLoader(
    "reports/rag_guide.pdf",
    custom_metadata={"department": "技术部"}
)
documents = loader.load()
print(f"加载了 {len(documents)} 个文档，每个都包含完整元数据")

# ===== 2. 分块阶段：保留元数据 =====
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = splitter.split_documents(documents)

# 验证：分块后元数据仍然保留
print(f"分块后得到 {len(chunks)} 个文本块")
print(f"第一个chunk的元数据: {chunks[0].metadata}")

# ===== 3. 向量化阶段：元数据存入向量库 =====
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    chunks,
    embeddings,
    persist_directory="./chroma_db"
)
print("元数据已存入向量库")

# ===== 4. 检索阶段：使用元数据过滤 =====
results = vectorstore.similarity_search(
    "RAG核心流程",
    k=3,
    filter={"department": "技术部"}  # 使用元数据过滤
)
print(f"检索到 {len(results)} 个相关文档")

# ===== 5. 生成阶段：引用元数据 =====
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

result = qa_chain({"query": "RAG的核心流程是什么？"})

# 显示答案和来源（使用元数据）
print(f"\n答案: {result['result']}\n")
print("来源:")
for doc in result['source_documents']:
    meta = doc.metadata
    print(f"- {meta.get('title', '未知')} (作者: {meta.get('author', '未知')})")
    print(f"  来源: {meta.get('source')} (第{meta.get('page')}页)")
```

---

## 总结

**元数据提取与保留的核心价值：**

1. **检索增强**：通过元数据过滤和排序提高检索质量
2. **可追溯性**：提供答案的来源和上下文
3. **多租户支持**：基于部门、项目等元数据实现权限控制
4. **质量保障**：通过元数据验证确保数据完整性
5. **业务集成**：元数据连接RAG系统与业务系统

**在RAG中的作用：**
- 提高检索的精准度和相关性
- 增强答案的可信度和可解释性
- 支持复杂的业务场景（多租户、权限控制）
- 为系统监控和优化提供数据支持

---

## 参考来源

> **参考来源：**
> - [LangChain Document Schema](https://python.langchain.com/docs/modules/data_connection/document_loaders/#document) - Document对象规范 (2025)
> - [Chroma Metadata Filtering](https://docs.trychroma.com/usage-guide#filtering-by-metadata) - 元数据过滤 (2025)
> - [RAG Metadata Best Practices](https://www.pinecone.io/learn/rag-metadata/) - 元数据最佳实践 (2025)

---

**版本：** v1.0
**最后更新：** 2026-02-15
**下一步：** 阅读 [03_核心概念_04_PDF解析技术.md](./03_核心概念_04_PDF解析技术.md)
