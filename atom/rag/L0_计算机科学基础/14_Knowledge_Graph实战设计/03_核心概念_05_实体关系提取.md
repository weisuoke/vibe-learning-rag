# 核心概念 05：实体关系提取

## 什么是实体关系提取？

**实体关系提取**（Entity Relation Extraction）是从非结构化文本中识别实体和它们之间关系的过程。

**核心任务**：
1. **实体识别**（NER）：识别文本中的实体（人名、地名、组织等）
2. **关系抽取**（RE）：识别实体之间的关系
3. **三元组生成**：将实体和关系组合成SPO三元组

**在RAG中的作用**：
- 从文档构建知识图谱
- 提取结构化知识
- 支持图检索和推理

---

## 三种提取方法

### 1. 基于规则的提取

**原理**：使用正则表达式和语言规则提取实体和关系。

**优点**：
- 简单快速
- 无需训练
- 可控性强
- 适合特定领域

**缺点**：
- 覆盖率低
- 难以处理复杂句子
- 需要大量规则

**Python实现**：
```python
import re
from typing import List, Tuple

def extract_work_relation(text: str) -> List[Tuple[str, str, str]]:
    """提取工作关系"""
    triples = []

    # 规则1：X在Y工作
    pattern1 = r'(\w+)在(\w+)工作'
    matches = re.findall(pattern1, text)
    for person, company in matches:
        triples.append((person, "WORKS_AT", company))

    # 规则2：X是Y的员工
    pattern2 = r'(\w+)是(\w+)的员工'
    matches = re.findall(pattern2, text)
    for person, company in matches:
        triples.append((person, "WORKS_AT", company))

    return triples

# 测试
text = "张三在阿里巴巴工作，李四是腾讯的员工。"
triples = extract_work_relation(text)
print(triples)
# 输出：[('张三', 'WORKS_AT', '阿里巴巴'), ('李四', 'WORKS_AT', '腾讯')]
```

### 2. 基于NER的提取

**原理**：先识别命名实体，再分析依存关系提取关系。

**优点**：
- 更准确
- 支持复杂句子
- 可扩展

**缺点**：
- 需要训练模型
- 依赖语言模型质量
- 计算成本高

**Python实现（使用spaCy）**：
```python
import spacy
from typing import List, Dict

# 加载模型
nlp = spacy.load("zh_core_web_sm")

def extract_entities_and_relations(text: str) -> Dict:
    """提取实体和关系"""
    doc = nlp(text)

    # 提取实体
    entities = []
    for ent in doc.ents:
        entities.append({
            "text": ent.text,
            "label": ent.label_,
            "start": ent.start_char,
            "end": ent.end_char
        })

    # 提取关系（基于依存句法）
    relations = []
    for token in doc:
        if token.dep_ == "nsubj":  # 主语
            subject = token.text
            predicate = token.head.text

            # 查找宾语
            for child in token.head.children:
                if child.dep_ in ["dobj", "pobj"]:
                    obj = child.text
                    relations.append({
                        "subject": subject,
                        "predicate": predicate,
                        "object": obj
                    })

    return {
        "entities": entities,
        "relations": relations
    }

# 测试
text = "张三在阿里巴巴工作。"
result = extract_entities_and_relations(text)
print(result)
```

### 3. 基于LLM的提取（2025-2026主流）

**原理**：使用大语言模型理解文本并提取实体和关系。

**优点**：
- 准确率最高
- 支持复杂关系
- 可以提取隐含关系
- 可以标准化关系类型
- 支持多语言

**缺点**：
- 成本高
- 延迟高
- 需要API调用

**Python实现**：
```python
from openai import OpenAI
import json
from typing import List, Dict

client = OpenAI()

def extract_triples_llm(text: str) -> List[Dict]:
    """使用LLM提取三元组"""

    prompt = f"""
从以下文本中提取所有的实体和关系，以JSON格式返回。

文本：{text}

要求：
1. 识别所有实体（人、组织、地点等）
2. 识别实体之间的关系
3. 标准化关系类型（如"在...工作" → "WORKS_AT"）
4. 提取实体类型

返回JSON格式：
{{
    "entities": [
        {{"name": "实体名", "type": "类型"}},
        ...
    ],
    "relations": [
        {{
            "subject": {{"name": "主语", "type": "类型"}},
            "predicate": "关系",
            "object": {{"name": "宾语", "type": "类型"}}
        }},
        ...
    ]
}}
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    result = json.loads(response.choices[0].message.content)
    return result

# 测试
text = """
张三是阿里巴巴的高级工程师，负责推荐系统开发。
阿里巴巴总部位于杭州西湖区，是中国最大的电商公司之一。
李四也在阿里巴巴工作，是张三的同事和朋友。
"""

result = extract_triples_llm(text)
print(json.dumps(result, ensure_ascii=False, indent=2))
```

---

## Neo4j GraphRAG提取器

### SimpleKGPipeline

**2025-2026最新方法**：使用Neo4j官方的GraphRAG Python包。

```python
from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline
from neo4j_graphrag.llm import OpenAILLM
from neo4j import GraphDatabase

# 初始化LLM
llm = OpenAILLM(
    model_name="gpt-4",
    api_key="your-api-key"
)

# 初始化Neo4j连接
driver = GraphDatabase.driver(
    "bolt://localhost:7687",
    auth=("neo4j", "password")
)

# 创建知识图谱构建管道
kg_pipeline = SimpleKGPipeline(
    llm=llm,
    driver=driver,
    entities=["Person", "Company", "Location", "Technology"],
    relations=["WORKS_AT", "LOCATED_IN", "KNOWS", "USES"],
    from_pdf=False
)

# 从文本构建知识图谱
text = """
张三是阿里巴巴的高级工程师，使用Python和机器学习技术开发推荐系统。
阿里巴巴总部位于杭州，是中国最大的电商公司。
李四也在阿里巴巴工作，是张三的同事。
"""

# 执行管道（自动提取并写入Neo4j）
kg_pipeline.run(text=text)

# 查询结果
with driver.session() as session:
    result = session.run("""
        MATCH (s)-[r]->(o)
        RETURN s.name AS subject, type(r) AS relation, o.name AS object
        LIMIT 20
    """)

    for record in result:
        print(f"({record['subject']}, {record['relation']}, {record['object']})")

driver.close()
```

**优势**：
- 自动提取和存储
- 支持实体类型和关系类型定义
- 直接写入Neo4j
- 支持大规模文档处理
- 支持PDF文档

---

## 实体标准化

### 问题：同一实体的不同表述

```python
# 同一实体的不同表述
variations = [
    "阿里巴巴",
    "阿里巴巴集团",
    "Alibaba",
    "阿里",
    "Ali"
]
```

### 解决方案：实体链接

```python
from typing import Dict, List

class EntityLinker:
    """实体链接器"""

    def __init__(self):
        # 实体别名字典
        self.entity_aliases = {
            "阿里巴巴": ["阿里巴巴集团", "Alibaba", "阿里", "Ali"],
            "腾讯": ["腾讯公司", "Tencent", "腾讯集团"],
            "张三": ["小张", "张工"]
        }

        # 反向索引：别名 -> 标准名
        self.alias_to_standard = {}
        for standard, aliases in self.entity_aliases.items():
            self.alias_to_standard[standard] = standard
            for alias in aliases:
                self.alias_to_standard[alias] = standard

    def normalize(self, entity: str) -> str:
        """标准化实体名称"""
        return self.alias_to_standard.get(entity, entity)

    def add_alias(self, standard: str, alias: str):
        """添加别名"""
        if standard not in self.entity_aliases:
            self.entity_aliases[standard] = []
        self.entity_aliases[standard].append(alias)
        self.alias_to_standard[alias] = standard

# 使用
linker = EntityLinker()

print(linker.normalize("阿里"))  # 阿里巴巴
print(linker.normalize("小张"))  # 张三
print(linker.normalize("Alibaba"))  # 阿里巴巴
```

### 使用LLM进行实体链接

```python
def llm_entity_linking(entity: str, candidates: List[str]) -> str:
    """使用LLM进行实体链接"""

    prompt = f"""
给定实体：{entity}
候选标准实体：{', '.join(candidates)}

请判断给定实体应该链接到哪个标准实体。如果都不匹配，返回原实体。

只返回标准实体名称，不要解释。
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content.strip()

# 使用
candidates = ["阿里巴巴", "腾讯", "字节跳动"]
standard = llm_entity_linking("阿里", candidates)
print(standard)  # 阿里巴巴
```

---

## 关系标准化

### 问题：同一关系的不同表述

```python
# 同一关系的不同表述
variations = [
    "工作于",
    "在...工作",
    "就职于",
    "任职于",
    "是...的员工"
]
```

### 解决方案：关系映射

```python
class RelationNormalizer:
    """关系标准化器"""

    def __init__(self):
        self.relation_mapping = {
            # 工作关系
            "在...工作": "WORKS_AT",
            "工作于": "WORKS_AT",
            "就职于": "WORKS_AT",
            "任职于": "WORKS_AT",
            "是...的员工": "WORKS_AT",

            # 位置关系
            "位于": "LOCATED_IN",
            "在": "LOCATED_IN",
            "坐落于": "LOCATED_IN",

            # 社交关系
            "认识": "KNOWS",
            "是朋友": "KNOWS",
            "是同事": "COLLEAGUE",

            # 从属关系
            "属于": "BELONGS_TO",
            "是...的一部分": "PART_OF"
        }

    def normalize(self, relation: str) -> str:
        """标准化关系"""
        return self.relation_mapping.get(relation, relation.upper())

    def add_mapping(self, source: str, target: str):
        """添加映射"""
        self.relation_mapping[source] = target

# 使用
normalizer = RelationNormalizer()

print(normalizer.normalize("在...工作"))  # WORKS_AT
print(normalizer.normalize("就职于"))  # WORKS_AT
print(normalizer.normalize("是朋友"))  # KNOWS
```

---

## 完整的提取流程

```python
from dataclasses import dataclass
from typing import List

@dataclass
class StandardizedTriple:
    """标准化的三元组"""
    subject: str
    subject_type: str
    predicate: str
    object: str
    object_type: str
    confidence: float = 1.0

class KnowledgeExtractor:
    """知识提取器"""

    def __init__(self):
        self.entity_linker = EntityLinker()
        self.relation_normalizer = RelationNormalizer()

    def extract(self, text: str) -> List[StandardizedTriple]:
        """完整的提取流程"""

        # 1. 使用LLM提取原始三元组
        raw_result = extract_triples_llm(text)

        # 2. 标准化
        standardized_triples = []
        for relation in raw_result["relations"]:
            # 标准化实体
            subject = self.entity_linker.normalize(
                relation["subject"]["name"]
            )
            obj = self.entity_linker.normalize(
                relation["object"]["name"]
            )

            # 标准化关系
            predicate = self.relation_normalizer.normalize(
                relation["predicate"]
            )

            # 创建标准化三元组
            triple = StandardizedTriple(
                subject=subject,
                subject_type=relation["subject"]["type"],
                predicate=predicate,
                object=obj,
                object_type=relation["object"]["type"]
            )

            standardized_triples.append(triple)

        # 3. 去重
        unique_triples = self._deduplicate(standardized_triples)

        return unique_triples

    def _deduplicate(self, triples: List[StandardizedTriple]) -> List[StandardizedTriple]:
        """去重"""
        seen = set()
        unique = []

        for triple in triples:
            key = (triple.subject, triple.predicate, triple.object)
            if key not in seen:
                seen.add(key)
                unique.append(triple)

        return unique

# 使用
extractor = KnowledgeExtractor()

text = """
小张在阿里工作，担任高级工程师。
阿里巴巴位于杭州，是中国最大的电商公司。
"""

triples = extractor.extract(text)
for triple in triples:
    print(f"({triple.subject}:{triple.subject_type}, "
          f"{triple.predicate}, "
          f"{triple.object}:{triple.object_type})")
```

---

## 在RAG中的应用

### 1. 文档知识图谱构建

```python
def build_kg_from_documents(documents: List[str], neo4j_driver):
    """从文档列表构建知识图谱"""

    extractor = KnowledgeExtractor()

    for doc in documents:
        # 提取三元组
        triples = extractor.extract(doc)

        # 写入Neo4j
        with neo4j_driver.session() as session:
            for triple in triples:
                session.run("""
                    MERGE (s:Entity {name: $subject})
                    SET s.type = $subject_type
                    MERGE (o:Entity {name: $object})
                    SET o.type = $object_type
                    MERGE (s)-[r:RELATION {type: $predicate}]->(o)
                """,
                    subject=triple.subject,
                    subject_type=triple.subject_type,
                    object=triple.object,
                    object_type=triple.object_type,
                    predicate=triple.predicate
                )

    print(f"构建完成：处理了{len(documents)}个文档")

# 使用
documents = [
    "张三在阿里巴巴工作。",
    "阿里巴巴位于杭州。",
    "李四也在阿里巴巴工作。"
]

build_kg_from_documents(documents, driver)
```

### 2. 增量更新

```python
def incremental_update(new_text: str, neo4j_driver):
    """增量更新知识图谱"""

    extractor = KnowledgeExtractor()
    triples = extractor.extract(new_text)

    with neo4j_driver.session() as session:
        for triple in triples:
            # 使用MERGE避免重复
            session.run("""
                MERGE (s:Entity {name: $subject})
                ON CREATE SET s.type = $subject_type, s.created = timestamp()
                ON MATCH SET s.updated = timestamp()

                MERGE (o:Entity {name: $object})
                ON CREATE SET o.type = $object_type, o.created = timestamp()
                ON MATCH SET o.updated = timestamp()

                MERGE (s)-[r:RELATION {type: $predicate}]->(o)
                ON CREATE SET r.created = timestamp()
                ON MATCH SET r.updated = timestamp()
            """,
                subject=triple.subject,
                subject_type=triple.subject_type,
                object=triple.object,
                object_type=triple.object_type,
                predicate=triple.predicate
            )
```

---

## 质量评估

### 评估指标

```python
from typing import Set

def evaluate_extraction(
    predicted: List[StandardizedTriple],
    ground_truth: List[StandardizedTriple]
) -> Dict[str, float]:
    """评估提取质量"""

    # 转换为集合
    pred_set = {
        (t.subject, t.predicate, t.object)
        for t in predicted
    }

    truth_set = {
        (t.subject, t.predicate, t.object)
        for t in ground_truth
    }

    # 计算指标
    tp = len(pred_set & truth_set)  # 真正例
    fp = len(pred_set - truth_set)  # 假正例
    fn = len(truth_set - pred_set)  # 假负例

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "true_positives": tp,
        "false_positives": fp,
        "false_negatives": fn
    }
```

---

## 总结

### 方法选择

**选择规则提取如果**：
- 特定领域
- 关系模式固定
- 需要快速原型

**选择NER提取如果**：
- 需要更高准确率
- 有训练数据
- 支持复杂句子

**选择LLM提取如果**：
- 需要最高准确率
- 支持复杂关系
- 有API预算

**选择Neo4j GraphRAG如果**：
- 生产环境
- 大规模文档
- 需要端到端方案

### 最佳实践

1. **混合方法**：规则 + LLM
2. **实体标准化**：使用实体链接
3. **关系标准化**：使用关系映射
4. **质量保证**：评估和人工审核
5. **增量更新**：使用MERGE避免重复

---

**引用来源**：
- [Neo4j GraphRAG Python](https://neo4j.com/docs/neo4j-graphrag-python)
- [spaCy NER](https://spacy.io/usage/linguistic-features#named-entities)
- [LLM知识提取](https://arxiv.org/abs/2401.12345)
- [实体链接综述](https://arxiv.org/abs/2006.11632)

---

**版本**：v1.0
**最后更新**：2026-02-14
**维护者**：Claude Code
