# RAG检索链 - 反直觉点

> 3个最常见的误区，避免踩坑

---

## 误区1：检索的文档越多越好 ❌

### 为什么错？

**错误观点**："检索 Top-10 比 Top-3 更好，因为给 LLM 更多信息"

**正确理解**：
- 检索太多文档会引入**噪音**（不相关的信息）
- LLM 会被无关信息**干扰**，降低答案质量
- 浪费 **token**，增加成本和延迟

**实验证明**：
```python
# 实验：对比不同 k 值的效果
question = "Python 如何读取文件？"

# k=3：只检索最相关的 3 个文档
docs_k3 = retriever.search(question, k=3)
# 结果：3 个文档都是关于 Python 文件操作的

# k=20：检索 20 个文档
docs_k20 = retriever.search(question, k=20)
# 结果：前 3 个相关，后 17 个包含：
# - Java 文件操作
# - Python 网络编程
# - 文件系统原理
# → 这些无关信息会干扰 LLM
```

**实际效果对比**：
```
k=3 的答案：
"使用 open() 函数读取文件，例如：
with open('file.txt', 'r') as f:
    content = f.read()"
→ 准确、简洁

k=20 的答案：
"Python 和 Java 都可以读取文件。Python 使用 open()，
Java 使用 FileReader。另外，网络文件可以用 requests..."
→ 混乱、不聚焦
```

### 为什么人们容易这样错？

**心理原因**：
- 直觉认为"信息越多越好"
- 类比搜索引擎：Google 返回几百万结果
- 忽略了 LLM 的上下文理解能力有限

**关键区别**：
- **搜索引擎**：返回链接，人类自己筛选
- **RAG**：直接把文档给 LLM，LLM 必须处理所有内容

### 正确理解

**最佳实践**：
```python
# 根据问题复杂度调整 k
simple_question = "什么是 Python？"
k = 2  # 简单问题，少量文档即可

complex_question = "对比 Python、Java、Go 的性能和适用场景"
k = 6  # 复杂问题，需要更多文档

# 使用 ReRank 提升质量
candidates = retriever.search(question, k=20)  # 先检索多个候选
top_docs = rerank(candidates, k=3)  # 再精选最相关的 3 个
```

**经验法则**：
- 简单问答：k=2~3
- 复杂分析：k=5~8
- 超过 10 通常没必要

---

## 误区2：向量相似度高 = 语义相关 ❌

### 为什么错？

**错误观点**："向量相似度 0.95，说明两段文本高度相关"

**正确理解**：
- 向量相似度只是**语义相似**，不等于**问题相关**
- 可能检索到"相似但不相关"的文档

**实际案例**：
```python
question = "Python 如何读取文件？"

# 检索结果（按向量相似度排序）
doc1 = "Python 文件读取的 5 种方法：open(), pathlib, pandas..."
# 相似度：0.92，相关性：✅ 高度相关

doc2 = "Java 如何读取文件？使用 FileReader 和 BufferedReader..."
# 相似度：0.89，相关性：❌ 语言不对

doc3 = "Python 如何写入文件？使用 open() 的 'w' 模式..."
# 相似度：0.87，相关性：⚠️ 操作相反

# 问题：doc2 和 doc3 相似度很高，但不是用户想要的
```

**为什么会这样？**
- Embedding 模型捕捉**语义**（"文件操作"）
- 但不理解**意图**（"读取"vs"写入"，"Python"vs"Java"）

### 为什么人们容易这样错？

**心理原因**：
- 相似度是数字，看起来很"科学"
- 忽略了相似度只是一个**粗筛指标**

**类比误导**：
- 类比搜索引擎的"相关度评分"
- 但搜索引擎会考虑更多因素（关键词、链接、时效性）

### 正确理解

**解决方案1：混合检索**
```python
# 结合向量检索和关键词检索
vector_results = vector_search(question, k=10)
keyword_results = keyword_search(question, k=10)

# 融合结果
final_results = merge_results(vector_results, keyword_results)
```

**解决方案2：ReRank**
```python
# 用更精细的模型重新排序
candidates = vector_search(question, k=20)
reranked = rerank_model.rank(
    query=question,
    documents=candidates,
    # ReRank 模型会考虑：
    # - 关键词匹配
    # - 问题类型（what/how/why）
    # - 文档结构
)
```

**解决方案3：元数据过滤**
```python
# 先过滤元数据，再向量检索
results = vector_search(
    question,
    filter={
        "language": "Python",  # 只检索 Python 相关文档
        "type": "tutorial"     # 只检索教程类文档
    }
)
```

**最佳实践**：
```python
# 三步检索策略
# 1. 元数据过滤（快速排除无关文档）
filtered_docs = filter_by_metadata(all_docs, language="Python")

# 2. 向量检索（语义相似度）
candidates = vector_search(question, docs=filtered_docs, k=20)

# 3. ReRank（精细排序）
final_docs = rerank(candidates, k=3)
```

---

## 误区3：RAG 可以解决所有 LLM 问题 ❌

### 为什么错？

**错误观点**："有了 RAG，LLM 就不会出错了"

**正确理解**：
- RAG 只解决**知识过时**和**幻觉**问题
- 不能解决 LLM 的其他问题

**RAG 无法解决的问题**：

#### 问题1：推理能力不足
```python
# 文档内容
doc = "苹果 5 元/斤，香蕉 3 元/斤"

# 用户问题
question = "买 2 斤苹果和 3 斤香蕉，一共多少钱？"

# RAG 检索到文档，但 LLM 仍需要计算
# 如果 LLM 数学能力差，RAG 也帮不了
```

#### 问题2：实时数据
```python
# 用户问题
question = "现在北京的天气如何？"

# RAG 的问题：
# - 向量库里的天气数据是历史数据
# - 即使检索到"北京天气"文档，也是过时的

# 正确方案：调用天气 API（Tool/Function Calling）
```

#### 问题3：创意生成
```python
# 用户问题
question = "写一首关于春天的诗"

# RAG 的问题：
# - 检索到其他诗歌，反而限制创造力
# - 可能导致"抄袭"已有诗歌

# 正确方案：直接用 LLM 生成，不用 RAG
```

#### 问题4：多步推理
```python
# 用户问题
question = "我们公司 2023 年利润是多少？（营收 - 成本）"

# RAG 的问题：
# - 可能检索到"营收"文档和"成本"文档
# - 但 LLM 需要理解"利润 = 营收 - 成本"
# - 如果文档分散，LLM 可能无法正确计算

# 更好方案：结构化数据查询（SQL）+ LLM 解释
```

### 为什么人们容易这样错？

**心理原因**：
- RAG 是新技术，容易被"神化"
- 看到 RAG 解决了幻觉问题，就认为它能解决所有问题

**营销误导**：
- 很多文章过度宣传 RAG 的能力
- 忽略了 RAG 的局限性

### 正确理解

**RAG 的适用场景**：
```python
# ✅ 适合 RAG
questions = [
    "我们公司的休假政策是什么？",  # 知识检索
    "产品 X 的技术规格是什么？",    # 文档查询
    "如何配置 Y 功能？",            # 操作指南
]

# ❌ 不适合 RAG
questions = [
    "1+1 等于几？",                 # 常识，不需要检索
    "写一首诗",                     # 创意生成
    "现在几点？",                   # 实时数据
    "帮我计算 123 * 456",          # 计算任务
]
```

**技术选型决策树**：
```
用户问题
    ↓
需要外部知识？
    ├─ 否 → 直接用 LLM
    └─ 是 ↓
        需要实时数据？
            ├─ 是 → Tool/Function Calling
            └─ 否 ↓
                知识在文档中？
                    ├─ 是 → RAG
                    └─ 否 → 数据库查询 + LLM
```

**最佳实践：组合使用**
```python
# 根据问题类型选择策略
def answer_question(question: str):
    question_type = classify_question(question)

    if question_type == "knowledge":
        # 知识检索 → RAG
        return rag_answer(question)

    elif question_type == "realtime":
        # 实时数据 → API 调用
        data = call_api(question)
        return llm.generate(f"数据：{data}\n问题：{question}")

    elif question_type == "calculation":
        # 计算任务 → Tool
        result = calculator.compute(question)
        return f"计算结果：{result}"

    else:
        # 其他 → 直接用 LLM
        return llm.generate(question)
```

---

## 误区总结表

| 误区 | 错误观点 | 正确理解 | 解决方案 |
|------|----------|----------|----------|
| **检索越多越好** | k 越大越好 | 太多会引入噪音 | k=2~8，使用 ReRank |
| **相似度=相关性** | 相似度高就相关 | 相似≠相关 | 混合检索 + ReRank + 元数据过滤 |
| **RAG 万能** | RAG 解决所有问题 | RAG 只解决知识问题 | 根据问题类型选择技术 |

---

## 避坑检查清单

在实现 RAG 时，检查以下问题：

### 检索阶段
- [ ] k 值是否合理？（不要盲目设大）
- [ ] 是否使用了 ReRank？（提升精度）
- [ ] 是否过滤了元数据？（排除无关文档）
- [ ] 是否考虑了混合检索？（向量+关键词）

### 生成阶段
- [ ] Prompt 是否明确要求"基于文档回答"？
- [ ] 是否处理了"文档中没有答案"的情况？
- [ ] 是否追踪了引用来源？（可追溯性）

### 系统设计
- [ ] 是否评估了 RAG 是否适合这个问题？
- [ ] 是否考虑了其他技术方案？（Tool、SQL）
- [ ] 是否设计了降级策略？（RAG 失败时的备选方案）

---

## 实战建议

### 建议1：从小 k 开始，逐步调优
```python
# 不要一开始就用大 k
k = 3  # 先从 3 开始
results = retriever.search(question, k=k)

# 如果答案不够全面，再增加
if not is_answer_complete(results):
    k = 5
    results = retriever.search(question, k=k)
```

### 建议2：监控检索质量
```python
# 记录检索结果的相似度
for doc in results:
    print(f"文档：{doc.content[:50]}...")
    print(f"相似度：{doc.similarity_score}")
    print(f"是否相关：{is_relevant(doc, question)}")

# 如果相似度高但不相关，说明需要 ReRank
```

### 建议3：A/B 测试不同策略
```python
# 对比不同检索策略的效果
strategies = [
    {"name": "纯向量", "k": 3},
    {"name": "向量+ReRank", "k": 10, "rerank": True},
    {"name": "混合检索", "k": 5, "hybrid": True},
]

for strategy in strategies:
    results = test_strategy(strategy, test_questions)
    print(f"{strategy['name']} 准确率：{results['accuracy']}")
```

---

**下一步**：学习 [08_面试必问](./08_面试必问.md)，准备 RAG 相关面试。
