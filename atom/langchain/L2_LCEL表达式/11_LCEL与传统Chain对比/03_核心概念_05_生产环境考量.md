# 核心概念 05：生产环境考量

> 生产环境部署 LCEL 的最佳实践和关键考量

---

## 概述

将 LCEL 应用部署到生产环境需要考虑性能、成本、监控、安全等多个维度。本文提供生产环境部署的完整指南，包括基准测试、成本优化、监控方案、部署模式和安全实践。

---

## 1. 生产环境性能基准

### 1.1 关键性能指标（KPIs）

| 指标 | 目标值 | 测量方法 |
|------|--------|----------|
| **P50 延迟** | <1s | LangSmith追踪 |
| **P95 延迟** | <2s | LangSmith追踪 |
| **P99 延迟** | <3s | LangSmith追踪 |
| **吞吐量** | >100 req/s | 负载测试 |
| **错误率** | <0.1% | 监控系统 |
| **可用性** | >99.9% | 监控系统 |

### 1.2 性能测试

```python
import time
import statistics
from concurrent.futures import ThreadPoolExecutor

def benchmark_chain(chain, inputs, concurrency=10):
    """性能基准测试"""
    latencies = []

    def process_one(input_data):
        start = time.time()
        try:
            result = chain.invoke(input_data)
            latency = time.time() - start
            return latency, None
        except Exception as e:
            return None, str(e)

    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        results = list(executor.map(process_one, inputs))

    latencies = [r[0] for r in results if r[0] is not None]
    errors = [r[1] for r in results if r[1] is not None]

    return {
        "p50": statistics.median(latencies),
        "p95": statistics.quantiles(latencies, n=20)[18],
        "p99": statistics.quantiles(latencies, n=100)[98],
        "mean": statistics.mean(latencies),
        "throughput": len(inputs) / sum(latencies),
        "error_rate": len(errors) / len(inputs),
        "errors": errors
    }

# 使用
test_inputs = [{"text": f"测试{i}"} for i in range(100)]
metrics = benchmark_chain(chain, test_inputs, concurrency=10)
print(f"P50: {metrics['p50']:.2f}s")
print(f"P95: {metrics['p95']:.2f}s")
print(f"P99: {metrics['p99']:.2f}s")
print(f"吞吐量: {metrics['throughput']:.2f} req/s")
print(f"错误率: {metrics['error_rate']:.2%}")
```

---

## 2. 成本优化

### 2.1 Token 使用追踪

```python
from langchain_core.callbacks import BaseCallbackHandler

class CostTracker(BaseCallbackHandler):
    """成本追踪回调"""

    def __init__(self):
        self.total_tokens = 0
        self.total_cost = 0.0
        self.pricing = {
            "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
            "gpt-4o": {"input": 0.005, "output": 0.015}
        }

    def on_llm_end(self, response, **kwargs):
        if hasattr(response, 'llm_output') and response.llm_output:
            usage = response.llm_output.get('token_usage', {})
            model = response.llm_output.get('model_name', 'gpt-4o-mini')

            input_tokens = usage.get('prompt_tokens', 0)
            output_tokens = usage.get('completion_tokens', 0)

            if model in self.pricing:
                cost = (
                    input_tokens * self.pricing[model]["input"] / 1000 +
                    output_tokens * self.pricing[model]["output"] / 1000
                )
                self.total_tokens += input_tokens + output_tokens
                self.total_cost += cost

    def get_report(self):
        return {
            "total_tokens": self.total_tokens,
            "total_cost": self.total_cost,
            "cost_per_request": self.total_cost / max(1, self.request_count)
        }

# 使用
tracker = CostTracker()
result = chain.invoke(input, config={"callbacks": [tracker]})
print(f"成本: ${tracker.total_cost:.4f}")
```

### 2.2 批处理优化（langasync）

**2026年新工具：langasync**

```python
# 传统方式：实时调用，成本高
results = [chain.invoke(inp) for inp in inputs]

# langasync：批处理API，成本降低50%
from langasync import wrap_chain

async_chain = wrap_chain(chain, batch_size=10)
results = await async_chain.abatch(inputs)
```

**适用场景**：
- 批量评估和测试
- 数据标注任务
- 离线分析和报告生成

### 2.3 语义缓存

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# 启用缓存
set_llm_cache(InMemoryCache())

# 第一次调用：2.0s，成本 $0.01
result1 = chain.invoke(input)

# 第二次调用（缓存命中）：0.01s，成本 $0
result2 = chain.invoke(input)
```

---

## 3. 监控和可观测性

### 3.1 LangSmith 集成（推荐）

```python
import os
from langchain_core.runnables import RunnableConfig

# 配置 LangSmith
os.environ["LANGSMITH_API_KEY"] = "your_key"
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "production"

# 使用
config = RunnableConfig(
    tags=["production", "rag", "v2"],
    metadata={
        "user_id": "123",
        "environment": "prod",
        "version": "2.0"
    },
    run_name="rag_query"
)

result = chain.invoke(input, config=config)
```

**LangSmith 提供**：
- 完整的调用链路追踪
- 每个步骤的输入输出
- 执行时间和 token 使用
- 错误和异常追踪
- 成本分析

### 3.2 Prometheus 集成

```python
from prometheus_client import Counter, Histogram, start_http_server
from langchain_core.callbacks import BaseCallbackHandler

# 定义指标
request_counter = Counter('chain_requests_total', 'Total requests')
latency_histogram = Histogram('chain_latency_seconds', 'Request latency')
error_counter = Counter('chain_errors_total', 'Total errors')

class PrometheusHandler(BaseCallbackHandler):
    """Prometheus 监控回调"""

    def on_chain_start(self, serialized, inputs, **kwargs):
        request_counter.inc()
        self.start_time = time.time()

    def on_chain_end(self, outputs, **kwargs):
        latency = time.time() - self.start_time
        latency_histogram.observe(latency)

    def on_chain_error(self, error, **kwargs):
        error_counter.inc()

# 启动 Prometheus 服务器
start_http_server(8000)

# 使用
result = chain.invoke(input, config={"callbacks": [PrometheusHandler()]})
```

### 3.3 自定义日志

```python
import logging
from langchain_core.callbacks import BaseCallbackHandler

logger = logging.getLogger(__name__)

class LoggingHandler(BaseCallbackHandler):
    """自定义日志回调"""

    def on_chain_start(self, serialized, inputs, **kwargs):
        logger.info(f"Chain started: {kwargs.get('name')}")
        logger.debug(f"Inputs: {inputs}")

    def on_chain_end(self, outputs, **kwargs):
        logger.info(f"Chain completed: {kwargs.get('name')}")
        logger.debug(f"Outputs: {outputs}")

    def on_chain_error(self, error, **kwargs):
        logger.error(f"Chain error: {error}")

# 使用
result = chain.invoke(input, config={"callbacks": [LoggingHandler()]})
```

---

## 4. 部署模式

### 4.1 FastAPI 部署

```python
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import asyncio

app = FastAPI()

class QueryRequest(BaseModel):
    query: str
    stream: bool = False

@app.post("/query")
async def query(request: QueryRequest):
    """查询端点"""
    try:
        if request.stream:
            # 流式响应
            async def generate():
                async for chunk in chain.astream({"query": request.query}):
                    yield f"data: {chunk}\n\n"

            return StreamingResponse(generate(), media_type="text/event-stream")
        else:
            # 批量响应
            result = await chain.ainvoke({"query": request.query})
            return {"result": result}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """健康检查"""
    return {"status": "healthy"}

# 运行
# uvicorn main:app --host 0.0.0.0 --port 8000
```

### 4.2 Docker 部署

```dockerfile
# Dockerfile
FROM python:3.13-slim

WORKDIR /app

# 安装依赖
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制代码
COPY . .

# 暴露端口
EXPOSE 8000

# 启动服务
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
# 构建镜像
docker build -t lcel-app:latest .

# 运行容器
docker run -d \
  -p 8000:8000 \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  -e LANGSMITH_API_KEY=$LANGSMITH_API_KEY \
  --name lcel-app \
  lcel-app:latest
```

### 4.3 Kubernetes 部署

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lcel-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: lcel-app
  template:
    metadata:
      labels:
        app: lcel-app
    spec:
      containers:
      - name: lcel-app
        image: lcel-app:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-keys
              key: openai
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: lcel-app
spec:
  selector:
    app: lcel-app
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

---

## 5. 安全考量

### 5.1 API 密钥管理

```python
import os
from dotenv import load_dotenv

# 从环境变量加载
load_dotenv()

# 不要硬编码密钥
# ❌ api_key = "sk-..."

# ✅ 从环境变量读取
api_key = os.getenv("OPENAI_API_KEY")

# ✅ 使用秘密管理服务（生产环境）
# from aws_secretsmanager import get_secret
# api_key = get_secret("openai_api_key")
```

### 5.2 输入验证

```python
from pydantic import BaseModel, validator

class QueryRequest(BaseModel):
    query: str

    @validator('query')
    def validate_query(cls, v):
        if len(v) > 10000:
            raise ValueError("Query too long")
        if not v.strip():
            raise ValueError("Query cannot be empty")
        return v
```

### 5.3 速率限制

```python
from fastapi import FastAPI, Request
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app = FastAPI()
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/query")
@limiter.limit("10/minute")
async def query(request: Request, query_request: QueryRequest):
    """限制每分钟10次请求"""
    result = await chain.ainvoke({"query": query_request.query})
    return {"result": result}
```

### 5.4 2025年安全修复

**CVE-2025-68664：序列化注入**

```python
from langchain.load import load

# ❌ 不安全（旧版本）
chain = load(data)

# ✅ 安全（2025+）
chain = load(data, allowed_objects=['core'])
```

---

## 6. 生产环境检查清单

### 6.1 性能

- [ ] P95 延迟 <2s
- [ ] 吞吐量 >100 req/s
- [ ] 错误率 <0.1%
- [ ] 启用缓存
- [ ] 使用批处理（适用场景）

### 6.2 成本

- [ ] Token 使用追踪
- [ ] 成本告警设置
- [ ] 使用 langasync（批处理场景）
- [ ] 语义缓存启用

### 6.3 监控

- [ ] LangSmith 集成
- [ ] Prometheus 指标
- [ ] 日志记录
- [ ] 告警配置

### 6.4 部署

- [ ] Docker 容器化
- [ ] 健康检查端点
- [ ] 资源限制配置
- [ ] 自动扩缩容

### 6.5 安全

- [ ] API 密钥安全存储
- [ ] 输入验证
- [ ] 速率限制
- [ ] 使用最新版本（安全修复）

---

## 7. 实战案例

### 7.1 案例：金融服务公司生产部署

**背景**：
- 内部知识库问答系统
- 1000+ 用户
- 高可用性要求（99.9%）

**架构**：
- FastAPI + LCEL
- Docker + Kubernetes
- LangSmith 监控
- Prometheus + Grafana

**性能指标**：
- P95 延迟：1.2s
- 吞吐量：150 req/s
- 可用性：99.95%
- 成本：$4.2M/年 → $2.8M/年（节省 33%）

**关键优化**：
1. 并行检索多个数据源
2. 语义缓存（命中率 40%）
3. 使用 langasync 批处理评估
4. 动态模型选择（简单问题用 gpt-4o-mini）

---

## 8. 总结

### 8.1 核心原则

1. **性能优先**：P95 延迟 <2s
2. **成本可控**：Token 追踪、缓存、批处理
3. **可观测性**：LangSmith + Prometheus
4. **高可用性**：健康检查、自动扩缩容
5. **安全第一**：密钥管理、输入验证、速率限制

### 8.2 推荐工具栈

- **框架**：LCEL
- **API 服务**：FastAPI
- **容器化**：Docker
- **编排**：Kubernetes
- **监控**：LangSmith + Prometheus
- **成本优化**：langasync

### 8.3 持续优化

1. **监控指标**：持续追踪性能和成本
2. **A/B 测试**：测试不同优化策略
3. **用户反馈**：收集用户体验数据
4. **技术更新**：关注 2025-2026 年新特性

---

**下一步**：阅读 `07_实战代码_01_基础迁移模式.md` 开始实战练习
