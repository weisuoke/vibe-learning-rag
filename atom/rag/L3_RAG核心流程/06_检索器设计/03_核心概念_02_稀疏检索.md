# 核心概念2：稀疏检索 (Sparse Retrieval)

> 基于词法匹配的检索方式，精准捕捉关键术语的经典技术

---

## 一句话定义

**稀疏检索是基于词频统计的检索方法，通过BM25等算法计算查询词与文档的词法匹配度，擅长精准匹配关键术语、专有名词和产品型号。**

---

## 核心原理

### 什么是"稀疏"？

**稀疏（Sparse）向量特征**：

```python
# 稀疏向量示例
# 假设词表大小为10000

# 文档："Python是编程语言"
sparse_vector = [0, 0, 0, ..., 1, 0, 0, ..., 1, 0, 0, ..., 1, 0, ...]
#                     位置3245:Python=1    位置7821:编程=1    位置9012:语言=1
# 只有3个位置有值，其余9997个位置都是0

# 稀疏度 = 非零元素数 / 总维度 = 3 / 10000 = 0.03%
```

**为什么叫"稀疏"？**
- 向量维度很高（10K-100K，等于词表大小）
- 大部分元素为0（只有出现的词对应位置为非零）
- 非零元素很少（稀疏度通常<1%）

**参考**: [Dense–Sparse Hybrid Retrieval - Emergent Mind 2025](https://www.emergentmind.com/topics/dense-sparse-hybrid-retrieval)

---

## BM25算法详解

### BM25公式

**BM25（Best Matching 25）是稀疏检索的标准算法**：

```
BM25(q, d) = Σ IDF(qi) × (f(qi, d) × (k1 + 1)) / (f(qi, d) + k1 × (1 - b + b × |d| / avgdl))

其中：
- q: 查询（包含多个词qi）
- d: 文档
- f(qi, d): 词qi在文档d中的词频
- |d|: 文档d的长度
- avgdl: 所有文档的平均长度
- k1: 词频饱和参数（通常1.2-2.0）
- b: 长度归一化参数（通常0.75）
- IDF(qi): 逆文档频率
```

**IDF（Inverse Document Frequency）**：

```
IDF(qi) = log((N - n(qi) + 0.5) / (n(qi) + 0.5))

其中：
- N: 文档总数
- n(qi): 包含词qi的文档数
```

**参考**: [Hybrid Dense-Sparse Retrieval for High-Recall 2026](https://www.researchgate.net/publication/399428523)

### BM25直觉理解

**三个核心思想**：

1. **词频（TF）**：词出现越多，越重要
   - 但有饱和效应：出现10次和100次差别不大

2. **逆文档频率（IDF）**：罕见词更重要
   - "Python"比"的"更有区分度

3. **长度归一化**：避免长文档占优
   - 长文档自然包含更多词，需要归一化

```python
"""
BM25直觉示例
"""
# 查询："Python教程"

# 文档1："Python编程教程"（短文档，精准匹配）
# - "Python"出现1次，IDF高（罕见词）
# - "教程"出现1次，IDF中等
# - 文档短，长度归一化影响小
# → BM25得分：高

# 文档2："这是一个很长的文档，讲了很多内容，包括Python、Java、C++等编程语言的教程..."（长文档）
# - "Python"出现1次，IDF高
# - "教程"出现1次，IDF中等
# - 文档长，长度归一化降低得分
# → BM25得分：中等

# 文档3："机器学习入门指南"（不匹配）
# - "Python"出现0次
# - "教程"出现0次
# → BM25得分：0
```

---

## 工作流程

### 1. 文档索引构建

```python
"""
BM25索引构建
"""
from rank_bm25 import BM25Okapi
import jieba  # 中文分词

# 文档列表
documents = [
    "Python是一种高级编程语言",
    "JavaScript用于Web开发",
    "机器学习是人工智能的子领域",
    "深度学习使用神经网络"
]

# 中文分词
tokenized_docs = [list(jieba.cut(doc)) for doc in documents]

print("=== 分词结果 ===")
for i, tokens in enumerate(tokenized_docs):
    print(f"文档{i+1}: {tokens}")

# 输出:
# 文档1: ['Python', '是', '一种', '高级', '编程', '语言']
# 文档2: ['JavaScript', '用于', 'Web', '开发']
# 文档3: ['机器', '学习', '是', '人工智能', '的', '子', '领域']
# 文档4: ['深度', '学习', '使用', '神经网络']

# 构建BM25索引
bm25 = BM25Okapi(tokenized_docs)

print("\n✅ BM25索引构建完成")
```

### 2. 查询处理

```python
"""
BM25查询
"""
# 用户查询
query = "如何学习编程"

# 查询分词
query_tokens = list(jieba.cut(query))
print(f"\n查询分词: {query_tokens}")
# 输出: ['如何', '学习', '编程']

# 计算BM25得分
scores = bm25.get_scores(query_tokens)

print("\n=== BM25得分 ===")
for i, (doc, score) in enumerate(zip(documents, scores)):
    print(f"{i+1}. [{score:.3f}] {doc}")

# 输出示例:
# 1. [2.145] Python是一种高级编程语言  ← "编程"匹配
# 2. [0.000] JavaScript用于Web开发
# 3. [1.234] 机器学习是人工智能的子领域  ← "学习"匹配
# 4. [1.234] 深度学习使用神经网络  ← "学习"匹配
```

### 3. Top-K检索

```python
"""
返回Top-K结果
"""
def bm25_search(bm25, query, documents, k=3):
    """
    BM25检索Top-K文档
    """
    # 查询分词
    query_tokens = list(jieba.cut(query))

    # 计算得分
    scores = bm25.get_scores(query_tokens)

    # 排序并返回Top-K
    top_k_indices = scores.argsort()[-k:][::-1]

    results = []
    for idx in top_k_indices:
        results.append({
            "doc": documents[idx],
            "score": scores[idx],
            "rank": len(results) + 1
        })

    return results

# 使用
query = "Python编程"
results = bm25_search(bm25, query, documents, k=3)

print(f"\n查询: {query}")
print("=== Top-3结果 ===")
for r in results:
    print(f"{r['rank']}. [{r['score']:.3f}] {r['doc']}")
```

---

## 核心优势

### 优势1：精准匹配关键术语

**对专有名词、产品型号等精准匹配**：

```python
"""
精准匹配示例
"""
# 查询: "iPhone 15 Pro Max 价格"
query_tokens = ["iPhone", "15", "Pro", "Max", "价格"]

documents = [
    "iPhone 15 Pro Max 官方售价9999元",  # ✅ 完全匹配
    "iPhone 15 系列对比",                 # ❌ 部分匹配
    "高端智能手机价格分析",               # ❌ 只匹配"价格"
    "iPhone 14 Pro Max 评测"             # ❌ 版本不对
]

# BM25会给第一个文档最高分
# 因为所有关键词都精准匹配
```

### 优势2：计算速度快

**无需深度学习模型，计算效率高**：

```python
"""
性能对比
"""
import time

# Sparse检索（BM25）
start = time.time()
scores = bm25.get_scores(query_tokens)
sparse_time = time.time() - start
print(f"Sparse检索耗时: {sparse_time:.4f}s")
# 输出: 0.0003s

# Dense检索（需要Embedding模型）
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')

start = time.time()
query_emb = model.encode([query])
dense_time = time.time() - start
print(f"Dense检索耗时: {dense_time:.4f}s")
# 输出: 0.0125s

print(f"Sparse是Dense的 {dense_time/sparse_time:.1f}倍快")
# 输出: Sparse是Dense的 41.7倍快
```

### 优势3：可解释性强

**可以清楚看到哪些词匹配了**：

```python
"""
可解释性示例
"""
def explain_bm25_score(bm25, query, doc_idx, documents):
    """
    解释BM25得分
    """
    query_tokens = list(jieba.cut(query))
    doc_tokens = list(jieba.cut(documents[doc_idx]))

    print(f"查询: {query}")
    print(f"文档: {documents[doc_idx]}")
    print("\n匹配词:")

    for token in query_tokens:
        if token in doc_tokens:
            # 计算该词的IDF
            doc_freq = sum(1 for doc in tokenized_docs if token in doc)
            idf = bm25.idf.get(token, 0)
            print(f"  ✅ '{token}' - IDF={idf:.3f} (出现在{doc_freq}个文档中)")
        else:
            print(f"  ❌ '{token}' - 未匹配")

# 使用
explain_bm25_score(bm25, "Python编程", 0, documents)

# 输出:
# 查询: Python编程
# 文档: Python是一种高级编程语言
#
# 匹配词:
#   ✅ 'Python' - IDF=1.386 (出现在1个文档中)
#   ✅ '编程' - IDF=1.386 (出现在1个文档中)
```

---

## 核心劣势

### 劣势1：无法理解语义

**不理解同义词和相关概念**：

```python
"""
语义理解问题
"""
# 查询: "便宜的手机"
query_tokens = ["便宜", "手机"]

documents = [
    "性价比高的智能机推荐",  # ❌ BM25无法匹配（没有"便宜"、"手机"）
    "iPhone 15 Pro Max",      # ❌ 无法匹配
    "经济实惠的通讯设备"      # ❌ 无法匹配（同义词）
]

# BM25得分都是0，因为没有词法匹配
# 但这些文档在语义上都相关
```

### 劣势2：对分词依赖强

**中文分词错误会影响检索效果**：

```python
"""
分词问题示例
"""
# 文档: "南京市长江大桥"

# 正确分词
tokens_correct = ["南京市", "长江大桥"]

# 错误分词
tokens_wrong = ["南京", "市长", "江大桥"]

# 查询: "长江大桥"
# 正确分词能匹配，错误分词无法匹配
```

### 劣势3：词袋模型，丢失语序

**无法理解词序和语法结构**：

```python
"""
词序问题
"""
# 查询: "Python比Java快"
query_tokens = ["Python", "比", "Java", "快"]

# 文档1: "Python比Java快"  ← 原意
# 文档2: "Java比Python快"  ← 相反意思

# BM25给两个文档相同的得分
# 因为都包含相同的词，只是顺序不同
```

---

## BM25参数调优

### 关键参数

```python
"""
BM25参数调优
"""
from rank_bm25 import BM25Okapi, BM25L, BM25Plus

# 1. BM25Okapi（标准版本，推荐）
bm25_okapi = BM25Okapi(
    tokenized_docs,
    k1=1.5,  # 词频饱和参数（1.2-2.0）
    b=0.75   # 长度归一化参数（0.0-1.0）
)

# 2. BM25Plus（改进版本，避免负分）
bm25_plus = BM25Plus(
    tokenized_docs,
    k1=1.5,
    b=0.75,
    delta=1.0  # 额外参数
)

# 3. BM25L（另一种改进版本）
bm25_l = BM25L(
    tokenized_docs,
    k1=1.5,
    b=0.75,
    delta=0.5
)
```

### 参数调优建议

| 参数 | 推荐值 | 说明 | 调优方向 |
|------|--------|------|----------|
| **k1** | 1.2-2.0 | 词频饱和参数 | 增大k1 → 词频影响更大 |
| **b** | 0.75 | 长度归一化 | 增大b → 长度影响更大 |

**调优实验**：

```python
"""
参数调优实验
"""
def tune_bm25_parameters(tokenized_docs, test_queries):
    """
    测试不同参数组合
    """
    k1_values = [1.2, 1.5, 2.0]
    b_values = [0.0, 0.5, 0.75, 1.0]

    best_score = 0
    best_params = {}

    for k1 in k1_values:
        for b in b_values:
            bm25 = BM25Okapi(tokenized_docs, k1=k1, b=b)

            # 评估
            score = evaluate_bm25(bm25, test_queries)

            print(f"k1={k1}, b={b} → Score={score:.3f}")

            if score > best_score:
                best_score = score
                best_params = {"k1": k1, "b": b}

    print(f"\n最佳参数: {best_params}")
    return best_params

# 使用
best_params = tune_bm25_parameters(tokenized_docs, test_queries)
```

**参考**: [Vector Similarity Search 2025: RAG Engineer's Guide](https://www.linkedin.com/pulse/vector-similarity-search-2025-rag-engineers-field-guide-gadiraju-gqucc)

---

## 在RAG中的应用

### 应用1：法律文档检索

```python
"""
法律文档检索（精准匹配重要）
"""
from rank_bm25 import BM25Okapi
import jieba

# 法律文档
legal_docs = [
    "中华人民共和国合同法第52条规定...",
    "中华人民共和国民法典第143条规定...",
    "最高人民法院关于合同纠纷的司法解释...",
]

# 分词
tokenized_docs = [list(jieba.cut(doc)) for doc in legal_docs]

# 构建BM25索引
bm25 = BM25Okapi(tokenized_docs)

# 查询："合同法第52条"
query = "合同法第52条"
query_tokens = list(jieba.cut(query))

# 检索
scores = bm25.get_scores(query_tokens)
top_idx = scores.argmax()

print(f"查询: {query}")
print(f"最相关文档: {legal_docs[top_idx]}")
# 输出: 中华人民共和国合同法第52条规定...
# ✅ 精准匹配"合同法"和"52条"
```

### 应用2：产品搜索

```python
"""
产品搜索（型号精准匹配）
"""
products = [
    "iPhone 15 Pro Max 256GB 深空黑",
    "iPhone 15 Pro 128GB 原色钛金属",
    "iPhone 15 Plus 256GB 粉色",
    "iPhone 14 Pro Max 512GB 暗紫色"
]

tokenized_products = [list(jieba.cut(p)) for p in products]
bm25 = BM25Okapi(tokenized_products)

# 查询："iPhone 15 Pro Max"
query = "iPhone 15 Pro Max"
query_tokens = list(jieba.cut(query))

scores = bm25.get_scores(query_tokens)
top_idx = scores.argmax()

print(f"查询: {query}")
print(f"最匹配产品: {products[top_idx]}")
# 输出: iPhone 15 Pro Max 256GB 深空黑
# ✅ 精准匹配型号
```

### 应用3：代码检索

```python
"""
代码检索（函数名精准匹配）
"""
code_snippets = [
    "def calculate_bm25_score(query, document): ...",
    "def compute_cosine_similarity(vec1, vec2): ...",
    "def bm25_ranking(queries, docs): ...",
    "class BM25Retriever: ..."
]

tokenized_code = [snippet.split() for snippet in code_snippets]
bm25 = BM25Okapi(tokenized_code)

# 查询："bm25"
query = "bm25"
query_tokens = query.split()

scores = bm25.get_scores(query_tokens)

print(f"查询: {query}")
print("=== 相关代码 ===")
for idx in scores.argsort()[-3:][::-1]:
    print(f"[{scores[idx]:.3f}] {code_snippets[idx]}")

# 输出:
# [2.145] def calculate_bm25_score(query, document): ...
# [2.145] def bm25_ranking(queries, docs): ...
# [1.386] class BM25Retriever: ...
# ✅ 精准匹配函数名中的"bm25"
```

---

## 2025-2026最佳实践

### 1. 使用rank-bm25库

```python
"""
rank-bm25是Python最流行的BM25实现
"""
# 安装
# pip install rank-bm25

from rank_bm25 import BM25Okapi

# 使用
bm25 = BM25Okapi(tokenized_docs)
scores = bm25.get_scores(query_tokens)

# 优点：
# - 简单易用
# - 性能优化
# - 支持多种BM25变体
```

### 2. 中文分词优化

```python
"""
中文分词最佳实践
"""
import jieba

# 1. 加载自定义词典
jieba.load_userdict("custom_dict.txt")
# 格式: 每行一个词，可选词频和词性
# 示例:
# RAG系统 100 n
# BM25算法 50 n

# 2. 添加动态词
jieba.add_word("检索器设计")
jieba.add_word("混合检索")

# 3. 使用精确模式（推荐）
text = "RAG系统使用混合检索"
tokens = jieba.lcut(text, cut_all=False)  # 精确模式
print(tokens)
# 输出: ['RAG系统', '使用', '混合检索']
```

### 3. 停用词过滤

```python
"""
停用词过滤
"""
# 停用词列表
stopwords = set(['的', '了', '在', '是', '我', '有', '和', '就', '不', '人', '都', '一', '一个', '上', '也', '很', '到', '说', '要', '去', '你', '会', '着', '没有', '看', '好', '自己', '这'])

def filter_stopwords(tokens):
    """过滤停用词"""
    return [t for t in tokens if t not in stopwords and len(t) > 1]

# 使用
text = "这是一个关于Python的教程"
tokens = list(jieba.cut(text))
filtered = filter_stopwords(tokens)

print(f"原始: {tokens}")
print(f"过滤后: {filtered}")
# 输出:
# 原始: ['这是', '一个', '关于', 'Python', '的', '教程']
# 过滤后: ['关于', 'Python', '教程']
```

### 4. 缓存BM25索引

```python
"""
缓存BM25索引
"""
import pickle

# 保存索引
with open('bm25_index.pkl', 'wb') as f:
    pickle.dump(bm25, f)

# 加载索引
with open('bm25_index.pkl', 'rb') as f:
    bm25 = pickle.load(f)

# 优点：
# - 避免重复构建索引
# - 加快启动速度
```

---

## 常见问题

### Q1: BM25和TF-IDF有什么区别？

**A**: BM25是TF-IDF的改进版本：
- TF-IDF：词频线性增长，长文档占优
- BM25：词频饱和效应，长度归一化
- BM25在实践中效果更好，是现代搜索引擎的标准

### Q2: 为什么不直接用Elasticsearch？

**A**:
- Elasticsearch内置BM25，适合大规模生产
- rank-bm25适合小规模、快速原型
- RAG系统通常文档量不大（<100万），rank-bm25足够

### Q3: 中文分词用什么工具？

**A**:
- jieba：最流行，速度快（推荐）
- pkuseg：准确率高，速度慢
- thulac：清华开源，平衡
- 通用场景用jieba即可

### Q4: BM25能处理多语言吗？

**A**: 可以，但需要对应的分词工具：
- 中文：jieba
- 英文：nltk, spacy
- 日文：MeCab
- 韩文：KoNLPy

---

## 总结

**Sparse检索的核心价值**：
1. ✅ 精准匹配：擅长专有名词、产品型号
2. ✅ 计算快速：无需深度学习模型
3. ✅ 可解释性：清楚看到匹配词

**核心局限**：
1. ❌ 无法理解语义：不识别同义词
2. ❌ 依赖分词：中文分词错误影响效果
3. ❌ 词袋模型：丢失语序信息

**2025-2026标准实践**：
- 不要只用Sparse检索，要结合Dense检索（混合检索）
- 使用rank-bm25库（简单高效）
- 优化中文分词（自定义词典+停用词）
- 缓存BM25索引（提升性能）

**下一步**：学习【核心概念3：混合检索】，理解如何结合Dense和Sparse的优势。
