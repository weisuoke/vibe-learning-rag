# 核心概念 05：流式响应架构

> **核心价值**: 流式响应架构通过 Event-based streaming 实现实时反馈，支持背压处理和错误恢复，是构建高性能、用户友好的 AI Agent 的关键机制。

---

## 概述

流式响应（Streaming Response）是 AI Agent 系统中的核心特性，它允许 Agent 在生成响应的过程中逐步向用户展示内容，而不是等待全部生成完成后一次性返回。

**核心优势**：
- **实时反馈**：用户可以立即看到 Agent 的响应，提升体验
- **降低延迟感知**：即使总时间相同，流式响应让用户感觉更快
- **可中断性**：用户可以在生成过程中中断（Steering message）
- **资源优化**：边生成边传输，减少内存占用

---

## 1. Event-based Streaming

### 1.1 定义

**Event-based Streaming** 是一种基于事件的流式传输模式，将响应过程分解为多个事件：

- **start 事件**：响应开始
- **delta 事件**：增量内容（逐字、逐句或逐块）
- **end 事件**：响应结束
- **error 事件**：错误发生

### 1.2 事件类型定义

```typescript
// 流式事件类型
export enum StreamEventType {
  START = 'start',
  DELTA = 'delta',
  END = 'end',
  ERROR = 'error',
  TOOL_CALL = 'tool-call',
  TOOL_RESULT = 'tool-result'
}

// 基础流式事件
export interface StreamEvent {
  type: StreamEventType;
  timestamp: number;
  metadata?: Record<string, any>;
}

// Start 事件
export interface StartEvent extends StreamEvent {
  type: StreamEventType.START;
  messageId: string;
}

// Delta 事件（增量内容）
export interface DeltaEvent extends StreamEvent {
  type: StreamEventType.DELTA;
  content: string;
  index?: number; // 内容索引
}

// End 事件
export interface EndEvent extends StreamEvent {
  type: StreamEventType.END;
  messageId: string;
  finishReason: 'stop' | 'length' | 'tool-calls' | 'interrupted';
}

// Error 事件
export interface ErrorEvent extends StreamEvent {
  type: StreamEventType.ERROR;
  error: {
    code: string;
    message: string;
    details?: any;
  };
}

// Tool Call 事件
export interface ToolCallEvent extends StreamEvent {
  type: StreamEventType.TOOL_CALL;
  toolCall: {
    id: string;
    name: string;
    args: Record<string, any>;
  };
}

// Tool Result 事件
export interface ToolResultEvent extends StreamEvent {
  type: StreamEventType.TOOL_RESULT;
  toolCallId: string;
  result: any;
}
```

### 1.3 流式响应流程

```typescript
/**
 * 流式响应处理器
 */
export class StreamingResponseHandler {
  private buffer: string = '';
  private events: StreamEvent[] = [];

  /**
   * 处理流式响应
   */
  async handleStream(stream: AsyncIterable<StreamEvent>): Promise<void> {
    try {
      // 1. 发送 start 事件
      await this.onStart();

      // 2. 处理流式事件
      for await (const event of stream) {
        this.events.push(event);

        switch (event.type) {
          case StreamEventType.DELTA:
            await this.onDelta(event as DeltaEvent);
            break;

          case StreamEventType.TOOL_CALL:
            await this.onToolCall(event as ToolCallEvent);
            break;

          case StreamEventType.TOOL_RESULT:
            await this.onToolResult(event as ToolResultEvent);
            break;

          case StreamEventType.ERROR:
            await this.onError(event as ErrorEvent);
            break;
        }
      }

      // 3. 发送 end 事件
      await this.onEnd();
    } catch (error) {
      await this.onError({
        type: StreamEventType.ERROR,
        timestamp: Date.now(),
        error: {
          code: 'STREAM_ERROR',
          message: error.message
        }
      });
    }
  }

  /**
   * 处理 start 事件
   */
  private async onStart(): Promise<void> {
    console.log('=== 开始生成响应 ===');
    this.buffer = '';
  }

  /**
   * 处理 delta 事件
   */
  private async onDelta(event: DeltaEvent): Promise<void> {
    this.buffer += event.content;
    process.stdout.write(event.content); // 实时显示
  }

  /**
   * 处理 tool call 事件
   */
  private async onToolCall(event: ToolCallEvent): Promise<void> {
    console.log(`\n[工具调用] ${event.toolCall.name}`);
  }

  /**
   * 处理 tool result 事件
   */
  private async onToolResult(event: ToolResultEvent): Promise<void> {
    console.log(`[工具结果] ${event.toolCallId}`);
  }

  /**
   * 处理 error 事件
   */
  private async onError(event: ErrorEvent): Promise<void> {
    console.error(`\n[错误] ${event.error.message}`);
  }

  /**
   * 处理 end 事件
   */
  private async onEnd(): Promise<void> {
    console.log('\n=== 响应完成 ===');
    console.log(`总字符数: ${this.buffer.length}`);
  }

  /**
   * 获取完整响应
   */
  getFullResponse(): string {
    return this.buffer;
  }
}
```

### 1.4 TypeScript/Node.js 类比

**类比：AsyncIterator**

```typescript
// Event-based streaming 类似于 AsyncIterator
async function* generateStream(): AsyncIterator<string> {
  yield 'Hello';
  yield ' ';
  yield 'World';
}

// 消费流
for await (const chunk of generateStream()) {
  process.stdout.write(chunk);
}
```

### 1.5 日常生活类比

**类比：视频流**

想象你在看视频：
- **start 事件** = 视频开始播放
- **delta 事件** = 逐帧显示视频内容
- **end 事件** = 视频播放完成
- **error 事件** = 播放出错（缓冲、网络问题）

---

## 2. 流式数据处理

### 2.1 流式生成器

```typescript
/**
 * LLM 流式生成器
 */
export async function* streamLLMResponse(
  prompt: string
): AsyncGenerator<StreamEvent> {
  // 1. 发送 start 事件
  yield {
    type: StreamEventType.START,
    timestamp: Date.now(),
    messageId: `msg-${Date.now()}`
  };

  // 2. 调用 LLM API（流式）
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`
    },
    body: JSON.stringify({
      model: 'gpt-4',
      messages: [{ role: 'user', content: prompt }],
      stream: true
    })
  });

  // 3. 处理流式响应
  const reader = response.body!.getReader();
  const decoder = new TextDecoder();

  try {
    while (true) {
      const { done, value } = await reader.read();

      if (done) break;

      // 解析 SSE 数据
      const chunk = decoder.decode(value);
      const lines = chunk.split('\n').filter(line => line.trim());

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);

          if (data === '[DONE]') {
            // 4. 发送 end 事件
            yield {
              type: StreamEventType.END,
              timestamp: Date.now(),
              messageId: `msg-${Date.now()}`,
              finishReason: 'stop'
            };
            return;
          }

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices[0]?.delta?.content;

            if (content) {
              // 发送 delta 事件
              yield {
                type: StreamEventType.DELTA,
                timestamp: Date.now(),
                content
              };
            }
          } catch (error) {
            console.error('Failed to parse chunk:', error);
          }
        }
      }
    }
  } catch (error) {
    // 发送 error 事件
    yield {
      type: StreamEventType.ERROR,
      timestamp: Date.now(),
      error: {
        code: 'STREAM_ERROR',
        message: error.message
      }
    };
  }
}
```

### 2.2 流式消费者

```typescript
/**
 * 消费流式响应
 */
async function consumeStream(prompt: string): Promise<string> {
  const handler = new StreamingResponseHandler();
  const stream = streamLLMResponse(prompt);

  await handler.handleStream(stream);

  return handler.getFullResponse();
}

// 使用示例
const response = await consumeStream('解释什么是流式响应');
console.log('完整响应:', response);
```

---

## 3. 背压处理（Backpressure）

### 3.1 定义

**背压（Backpressure）** 是指当生产者（LLM）生成数据的速度超过消费者（客户端）处理速度时，需要采取的流量控制机制。

**核心问题**：
- LLM 生成速度很快（每秒数千 tokens）
- 网络传输速度有限
- 客户端处理速度有限
- 如果不控制，会导致内存溢出或丢失数据

### 3.2 背压处理策略

```typescript
/**
 * 带背压处理的流式生成器
 */
export class BackpressureStream {
  private buffer: StreamEvent[] = [];
  private maxBufferSize: number = 100;
  private isPaused: boolean = false;

  /**
   * 生成流式事件（带背压控制）
   */
  async* generate(
    source: AsyncGenerator<StreamEvent>
  ): AsyncGenerator<StreamEvent> {
    for await (const event of source) {
      // 1. 检查缓冲区大小
      if (this.buffer.length >= this.maxBufferSize) {
        // 2. 暂停生产
        this.isPaused = true;
        await this.waitForDrain();
      }

      // 3. 添加到缓冲区
      this.buffer.push(event);

      // 4. 发送事件
      yield event;

      // 5. 从缓冲区移除
      this.buffer.shift();
    }
  }

  /**
   * 等待缓冲区排空
   */
  private async waitForDrain(): Promise<void> {
    while (this.buffer.length > this.maxBufferSize / 2) {
      await new Promise(resolve => setTimeout(resolve, 10));
    }
    this.isPaused = false;
  }

  /**
   * 获取缓冲区状态
   */
  getBufferStatus(): { size: number; isPaused: boolean } {
    return {
      size: this.buffer.length,
      isPaused: this.isPaused
    };
  }
}
```

### 3.3 TypeScript/Node.js 类比

**类比：Stream Backpressure**

```typescript
// Node.js Stream 的背压处理
const readStream = fs.createReadStream('large-file.txt');
const writeStream = fs.createWriteStream('output.txt');

readStream.on('data', (chunk) => {
  const canContinue = writeStream.write(chunk);

  if (!canContinue) {
    // 背压：暂停读取
    readStream.pause();
  }
});

writeStream.on('drain', () => {
  // 缓冲区排空：恢复读取
  readStream.resume();
});
```

### 3.4 日常生活类比

**类比：水管流量控制**

想象水管系统：
- **生产者** = 水龙头（LLM）
- **消费者** = 水桶（客户端）
- **背压** = 水桶快满时，关小水龙头

---

## 4. 错误恢复

### 4.1 错误类型

```typescript
// 流式响应错误类型
export enum StreamErrorType {
  NETWORK_ERROR = 'network_error',       // 网络错误
  TIMEOUT = 'timeout',                   // 超时
  RATE_LIMIT = 'rate_limit',             // 速率限制
  INVALID_RESPONSE = 'invalid_response', // 无效响应
  INTERRUPTED = 'interrupted'            // 用户中断
}

// 错误处理策略
export interface ErrorRecoveryStrategy {
  type: StreamErrorType;
  maxRetries: number;
  retryDelay: number;
  shouldRetry: (error: Error) => boolean;
}
```

### 4.2 错误恢复实现

```typescript
/**
 * 带错误恢复的流式生成器
 */
export class ResilientStreamGenerator {
  private retryStrategies: Map<StreamErrorType, ErrorRecoveryStrategy>;

  constructor() {
    this.retryStrategies = new Map([
      [StreamErrorType.NETWORK_ERROR, {
        type: StreamErrorType.NETWORK_ERROR,
        maxRetries: 3,
        retryDelay: 1000,
        shouldRetry: () => true
      }],
      [StreamErrorType.TIMEOUT, {
        type: StreamErrorType.TIMEOUT,
        maxRetries: 2,
        retryDelay: 2000,
        shouldRetry: () => true
      }],
      [StreamErrorType.RATE_LIMIT, {
        type: StreamErrorType.RATE_LIMIT,
        maxRetries: 5,
        retryDelay: 5000,
        shouldRetry: () => true
      }]
    ]);
  }

  /**
   * 生成流式响应（带错误恢复）
   */
  async* generateWithRecovery(
    prompt: string
  ): AsyncGenerator<StreamEvent> {
    let retries = 0;
    const maxRetries = 3;

    while (retries <= maxRetries) {
      try {
        // 尝试生成流式响应
        yield* streamLLMResponse(prompt);
        return; // 成功，退出
      } catch (error) {
        retries++;

        // 判断是否应该重试
        const errorType = this.classifyError(error);
        const strategy = this.retryStrategies.get(errorType);

        if (!strategy || retries > strategy.maxRetries) {
          // 不重试，发送错误事件
          yield {
            type: StreamEventType.ERROR,
            timestamp: Date.now(),
            error: {
              code: errorType,
              message: error.message,
              details: { retries }
            }
          };
          throw error;
        }

        // 等待后重试
        console.log(`重试 ${retries}/${strategy.maxRetries}...`);
        await new Promise(resolve =>
          setTimeout(resolve, strategy.retryDelay)
        );
      }
    }
  }

  /**
   * 分类错误类型
   */
  private classifyError(error: Error): StreamErrorType {
    if (error.message.includes('network')) {
      return StreamErrorType.NETWORK_ERROR;
    }
    if (error.message.includes('timeout')) {
      return StreamErrorType.TIMEOUT;
    }
    if (error.message.includes('rate limit')) {
      return StreamErrorType.RATE_LIMIT;
    }
    return StreamErrorType.INVALID_RESPONSE;
  }
}
```

---

## 5. 2025-2026 可恢复流机制

### 5.1 MCP 可恢复流提案

> **2025-2026 最新实践**: MCP 可恢复流提案支持长运行任务的流式传输，允许连接断开后从断点恢复。

**核心特性**：
```typescript
// 可恢复流接口
export interface ResumableStream {
  // 流 ID（唯一标识）
  streamId: string;

  // 当前位置（已传输的事件数量）
  position: number;

  // 检查点（定期保存状态）
  checkpoints: StreamCheckpoint[];

  // 恢复流（从指定位置继续）
  resume(position: number): AsyncGenerator<StreamEvent>;

  // 保存检查点
  saveCheckpoint(): Promise<void>;
}

// 流检查点
export interface StreamCheckpoint {
  position: number;
  timestamp: number;
  state: any; // 流状态快照
}
```

**实现示例**：
```typescript
/**
 * 可恢复流生成器
 */
export class ResumableStreamGenerator implements ResumableStream {
  streamId: string;
  position: number = 0;
  checkpoints: StreamCheckpoint[] = [];
  private events: StreamEvent[] = [];

  constructor(streamId: string) {
    this.streamId = streamId;
  }

  /**
   * 生成流式响应（可恢复）
   */
  async* generate(prompt: string): AsyncGenerator<StreamEvent> {
    // 1. 生成流式响应
    const source = streamLLMResponse(prompt);

    for await (const event of source) {
      // 2. 保存事件
      this.events.push(event);
      this.position++;

      // 3. 定期保存检查点
      if (this.position % 10 === 0) {
        await this.saveCheckpoint();
      }

      // 4. 发送事件
      yield event;
    }
  }

  /**
   * 恢复流（从指定位置继续）
   */
  async* resume(position: number): AsyncGenerator<StreamEvent> {
    // 1. 验证位置
    if (position < 0 || position > this.events.length) {
      throw new Error(`Invalid position: ${position}`);
    }

    // 2. 从指定位置继续发送事件
    for (let i = position; i < this.events.length; i++) {
      yield this.events[i];
    }

    // 3. 如果流还在生成，继续生成新事件
    // ...
  }

  /**
   * 保存检查点
   */
  async saveCheckpoint(): Promise<void> {
    const checkpoint: StreamCheckpoint = {
      position: this.position,
      timestamp: Date.now(),
      state: {
        eventsCount: this.events.length,
        lastEvent: this.events[this.events.length - 1]
      }
    };

    this.checkpoints.push(checkpoint);

    // 持久化到存储（如 Redis、数据库）
    await this.persistCheckpoint(checkpoint);
  }

  /**
   * 持久化检查点
   */
  private async persistCheckpoint(checkpoint: StreamCheckpoint): Promise<void> {
    // 保存到 Redis 或数据库
    console.log(`保存检查点: position=${checkpoint.position}`);
  }
}
```

**引用来源**：
- MCP Resumable Streams Proposal - Long-running tasks support
- https://github.com/modelcontextprotocol/proposals/resumable-streams

### 5.2 使用示例

```typescript
// 创建可恢复流
const stream = new ResumableStreamGenerator('stream-123');

// 开始生成
const generator = stream.generate('写一篇长文章');

try {
  for await (const event of generator) {
    console.log(event);
  }
} catch (error) {
  // 连接断开，保存当前位置
  const lastPosition = stream.position;

  // 稍后恢复
  setTimeout(async () => {
    console.log(`从位置 ${lastPosition} 恢复...`);
    const resumedGenerator = stream.resume(lastPosition);

    for await (const event of resumedGenerator) {
      console.log(event);
    }
  }, 5000);
}
```

---

## 6. 实际应用示例

### 6.1 pi-mono 中的流式响应

```typescript
// packages/pi-agent-core/src/streaming.ts

export class AgentStreamingResponse {
  private transport: SSETransport | WebSocketTransport;

  constructor(transport: SSETransport | WebSocketTransport) {
    this.transport = transport;
  }

  /**
   * 流式生成响应
   */
  async* stream(prompt: string): AsyncGenerator<StreamEvent> {
    // 1. 发送 start 事件
    yield { type: StreamEventType.START, timestamp: Date.now() };

    // 2. 调用 LLM
    const llmStream = await this.callLLM(prompt);

    // 3. 处理流式响应
    for await (const chunk of llmStream) {
      yield {
        type: StreamEventType.DELTA,
        timestamp: Date.now(),
        content: chunk
      };
    }

    // 4. 发送 end 事件
    yield {
      type: StreamEventType.END,
      timestamp: Date.now(),
      finishReason: 'stop'
    };
  }
}
```

---

## 7. 最佳实践

### 7.1 合理的缓冲区大小

```typescript
// 推荐：设置合理的缓冲区大小
const stream = new BackpressureStream();
stream.maxBufferSize = 100; // 100 个事件
```

### 7.2 错误处理和重试

```typescript
// 推荐：实现错误恢复机制
const resilientStream = new ResilientStreamGenerator();
const stream = resilientStream.generateWithRecovery(prompt);
```

### 7.3 定期保存检查点

```typescript
// 推荐：对于长运行任务，定期保存检查点
if (position % 10 === 0) {
  await stream.saveCheckpoint();
}
```

---

## 8. 总结

### 8.1 核心要点

1. **Event-based Streaming**：start、delta、end、error 事件
2. **背压处理**：控制生产和消费速度，避免内存溢出
3. **错误恢复**：重试机制、错误分类、恢复策略
4. **可恢复流**：2025-2026 新标准，支持断点恢复
5. **实时反馈**：提升用户体验，降低延迟感知

### 8.2 TypeScript/Node.js 类比

- **Event-based Streaming** = AsyncIterator
- **背压处理** = Stream Backpressure
- **可恢复流** = Resumable AsyncIterator

### 8.3 日常生活类比

- **Event-based Streaming** = 视频流（逐帧播放）
- **背压处理** = 水管流量控制
- **可恢复流** = 断点续传

### 8.4 2025-2026 最新实践

- **MCP 可恢复流提案**：支持长运行任务的断点恢复
- **检查点机制**：定期保存状态，允许从断点继续
- **错误恢复**：自动重试和恢复机制

### 8.5 学习检查

- [ ] 理解 Event-based Streaming 的事件类型
- [ ] 理解背压处理的原理和实现
- [ ] 理解错误恢复的策略
- [ ] 了解 2025-2026 可恢复流机制
- [ ] 能够实现流式响应处理器

### 8.6 下一步

- **03_核心概念_06_队列优化策略.md**：学习批处理和去重优化
- **07_实战代码_04_SSE流式实现.md**：手写 SSE 流式传输
- **07_实战代码_05_WebSocket流式实现.md**：手写 WebSocket 流式传输

---

**版本**: v1.0
**最后更新**: 2026-02-19
**维护者**: Claude Code
