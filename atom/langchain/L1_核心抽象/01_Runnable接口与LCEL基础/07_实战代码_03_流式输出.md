# å®æˆ˜ä»£ç  03: æµå¼è¾“å‡º

> æµå¼èŠå¤©åº”ç”¨ä¸ FastAPI é›†æˆå®æˆ˜

---

## ä»£ç æ¦‚è¿°

æœ¬ç¤ºä¾‹æ¼”ç¤ºï¼š
1. æµå¼èŠå¤©åº”ç”¨
2. FastAPI æµå¼æ¥å£
3. å‰ç«¯é›†æˆæ¨¡å¼
4. è¿›åº¦è¿½è¸ªå’Œå–æ¶ˆ
5. å¼‚æ­¥æµå¼å¤„ç†

---

## å®Œæ•´ä»£ç 

```python
"""
æµå¼è¾“å‡ºå®æˆ˜ç¤ºä¾‹
æ¼”ç¤ºï¼šæµå¼èŠå¤©åº”ç”¨ä¸ FastAPI é›†æˆ

Requirements:
- Python 3.13+
- langchain-core
- langchain-openai
- fastapi
- uvicorn
- python-dotenv
"""

import os
import asyncio
from typing import AsyncIterator
from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ===== 1. åŸºç¡€æµå¼è¾“å‡º =====
print("=" * 60)
print("1. åŸºç¡€æµå¼è¾“å‡º")
print("=" * 60)

# æ£€æŸ¥ API å¯†é’¥
if not os.getenv("OPENAI_API_KEY"):
    print("âš ï¸  è­¦å‘Š: æœªè®¾ç½® OPENAI_API_KEY")
    print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY")
    USE_LLM = False
else:
    USE_LLM = True

    # å®šä¹‰èŠå¤©é“¾
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"),
        ("user", "{message}")
    ])

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        streaming=True,  # å¯ç”¨æµå¼
        temperature=0.7
    )

    parser = StrOutputParser()

    chat_chain = prompt | llm | parser

    print("\nâœ“ èŠå¤©é“¾å·²åˆ›å»ºï¼ˆå¯ç”¨æµå¼ï¼‰")

# ===== 2. åŒæ­¥æµå¼è¾“å‡º =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("2. åŒæ­¥æµå¼è¾“å‡º")
    print("=" * 60)

    print("\nåŠ©æ‰‹: ", end="", flush=True)

    full_response = ""
    for chunk in chat_chain.stream({"message": "ä»‹ç»ä¸€ä¸‹ LangChain"}):
        print(chunk, end="", flush=True)
        full_response += chunk

    print("\n")
    print(f"å®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")

# ===== 3. å¼‚æ­¥æµå¼è¾“å‡º =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("3. å¼‚æ­¥æµå¼è¾“å‡º")
    print("=" * 60)

    async def async_stream_example():
        """å¼‚æ­¥æµå¼è¾“å‡ºç¤ºä¾‹"""
        print("\nåŠ©æ‰‹: ", end="", flush=True)

        full_response = ""
        async for chunk in chat_chain.astream({"message": "ä»€ä¹ˆæ˜¯ Runnableï¼Ÿ"}):
            print(chunk, end="", flush=True)
            full_response += chunk

        print("\n")
        return full_response

    # è¿è¡Œå¼‚æ­¥ç¤ºä¾‹
    response = asyncio.run(async_stream_example())
    print(f"å®Œæ•´å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")

# ===== 4. å¸¦è¿›åº¦è¿½è¸ªçš„æµå¼è¾“å‡º =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("4. å¸¦è¿›åº¦è¿½è¸ªçš„æµå¼è¾“å‡º")
    print("=" * 60)

    import time

    async def stream_with_progress():
        """å¸¦è¿›åº¦è¿½è¸ªçš„æµå¼è¾“å‡º"""
        print("\nåŠ©æ‰‹: ", end="", flush=True)

        chunk_count = 0
        char_count = 0
        start_time = time.time()
        first_chunk_time = None

        async for chunk in chat_chain.astream({"message": "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"}):
            if first_chunk_time is None:
                first_chunk_time = time.time()
                print(f"\n[é¦– token å»¶è¿Ÿ: {first_chunk_time - start_time:.2f}ç§’]\n")
                print("åŠ©æ‰‹: ", end="", flush=True)

            print(chunk, end="", flush=True)
            chunk_count += 1
            char_count += len(chunk)

        total_time = time.time() - start_time
        print("\n")
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  - æ€»å—æ•°: {chunk_count}")
        print(f"  - æ€»å­—ç¬¦æ•°: {char_count}")
        print(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"  - é¦– token å»¶è¿Ÿ: {first_chunk_time - start_time:.2f}ç§’")
        print(f"  - å¹³å‡é€Ÿåº¦: {char_count / total_time:.1f} å­—ç¬¦/ç§’")

    asyncio.run(stream_with_progress())

# ===== 5. FastAPI æµå¼æ¥å£ =====
print("\n" + "=" * 60)
print("5. FastAPI æµå¼æ¥å£")
print("=" * 60)

print("""
ä»¥ä¸‹æ˜¯ FastAPI æµå¼èŠå¤©æ¥å£çš„å®Œæ•´å®ç°ã€‚

ä¿å­˜ä¸º app.py å¹¶è¿è¡Œ:
  uvicorn app:app --reload

ç„¶åè®¿é—®:
  http://localhost:8000/docs
""")

# FastAPI åº”ç”¨ä»£ç 
fastapi_code = '''
"""
FastAPI æµå¼èŠå¤© API
è¿è¡Œ: uvicorn app:app --reload
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
import json
import os
from dotenv import load_dotenv

load_dotenv()

app = FastAPI(title="æµå¼èŠå¤© API", version="1.0.0")

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# å®šä¹‰èŠå¤©é“¾
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"),
    ("user", "{message}")
])

llm = ChatOpenAI(
    model="gpt-4o-mini",
    streaming=True,
    temperature=0.7
)

parser = StrOutputParser()
chat_chain = prompt | llm | parser

# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    message: str
    stream: bool = True

class ChatResponse(BaseModel):
    response: str

@app.post("/chat")
async def chat(request: ChatRequest):
    """
    èŠå¤©æ¥å£ï¼ˆæ”¯æŒæµå¼å’Œéæµå¼ï¼‰

    æµå¼å“åº”æ ¼å¼ï¼ˆSSEï¼‰:
    data: {"type": "chunk", "content": "æ–‡æœ¬å—"}
    data: {"type": "end"}

    éæµå¼å“åº”æ ¼å¼ï¼ˆJSONï¼‰:
    {"response": "å®Œæ•´å“åº”"}
    """
    try:
        if request.stream:
            # æµå¼å“åº”
            async def generate():
                try:
                    async for chunk in chat_chain.astream({"message": request.message}):
                        data = {
                            "type": "chunk",
                            "content": chunk
                        }
                        yield f"data: {json.dumps(data, ensure_ascii=False)}\\n\\n"

                    # å‘é€ç»“æŸæ ‡è®°
                    yield f"data: {json.dumps({'type': 'end'})}\\n\\n"

                except Exception as e:
                    error_data = {
                        "type": "error",
                        "message": str(e)
                    }
                    yield f"data: {json.dumps(error_data)}\\n\\n"

            return StreamingResponse(
                generate(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                }
            )
        else:
            # éæµå¼å“åº”
            result = await chat_chain.ainvoke({"message": request.message})
            return ChatResponse(response=result)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "model": "gpt-4o-mini"}

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "æµå¼èŠå¤© API",
        "docs": "/docs",
        "health": "/health"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
'''

print("\n" + "=" * 60)
print("FastAPI åº”ç”¨ä»£ç ")
print("=" * 60)
print(fastapi_code)

# ===== 6. å‰ç«¯é›†æˆç¤ºä¾‹ =====
print("\n" + "=" * 60)
print("6. å‰ç«¯é›†æˆç¤ºä¾‹")
print("=" * 60)

print("""
ä»¥ä¸‹æ˜¯å‰ç«¯é›†æˆæµå¼èŠå¤©çš„ç¤ºä¾‹ä»£ç ã€‚

ä¿å­˜ä¸º index.html å¹¶åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ã€‚
""")

html_code = '''
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æµå¼èŠå¤©æ¼”ç¤º</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        #chat-container {
            border: 1px solid #ccc;
            height: 400px;
            overflow-y: auto;
            padding: 20px;
            margin-bottom: 20px;
            background: #f9f9f9;
        }
        .message {
            margin-bottom: 15px;
            padding: 10px;
            border-radius: 5px;
        }
        .user-message {
            background: #e3f2fd;
            text-align: right;
        }
        .assistant-message {
            background: #fff;
            border: 1px solid #ddd;
        }
        #input-container {
            display: flex;
            gap: 10px;
        }
        #message-input {
            flex: 1;
            padding: 10px;
            font-size: 16px;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
            cursor: pointer;
        }
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
    </style>
</head>
<body>
    <h1>æµå¼èŠå¤©æ¼”ç¤º</h1>

    <div id="chat-container"></div>

    <div id="input-container">
        <input
            type="text"
            id="message-input"
            placeholder="è¾“å…¥æ¶ˆæ¯..."
            onkeypress="if(event.key==='Enter') sendMessage()"
        />
        <button onclick="sendMessage()" id="send-btn">å‘é€</button>
    </div>

    <script>
        const chatContainer = document.getElementById('chat-container');
        const messageInput = document.getElementById('message-input');
        const sendBtn = document.getElementById('send-btn');

        let currentEventSource = null;

        function addMessage(content, isUser) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${isUser ? 'user-message' : 'assistant-message'}`;
            messageDiv.textContent = content;
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
            return messageDiv;
        }

        function sendMessage() {
            const message = messageInput.value.trim();
            if (!message) return;

            // æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            addMessage(message, true);
            messageInput.value = '';

            // ç¦ç”¨è¾“å…¥
            sendBtn.disabled = true;
            messageInput.disabled = true;

            // åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å®¹å™¨
            const assistantDiv = addMessage('', false);

            // ä½¿ç”¨ EventSource æ¥æ”¶æµå¼å“åº”
            const eventSource = new EventSource(
                `http://localhost:8000/chat?message=${encodeURIComponent(message)}&stream=true`
            );

            currentEventSource = eventSource;

            eventSource.onmessage = (event) => {
                const data = JSON.parse(event.data);

                if (data.type === 'chunk') {
                    assistantDiv.textContent += data.content;
                    chatContainer.scrollTop = chatContainer.scrollHeight;
                } else if (data.type === 'end') {
                    eventSource.close();
                    currentEventSource = null;
                    sendBtn.disabled = false;
                    messageInput.disabled = false;
                    messageInput.focus();
                } else if (data.type === 'error') {
                    assistantDiv.textContent = `é”™è¯¯: ${data.message}`;
                    assistantDiv.style.color = 'red';
                    eventSource.close();
                    currentEventSource = null;
                    sendBtn.disabled = false;
                    messageInput.disabled = false;
                }
            };

            eventSource.onerror = () => {
                assistantDiv.textContent += '\\n[è¿æ¥é”™è¯¯]';
                eventSource.close();
                currentEventSource = null;
                sendBtn.disabled = false;
                messageInput.disabled = false;
            };
        }

        // é¡µé¢åŠ è½½æ—¶èšç„¦è¾“å…¥æ¡†
        messageInput.focus();
    </script>
</body>
</html>
'''

print("\n" + "=" * 60)
print("å‰ç«¯ HTML ä»£ç ")
print("=" * 60)
print(html_code)

# ===== 7. å¯å–æ¶ˆçš„æµå¼è¾“å‡º =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("7. å¯å–æ¶ˆçš„æµå¼è¾“å‡º")
    print("=" * 60)

    import signal

    class StreamCanceller:
        """æµå¼è¾“å‡ºå–æ¶ˆå™¨"""

        def __init__(self):
            self.cancelled = False

        def cancel(self):
            """å–æ¶ˆæµå¼è¾“å‡º"""
            self.cancelled = True

        async def stream_with_cancel(self, chain, input_data):
            """æ”¯æŒå–æ¶ˆçš„æµå¼è¾“å‡º"""
            print("\næç¤º: æŒ‰ Ctrl+C å¯ä»¥å–æ¶ˆè¾“å‡º\n")
            print("åŠ©æ‰‹: ", end="", flush=True)

            try:
                async for chunk in chain.astream(input_data):
                    if self.cancelled:
                        print("\n\nâš ï¸  æµå¼è¾“å‡ºå·²å–æ¶ˆ")
                        break
                    print(chunk, end="", flush=True)
                else:
                    print("\n\nâœ“ æµå¼è¾“å‡ºå®Œæˆ")

            except KeyboardInterrupt:
                print("\n\nâš ï¸  æµå¼è¾“å‡ºå·²ä¸­æ–­")
                self.cancelled = True

    # ä½¿ç”¨ç¤ºä¾‹
    canceller = StreamCanceller()

    # æ³¨å†Œä¿¡å·å¤„ç†
    def signal_handler(signum, frame):
        canceller.cancel()

    signal.signal(signal.SIGINT, signal_handler)

    print("\næ¼”ç¤ºå¯å–æ¶ˆçš„æµå¼è¾“å‡º:")
    asyncio.run(
        canceller.stream_with_cancel(
            chat_chain,
            {"message": "è¯¦ç»†ä»‹ç»ä¸€ä¸‹ Python çš„å†å²"}
        )
    )

# ===== 8. å¤šè½®å¯¹è¯æµå¼è¾“å‡º =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("8. å¤šè½®å¯¹è¯æµå¼è¾“å‡º")
    print("=" * 60)

    from langchain_core.messages import HumanMessage, AIMessage

    async def multi_turn_chat():
        """å¤šè½®å¯¹è¯ç¤ºä¾‹"""
        # å¸¦å†å²çš„æç¤ºè¯
        prompt_with_history = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"),
            ("placeholder", "{history}"),
            ("user", "{message}")
        ])

        chain_with_history = prompt_with_history | llm | parser

        # å¯¹è¯å†å²
        history = []

        # ç¬¬ä¸€è½®
        print("\nç”¨æˆ·: ä½ å¥½")
        print("åŠ©æ‰‹: ", end="", flush=True)

        response1 = ""
        async for chunk in chain_with_history.astream({
            "history": history,
            "message": "ä½ å¥½"
        }):
            print(chunk, end="", flush=True)
            response1 += chunk

        print("\n")

        # æ›´æ–°å†å²
        history.extend([
            HumanMessage(content="ä½ å¥½"),
            AIMessage(content=response1)
        ])

        # ç¬¬äºŒè½®
        print("ç”¨æˆ·: ä½ èƒ½åšä»€ä¹ˆï¼Ÿ")
        print("åŠ©æ‰‹: ", end="", flush=True)

        response2 = ""
        async for chunk in chain_with_history.astream({
            "history": history,
            "message": "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ"
        }):
            print(chunk, end="", flush=True)
            response2 += chunk

        print("\n")

    asyncio.run(multi_turn_chat())

# ===== 9. æ€§èƒ½å¯¹æ¯” =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("9. æ€§èƒ½å¯¹æ¯”ï¼šinvoke vs stream")
    print("=" * 60)

    import time

    async def compare_performance():
        """å¯¹æ¯” invoke å’Œ stream çš„æ€§èƒ½"""
        message = {"message": "ç”¨ä¸€å¥è¯ä»‹ç» LangChain"}

        # æµ‹è¯• invoke
        print("\næµ‹è¯• invokeï¼ˆéæµå¼ï¼‰:")
        start = time.time()
        result_invoke = await chat_chain.ainvoke(message)
        time_invoke = time.time() - start
        print(f"  å“åº”: {result_invoke}")
        print(f"  æ€»è€—æ—¶: {time_invoke:.2f}ç§’")

        # æµ‹è¯• stream
        print("\næµ‹è¯• streamï¼ˆæµå¼ï¼‰:")
        start = time.time()
        first_chunk_time = None
        result_stream = ""

        async for chunk in chat_chain.astream(message):
            if first_chunk_time is None:
                first_chunk_time = time.time()
            result_stream += chunk

        time_stream = time.time() - start

        print(f"  å“åº”: {result_stream}")
        print(f"  é¦– token å»¶è¿Ÿ: {first_chunk_time - start:.2f}ç§’")
        print(f"  æ€»è€—æ—¶: {time_stream:.2f}ç§’")

        # å¯¹æ¯”
        print(f"\næ€§èƒ½å¯¹æ¯”:")
        print(f"  invoke æ€»è€—æ—¶: {time_invoke:.2f}ç§’")
        print(f"  stream é¦– token: {first_chunk_time - start:.2f}ç§’")
        print(f"  stream æ€»è€—æ—¶: {time_stream:.2f}ç§’")
        print(f"  æ„ŸçŸ¥å»¶è¿Ÿé™ä½: {(1 - (first_chunk_time - start) / time_invoke) * 100:.1f}%")

    asyncio.run(compare_performance())

# ===== 10. æœ€ä½³å®è·µæ€»ç»“ =====
print("\n" + "=" * 60)
print("10. æœ€ä½³å®è·µæ€»ç»“")
print("=" * 60)

print("""
æµå¼è¾“å‡ºæœ€ä½³å®è·µ:

1. å¯ç”¨æµå¼
   âœ“ åˆ›å»º LLM æ—¶è®¾ç½® streaming=True
   âœ“ ä½¿ç”¨ astream è€Œé streamï¼ˆå¼‚æ­¥æ›´é«˜æ•ˆï¼‰

2. UI é›†æˆ
   âœ“ FastAPI + StreamingResponse
   âœ“ å‰ç«¯ EventSource æˆ– fetch ReadableStream
   âœ“ å®æ—¶æ˜¾ç¤ºï¼Œæå‡ç”¨æˆ·ä½“éªŒ

3. é”™è¯¯å¤„ç†
   âœ“ try-except æ•è·å¼‚å¸¸
   âœ“ å‘é€é”™è¯¯æ¶ˆæ¯ç»™å‰ç«¯
   âœ“ ä¼˜é›…å…³é—­è¿æ¥

4. æ€§èƒ½ä¼˜åŒ–
   âœ“ è¿½è¸ªé¦– token å»¶è¿Ÿ
   âœ“ ç›‘æ§æµå¼é€Ÿåº¦
   âœ“ ä½¿ç”¨å¼‚æ­¥æå‡å¹¶å‘

5. ç”¨æˆ·ä½“éªŒ
   âœ“ æ˜¾ç¤ºæ‰“å­—æ•ˆæœ
   âœ“ æ”¯æŒå–æ¶ˆæ“ä½œ
   âœ“ æä¾›è¿›åº¦åé¦ˆ

6. ç”Ÿäº§ç¯å¢ƒ
   âœ“ é…ç½® CORS
   âœ“ æ·»åŠ å¥åº·æ£€æŸ¥
   âœ“ å®ç°è¶…æ—¶æœºåˆ¶
   âœ“ æ—¥å¿—å’Œç›‘æ§
""")

# ===== æ€»ç»“ =====
print("\n" + "=" * 60)
print("æ€»ç»“")
print("=" * 60)

if USE_LLM:
    print("""
æœ¬ç¤ºä¾‹æ¼”ç¤ºäº†:

1. âœ“ åŸºç¡€æµå¼è¾“å‡ºï¼ˆåŒæ­¥å’Œå¼‚æ­¥ï¼‰
2. âœ“ è¿›åº¦è¿½è¸ªå’Œæ€§èƒ½ç»Ÿè®¡
3. âœ“ FastAPI æµå¼æ¥å£å®ç°
4. âœ“ å‰ç«¯é›†æˆï¼ˆEventSourceï¼‰
5. âœ“ å¯å–æ¶ˆçš„æµå¼è¾“å‡º
6. âœ“ å¤šè½®å¯¹è¯æµå¼
7. âœ“ æ€§èƒ½å¯¹æ¯”ï¼ˆinvoke vs streamï¼‰

å…³é”®è¦ç‚¹:
- stream é™ä½ 50-70% æ„ŸçŸ¥å»¶è¿Ÿ
- ä½¿ç”¨ astream æå‡æ€§èƒ½
- FastAPI + StreamingResponse å®ç° SSE
- å‰ç«¯ EventSource æ¥æ”¶æµå¼æ•°æ®
- æ”¯æŒå–æ¶ˆå’Œé”™è¯¯å¤„ç†

ä¸‹ä¸€æ­¥:
- éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
- é›†æˆ LangSmith ç›‘æ§
- ä¼˜åŒ–æµå¼æ€§èƒ½
- æ·»åŠ æ›´å¤šåŠŸèƒ½ï¼ˆå†å²è®°å½•ã€å¤šæ¨¡æ€ç­‰ï¼‰
""")
else:
    print("""
æœ¬ç¤ºä¾‹æ¼”ç¤ºäº†æµå¼è¾“å‡ºçš„æ¦‚å¿µå’Œæœ€ä½³å®è·µã€‚

è¦è¿è¡Œå®Œæ•´ç¤ºä¾‹ï¼Œè¯·:
1. è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡
2. é‡æ–°è¿è¡Œè„šæœ¬

å®Œæ•´æ–‡æ¡£åŒ…å«:
- FastAPI æµå¼æ¥å£ä»£ç 
- å‰ç«¯é›†æˆ HTML ç¤ºä¾‹
- å¯ç›´æ¥ä½¿ç”¨çš„ç”Ÿäº§ä»£ç 
""")
```

---

## ä½¿ç”¨è¯´æ˜

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
uv add langchain langchain-core langchain-openai fastapi uvicorn python-dotenv

# é…ç½® API å¯†é’¥
cp .env.example .env
# ç¼–è¾‘ .envï¼Œæ·»åŠ  OPENAI_API_KEY
```

### 2. è¿è¡ŒåŸºç¡€ç¤ºä¾‹

```bash
python 07_å®æˆ˜ä»£ç _03_æµå¼è¾“å‡º.py
```

### 3. è¿è¡Œ FastAPI åº”ç”¨

```bash
# ä¿å­˜ FastAPI ä»£ç ä¸º app.py
# è¿è¡ŒæœåŠ¡å™¨
uvicorn app:app --reload

# è®¿é—® API æ–‡æ¡£
open http://localhost:8000/docs
```

### 4. æµ‹è¯•å‰ç«¯é›†æˆ

```bash
# ä¿å­˜ HTML ä»£ç ä¸º index.html
# åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€
open index.html
```

---

## å­¦ä¹ è¦ç‚¹

### æ ¸å¿ƒæ¦‚å¿µ

1. **stream æ–¹æ³•**ï¼šé€å—è¾“å‡ºç»“æœ
2. **å¼‚æ­¥æµå¼**ï¼šastream æ›´é«˜æ•ˆ
3. **FastAPI é›†æˆ**ï¼šStreamingResponse + SSE
4. **å‰ç«¯é›†æˆ**ï¼šEventSource æ¥æ”¶æµå¼æ•°æ®
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šé™ä½æ„ŸçŸ¥å»¶è¿Ÿ

### æ€§èƒ½ä¼˜åŠ¿

- é¦– token å»¶è¿Ÿï¼š0.5-1 ç§’ï¼ˆvs 5-10 ç§’ï¼‰
- æ„ŸçŸ¥å»¶è¿Ÿé™ä½ï¼š50-70%
- ç”¨æˆ·ä½“éªŒï¼šå®æ—¶åé¦ˆ

### ç”Ÿäº§å®è·µ

- å¯ç”¨ streaming=True
- ä½¿ç”¨ astream å¼‚æ­¥æµå¼
- å®ç°é”™è¯¯å¤„ç†å’Œå–æ¶ˆ
- é…ç½® CORS å’Œå¥åº·æ£€æŸ¥
- é›†æˆç›‘æ§å’Œæ—¥å¿—

---

**å®Œæˆ**: æ‰€æœ‰å®æˆ˜ä»£ç ç¤ºä¾‹å·²å®Œæˆï¼ç»§ç»­å­¦ä¹  L2_LCELè¡¨è¾¾å¼ æ·±å…¥æŒæ¡ LangChainã€‚
