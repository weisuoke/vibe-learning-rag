# 核心概念2：数据流转机制

> 深入理解数据如何在管道中流转、转换和传递

---

## 概述

本文档深入探讨 LCEL 管道中的数据流转机制，包括：
- 输入输出类型匹配规则
- 数据转换和传递过程
- 上下文保持机制
- 错误传播和处理
- 流式 vs 批处理的数据流

---

## 1. 数据流转基础

### 1.1 基本流转模型

**核心原则**：前一个 Runnable 的输出自动成为后一个 Runnable 的输入

```python
# 简单示例
chain = step_a | step_b | step_c

# 数据流转
input_data → step_a → intermediate_1 → step_b → intermediate_2 → step_c → output_data
```

**可视化**：

```
┌─────────┐      ┌─────────┐      ┌─────────┐      ┌─────────┐
│  Input  │ ───> │ Step A  │ ───> │ Step B  │ ───> │ Step C  │ ───> Output
└─────────┘      └─────────┘      └─────────┘      └─────────┘
                      ↓                ↓                ↓
                 Output A         Output B         Output C
                 (Input B)        (Input C)
```

### 1.2 类型匹配规则

**规则1：输出类型必须匹配输入类型**

```python
from typing import TypedDict

class StepAOutput(TypedDict):
    result: str

class StepBInput(TypedDict):
    result: str  # 必须匹配 StepAOutput

# ✅ 类型匹配
step_a: Runnable[dict, StepAOutput] = ...
step_b: Runnable[StepBInput, str] = ...
chain = step_a | step_b  # 正常工作

# ❌ 类型不匹配
class StepCInput(TypedDict):
    data: str  # 不匹配 StepAOutput

step_c: Runnable[StepCInput, str] = ...
invalid_chain = step_a | step_c  # 运行时错误
```

**规则2：自动类型转换**

LangChain 会尝试自动转换某些类型：

```python
# str → ChatPromptValue
prompt = ChatPromptTemplate.from_template("...")
llm = ChatOpenAI()

# prompt 输出 ChatPromptValue
# llm 接受 str 或 ChatPromptValue
chain = prompt | llm  # 自动转换

# dict → ChatPromptValue
chain = {"question": "..."} | prompt | llm  # 自动转换
```

**规则3：严格模式 vs 宽松模式**

```python
# 宽松模式（默认）：尝试自动转换
chain = step_a | step_b  # 如果类型不完全匹配，尝试转换

# 严格模式：使用 TypedDict 强制类型检查
from typing import TypedDict

class StrictInput(TypedDict):
    field1: str
    field2: int

step: Runnable[StrictInput, str] = ...
# 必须提供完全匹配的输入
```

---

## 2. 数据转换过程

### 2.1 简单数据转换

**场景**：每个步骤转换数据格式

```python
from langchain_core.runnables import RunnableLambda

# 步骤1：字符串 → 字典
def string_to_dict(text: str) -> dict:
    return {"text": text, "length": len(text)}

# 步骤2：字典 → 列表
def dict_to_list(data: dict) -> list:
    return [data["text"], data["length"]]

# 步骤3：列表 → 字符串
def list_to_string(data: list) -> str:
    return f"Text: {data[0]}, Length: {data[1]}"

# 组合
chain = (
    RunnableLambda(string_to_dict)
    | RunnableLambda(dict_to_list)
    | RunnableLambda(list_to_string)
)

# 执行
result = chain.invoke("Hello")
print(result)  # "Text: Hello, Length: 5"

# 数据流转
# "Hello" → {"text": "Hello", "length": 5} → ["Hello", 5] → "Text: Hello, Length: 5"
```

### 2.2 复杂数据转换

**场景**：RAG 管道中的数据转换

```python
from typing import TypedDict, List

# 定义数据结构
class QueryInput(TypedDict):
    query: str

class RetrievalOutput(TypedDict):
    docs: List[str]
    scores: List[float]

class ContextOutput(TypedDict):
    context: str
    query: str

class PromptOutput(TypedDict):
    prompt: str

# 步骤1：查询 → 检索结果
def retrieve(input: QueryInput) -> RetrievalOutput:
    # 模拟检索
    return {
        "docs": ["Doc 1", "Doc 2"],
        "scores": [0.9, 0.8]
    }

# 步骤2：检索结果 → 格式化上下文
def format_context(input: RetrievalOutput) -> ContextOutput:
    context = "\n\n".join(input["docs"])
    return {
        "context": context,
        "query": "original query"  # 需要保持原始查询
    }

# 问题：如何在 format_context 中访问原始查询？
```

**解决方案：使用 RunnablePassthrough**

```python
from langchain_core.runnables import RunnablePassthrough, RunnableParallel

# 保持原始输入并添加检索结果
chain = (
    RunnablePassthrough.assign(
        retrieval=RunnableLambda(retrieve)
    )
    | RunnableLambda(format_context_with_query)
)

def format_context_with_query(input: dict) -> ContextOutput:
    # input 包含原始查询和检索结果
    return {
        "context": "\n\n".join(input["retrieval"]["docs"]),
        "query": input["query"]
    }
```

### 2.3 数据合并

**场景**：合并多个数据源

```python
from langchain_core.runnables import RunnableParallel

# 并行执行多个步骤
parallel = RunnableParallel({
    "search_results": search_tool,
    "user_profile": profile_loader,
    "context": context_retriever
})

# 输入
input_data = {"query": "What is AI?"}

# 输出（合并结果）
result = parallel.invoke(input_data)
# {
#   "search_results": [...],
#   "user_profile": {...},
#   "context": "..."
# }

# 后续步骤可以访问所有数据
chain = parallel | result_processor
```

---

## 3. 上下文保持机制

### 3.1 问题：数据丢失

**常见问题**：管道中间步骤丢失原始输入

```python
# 问题示例
chain = (
    embed_query        # 输入: {"query": "..."} → 输出: [0.1, 0.2, ...]
    | retrieve_docs    # 输入: [0.1, 0.2, ...] → 输出: ["doc1", "doc2"]
    | format_prompt    # 需要原始 query，但已经丢失！
)
```

### 3.2 解决方案1：RunnablePassthrough.assign()

**保持原始输入并添加新字段**

```python
from langchain_core.runnables import RunnablePassthrough

chain = (
    # 保持原始输入，添加 embedding 字段
    RunnablePassthrough.assign(
        embedding=embed_query
    )
    # 现在输入是: {"query": "...", "embedding": [0.1, 0.2, ...]}
    | RunnablePassthrough.assign(
        docs=retrieve_docs  # 使用 embedding 字段
    )
    # 现在输入是: {"query": "...", "embedding": [...], "docs": [...]}
    | format_prompt  # 可以访问 query 和 docs
)
```

**完整示例**：

```python
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

def embed_query(input: dict) -> list:
    """从输入中提取 query 并生成 embedding"""
    query = input["query"]
    # 模拟 embedding
    return [0.1, 0.2, 0.3]

def retrieve_docs(input: dict) -> list:
    """使用 embedding 检索文档"""
    embedding = input["embedding"]
    # 模拟检索
    return ["Doc 1", "Doc 2"]

def format_prompt(input: dict) -> str:
    """使用 query 和 docs 格式化 prompt"""
    query = input["query"]
    docs = input["docs"]
    return f"Context: {', '.join(docs)}\n\nQuestion: {query}"

# 组合
chain = (
    RunnablePassthrough.assign(embedding=RunnableLambda(embed_query))
    | RunnablePassthrough.assign(docs=RunnableLambda(retrieve_docs))
    | RunnableLambda(format_prompt)
)

# 执行
result = chain.invoke({"query": "What is AI?"})
print(result)
# Context: Doc 1, Doc 2
#
# Question: What is AI?
```

### 3.3 解决方案2：RunnableParallel

**并行保持和处理数据**

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

chain = (
    RunnableParallel({
        "original": RunnablePassthrough(),  # 保持原始输入
        "processed": processing_step        # 处理数据
    })
    | merger  # 合并原始和处理后的数据
)
```

### 3.4 解决方案3：自定义 Runnable

**完全控制数据流转**

```python
from langchain_core.runnables import Runnable

class ContextPreservingRunnable(Runnable):
    """保持上下文的 Runnable"""

    def __init__(self, processor):
        self.processor = processor

    def invoke(self, input, config=None):
        # 处理数据
        processed = self.processor(input)

        # 保持原始输入
        return {
            **input,  # 保留所有原始字段
            "processed": processed  # 添加处理结果
        }

# 使用
chain = (
    ContextPreservingRunnable(embed_query)
    | ContextPreservingRunnable(retrieve_docs)
    | format_prompt
)
```

---

## 4. 错误传播和处理

### 4.1 错误传播机制

**默认行为**：错误向上传播，中断整个管道

```python
chain = step_a | step_b | step_c

try:
    result = chain.invoke(input)
except Exception as e:
    print(f"Pipeline failed: {e}")
    # 无法知道哪个步骤失败
```

### 4.2 错误定位

**方法1：逐步测试**

```python
# 测试每个步骤
try:
    output_a = step_a.invoke(input)
    print(f"Step A output: {output_a}")

    output_b = step_b.invoke(output_a)
    print(f"Step B output: {output_b}")

    output_c = step_c.invoke(output_b)
    print(f"Step C output: {output_c}")
except Exception as e:
    print(f"Failed at current step: {e}")
```

**方法2：包装 Runnable**

```python
from langchain_core.runnables import Runnable

class ErrorHandlingRunnable(Runnable):
    """带错误处理的 Runnable 包装器"""

    def __init__(self, name: str, wrapped: Runnable):
        self.name = name
        self.wrapped = wrapped

    def invoke(self, input, config=None):
        try:
            output = self.wrapped.invoke(input, config)
            return output
        except Exception as e:
            print(f"[{self.name}] Error: {e}")
            print(f"[{self.name}] Input: {input}")
            raise  # 重新抛出异常

# 使用
chain = (
    ErrorHandlingRunnable("Step A", step_a)
    | ErrorHandlingRunnable("Step B", step_b)
    | ErrorHandlingRunnable("Step C", step_c)
)
```

### 4.3 错误恢复

**方法1：提供默认值**

```python
class FallbackRunnable(Runnable):
    """带降级的 Runnable"""

    def __init__(self, primary: Runnable, fallback_value):
        self.primary = primary
        self.fallback_value = fallback_value

    def invoke(self, input, config=None):
        try:
            return self.primary.invoke(input, config)
        except Exception as e:
            print(f"Primary failed: {e}, using fallback")
            return self.fallback_value

# 使用
safe_step = FallbackRunnable(risky_step, default_value)
chain = step_a | safe_step | step_c
```

**方法2：使用 RunnableBranch**

```python
from langchain_core.runnables import RunnableBranch

# 条件路由：如果主路径失败，使用备用路径
chain = RunnableBranch(
    (lambda x: is_valid(x), primary_path),
    fallback_path  # 默认路径
)
```

---

## 5. 流式执行的数据流

### 5.1 流式 vs 批处理

**批处理模式**：等待完整输出

```python
chain = prompt | llm | parser

# 批处理：等待 LLM 完成整个响应
result = chain.invoke({"question": "..."})
print(result)  # 一次性输出完整结果
```

**流式模式**：逐步输出

```python
# 流式：逐步接收 LLM 输出
for chunk in chain.stream({"question": "..."}):
    print(chunk, end="", flush=True)
# 逐字符或逐 token 输出
```

### 5.2 流式数据流转

**关键点**：只有最后一个步骤流式输出

```python
chain = step_a | step_b | step_c

# 流式执行
for chunk in chain.stream(input):
    yield chunk

# 实际执行过程
# 1. step_a.invoke(input) → output_a（非流式）
# 2. step_b.invoke(output_a) → output_b（非流式）
# 3. step_c.stream(output_b) → chunks（流式）
```

**可视化**：

```
┌─────────┐      ┌─────────┐      ┌─────────┐
│ Step A  │ ───> │ Step B  │ ───> │ Step C  │ ───> Chunks
└─────────┘      └─────────┘      └─────────┘
   (invoke)        (invoke)         (stream)
     ↓               ↓                 ↓
  Complete        Complete         Streaming
```

### 5.3 流式优化

**2025-2026 优化**：降低首 token 延迟 67%

```python
from langchain_openai import ChatOpenAI

# 启用流式
llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)

chain = prompt | llm | parser

# 流式执行
import time
start = time.time()
first_chunk_time = None

for i, chunk in enumerate(chain.stream({"question": "..."})):
    if i == 0:
        first_chunk_time = time.time() - start
        print(f"First chunk after {first_chunk_time:.2f}s")
    print(chunk, end="", flush=True)

# 结果：
# 传统模式：首 token 2.5s
# 流式模式：首 token 0.8s（降低 67%）
```

---

## 6. 批处理的数据流

### 6.1 批处理基础

**批处理**：一次处理多个输入

```python
chain = prompt | llm | parser

# 单个输入
result = chain.invoke({"question": "What is AI?"})

# 批量输入
results = chain.batch([
    {"question": "What is AI?"},
    {"question": "What is ML?"},
    {"question": "What is DL?"}
])

# results = ["AI is...", "ML is...", "DL is..."]
```

### 6.2 批处理数据流

**每个输入独立流转**

```python
# 批处理执行过程
inputs = [input1, input2, input3]

# 方式1：顺序处理
results = []
for inp in inputs:
    result = chain.invoke(inp)
    results.append(result)

# 方式2：并行处理（如果支持）
results = chain.batch(inputs)  # 内部可能并行
```

### 6.3 批处理优化

**2026 年优化**：降低成本 50%

```python
# 使用批处理 API
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# 批量调用（使用批处理 API）
chain = prompt | llm | parser

# 批处理模式
results = chain.batch(inputs)

# 成本对比
# 单次调用：100 次 × $0.01 = $1.00
# 批处理：1 次批处理 = $0.50（50% 折扣）
```

---

## 7. 配置传递

### 7.1 配置对象

**RunnableConfig**：在管道中传递配置

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    tags=["production"],
    metadata={"user_id": "123"},
    callbacks=[my_callback],
    max_concurrency=5
)

# 配置传递给所有步骤
result = chain.invoke(input, config=config)
```

### 7.2 配置继承

**配置自动传递给所有子步骤**

```python
chain = step_a | step_b | step_c

# 配置传递
result = chain.invoke(input, config=config)

# 等价于
output_a = step_a.invoke(input, config=config)
output_b = step_b.invoke(output_a, config=config)
output_c = step_c.invoke(output_b, config=config)
```

### 7.3 配置覆盖

**子步骤可以覆盖配置**

```python
class ConfigOverrideRunnable(Runnable):
    def invoke(self, input, config=None):
        # 修改配置
        new_config = {
            **config,
            "temperature": 0.9  # 覆盖温度
        }
        return self.process(input, new_config)
```

---

## 8. 实战示例

### 8.1 完整 RAG 管道数据流

```python
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# 定义各个步骤
embeddings = OpenAIEmbeddings()
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

def embed_query(input: dict) -> list:
    return embeddings.embed_query(input["query"])

def retrieve_docs(input: dict) -> list:
    # 使用 embedding 检索
    embedding = input["embedding"]
    # 模拟检索
    return ["Doc 1 content", "Doc 2 content"]

def format_prompt(input: dict) -> dict:
    context = "\n\n".join(input["docs"])
    return {
        "context": context,
        "question": input["query"]
    }

# 组合管道
rag_chain = (
    # 步骤1：保持原始查询，添加 embedding
    RunnablePassthrough.assign(
        embedding=RunnableLambda(embed_query)
    )
    # 数据: {"query": "...", "embedding": [...]}

    # 步骤2：保持所有数据，添加检索结果
    | RunnablePassthrough.assign(
        docs=RunnableLambda(retrieve_docs)
    )
    # 数据: {"query": "...", "embedding": [...], "docs": [...]}

    # 步骤3：格式化 prompt
    | RunnableLambda(format_prompt)
    # 数据: {"context": "...", "question": "..."}

    # 步骤4：生成响应
    | ChatPromptTemplate.from_template(
        "Context: {context}\n\nQuestion: {question}\n\nAnswer:"
    )
    | llm
    | parser
)

# 执行
result = rag_chain.invoke({"query": "What is LangChain?"})
print(result)
```

**数据流转可视化**：

```
{"query": "What is LangChain?"}
    ↓
{"query": "...", "embedding": [0.1, 0.2, ...]}
    ↓
{"query": "...", "embedding": [...], "docs": ["Doc 1", "Doc 2"]}
    ↓
{"context": "Doc 1\n\nDoc 2", "question": "What is LangChain?"}
    ↓
ChatPromptValue
    ↓
AIMessage
    ↓
"LangChain is a framework..."
```

### 8.2 多路并行数据流

```python
from langchain_core.runnables import RunnableParallel

# 并行执行多个工具
parallel_chain = (
    RunnableParallel({
        "search": search_tool,
        "calculate": calculator_tool,
        "summarize": summarizer_tool
    })
    | result_merger
    | prompt
    | llm
    | parser
)

# 数据流转
input_data = {"query": "Calculate 2+2 and search for AI"}
    ↓
{
    "search": "AI is...",
    "calculate": "4",
    "summarize": "Summary..."
}
    ↓
"Combined: Search found..., Calculation is 4, Summary..."
    ↓
ChatPromptValue
    ↓
AIMessage
    ↓
"Based on the search and calculation..."
```

---

## 9. 性能考虑

### 9.1 数据复制开销

**问题**：RunnablePassthrough.assign() 复制数据

```python
# 每次 assign 都复制整个输入
chain = (
    RunnablePassthrough.assign(field1=step1)  # 复制1次
    | RunnablePassthrough.assign(field2=step2)  # 复制2次
    | RunnablePassthrough.assign(field3=step3)  # 复制3次
)
```

**优化**：合并 assign

```python
# 一次 assign 多个字段
chain = (
    RunnablePassthrough.assign(
        field1=step1,
        field2=step2,
        field3=step3
    )
)
```

### 9.2 中间数据大小

**问题**：大数据在管道中传递

```python
# 检索大量文档
chain = (
    retriever  # 返回 1000 个文档
    | reranker  # 处理 1000 个文档
    | top_k_selector  # 只需要前 10 个
)
```

**优化**：提前过滤

```python
# 在检索时限制数量
chain = (
    retriever.with_config(top_k=10)  # 只返回 10 个
    | reranker
)
```

---

## 10. 总结

### 核心要点

1. **数据流转**：
   - 前一个输出 = 后一个输入
   - 类型必须匹配
   - 支持自动转换

2. **上下文保持**：
   - 使用 RunnablePassthrough.assign()
   - 使用 RunnableParallel
   - 自定义 Runnable

3. **错误处理**：
   - 错误向上传播
   - 使用包装器定位错误
   - 提供降级方案

4. **执行模式**：
   - 流式：降低延迟 67%
   - 批处理：降低成本 50%
   - 配置传递：统一管理

### 学习检查

完成本文档学习后，你应该能够：

- [ ] 理解数据如何在管道中流转
- [ ] 解决类型不匹配问题
- [ ] 使用 RunnablePassthrough 保持上下文
- [ ] 处理管道中的错误
- [ ] 选择合适的执行模式（流式/批处理）
- [ ] 优化数据流转性能

---

**版本**: v1.0
**最后更新**: 2026-02-19
**维护者**: Claude Code

**参考资源**:
- [LangChain 性能优化指南](https://blog.langchain.com/how-do-i-speed-up-my-agent)
- [生产级 LCEL 指南](https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557)

**下一步**: 阅读 `03_核心概念_03_类型推断.md` 深入理解类型系统
