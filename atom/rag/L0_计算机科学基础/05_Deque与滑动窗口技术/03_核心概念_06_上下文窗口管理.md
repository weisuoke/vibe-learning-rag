# 核心概念 06：上下文窗口管理

> 深入理解 LLM 上下文窗口管理的策略与优化技术

---

## 概述

上下文窗口管理是 AI Agent 开发的核心挑战之一。LLM 的上下文窗口有限，而实际对话可能无限长，因此需要智能地管理上下文，保留最相关的信息。

**核心挑战：**
- 上下文窗口有限（4K-1M tokens）
- Token 计数复杂（不同模型不同编码）
- 需要平衡信息完整性和窗口限制
- 需要保持语义连贯性

**解决方案：** 滑动窗口 + Token 计数 + 语义压缩

---

## 1. 上下文窗口基础

### 1.1 主流 LLM 的上下文窗口

| 模型 | 上下文窗口 | 发布时间 |
|------|-----------|---------|
| GPT-4 | 8K / 32K / 128K | 2023-2024 |
| GPT-4 Turbo | 128K | 2024 |
| Claude 3 Opus | 200K | 2024 |
| Claude 3.5 Sonnet | 200K | 2024 |
| Gemini 1.5 Pro | 1M | 2024 |
| Gemini 1.5 Flash | 1M | 2024 |

**趋势：** 上下文窗口持续增长，但仍需要管理。

### 1.2 Token 计数

**什么是 Token？**
- Token 是 LLM 处理文本的基本单位
- 1 token ≈ 4 个字符（英文）
- 1 token ≈ 1-2 个汉字（中文）

**示例：**
```python
import tiktoken

# GPT-4 的编码器
encoding = tiktoken.encoding_for_model("gpt-4")

text = "Hello, world!"
tokens = encoding.encode(text)
print(f"文本: {text}")
print(f"Token 数: {len(tokens)}")  # 4
print(f"Tokens: {tokens}")  # [9906, 11, 1917, 0]

# 中文示例
text_cn = "你好，世界！"
tokens_cn = encoding.encode(text_cn)
print(f"文本: {text_cn}")
print(f"Token 数: {len(tokens_cn)}")  # 6
```

---

## 2. 基于消息数量的窗口管理

### 2.1 固定消息数量

```python
from collections import deque

class SimpleWindowManager:
    """简单的窗口管理器 - 固定消息数量"""

    def __init__(self, max_messages: int = 20):
        self.messages = deque(maxlen=max_messages)

    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})

    def get_context(self) -> list:
        return list(self.messages)

# 使用示例
manager = SimpleWindowManager(max_messages=10)

for i in range(15):
    manager.add_message("user", f"消息 {i}")

print(f"消息数: {len(manager.get_context())}")  # 10
```

**优点：**
- 实现简单
- 性能高（O(1)）

**缺点：**
- 不考虑消息长度
- 可能超过 token 限制

---

## 3. 基于 Token 计数的窗口管理

### 3.1 基础实现

```python
from collections import deque
import tiktoken

class TokenWindowManager:
    """基于 Token 计数的窗口管理器"""

    def __init__(self, max_tokens: int = 4096, model: str = "gpt-4"):
        self.max_tokens = max_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)

    def count_tokens(self, text: str) -> int:
        """计算文本的 token 数量"""
        return len(self.encoding.encode(text))

    def get_total_tokens(self) -> int:
        """计算当前所有消息的 token 总数"""
        total = 0
        for msg in self.messages:
            # 消息格式开销：role + content
            total += 4  # 每条消息的格式开销
            total += self.count_tokens(msg["content"])
        return total

    def add_message(self, role: str, content: str):
        """添加消息，自动淘汰超出窗口的消息"""
        # 添加新消息
        self.messages.append({"role": role, "content": content})

        # 移除旧消息直到满足 token 限制
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.popleft()

    def get_context(self) -> list:
        return list(self.messages)

# 使用示例
manager = TokenWindowManager(max_tokens=100)

manager.add_message("user", "这是一个很长的消息" * 10)
manager.add_message("assistant", "这是回复" * 5)
manager.add_message("user", "继续对话")

print(f"当前 token 数: {manager.get_total_tokens()}")
print(f"消息数量: {len(manager.get_context())}")
```

**2025-2026 实际应用：**

**来源**: [Context Length Optimization Guide 2025](https://local-ai-zone.github.io/guides/context-length-optimization-ultimate-guide-2025.html)
**时间**: 2025
**关键点**: 使用滑动窗口 + Token 计数优化上下文管理

### 3.2 带系统提示的实现

```python
from collections import deque
import tiktoken

class TokenWindowWithSystem:
    """带系统提示的 Token 窗口管理器"""

    def __init__(
        self,
        system_prompt: str,
        max_tokens: int = 4096,
        model: str = "gpt-4"
    ):
        self.system_prompt = system_prompt
        self.max_tokens = max_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)

        # 计算系统提示的 token 数
        self.system_tokens = self.count_tokens(system_prompt) + 4

    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def get_total_tokens(self) -> int:
        """计算总 token 数（包括系统提示）"""
        total = self.system_tokens
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total

    def add_message(self, role: str, content: str):
        """添加消息"""
        self.messages.append({"role": role, "content": content})

        # 移除旧消息直到满足限制
        available_tokens = self.max_tokens - self.system_tokens
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.popleft()

    def get_context(self) -> list:
        """获取上下文（包括系统提示）"""
        return [
            {"role": "system", "content": self.system_prompt},
            *list(self.messages)
        ]

# 使用示例
manager = TokenWindowWithSystem(
    system_prompt="你是一个友好的 AI 助手。",
    max_tokens=200
)

manager.add_message("user", "你好")
manager.add_message("assistant", "你好！有什么可以帮你的？")

print(f"总 token 数: {manager.get_total_tokens()}")
```

---

## 4. 语义压缩策略

### 4.1 摘要压缩

```python
from collections import deque
import tiktoken

class SummarizingWindowManager:
    """带摘要功能的窗口管理器"""

    def __init__(
        self,
        max_tokens: int = 4096,
        summary_threshold: int = 2000,
        model: str = "gpt-4"
    ):
        self.max_tokens = max_tokens
        self.summary_threshold = summary_threshold
        self.messages = deque()
        self.summaries = []  # 存储历史摘要
        self.encoding = tiktoken.encoding_for_model(model)

    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def get_total_tokens(self) -> int:
        total = 0
        # 摘要的 tokens
        for summary in self.summaries:
            total += self.count_tokens(summary)
        # 当前消息的 tokens
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total

    def add_message(self, role: str, content: str):
        """添加消息"""
        self.messages.append({"role": role, "content": content})

        # 如果超过摘要阈值，触发摘要
        if self.get_total_tokens() > self.summary_threshold:
            self._summarize_old_messages()

        # 如果仍然超过限制，移除最旧的消息
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.popleft()

    def _summarize_old_messages(self):
        """摘要旧消息"""
        if len(self.messages) < 4:
            return

        # 取前一半消息进行摘要
        num_to_summarize = len(self.messages) // 2
        messages_to_summarize = [self.messages.popleft() for _ in range(num_to_summarize)]

        # 生成摘要（实际应该调用 LLM）
        summary = f"[摘要] 讨论了 {num_to_summarize} 条消息的内容"
        self.summaries.append(summary)

    def get_context(self) -> list:
        """获取上下文（摘要 + 最近消息）"""
        context = []

        # 添加摘要
        for summary in self.summaries:
            context.append({"role": "system", "content": summary})

        # 添加最近消息
        context.extend(list(self.messages))

        return context

# 使用示例
manager = SummarizingWindowManager(
    max_tokens=500,
    summary_threshold=300
)

for i in range(20):
    manager.add_message("user", f"问题 {i}")
    manager.add_message("assistant", f"回答 {i}")

print(f"摘要数量: {len(manager.summaries)}")
print(f"当前消息数: {len(manager.messages)}")
print(f"总 token 数: {manager.get_total_tokens()}")
```

### 4.2 重要性评分

```python
from collections import deque
import tiktoken

class ImportanceBasedWindowManager:
    """基于重要性的窗口管理器"""

    def __init__(self, max_tokens: int = 4096, model: str = "gpt-4"):
        self.max_tokens = max_tokens
        self.messages = []  # 使用列表而非 deque，需要排序
        self.encoding = tiktoken.encoding_for_model(model)

    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def get_total_tokens(self) -> int:
        total = 0
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total

    def calculate_importance(self, message: dict) -> float:
        """计算消息重要性（简化版本）"""
        # 实际应该使用更复杂的算法
        importance = 1.0

        # 用户消息更重要
        if message["role"] == "user":
            importance *= 1.5

        # 包含关键词的消息更重要
        keywords = ["重要", "关键", "问题", "错误"]
        for keyword in keywords:
            if keyword in message["content"]:
                importance *= 1.2

        return importance

    def add_message(self, role: str, content: str):
        """添加消息"""
        message = {
            "role": role,
            "content": content,
            "importance": self.calculate_importance({"role": role, "content": content})
        }
        self.messages.append(message)

        # 如果超过限制，移除最不重要的消息
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            # 按重要性排序
            self.messages.sort(key=lambda x: x["importance"])
            # 移除最不重要的
            self.messages.pop(0)

    def get_context(self) -> list:
        """获取上下文"""
        return [
            {"role": msg["role"], "content": msg["content"]}
            for msg in self.messages
        ]

# 使用示例
manager = ImportanceBasedWindowManager(max_tokens=200)

manager.add_message("user", "这是一个重要的问题")
manager.add_message("assistant", "这是回答")
manager.add_message("user", "普通问题")
manager.add_message("assistant", "普通回答")

print(f"消息数量: {len(manager.get_context())}")
```

---

## 5. 2025-2026 年最佳实践

### 5.1 Redis 上下文窗口管理 (2026)

```python
from collections import deque
import tiktoken

class RedisStyleWindowManager:
    """Redis 2026 风格的上下文窗口管理"""

    def __init__(self, max_tokens: int = 4096, model: str = "gpt-4"):
        self.max_tokens = max_tokens
        self.token_window = deque()
        self.encoding = tiktoken.encoding_for_model(model)

    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def add_tokens(self, tokens: list):
        """添加 tokens"""
        for token in tokens:
            self.token_window.append(token)

            # 超过限制时移除最旧的 token
            while len(self.token_window) > self.max_tokens:
                self.token_window.popleft()

    def add_message(self, role: str, content: str):
        """添加消息"""
        tokens = self.encoding.encode(content)
        self.add_tokens(tokens)

    def get_context_text(self) -> str:
        """获取上下文文本"""
        return self.encoding.decode(list(self.token_window))

# 使用示例
manager = RedisStyleWindowManager(max_tokens=100)

manager.add_message("user", "这是一个很长的消息" * 10)
manager.add_message("assistant", "这是回复")

print(f"当前 token 数: {len(manager.token_window)}")
print(f"上下文: {manager.get_context_text()[:50]}...")
```

**来源**: [Redis - Context Window Overflow](https://redis.io/blog/context-window-overflow)
**时间**: 2026
**关键点**: 使用滑动窗口防止 LLM 上下文溢出

### 5.2 OneUptime 上下文管理策略 (2026)

```python
from collections import deque
import tiktoken

class OneUptimeStyleManager:
    """OneUptime 2026 风格的上下文管理"""

    def __init__(
        self,
        max_tokens: int = 4096,
        reserve_tokens: int = 500,
        model: str = "gpt-4"
    ):
        """
        Args:
            max_tokens: 最大 token 数
            reserve_tokens: 为响应预留的 token 数
        """
        self.max_tokens = max_tokens
        self.reserve_tokens = reserve_tokens
        self.available_tokens = max_tokens - reserve_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)

    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def get_total_tokens(self) -> int:
        total = 0
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total

    def add_message(self, role: str, content: str):
        """添加消息"""
        self.messages.append({"role": role, "content": content})

        # 确保不超过可用 token 数
        while self.get_total_tokens() > self.available_tokens and len(self.messages) > 1:
            self.messages.popleft()

    def get_context(self) -> list:
        return list(self.messages)

    def get_available_response_tokens(self) -> int:
        """获取可用于响应的 token 数"""
        used_tokens = self.get_total_tokens()
        return self.max_tokens - used_tokens

# 使用示例
manager = OneUptimeStyleManager(
    max_tokens=4096,
    reserve_tokens=1000  # 为响应预留 1000 tokens
)

manager.add_message("user", "问题")
manager.add_message("assistant", "回答")

print(f"可用响应 tokens: {manager.get_available_response_tokens()}")
```

**来源**: [OneUptime - Context Window Management](https://oneuptime.com/blog/post/2026-01-30-context-window-management/view)
**时间**: 2026
**关键点**: 为响应预留 token 空间

---

## 6. 性能优化

### 6.1 缓存 Token 计数

```python
from collections import deque
import tiktoken
from functools import lru_cache

class CachedTokenWindowManager:
    """带缓存的 Token 窗口管理器"""

    def __init__(self, max_tokens: int = 4096, model: str = "gpt-4"):
        self.max_tokens = max_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)
        self.token_cache = {}  # 缓存消息的 token 数

    @lru_cache(maxsize=1000)
    def count_tokens(self, text: str) -> int:
        """缓存 token 计数"""
        return len(self.encoding.encode(text))

    def get_total_tokens(self) -> int:
        total = 0
        for i, msg in enumerate(self.messages):
            # 使用缓存
            if i not in self.token_cache:
                self.token_cache[i] = 4 + self.count_tokens(msg["content"])
            total += self.token_cache[i]
        return total

    def add_message(self, role: str, content: str):
        """添加消息"""
        self.messages.append({"role": role, "content": content})

        # 更新缓存
        msg_index = len(self.messages) - 1
        self.token_cache[msg_index] = 4 + self.count_tokens(content)

        # 移除旧消息
        while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
            self.messages.popleft()
            # 清理缓存
            if 0 in self.token_cache:
                del self.token_cache[0]
            # 重新索引缓存
            self.token_cache = {k-1: v for k, v in self.token_cache.items() if k > 0}

    def get_context(self) -> list:
        return list(self.messages)
```

### 6.2 批量处理

```python
from collections import deque
import tiktoken

class BatchWindowManager:
    """批量处理的窗口管理器"""

    def __init__(self, max_tokens: int = 4096, model: str = "gpt-4"):
        self.max_tokens = max_tokens
        self.messages = deque()
        self.encoding = tiktoken.encoding_for_model(model)
        self.pending_messages = []

    def count_tokens(self, text: str) -> int:
        return len(self.encoding.encode(text))

    def get_total_tokens(self) -> int:
        total = 0
        for msg in self.messages:
            total += 4 + self.count_tokens(msg["content"])
        return total

    def add_message(self, role: str, content: str):
        """添加消息到待处理队列"""
        self.pending_messages.append({"role": role, "content": content})

    def flush(self):
        """批量处理待处理消息"""
        for msg in self.pending_messages:
            self.messages.append(msg)

            # 移除旧消息
            while self.get_total_tokens() > self.max_tokens and len(self.messages) > 1:
                self.messages.popleft()

        self.pending_messages.clear()

    def get_context(self) -> list:
        """获取上下文（自动 flush）"""
        self.flush()
        return list(self.messages)

# 使用示例
manager = BatchWindowManager(max_tokens=200)

# 批量添加消息
for i in range(10):
    manager.add_message("user", f"问题 {i}")
    manager.add_message("assistant", f"回答 {i}")

# 一次性处理
context = manager.get_context()
print(f"消息数量: {len(context)}")
```

---

## 7. 常见问题

### Q1: 如何选择窗口大小？

**答案：** 根据以下公式计算：

```python
def calculate_window_size(
    context_window: int,
    system_prompt_tokens: int,
    reserve_tokens: int,
    avg_message_tokens: int
) -> int:
    """
    计算推荐的窗口大小

    Args:
        context_window: LLM 上下文窗口大小
        system_prompt_tokens: 系统提示的 token 数
        reserve_tokens: 为响应预留的 token 数
        avg_message_tokens: 平均每条消息的 token 数

    Returns:
        推荐的最大消息数
    """
    available_tokens = context_window - system_prompt_tokens - reserve_tokens
    max_messages = available_tokens // avg_message_tokens

    # 留出 20% 余量
    return int(max_messages * 0.8)

# 示例
window_size = calculate_window_size(
    context_window=4096,
    system_prompt_tokens=200,
    reserve_tokens=1000,
    avg_message_tokens=100
)
print(f"推荐窗口大小: {window_size} 条消息")  # 约 23 条
```

### Q2: 如何处理超长消息？

**答案：** 使用截断或分块策略：

```python
def truncate_message(content: str, max_tokens: int, encoding) -> str:
    """截断消息"""
    tokens = encoding.encode(content)
    if len(tokens) <= max_tokens:
        return content

    # 截断并添加省略号
    truncated_tokens = tokens[:max_tokens-3]
    return encoding.decode(truncated_tokens) + "..."

def chunk_message(content: str, max_tokens: int, encoding) -> list:
    """分块消息"""
    tokens = encoding.encode(content)
    chunks = []

    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i+max_tokens]
        chunks.append(encoding.decode(chunk_tokens))

    return chunks
```

### Q3: 如何与向量检索结合？

**答案：**
```python
class HybridWindowManager:
    """混合窗口管理器 - 结合向量检索"""

    def __init__(self, max_tokens: int = 4096):
        self.max_tokens = max_tokens
        self.recent_messages = deque(maxlen=10)
        self.vector_store = []  # 实际应该是向量数据库

    def add_message(self, role: str, content: str):
        # 添加到最近消息
        self.recent_messages.append({"role": role, "content": content})

        # 同时存储到向量库
        self.vector_store.append({"role": role, "content": content})

    def get_context(self, query: str) -> list:
        # 从向量库检索相关消息
        relevant = self._retrieve_relevant(query, top_k=5)

        # 合并最近消息和相关消息
        context = relevant + list(self.recent_messages)

        return context
```

---

## 学习检查清单

- [ ] 理解主流 LLM 的上下文窗口大小
- [ ] 能够使用 tiktoken 计算 token 数量
- [ ] 能够实现基于 token 计数的窗口管理
- [ ] 理解语义压缩策略（摘要、重要性评分）
- [ ] 了解 2025-2026 年的最佳实践
- [ ] 能够优化 token 计数性能（缓存、批量处理）
- [ ] 知道如何选择合适的窗口大小

---

## 下一步学习

### 实战代码
→ **07_实战代码_05_上下文窗口优化.md** - 完整代码示例

### 生产实践
→ **07_实战代码_06_生产级实践.md** - 生产级实现

### 概览
→ **00_概览.md** - 完整知识体系

---

**版本**: v1.0
**最后更新**: 2026-02-13
**适用于**: Python 3.13+, AI Agent 开发, RAG 系统
