# 核心概念1：上下文管理

> 多轮对话的上下文维护策略

---

## 概念定义

**上下文管理（Context Management）**是对话式RAG的第一大核心技术，指在多轮对话中维护和组织历史信息，确保LLM能够理解对话上下文，从而给出连贯、相关的回答。

---

## 第一性原理

### 为什么需要上下文管理？

**根本问题**：LLM本身是无状态的，每次调用都是独立的。

```python
# LLM的无状态特性
response1 = llm.invoke("Python装饰器是什么？")
# LLM回答："装饰器是一种设计模式..."

response2 = llm.invoke("它有什么用？")
# LLM无法理解"它"指什么，因为没有历史上下文
# 回答："请问您指的是什么？"
```

**解决方案**：在每次调用时，将历史对话作为上下文传递给LLM。

```python
# 带上下文的调用
context = """
用户：Python装饰器是什么？
系统：装饰器是一种设计模式...
"""

response2 = llm.invoke(f"{context}\n用户：它有什么用？")
# LLM理解"它"指"Python装饰器"
# 回答："Python装饰器主要用于..."
```

### 核心挑战

1. **存储挑战**：如何高效存储和检索历史对话？
2. **选择挑战**：保留哪些历史？丢弃哪些历史？
3. **注入挑战**：如何将历史注入到当前查询中？
4. **成本挑战**：如何控制Token消耗？

---

## 技术演进（2025-2026）

### 传统方法（2023-2024）

**简单Buffer**：保留所有历史对话

```python
chat_history = []

def chat(query):
    chat_history.append({"user": query})
    response = llm.invoke(chat_history)
    chat_history.append({"bot": response})
    return response
```

**问题**：
- Token消耗线性增长
- 无法处理长对话
- 噪声干扰检索

### 最新进展（2025-2026）

#### 1. DH-RAG：动态历史上下文学习

**论文**：Dynamic History Context Learning for Multi-turn Dialogue RAG (2025)
**来源**：https://arxiv.org/abs/2502.13847

**核心思想**：
- **查询重构**：根据历史动态重构当前查询
- **历史信息更新**：智能更新历史信息的重要性
- **自适应上下文**：根据查询类型调整上下文长度

**实现原理**：

```python
class DH_RAG:
    def __init__(self):
        self.history = []
        self.importance_scores = []  # 每条历史的重要性分数

    def update_importance(self, query):
        """根据当前查询更新历史重要性"""
        for i, msg in enumerate(self.history):
            # 计算相关性
            relevance = compute_relevance(query, msg)
            # 更新重要性分数
            self.importance_scores[i] = relevance * decay_factor(i)

    def reconstruct_query(self, query):
        """根据历史重构查询"""
        # 选择重要的历史
        important_history = self.select_important_history()
        # 重构查询
        reconstructed = f"{important_history}\n{query}"
        return reconstructed

    def chat(self, query):
        # 1. 更新历史重要性
        self.update_importance(query)
        # 2. 重构查询
        reconstructed_query = self.reconstruct_query(query)
        # 3. 检索和生成
        response = self.rag_pipeline(reconstructed_query)
        # 4. 保存历史
        self.history.append({"user": query, "bot": response})
        return response
```

**优势**：
- 自动识别重要历史
- 减少无关历史的干扰
- 适应不同查询类型

#### 2. EvoRAG：演化图结构建模

**论文**：Evolutionary Graph for Multi-turn Dialogue Context (2025)
**来源**：https://dl.acm.org/doi/10.1145/3746252.3761355

**核心思想**：
- 使用**图结构**组织对话历史
- 节点：对话轮次
- 边：语义关联
- 演化：随对话动态更新图结构

**实现原理**：

```python
class EvoRAG:
    def __init__(self):
        self.graph = ConversationGraph()

    def add_turn(self, query, response):
        """添加新的对话轮次"""
        # 创建新节点
        node = self.graph.add_node(query, response)

        # 建立与历史节点的边
        for prev_node in self.graph.nodes:
            similarity = compute_similarity(node, prev_node)
            if similarity > threshold:
                self.graph.add_edge(node, prev_node, weight=similarity)

    def retrieve_context(self, query):
        """检索相关上下文"""
        # 找到与当前查询最相关的节点
        relevant_nodes = self.graph.find_relevant_nodes(query)

        # 通过图遍历获取相关历史
        context = self.graph.traverse(relevant_nodes)
        return context
```

**优势**：
- 捕捉非线性对话关系
- 支持话题跳转
- 更精准的上下文选择

#### 3. LangChain Memory系统

**来源**：https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory

**核心组件**：

| Memory类型 | 特点 | 适用场景 |
|-----------|------|---------|
| **ConversationBufferMemory** | 保留所有历史 | 短对话（<10轮） |
| **ConversationBufferWindowMemory** | 滑动窗口 | 中等对话（10-50轮） |
| **ConversationSummaryMemory** | 摘要压缩 | 长对话（>50轮） |
| **ConversationSummaryBufferMemory** | 窗口+摘要 | 混合场景 |
| **ConversationKGMemory** | 知识图谱 | 复杂关系 |
| **VectorStoreBackedMemory** | 向量检索 | 语义相关 |

---

## 核心策略

### 策略1：Buffer Memory（全量保留）

**原理**：保留所有历史对话

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# 保存对话
memory.save_context(
    {"input": "Python装饰器是什么？"},
    {"output": "装饰器是一种设计模式..."}
)

# 获取历史
history = memory.load_memory_variables({})
print(history["chat_history"])
# [
#   HumanMessage(content="Python装饰器是什么？"),
#   AIMessage(content="装饰器是一种设计模式...")
# ]
```

**优点**：
- 信息完整，不丢失任何细节
- 实现简单

**缺点**：
- Token消耗大
- 响应时间长
- 可能超出context window

**适用场景**：
- 短对话（<10轮）
- 需要精确历史的场景（如代码调试）

### 策略2：Window Memory（滑动窗口）

**原理**：只保留最近N轮对话

```python
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(
    k=5,  # 只保留最近5轮
    memory_key="chat_history",
    return_messages=True
)

# 自动管理窗口
for i in range(10):
    memory.save_context(
        {"input": f"问题{i}"},
        {"output": f"回答{i}"}
    )

# 只保留最近5轮
history = memory.load_memory_variables({})
print(len(history["chat_history"]))  # 10条消息（5轮 * 2）
```

**优点**：
- Token消耗可控
- 响应时间稳定
- 实现简单

**缺点**：
- 丢失早期历史
- 可能丢失重要信息

**适用场景**：
- 中等对话（10-50轮）
- 大多数RAG应用

**推荐配置**：
- 技术文档问答：k=5
- 客服对话：k=10
- 教育辅导：k=10

### 策略3：Summary Memory（摘要压缩）

**原理**：使用LLM生成对话摘要

```python
from langchain.memory import ConversationSummaryMemory
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)
memory = ConversationSummaryMemory(
    llm=llm,
    memory_key="chat_history",
    return_messages=True
)

# 自动生成摘要
for i in range(20):
    memory.save_context(
        {"input": f"问题{i}"},
        {"output": f"回答{i}"}
    )

# 获取摘要
history = memory.load_memory_variables({})
print(history["chat_history"])
# "用户询问了关于Python的20个问题，主要涉及装饰器、闭包、生成器等主题..."
```

**优点**：
- Token消耗小
- 保留核心信息
- 适合长对话

**缺点**：
- 丢失细节
- 需要额外LLM调用（成本）
- 摘要质量依赖LLM

**适用场景**：
- 长对话（>50轮）
- 成本敏感的场景

### 策略4：Hybrid Memory（混合策略）

**原理**：窗口 + 摘要

```python
from langchain.memory import ConversationSummaryBufferMemory

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=2000,  # Token阈值
    memory_key="chat_history",
    return_messages=True
)

# 自动管理：
# - 最近的对话保留完整
# - 早期的对话压缩为摘要
# - 总Token不超过2000
```

**优点**：
- 平衡信息完整性和Token消耗
- 自动管理
- 适应性强

**缺点**：
- 实现复杂
- 需要调优参数

**适用场景**：
- 生产环境
- 需要平衡性能和成本的场景

### 策略5：Semantic Memory（语义过滤）

**原理**：基于语义相似度选择相关历史

```python
from langchain.memory import VectorStoreRetrieverMemory
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# 创建向量存储
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)

# 创建Memory
memory = VectorStoreRetrieverMemory(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5})
)

# 保存对话
memory.save_context(
    {"input": "Python装饰器是什么？"},
    {"output": "装饰器是一种设计模式..."}
)

# 检索相关历史
query = "装饰器有什么用？"
relevant_history = memory.load_memory_variables({"prompt": query})
# 自动检索与"装饰器"相关的历史
```

**优点**：
- 智能选择相关历史
- 减少噪声干扰
- 适合话题跳转

**缺点**：
- 需要向量化（成本）
- 可能丢失时序信息
- 实现复杂

**适用场景**：
- 多话题对话
- 需要精准上下文的场景

---

## 实现细节

### 1. 上下文注入方式

#### 方式1：直接拼接

```python
def inject_context_simple(query, chat_history):
    context = "\n".join([
        f"用户：{msg['user']}\n系统：{msg['bot']}"
        for msg in chat_history
    ])
    return f"{context}\n用户：{query}"
```

#### 方式2：结构化注入

```python
def inject_context_structured(query, chat_history):
    messages = []
    for msg in chat_history:
        messages.append({"role": "user", "content": msg["user"]})
        messages.append({"role": "assistant", "content": msg["bot"]})
    messages.append({"role": "user", "content": query})
    return messages
```

#### 方式3：模板注入

```python
from langchain.prompts import ChatPromptTemplate

template = ChatPromptTemplate.from_messages([
    ("system", "你是一个RAG助手，根据对话历史和检索到的文档回答问题。"),
    ("placeholder", "{chat_history}"),
    ("human", "{query}"),
])

def inject_context_template(query, chat_history):
    return template.format_messages(
        chat_history=chat_history,
        query=query
    )
```

### 2. 历史清理策略

```python
class ContextManager:
    def __init__(self, max_tokens=2000):
        self.history = []
        self.max_tokens = max_tokens

    def add_message(self, user_msg, bot_msg):
        self.history.append({"user": user_msg, "bot": bot_msg})
        self._cleanup()

    def _cleanup(self):
        """清理策略"""
        total_tokens = self._count_tokens()

        if total_tokens > self.max_tokens:
            # 策略1：删除最早的消息
            self.history.pop(0)

            # 策略2：压缩早期消息
            # old_messages = self.history[:5]
            # summary = self._summarize(old_messages)
            # self.history = [{"summary": summary}] + self.history[5:]

            # 策略3：删除低重要性消息
            # self.history = self._filter_by_importance(self.history)

    def _count_tokens(self):
        import tiktoken
        encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        text = str(self.history)
        return len(encoding.encode(text))
```

### 3. 会话持久化

```python
import json

class PersistentMemory:
    def __init__(self, session_id):
        self.session_id = session_id
        self.file_path = f"sessions/{session_id}.json"
        self.history = self._load()

    def _load(self):
        """从文件加载历史"""
        try:
            with open(self.file_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return []

    def _save(self):
        """保存历史到文件"""
        with open(self.file_path, 'w') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)

    def add_message(self, user_msg, bot_msg):
        self.history.append({"user": user_msg, "bot": bot_msg})
        self._save()

    def get_history(self):
        return self.history

    def clear(self):
        self.history = []
        self._save()
```

---

## RAG应用场景

### 场景1：技术文档问答

**需求**：用户连续追问技术细节

```python
# 配置
memory = ConversationBufferWindowMemory(k=5)

# 对话示例
用户："Python装饰器是什么？"
系统："装饰器是一种设计模式..."

用户："它有什么用？"  # 系统理解"它"指"装饰器"
系统："装饰器主要用于..."

用户："给个例子"  # 系统理解需要"装饰器的例子"
系统："这是一个装饰器的例子：..."
```

### 场景2：智能客服

**需求**：长对话，需要压缩历史

```python
# 配置
memory = ConversationSummaryMemory(llm=llm)

# 对话示例（50轮后）
用户："之前提到的订单号是多少？"
系统：[从摘要中提取] "您的订单号是123456"
```

### 场景3：教育辅导

**需求**：支持追问和澄清

```python
# 配置
memory = ConversationBufferWindowMemory(k=10)  # 保留更多历史

# 对话示例
用户："什么是二叉树？"
系统："二叉树是一种树形数据结构..."

用户："它和链表有什么区别？"
系统："二叉树和链表的主要区别是..."

用户："能画个图吗？"
系统："这是二叉树的结构图：..."
```

### 场景4：多话题对话

**需求**：话题跳转，需要语义过滤

```python
# 配置
memory = VectorStoreRetrieverMemory(retriever=retriever)

# 对话示例
用户："Python装饰器是什么？"
系统："装饰器是..."

用户："Java的接口是什么？"  # 话题切换
系统："接口是..."

用户："装饰器和接口有什么相似之处？"  # 跨话题查询
系统：[检索两个话题的历史] "装饰器和接口都是..."
```

---

## 性能优化

### 1. Token优化

```python
# 优化前：保留所有历史
memory = ConversationBufferMemory()
# 100轮对话 = 20000 tokens

# 优化后：滑动窗口
memory = ConversationBufferWindowMemory(k=5)
# 100轮对话 = 1000 tokens（节省95%）
```

### 2. 响应时间优化

```python
# 优化前：每次都处理所有历史
def chat(query, history):
    context = process_all_history(history)  # 慢
    return llm.invoke(context + query)

# 优化后：缓存处理结果
from functools import lru_cache

@lru_cache(maxsize=100)
def process_history(history_hash):
    return process_all_history(history)

def chat(query, history):
    history_hash = hash(str(history))
    context = process_history(history_hash)  # 快
    return llm.invoke(context + query)
```

### 3. 内存优化

```python
# 优化前：在内存中保留所有历史
history = []  # 可能占用大量内存

# 优化后：使用数据库
import sqlite3

class DatabaseMemory:
    def __init__(self, session_id):
        self.conn = sqlite3.connect('memory.db')
        self.session_id = session_id

    def add_message(self, user_msg, bot_msg):
        self.conn.execute(
            "INSERT INTO messages (session_id, user_msg, bot_msg) VALUES (?, ?, ?)",
            (self.session_id, user_msg, bot_msg)
        )

    def get_recent(self, k=5):
        cursor = self.conn.execute(
            "SELECT user_msg, bot_msg FROM messages WHERE session_id = ? ORDER BY id DESC LIMIT ?",
            (self.session_id, k)
        )
        return cursor.fetchall()
```

---

## 评估指标

### 1. 上下文理解准确率

```python
def evaluate_context_understanding(test_cases):
    correct = 0
    for case in test_cases:
        # 设置历史
        memory.clear()
        for msg in case["history"]:
            memory.save_context(msg["input"], msg["output"])

        # 测试查询
        response = chain({"question": case["query"]})

        # 检查是否正确理解上下文
        if case["expected_entity"] in response["answer"]:
            correct += 1

    return correct / len(test_cases)

# 示例测试用例
test_cases = [
    {
        "history": [
            {"input": {"question": "Python装饰器是什么？"}, "output": {"answer": "装饰器是..."}}
        ],
        "query": "它有什么用？",
        "expected_entity": "装饰器"
    }
]

accuracy = evaluate_context_understanding(test_cases)
print(f"上下文理解准确率：{accuracy:.2%}")
```

### 2. Token效率

```python
def evaluate_token_efficiency(memory_strategy):
    total_tokens = 0
    num_turns = 100

    for i in range(num_turns):
        query = f"问题{i}"
        response = chain({"question": query})

        # 统计token
        history = memory.load_memory_variables({})
        tokens = count_tokens(str(history))
        total_tokens += tokens

    avg_tokens = total_tokens / num_turns
    return avg_tokens

# 对比不同策略
strategies = {
    "Buffer": ConversationBufferMemory(),
    "Window-5": ConversationBufferWindowMemory(k=5),
    "Summary": ConversationSummaryMemory(llm=llm)
}

for name, memory in strategies.items():
    avg_tokens = evaluate_token_efficiency(memory)
    print(f"{name}: {avg_tokens:.0f} tokens/turn")
```

### 3. 响应时间

```python
import time

def evaluate_response_time(memory_strategy, num_turns=50):
    times = []

    for i in range(num_turns):
        start = time.time()
        response = chain({"question": f"问题{i}"})
        end = time.time()
        times.append(end - start)

    return {
        "avg": sum(times) / len(times),
        "p50": sorted(times)[len(times) // 2],
        "p95": sorted(times)[int(len(times) * 0.95)]
    }

# 测试
metrics = evaluate_response_time(memory)
print(f"平均响应时间：{metrics['avg']:.2f}秒")
print(f"P50响应时间：{metrics['p50']:.2f}秒")
print(f"P95响应时间：{metrics['p95']:.2f}秒")
```

---

## 最佳实践

### 1. 选择合适的Memory策略

```python
def choose_memory_strategy(scenario):
    if scenario == "short_conversation":
        # 短对话：使用Buffer
        return ConversationBufferMemory()

    elif scenario == "medium_conversation":
        # 中等对话：使用Window
        return ConversationBufferWindowMemory(k=5)

    elif scenario == "long_conversation":
        # 长对话：使用Summary
        return ConversationSummaryMemory(llm=llm)

    elif scenario == "production":
        # 生产环境：使用Hybrid
        return ConversationSummaryBufferMemory(
            llm=llm,
            max_token_limit=2000
        )

    elif scenario == "multi_topic":
        # 多话题：使用Semantic
        return VectorStoreRetrieverMemory(retriever=retriever)
```

### 2. 监控和调优

```python
class MonitoredMemory:
    def __init__(self, base_memory):
        self.memory = base_memory
        self.metrics = {
            "total_turns": 0,
            "total_tokens": 0,
            "avg_response_time": 0
        }

    def save_context(self, input, output):
        start = time.time()
        self.memory.save_context(input, output)
        end = time.time()

        # 更新指标
        self.metrics["total_turns"] += 1
        self.metrics["total_tokens"] += count_tokens(str(input) + str(output))
        self.metrics["avg_response_time"] = (
            (self.metrics["avg_response_time"] * (self.metrics["total_turns"] - 1) + (end - start))
            / self.metrics["total_turns"]
        )

    def get_metrics(self):
        return self.metrics
```

### 3. 错误处理

```python
class RobustMemory:
    def __init__(self, base_memory):
        self.memory = base_memory

    def save_context(self, input, output):
        try:
            self.memory.save_context(input, output)
        except Exception as e:
            print(f"保存上下文失败：{e}")
            # 降级策略：只保留最近的消息
            self.memory.clear()
            self.memory.save_context(input, output)

    def load_memory_variables(self, inputs):
        try:
            return self.memory.load_memory_variables(inputs)
        except Exception as e:
            print(f"加载上下文失败：{e}")
            # 降级策略：返回空上下文
            return {"chat_history": []}
```

---

## 总结

上下文管理是对话式RAG的基础，核心要点：

1. **理解无状态性**：LLM本身无状态，需要显式传递历史
2. **选择合适策略**：根据场景选择Buffer、Window、Summary等
3. **平衡权衡**：信息完整性 vs Token消耗 vs 响应时间
4. **持续优化**：监控指标，调整策略
5. **关注最新技术**：DH-RAG、EvoRAG等新方法

**记住**：上下文管理不是越多越好，而是越相关越好。

---

**版本**：v1.0
**最后更新**：2026-02-17
**维护者**：Claude Code
**基于**：2025-2026年最新研究和生产实践
