# æ ¸å¿ƒæ¦‚å¿µ4ï¼šå¼•ç”¨æº¯æºï¼ˆAttributionï¼‰

> **ä¸ºæ¯ä¸ªç”Ÿæˆå£°æ˜æä¾›æºæ–‡æ¡£å¼•ç”¨ï¼Œæå‡RAGç³»ç»Ÿçš„å¯ä¿¡åº¦å’Œé€æ˜åº¦**

---

## æ¦‚å¿µå®šä¹‰

**å¼•ç”¨æº¯æºï¼ˆAttributionï¼‰**ï¼šä¸ºç”Ÿæˆå†…å®¹ä¸­çš„æ¯ä¸ªå£°æ˜æ ‡æ³¨å…¶æ¥æºæ–‡æ¡£çš„ä½ç½®ï¼Œä½¿ç”¨æˆ·å¯ä»¥è¿½æº¯åˆ°åŸå§‹ä¿¡æ¯æºï¼ŒéªŒè¯å£°æ˜çš„çœŸå®æ€§ã€‚

**æ ¸å¿ƒç›®æ ‡ï¼š**
- **å¯è¿½æº¯æ€§**ï¼šç”¨æˆ·å¯ä»¥ç‚¹å‡»å¼•ç”¨æŸ¥çœ‹åŸå§‹æ–‡æ¡£
- **å¯éªŒè¯æ€§**ï¼šç”¨æˆ·å¯ä»¥éªŒè¯å£°æ˜æ˜¯å¦è¢«æºæ–‡æ¡£æ”¯æŒ
- **é€æ˜åº¦**ï¼šæ˜ç¡®å±•ç¤ºä¿¡æ¯æ¥æºï¼Œå¢å¼ºä¿¡ä»»

---

## ä¸ºä»€ä¹ˆéœ€è¦å¼•ç”¨æº¯æºï¼Ÿ

### é—®é¢˜ï¼šç¼ºä¹å¼•ç”¨çš„é£é™©

```
ç”Ÿæˆå†…å®¹ï¼š"Pythonç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚"

ç”¨æˆ·ç–‘é—®ï¼š
- è¿™ä¸ªä¿¡æ¯ä»å“ªé‡Œæ¥çš„ï¼Ÿ
- æˆ‘å¦‚ä½•éªŒè¯è¿™æ˜¯çœŸçš„ï¼Ÿ
- å¦‚æœæœ‰é”™è¯¯ï¼Œè´£ä»»åœ¨è°ï¼Ÿ

è§£å†³æ–¹æ¡ˆï¼šæ·»åŠ å¼•ç”¨
"Pythonç”±Guido van Rossumäº1991å¹´åˆ›å»º[1]ã€‚"

[1] Pythonå®˜æ–¹æ–‡æ¡£ï¼Œhttps://docs.python.org/3/faq/general.html
```

### å¼•ç”¨æº¯æºçš„ä»·å€¼

| ç»´åº¦ | æ— å¼•ç”¨ | æœ‰å¼•ç”¨ |
|------|--------|--------|
| **ç”¨æˆ·ä¿¡ä»»åº¦** | ä½ï¼ˆæ— æ³•éªŒè¯ï¼‰ | é«˜ï¼ˆå¯è¿½æº¯ï¼‰ |
| **æ³•å¾‹åˆè§„** | é£é™©é«˜ | ç¬¦åˆè¦æ±‚ |
| **è°ƒè¯•æ•ˆç‡** | éš¾ä»¥å®šä½é”™è¯¯æ¥æº | å¿«é€Ÿå®šä½é—®é¢˜ |
| **ç”¨æˆ·ä½“éªŒ** | è¢«åŠ¨æ¥å— | ä¸»åŠ¨éªŒè¯ |

---

## å¼•ç”¨æº¯æºçš„å®ç°æ–¹æ³•

### æ–¹æ³•1ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆæ¨èï¼‰

```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('all-MiniLM-L6-v2')

def find_supporting_context(claim: str, contexts: list[str], threshold: float = 0.7) -> dict:
    """ä¸ºå£°æ˜æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡"""

    # ç¼–ç å£°æ˜å’Œä¸Šä¸‹æ–‡
    claim_emb = model.encode(claim, convert_to_tensor=True)
    context_embs = model.encode(contexts, convert_to_tensor=True)

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = util.cos_sim(claim_emb, context_embs)[0]

    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡
    best_idx = similarities.argmax().item()
    best_score = similarities[best_idx].item()

    if best_score >= threshold:
        return {
            "supported": True,
            "source_index": best_idx,
            "source_text": contexts[best_idx],
            "similarity": best_score
        }
    else:
        return {
            "supported": False,
            "similarity": best_score
        }

# ä½¿ç”¨ç¤ºä¾‹
claim = "Pythonç”±Guido van Rossumåˆ›å»º"
contexts = [
    "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚",
    "Pythonç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚",
    "Pythonå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦ã€‚"
]

result = find_supporting_context(claim, contexts)
if result["supported"]:
    print(f"âœ… å£°æ˜è¢«æ”¯æŒ")
    print(f"   æ¥æº: {result['source_text']}")
    print(f"   ç›¸ä¼¼åº¦: {result['similarity']:.2f}")
```

### æ–¹æ³•2ï¼šç²¾ç¡®åŒ¹é…

```python
def exact_match_attribution(claim: str, contexts: list[str]) -> dict:
    """ç²¾ç¡®åŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«å£°æ˜çš„ä¸Šä¸‹æ–‡"""

    claim_lower = claim.lower()

    for idx, context in enumerate(contexts):
        if claim_lower in context.lower():
            return {
                "supported": True,
                "source_index": idx,
                "source_text": context,
                "match_type": "exact"
            }

    return {"supported": False}

# ä½¿ç”¨ç¤ºä¾‹
claim = "Pythonç”±Guidoåˆ›å»º"
contexts = [
    "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚",
    "Pythonç”±Guido van Rossumåˆ›å»ºã€‚",
    "Pythonå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦ã€‚"
]

result = exact_match_attribution(claim, contexts)
if result["supported"]:
    print(f"âœ… ç²¾ç¡®åŒ¹é…")
    print(f"   æ¥æº: {result['source_text']}")
```

### æ–¹æ³•3ï¼šNLIéªŒè¯ + æº¯æº

```python
from transformers import pipeline

nli_model = pipeline("text-classification", model="microsoft/deberta-v3-base-mnli-fever-anli")

def nli_based_attribution(claim: str, contexts: list[str]) -> dict:
    """åŸºäºNLIçš„å¼•ç”¨æº¯æº"""

    best_result = {
        "supported": False,
        "best_score": 0.0,
        "source_index": -1
    }

    for idx, context in enumerate(contexts):
        # NLIéªŒè¯
        input_text = f"{context} [SEP] {claim}"
        result = nli_model(input_text)[0]

        if result["label"] == "entailment" and result["score"] > best_result["best_score"]:
            best_result = {
                "supported": True,
                "best_score": result["score"],
                "source_index": idx,
                "source_text": context
            }

    return best_result

# ä½¿ç”¨ç¤ºä¾‹
result = nli_based_attribution(claim, contexts)
if result["supported"]:
    print(f"âœ… NLIéªŒè¯é€šè¿‡")
    print(f"   æ¥æº: {result['source_text']}")
    print(f"   ç½®ä¿¡åº¦: {result['best_score']:.2f}")
```

---

## å¼•ç”¨æ ¼å¼

### æ ¼å¼1ï¼šè¡Œå†…å¼•ç”¨ï¼ˆæ¨èï¼‰

```python
def format_inline_citation(claims: list[dict]) -> str:
    """ç”Ÿæˆè¡Œå†…å¼•ç”¨æ ¼å¼"""

    cited_text = ""
    citations = []

    for i, claim_info in enumerate(claims):
        if claim_info["supported"]:
            citation_num = claim_info["source_index"] + 1
            cited_text += f"{claim_info['claim']}[{citation_num}]"
            citations.append({
                "num": citation_num,
                "text": claim_info["source_text"]
            })
        else:
            # æœªè¢«æ”¯æŒçš„å£°æ˜ä¸æ·»åŠ å¼•ç”¨
            cited_text += f"[æœªéªŒè¯ï¼š{claim_info['claim']}]"

    # ç”Ÿæˆå¼•ç”¨åˆ—è¡¨
    citation_list = "\n\nå¼•ç”¨æ¥æºï¼š\n"
    for cite in citations:
        citation_list += f"[{cite['num']}] {cite['text']}\n"

    return cited_text + citation_list

# ä½¿ç”¨ç¤ºä¾‹
claims = [
    {"claim": "Pythonæ˜¯é«˜çº§è¯­è¨€", "supported": True, "source_index": 0, "source_text": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚"},
    {"claim": "Pythonç”±Guidoåˆ›å»º", "supported": True, "source_index": 1, "source_text": "Pythonç”±Guido van Rossumåˆ›å»ºã€‚"},
    {"claim": "Pythonç”¨äºAIå¼€å‘", "supported": False}
]

result = format_inline_citation(claims)
print(result)
```

**è¾“å‡ºï¼š**
```
Pythonæ˜¯é«˜çº§è¯­è¨€[1]Pythonç”±Guidoåˆ›å»º[2][æœªéªŒè¯ï¼šPythonç”¨äºAIå¼€å‘]

å¼•ç”¨æ¥æºï¼š
[1] Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚
[2] Pythonç”±Guido van Rossumåˆ›å»ºã€‚
```

### æ ¼å¼2ï¼šè„šæ³¨å¼•ç”¨

```python
def format_footnote_citation(claims: list[dict]) -> str:
    """ç”Ÿæˆè„šæ³¨å¼•ç”¨æ ¼å¼"""

    cited_text = ""
    footnotes = []

    for i, claim_info in enumerate(claims):
        if claim_info["supported"]:
            footnote_num = len(footnotes) + 1
            cited_text += f"{claim_info['claim']}^{footnote_num}"
            footnotes.append({
                "num": footnote_num,
                "text": claim_info["source_text"],
                "url": claim_info.get("source_url", "")
            })

    # ç”Ÿæˆè„šæ³¨
    footnote_list = "\n\n---\n"
    for fn in footnotes:
        if fn["url"]:
            footnote_list += f"^{fn['num']} {fn['text']} ({fn['url']})\n"
        else:
            footnote_list += f"^{fn['num']} {fn['text']}\n"

    return cited_text + footnote_list
```

### æ ¼å¼3ï¼šSpan-levelå¼•ç”¨ï¼ˆé«˜çº§ï¼‰

```python
def format_span_citation(text: str, attributions: list[dict]) -> str:
    """ç”ŸæˆSpan-levelå¼•ç”¨ï¼ˆHTMLæ ¼å¼ï¼‰"""

    html = "<div>"

    for attr in attributions:
        span_text = attr["span_text"]
        source_idx = attr["source_index"]

        # æ›¿æ¢ä¸ºå¸¦å¼•ç”¨çš„span
        cited_span = f'<span data-source="{source_idx}" class="cited">{span_text}</span>'
        html = html.replace(span_text, cited_span)

    html += "</div>"

    # æ·»åŠ å¼•ç”¨åˆ—è¡¨
    html += "\n<div class='citations'>"
    for i, source in enumerate(attr["sources"]):
        html += f"<div id='source-{i}'>[{i+1}] {source}</div>"
    html += "</div>"

    return html
```

---

## åœ¨RAGä¸­çš„é›†æˆ

### å®Œæ•´å¼•ç”¨æº¯æºæµç¨‹

```python
def rag_with_attribution(question: str) -> dict:
    """é›†æˆå¼•ç”¨æº¯æºçš„RAGç³»ç»Ÿ"""

    # 1. æ£€ç´¢
    contexts = retrieve_contexts(question)

    # 2. ç”Ÿæˆ
    answer = generate_answer(question, contexts)

    # 3. å£°æ˜åˆ†è§£
    claims = decompose_claims(answer)

    # 4. ä¸ºæ¯ä¸ªå£°æ˜æ‰¾åˆ°å¼•ç”¨
    attributed_claims = []
    for claim in claims:
        attribution = find_supporting_context(claim, contexts)

        attributed_claims.append({
            "claim": claim,
            "supported": attribution["supported"],
            "source_index": attribution.get("source_index", -1),
            "source_text": attribution.get("source_text", ""),
            "similarity": attribution.get("similarity", 0.0)
        })

    # 5. ç”Ÿæˆå¸¦å¼•ç”¨çš„å›ç­”
    cited_answer = format_inline_citation(attributed_claims)

    # 6. è®¡ç®—æ”¯æŒç‡
    support_ratio = sum(1 for c in attributed_claims if c["supported"]) / len(attributed_claims)

    return {
        "answer": cited_answer,
        "original_answer": answer,
        "support_ratio": support_ratio,
        "attributed_claims": attributed_claims,
        "contexts": contexts
    }
```

---

## 2026å¹´æœ€æ–°æŠ€æœ¯

### 1. ç»Ÿä¸€å½’å› ç®¡é“ï¼ˆarXiv 2601.19927, 2026ï¼‰

```python
from attribution_pipeline import UnifiedAttributionPipeline

pipeline = UnifiedAttributionPipeline()

def unified_attribution(response: str, contexts: list[str]) -> dict:
    """ä½¿ç”¨ç»Ÿä¸€å½’å› ç®¡é“"""

    result = pipeline.attribute(
        generated_text=response,
        source_documents=contexts,
        granularity="claim",  # å£°æ˜çº§å½’å› 
        method="hybrid"  # æ··åˆæ–¹æ³•ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ + NLIï¼‰
    )

    return {
        "attributed_text": result["attributed_text"],
        "attributions": result["attributions"],
        "confidence_scores": result["confidence_scores"]
    }
```

### 2. å¤šæºå¼•ç”¨

```python
def multi_source_attribution(claim: str, contexts: list[str], top_k: int = 3) -> dict:
    """ä¸ºä¸€ä¸ªå£°æ˜æ‰¾åˆ°å¤šä¸ªæ”¯æŒæ¥æº"""

    model = SentenceTransformer('all-MiniLM-L6-v2')

    claim_emb = model.encode(claim, convert_to_tensor=True)
    context_embs = model.encode(contexts, convert_to_tensor=True)

    similarities = util.cos_sim(claim_emb, context_embs)[0]

    # æ‰¾åˆ°Top-Kæœ€ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡
    top_k_indices = similarities.argsort(descending=True)[:top_k]

    sources = []
    for idx in top_k_indices:
        if similarities[idx] > 0.7:  # é˜ˆå€¼
            sources.append({
                "source_index": idx.item(),
                "source_text": contexts[idx.item()],
                "similarity": similarities[idx].item()
            })

    return {
        "claim": claim,
        "sources": sources,
        "num_sources": len(sources)
    }

# ä½¿ç”¨ç¤ºä¾‹
claim = "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€"
contexts = [
    "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œæ˜“äºå­¦ä¹ ã€‚",
    "Pythonæ˜¯é«˜çº§è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦ã€‚",
    "Pythonè¯­æ³•ç®€æ´ï¼Œæ˜¯é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚"
]

result = multi_source_attribution(claim, contexts, top_k=3)
print(f"å£°æ˜ï¼š{result['claim']}")
print(f"æ‰¾åˆ°{result['num_sources']}ä¸ªæ”¯æŒæ¥æºï¼š")
for i, source in enumerate(result['sources'], 1):
    print(f"  [{i}] {source['source_text']} (ç›¸ä¼¼åº¦: {source['similarity']:.2f})")
```

### 3. äº¤äº’å¼å¼•ç”¨

```python
def interactive_citation(attributed_claims: list[dict]) -> str:
    """ç”Ÿæˆäº¤äº’å¼å¼•ç”¨ï¼ˆMarkdownæ ¼å¼ï¼‰"""

    markdown = ""

    for claim_info in attributed_claims:
        if claim_info["supported"]:
            source_idx = claim_info["source_index"]
            claim = claim_info["claim"]

            # ç”Ÿæˆå¯ç‚¹å‡»çš„å¼•ç”¨é“¾æ¥
            markdown += f"{claim} [[{source_idx+1}]](#source-{source_idx})\n\n"

    # æ·»åŠ å¼•ç”¨åˆ—è¡¨ï¼ˆå¸¦é”šç‚¹ï¼‰
    markdown += "\n---\n\n## å¼•ç”¨æ¥æº\n\n"
    for i, claim_info in enumerate(attributed_claims):
        if claim_info["supported"]:
            markdown += f"<a id='source-{i}'></a>\n"
            markdown += f"**[{i+1}]** {claim_info['source_text']}\n\n"

    return markdown
```

---

## ç”¨æˆ·ä½“éªŒä¼˜åŒ–

### 1. æ‚¬æµ®é¢„è§ˆ

```javascript
// å‰ç«¯å®ç°ï¼šé¼ æ ‡æ‚¬åœæ˜¾ç¤ºæºæ–‡æ¡£ç‰‡æ®µ
document.querySelectorAll('.citation').forEach(cite => {
    cite.addEventListener('mouseover', (e) => {
        const sourceId = e.target.dataset.source;
        const sourceText = document.getElementById(`source-${sourceId}`).textContent;

        // æ˜¾ç¤ºæ‚¬æµ®çª—
        showTooltip(e.target, sourceText);
    });
});
```

### 2. é«˜äº®åŒ¹é…

```python
def highlight_supporting_text(claim: str, source_text: str) -> str:
    """åœ¨æºæ–‡æ¡£ä¸­é«˜äº®æ”¯æŒè¯¥å£°æ˜çš„éƒ¨åˆ†"""

    # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°æœ€ç›¸å…³çš„ç‰‡æ®µ
    from difflib import SequenceMatcher

    claim_words = claim.lower().split()
    source_words = source_text.lower().split()

    # æ‰¾åˆ°æœ€é•¿å…¬å…±å­åºåˆ—
    matcher = SequenceMatcher(None, claim_words, source_words)
    match = matcher.find_longest_match(0, len(claim_words), 0, len(source_words))

    if match.size > 0:
        # é«˜äº®åŒ¹é…éƒ¨åˆ†
        highlighted = source_text
        start_idx = sum(len(w) + 1 for w in source_words[:match.b])
        end_idx = start_idx + sum(len(w) + 1 for w in source_words[match.b:match.b + match.size])

        highlighted = (
            source_text[:start_idx] +
            f"**{source_text[start_idx:end_idx]}**" +
            source_text[end_idx:]
        )

        return highlighted

    return source_text
```

### 3. ç½®ä¿¡åº¦æ˜¾ç¤º

```python
def format_citation_with_confidence(claim_info: dict) -> str:
    """æ˜¾ç¤ºå¼•ç”¨çš„ç½®ä¿¡åº¦"""

    claim = claim_info["claim"]
    source_idx = claim_info["source_index"]
    confidence = claim_info["similarity"]

    # æ ¹æ®ç½®ä¿¡åº¦é€‰æ‹©å›¾æ ‡
    if confidence > 0.9:
        icon = "ğŸŸ¢"  # é«˜ç½®ä¿¡åº¦
    elif confidence > 0.7:
        icon = "ğŸŸ¡"  # ä¸­ç½®ä¿¡åº¦
    else:
        icon = "ğŸ”´"  # ä½ç½®ä¿¡åº¦

    return f"{claim} [{source_idx+1}] {icon} {confidence:.0%}"

# ä½¿ç”¨ç¤ºä¾‹
claim_info = {
    "claim": "Pythonç”±Guidoåˆ›å»º",
    "source_index": 1,
    "similarity": 0.95
}

print(format_citation_with_confidence(claim_info))
# è¾“å‡º: Pythonç”±Guidoåˆ›å»º [2] ğŸŸ¢ 95%
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†ä¸€ä¸ªå£°æ˜éœ€è¦å¤šä¸ªæ¥æºæ”¯æŒçš„æƒ…å†µï¼Ÿ

**A:** ä½¿ç”¨å¤šæºå¼•ç”¨

```python
def multi_source_citation(claim: str, contexts: list[str]) -> str:
    """ä¸ºä¸€ä¸ªå£°æ˜æ·»åŠ å¤šä¸ªå¼•ç”¨"""

    sources = find_all_supporting_contexts(claim, contexts, threshold=0.7)

    if len(sources) == 0:
        return f"[æœªéªŒè¯ï¼š{claim}]"
    elif len(sources) == 1:
        return f"{claim}[{sources[0]['index']+1}]"
    else:
        # å¤šä¸ªæ¥æº
        indices = ",".join(str(s['index']+1) for s in sources)
        return f"{claim}[{indices}]"

# ç¤ºä¾‹
claim = "Pythonæ˜¯é«˜çº§è¯­è¨€"
contexts = [
    "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚",
    "Pythonæ˜¯é«˜çº§è¯­è¨€ï¼Œæ˜“äºå­¦ä¹ ã€‚",
    "Pythonå±äºé«˜çº§ç¼–ç¨‹è¯­è¨€ç±»åˆ«ã€‚"
]

result = multi_source_citation(claim, contexts)
print(result)
# è¾“å‡º: Pythonæ˜¯é«˜çº§è¯­è¨€[1,2,3]
```

### Q2: å¦‚ä½•å¤„ç†éƒ¨åˆ†æ”¯æŒçš„æƒ…å†µï¼Ÿ

**A:** æ ‡æ³¨æ”¯æŒç¨‹åº¦

```python
def partial_support_citation(claim: str, context: str, similarity: float) -> str:
    """æ ‡æ³¨éƒ¨åˆ†æ”¯æŒçš„å£°æ˜"""

    if similarity > 0.9:
        support_level = "å®Œå…¨æ”¯æŒ"
    elif similarity > 0.7:
        support_level = "éƒ¨åˆ†æ”¯æŒ"
    else:
        support_level = "å¼±æ”¯æŒ"

    return f"{claim} [{support_level}, ç›¸ä¼¼åº¦: {similarity:.0%}]"
```

### Q3: å¼•ç”¨æº¯æºçš„æ€§èƒ½å¼€é”€å¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**A:** 3ç§ä¼˜åŒ–ç­–ç•¥

1. **æ‰¹é‡Embedding**
   ```python
   # ä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰å£°æ˜å’Œä¸Šä¸‹æ–‡
   claim_embs = model.encode(claims)
   context_embs = model.encode(contexts)

   # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
   similarities = util.cos_sim(claim_embs, context_embs)
   ```

2. **ç¼“å­˜Embedding**
   ```python
   # ç¼“å­˜ä¸Šä¸‹æ–‡çš„Embedding
   context_emb_cache = {}

   def get_context_embedding(context: str):
       if context not in context_emb_cache:
           context_emb_cache[context] = model.encode(context)
       return context_emb_cache[context]
   ```

3. **å¼‚æ­¥å¤„ç†**
   ```python
   import asyncio

   async def async_attribution(claims: list[str], contexts: list[str]):
       tasks = [
           find_supporting_context_async(claim, contexts)
           for claim in claims
       ]
       return await asyncio.gather(*tasks)
   ```

---

## å®é™…æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šå­¦æœ¯è®ºæ–‡å¼•ç”¨

```python
# åœºæ™¯ï¼šå­¦æœ¯é—®ç­”ç³»ç»Ÿ
question = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
contexts = [
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚ï¼ˆLeCun et al., Nature 2015ï¼‰",
    "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚ï¼ˆKrizhevsky et al., NIPS 2012ï¼‰"
]

answer = "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œï¼Œåœ¨å›¾åƒè¯†åˆ«ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚"

# å¼•ç”¨æº¯æº
claims = [
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯",
    "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ",
    "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•"
]

attributed_answer = """
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯[1]ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ[1]ï¼Œåœ¨å›¾åƒè¯†åˆ«ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•[2]ã€‚

å¼•ç”¨æ¥æºï¼š
[1] LeCun et al., "Deep learning", Nature 2015
[2] Krizhevsky et al., "ImageNet Classification with Deep CNNs", NIPS 2012
"""
```

### æ¡ˆä¾‹2ï¼šåŒ»ç–—å’¨è¯¢å¼•ç”¨

```python
# åœºæ™¯ï¼šåŒ»ç–—é—®ç­”ç³»ç»Ÿ
question = "æ„Ÿå†’åº”è¯¥å¦‚ä½•æ²»ç–—ï¼Ÿ"
contexts = [
    "æ„Ÿå†’çš„å¸¸è§æ²»ç–—æ–¹æ³•åŒ…æ‹¬å¤šå–æ°´ã€ä¼‘æ¯å’Œæœç”¨é€€çƒ§è¯ã€‚ï¼ˆã€Šæ„Ÿå†’è¯Šç–—æŒ‡å—2025ç‰ˆã€‹ç¬¬5é¡µï¼‰",
    "æ„Ÿå†’æ‚£è€…åº”é¿å…å‰§çƒˆè¿åŠ¨ã€‚ï¼ˆã€Šå®¶åº­åŒ»ç–—æ‰‹å†Œã€‹ç¬¬120é¡µï¼‰"
]

answer = "æ„Ÿå†’çš„æ²»ç–—æ–¹æ³•åŒ…æ‹¬å¤šå–æ°´ã€ä¼‘æ¯ã€æœç”¨é€€çƒ§è¯ï¼Œå¹¶é¿å…å‰§çƒˆè¿åŠ¨ã€‚"

# å¼•ç”¨æº¯æºï¼ˆåŒ»ç–—åœºæ™¯è¦æ±‚æ›´ä¸¥æ ¼ï¼‰
attributed_answer = """
æ„Ÿå†’çš„æ²»ç–—æ–¹æ³•åŒ…æ‹¬ï¼š
- å¤šå–æ°´[1]
- ä¼‘æ¯[1]
- æœç”¨é€€çƒ§è¯[1]
- é¿å…å‰§çƒˆè¿åŠ¨[2]

å¼•ç”¨æ¥æºï¼š
[1] ã€Šæ„Ÿå†’è¯Šç–—æŒ‡å—2025ç‰ˆã€‹ç¬¬5é¡µ
[2] ã€Šå®¶åº­åŒ»ç–—æ‰‹å†Œã€‹ç¬¬120é¡µ

âš ï¸ ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œè¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚
"""
```

---

## å­¦ä¹ èµ„æº

### è®ºæ–‡

- **Attribution Techniques for RAG Systems** (arXiv 2601.19927, 2026)
- **Faithful Attribution in RAG** (2025)
- **Span-level Attribution for LLMs** (2024)

### å·¥å…·

- **LangChain Attribution**: https://python.langchain.com/docs/modules/chains/additional/attribution
- **LlamaIndex Citation**: https://docs.llamaindex.ai/en/stable/examples/query_engine/citation_query_engine.html

### æœ€ä½³å®è·µ

- **Datadog LLM Observability** (2025)
- **AWS RAG Attribution Guide** (2025)

---

**è®°ä½ï¼šå¼•ç”¨æº¯æºä¸ä»…æ˜¯æŠ€æœ¯è¦æ±‚ï¼Œæ›´æ˜¯æå‡ç”¨æˆ·ä¿¡ä»»å’Œç³»ç»Ÿé€æ˜åº¦çš„å…³é”®ã€‚åœ¨åŒ»ç–—ã€æ³•å¾‹ç­‰é«˜é£é™©åœºæ™¯ä¸­ï¼Œå¼•ç”¨æº¯æºæ˜¯å¿…éœ€çš„ã€‚**
