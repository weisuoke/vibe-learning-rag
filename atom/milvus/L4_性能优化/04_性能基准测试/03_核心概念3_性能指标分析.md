# 核心概念3：性能指标分析

> 深入理解性能指标的含义、计算方法和分析技巧

---

## 什么是性能指标？

**性能指标是量化系统性能表现的可测量数值。**

类比：
- **前端开发**：FCP、LCP、TTI 是网页性能指标
- **日常生活**：体温、血压、心率是健康指标

---

## 指标体系

```
性能指标体系
├── 1. 吞吐量指标（QPS、TPS）
├── 2. 响应时间指标（延迟、百分位数）
├── 3. 准确性指标（召回率、精确率）
├── 4. 资源使用指标（CPU、内存、磁盘、网络）
└── 5. 稳定性指标（错误率、可用性）
```

---

## 指标1：QPS（每秒查询数）

### 1.1 定义

**QPS (Queries Per Second) = 单位时间内系统能处理的查询请求数量**

```python
QPS = 总查询数 / 总耗时（秒）
```

### 1.2 计算方法

```python
import time
from pymilvus import Collection

def calculate_qps(collection, iterations=1000):
    """计算 QPS"""

    test_vector = [[0.1] * 768]

    start_time = time.time()
    for _ in range(iterations):
        collection.search(
            data=test_vector,
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=10
        )
    total_time = time.time() - start_time

    qps = iterations / total_time
    print(f"QPS: {qps:.2f}")
    return qps
```

### 1.3 影响因素

**QPS 受以下因素影响：**

```python
影响因素 = {
    "索引类型": {
        "FLAT": "QPS 低（精确检索，计算量大）",
        "IVF_FLAT": "QPS 中等",
        "HNSW": "QPS 高（近似检索，计算量小）"
    },
    "数据规模": {
        "100万": "QPS 高",
        "1000万": "QPS 中等",
        "1亿": "QPS 低"
    },
    "并发数": {
        "1": "QPS = 1 / 延迟",
        "10": "QPS 增加，但不是线性",
        "100": "QPS 达到瓶颈"
    },
    "硬件": {
        "CPU": "核心数越多，QPS 越高",
        "内存": "内存不足会导致 QPS 下降",
        "网络": "网络延迟影响 QPS"
    }
}
```

### 1.4 优化策略

```python
# 策略1：使用更高效的索引
索引优化 = {
    "FLAT → IVF_FLAT": "QPS 提升 5-10x",
    "IVF_FLAT → HNSW": "QPS 提升 2-3x"
}

# 策略2：批量查询
def batch_search(collection, vectors, batch_size=10):
    """批量查询提升 QPS"""
    results = []
    for i in range(0, len(vectors), batch_size):
        batch = vectors[i:i+batch_size]
        result = collection.search(
            data=batch,
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=10
        )
        results.extend(result)
    return results

# 策略3：减少 nprobe/ef 值
参数优化 = {
    "nprobe=100 → nprobe=10": "QPS 提升 3-5x",
    "ef=128 → ef=64": "QPS 提升 1.5-2x"
}
```

---

## 指标2：延迟（响应时间）

### 2.1 定义

**延迟 (Latency) = 单个请求从发起到返回结果的时间**

```python
延迟 = 结束时间 - 开始时间
```

### 2.2 百分位数（Percentile）

**为什么需要百分位数？**

```python
# 示例：100次查询的延迟分布
延迟数据 = [10, 12, 11, 13, 15, 14, 16, 18, 20, 150]  # 最后一个是异常值

平均延迟 = sum(延迟数据) / len(延迟数据) = 27.9ms  # 被异常值拉高
中位数延迟 = 14.5ms  # 更能代表真实情况

# 百分位数的含义
P50 = 14.5ms  # 50%的请求延迟 <= 14.5ms
P95 = 20ms    # 95%的请求延迟 <= 20ms
P99 = 150ms   # 99%的请求延迟 <= 150ms
```

**计算百分位数：**

```python
import numpy as np

def calculate_percentiles(latencies):
    """计算延迟的百分位数"""

    return {
        "平均值": np.mean(latencies),
        "中位数 (P50)": np.percentile(latencies, 50),
        "P75": np.percentile(latencies, 75),
        "P90": np.percentile(latencies, 90),
        "P95": np.percentile(latencies, 95),
        "P99": np.percentile(latencies, 99),
        "P99.9": np.percentile(latencies, 99.9),
        "最大值": np.max(latencies)
    }

# 使用示例
latencies = [15.2, 16.8, 14.3, 18.9, 20.1, 13.7, 150.5, 17.2, 19.4, 15.8]
percentiles = calculate_percentiles(latencies)

for key, value in percentiles.items():
    print(f"{key}: {value:.2f}ms")
```

### 2.3 延迟分布分析

**正常分布 vs 长尾分布：**

```python
# 正常分布（理想情况）
正常分布 = {
    "P50": "15ms",
    "P95": "25ms",
    "P99": "30ms",
    "特征": "延迟稳定，波动小"
}

# 长尾分布（有问题）
长尾分布 = {
    "P50": "15ms",
    "P95": "50ms",
    "P99": "200ms",
    "特征": "延迟不稳定，有异常值",
    "可能原因": [
        "GC（垃圾回收）",
        "磁盘 I/O 阻塞",
        "网络抖动",
        "CPU 调度延迟"
    ]
}
```

### 2.4 延迟优化策略

```python
# 策略1：优化索引参数
def optimize_for_latency(collection):
    """优化延迟"""

    # 减少 nprobe（IVF 索引）
    search_params = {"metric_type": "L2", "params": {"nprobe": 10}}  # 而非 100

    # 减少 ef（HNSW 索引）
    search_params = {"metric_type": "L2", "params": {"ef": 64}}  # 而非 128

    return search_params

# 策略2：使用分区减少搜索范围
def search_with_partition(collection, vector, partition_name):
    """使用分区减少延迟"""
    results = collection.search(
        data=[vector],
        anns_field="embedding",
        param={"metric_type": "L2", "params": {"nprobe": 10}},
        limit=10,
        partition_names=[partition_name]  # 只搜索特定分区
    )
    return results

# 策略3：预加载数据到内存
collection.load()  # 确保数据已加载到内存
```

---

## 指标3：召回率（Recall）

### 3.1 定义

**召回率 (Recall@K) = 检索到的相关结果数 / 真实相关结果数**

```python
Recall@K = |检索结果 ∩ 真实结果| / |真实结果|
```

### 3.2 计算方法

```python
def calculate_recall(collection, test_vector, search_params, k=10):
    """计算 Recall@K"""

    # 1. 使用 FLAT 索引获取 ground truth
    ground_truth = collection.search(
        data=[test_vector],
        anns_field="embedding",
        param={"metric_type": "L2", "params": {}},  # FLAT 索引
        limit=k
    )
    ground_truth_ids = set([hit.id for hit in ground_truth[0]])

    # 2. 使用目标索引获取检索结果
    search_result = collection.search(
        data=[test_vector],
        anns_field="embedding",
        param=search_params,
        limit=k
    )
    search_result_ids = set([hit.id for hit in search_result[0]])

    # 3. 计算召回率
    intersection = ground_truth_ids & search_result_ids
    recall = len(intersection) / len(ground_truth_ids)

    print(f"Recall@{k}: {recall:.2%}")
    return recall
```

### 3.3 召回率 vs 性能权衡

```python
# nprobe 对召回率和延迟的影响
nprobe_analysis = {
    "nprobe=5": {"召回率": "85%", "延迟": "8ms"},
    "nprobe=10": {"召回率": "92%", "延迟": "15ms"},
    "nprobe=20": {"召回率": "96%", "延迟": "28ms"},
    "nprobe=50": {"召回率": "98%", "延迟": "65ms"},
    "nprobe=100": {"召回率": "99%", "延迟": "120ms"}
}

# 如何选择？
选择策略 = {
    "实时交互": "nprobe=10（召回率92%，延迟15ms）",
    "高精度场景": "nprobe=50（召回率98%，延迟65ms）",
    "批量处理": "nprobe=20（召回率96%，延迟28ms）"
}
```

### 3.4 提升召回率的方法

```python
# 方法1：增加 nprobe/ef 值
def improve_recall_by_params(collection, test_vector):
    """通过增加参数提升召回率"""

    # IVF 索引：增加 nprobe
    search_params = {"metric_type": "L2", "params": {"nprobe": 50}}  # 从 10 增加到 50

    # HNSW 索引：增加 ef
    search_params = {"metric_type": "L2", "params": {"ef": 128}}  # 从 64 增加到 128

    return collection.search(
        data=[test_vector],
        anns_field="embedding",
        param=search_params,
        limit=10
    )

# 方法2：使用更精确的索引类型
索引选择 = {
    "FLAT": "召回率 100%（精确检索）",
    "IVF_FLAT": "召回率 95-99%",
    "HNSW": "召回率 95-99%",
    "IVF_SQ8": "召回率 90-95%（量化损失）"
}

# 方法3：优化向量质量
def improve_vector_quality(vectors):
    """优化向量质量"""

    # 归一化向量（如果使用 IP 距离）
    import numpy as np
    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)

    return vectors
```

---

## 指标4：资源使用率

### 4.1 CPU 使用率

```python
import psutil

def monitor_cpu_usage():
    """监控 CPU 使用率"""

    # 总体 CPU 使用率
    cpu_percent = psutil.cpu_percent(interval=1)
    print(f"CPU 使用率: {cpu_percent}%")

    # 每个核心的使用率
    cpu_per_core = psutil.cpu_percent(interval=1, percpu=True)
    for i, percent in enumerate(cpu_per_core):
        print(f"  核心 {i}: {percent}%")

    # CPU 瓶颈判断
    if cpu_percent > 90:
        print("⚠️ CPU 瓶颈：考虑优化索引类型或增加 CPU 核心数")
```

### 4.2 内存使用量

```python
def monitor_memory_usage():
    """监控内存使用量"""

    memory = psutil.virtual_memory()

    print(f"总内存: {memory.total / (1024**3):.2f} GB")
    print(f"已使用: {memory.used / (1024**3):.2f} GB")
    print(f"可用: {memory.available / (1024**3):.2f} GB")
    print(f"使用率: {memory.percent}%")

    # 内存瓶颈判断
    if memory.percent > 90:
        print("⚠️ 内存瓶颈：考虑使用量化索引或增加内存")
```

### 4.3 磁盘 I/O

```python
def monitor_disk_io():
    """监控磁盘 I/O"""

    disk_io = psutil.disk_io_counters()

    print(f"读取字节数: {disk_io.read_bytes / (1024**3):.2f} GB")
    print(f"写入字节数: {disk_io.write_bytes / (1024**3):.2f} GB")
    print(f"读取次数: {disk_io.read_count}")
    print(f"写入次数: {disk_io.write_count}")

    # 磁盘 I/O 瓶颈判断
    # 需要对比前后两次采样
```

### 4.4 网络带宽

```python
def monitor_network():
    """监控网络使用"""

    net_io = psutil.net_io_counters()

    print(f"发送字节数: {net_io.bytes_sent / (1024**3):.2f} GB")
    print(f"接收字节数: {net_io.bytes_recv / (1024**3):.2f} GB")
    print(f"发送包数: {net_io.packets_sent}")
    print(f"接收包数: {net_io.packets_recv}")
```

---

## 指标5：稳定性指标

### 5.1 错误率

```python
def calculate_error_rate(total_requests, failed_requests):
    """计算错误率"""

    error_rate = failed_requests / total_requests
    print(f"错误率: {error_rate:.2%}")

    # SLA 标准
    if error_rate < 0.001:
        print("✅ 优秀：错误率 < 0.1%")
    elif error_rate < 0.01:
        print("⚠️ 良好：错误率 < 1%")
    else:
        print("❌ 较差：错误率 >= 1%")

    return error_rate
```

### 5.2 可用性

```python
def calculate_availability(uptime_seconds, total_seconds):
    """计算可用性"""

    availability = uptime_seconds / total_seconds
    print(f"可用性: {availability:.4%}")

    # SLA 等级
    sla_levels = {
        0.99: "两个9（99%）- 年停机时间 3.65 天",
        0.999: "三个9（99.9%）- 年停机时间 8.76 小时",
        0.9999: "四个9（99.99%）- 年停机时间 52.56 分钟",
        0.99999: "五个9（99.999%）- 年停机时间 5.26 分钟"
    }

    for level, desc in sla_levels.items():
        if availability >= level:
            print(f"✅ 达到 {desc}")
            break

    return availability
```

---

## 综合指标分析

### 完整的性能分析框架

```python
import time
import numpy as np
import psutil
from pymilvus import Collection

class PerformanceAnalyzer:
    """性能分析器"""

    def __init__(self, collection_name):
        self.collection = Collection(collection_name)
        self.collection.load()

    def comprehensive_analysis(self, test_vectors, search_params, iterations=100):
        """综合性能分析"""

        print("=== 开始综合性能分析 ===\n")

        # 1. 吞吐量分析
        print("1. 吞吐量分析")
        qps = self._analyze_throughput(test_vectors, search_params, iterations)

        # 2. 响应时间分析
        print("\n2. 响应时间分析")
        latency_stats = self._analyze_latency(test_vectors, search_params, iterations)

        # 3. 准确性分析
        print("\n3. 准确性分析")
        recall = self._analyze_recall(test_vectors[0], search_params)

        # 4. 资源使用分析
        print("\n4. 资源使用分析")
        resource_stats = self._analyze_resources()

        # 5. 综合评分
        print("\n5. 综合评分")
        score = self._calculate_overall_score(qps, latency_stats, recall, resource_stats)

        return {
            "qps": qps,
            "latency": latency_stats,
            "recall": recall,
            "resources": resource_stats,
            "score": score
        }

    def _analyze_throughput(self, test_vectors, search_params, iterations):
        """分析吞吐量"""
        start_time = time.time()
        for vector in test_vectors[:iterations]:
            self.collection.search(
                data=[vector],
                anns_field="embedding",
                param=search_params,
                limit=10
            )
        total_time = time.time() - start_time
        qps = iterations / total_time
        print(f"  QPS: {qps:.2f}")
        return qps

    def _analyze_latency(self, test_vectors, search_params, iterations):
        """分析响应时间"""
        latencies = []
        for vector in test_vectors[:iterations]:
            start = time.time()
            self.collection.search(
                data=[vector],
                anns_field="embedding",
                param=search_params,
                limit=10
            )
            latencies.append((time.time() - start) * 1000)

        stats = {
            "平均": np.mean(latencies),
            "P50": np.percentile(latencies, 50),
            "P95": np.percentile(latencies, 95),
            "P99": np.percentile(latencies, 99),
            "标准差": np.std(latencies)
        }

        for key, value in stats.items():
            print(f"  {key}: {value:.2f}ms")

        return stats

    def _analyze_recall(self, test_vector, search_params):
        """分析召回率"""
        # Ground truth
        ground_truth = self.collection.search(
            data=[test_vector],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {}},
            limit=10
        )
        ground_truth_ids = set([hit.id for hit in ground_truth[0]])

        # Search result
        result = self.collection.search(
            data=[test_vector],
            anns_field="embedding",
            param=search_params,
            limit=10
        )
        result_ids = set([hit.id for hit in result[0]])

        recall = len(ground_truth_ids & result_ids) / len(ground_truth_ids)
        print(f"  Recall@10: {recall:.2%}")

        return recall

    def _analyze_resources(self):
        """分析资源使用"""
        cpu = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()

        stats = {
            "CPU使用率": cpu,
            "内存使用率": memory.percent
        }

        print(f"  CPU 使用率: {cpu}%")
        print(f"  内存使用率: {memory.percent}%")

        return stats

    def _calculate_overall_score(self, qps, latency_stats, recall, resource_stats):
        """计算综合评分"""

        # 归一化各项指标（0-100分）
        qps_score = min(qps / 10, 100)  # 假设 1000 QPS = 100分
        latency_score = max(0, 100 - latency_stats['P95'])  # 延迟越低越好
        recall_score = recall * 100
        resource_score = max(0, 100 - resource_stats['CPU使用率'])

        # 加权平均
        weights = {
            "qps": 0.3,
            "latency": 0.3,
            "recall": 0.3,
            "resource": 0.1
        }

        overall_score = (
            qps_score * weights['qps'] +
            latency_score * weights['latency'] +
            recall_score * weights['recall'] +
            resource_score * weights['resource']
        )

        print(f"  综合评分: {overall_score:.2f}/100")

        return overall_score

# 使用示例
analyzer = PerformanceAnalyzer("my_collection")
test_vectors = [[0.1] * 768 for _ in range(100)]
search_params = {"metric_type": "L2", "params": {"nprobe": 10}}

results = analyzer.comprehensive_analysis(test_vectors, search_params)
```

---

## 关键洞察

1. **不同指标关注不同维度**
   - QPS → 吞吐能力
   - 延迟 → 用户体验
   - 召回率 → 检索质量
   - 资源使用 → 成本效率

2. **百分位数比平均值更重要**
   - P95/P99 代表最差情况
   - 平均值容易被异常值影响

3. **指标之间存在权衡**
   - QPS ↑ 可能导致延迟 ↑
   - 召回率 ↑ 可能导致延迟 ↑
   - 需要根据业务需求平衡

4. **资源瓶颈影响性能**
   - CPU 瓶颈 → QPS 下降
   - 内存瓶颈 → 延迟增加
   - 磁盘 I/O 瓶颈 → 加载慢
   - 网络瓶颈 → 延迟增加

5. **综合分析才能全面评估**
   - 单一指标不能代表整体性能
   - 需要多维度综合评估

---

## 下一步

理解了性能指标分析后，接下来学习：
- **实战代码** - 使用这些指标进行实际性能测试
- **化骨绵掌** - 10个2分钟知识卡片巩固理解
