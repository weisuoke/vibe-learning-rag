# 最小可用

掌握以下内容，就能在 RAG 开发中正确使用文本分块，并应用 2025-2026 年的最新研究成果。

---

## 4.1 使用 LangChain 的 RecursiveCharacterTextSplitter（2025-2026 推荐配置）

**这是 90% 场景的最佳选择，开箱即用。**

### 基础配置（快速原型）

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 创建分块器（2025-2026 推荐配置）
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,      # NVIDIA 2025: 适合事实查询
    chunk_overlap=77,    # 15% overlap（NVIDIA 最优）
    separators=["\n\n", "\n", "。", ".", "！", "？", " ", ""]
)

# 分块
text = "你的长文本内容..."
chunks = splitter.split_text(text)

# 查看结果
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {len(chunk)} chars")
```

**记住这个配置（2025-2026 标准）：**
- `chunk_size=512`：NVIDIA 研究推荐，适合事实查询
- `chunk_overlap=77`：15% 重叠率（NVIDIA 验证最优）
- 中文分隔符：`"。", "！", "？"`

**研究来源**: [NVIDIA 2025 Chunking Benchmark](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)

---

## 4.2 根据查询类型调整块大小（NVIDIA 2025 研究）

**NVIDIA 2025 年研究发现：查询类型决定最优块大小**

| 查询类型 | chunk_size | chunk_overlap | 原因 | 适用场景 |
|---------|------------|---------------|------|---------|
| **事实查询** | 256-512 | 15% | 需要精准定位具体信息 | 问答系统、知识检索 |
| **分析查询** | 1024+ | 15% | 需要更多上下文进行推理 | 文档摘要、深度分析 |
| **混合查询** | 512-768 | 15% | 平衡精度和上下文 | 通用 RAG 系统 |
| **代码检索** | 500-1000 | 20% | 保持函数完整 | 代码问答、API 文档 |

### 实现代码

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def create_splitter_by_query_type(query_type: str) -> RecursiveCharacterTextSplitter:
    """根据查询类型创建最优分块器（NVIDIA 2025 推荐）"""

    if query_type == "factual":
        # 事实查询：小块，精准匹配
        chunk_size = 512
    elif query_type == "analytical":
        # 分析查询：大块，更多上下文
        chunk_size = 1024
    elif query_type == "code":
        # 代码查询：中等块，保持函数完整
        chunk_size = 800
    else:
        # 混合查询：平衡配置
        chunk_size = 768

    # 15% 重叠率（NVIDIA 最优）
    chunk_overlap = int(chunk_size * 0.15)

    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", "。", ".", "！", "？", " ", ""]
    )

# 使用示例
factual_splitter = create_splitter_by_query_type("factual")
analytical_splitter = create_splitter_by_query_type("analytical")

# 根据场景选择
chunks = factual_splitter.split_text(document)
```

**研究来源**: [NVIDIA 2025 Chunking Benchmark](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)

---

## 4.3 处理中文文档（2025-2026 最佳实践）

**中文需要添加中文标点作为分隔符，并考虑字符编码：**

```python
# 中文文档分块器（2025-2026 推荐）
chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,      # NVIDIA 推荐
    chunk_overlap=77,    # 15% overlap
    separators=[
        "\n\n",      # 段落（最优先）
        "\n",        # 换行
        "。",        # 句号
        "！",        # 感叹号
        "？",        # 问号
        "；",        # 分号
        "，",        # 逗号
        " ",         # 空格
        "",          # 字符（最后手段）
    ],
    length_function=len,  # 使用字符数而非 token 数
)

text = """
第一段内容，介绍了Python的基本概念。Python是一种简单易学的编程语言。

第二段内容，详细讲解了变量和数据类型。变量是存储数据的容器。
"""

chunks = chinese_splitter.split_text(text)

# 验证分块效果
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {len(chunk)} 字符")
    print(f"内容: {chunk[:50]}...")
    print("---")
```

**中文分块注意事项：**
- ✅ 优先在段落边界（`\n\n`）切分
- ✅ 其次在句子边界（`。！？`）切分
- ✅ 避免在词语中间切分
- ✅ 使用 `len()` 计算字符数，而非 token 数

---

## 4.4 与文档加载器配合使用

**完整的文档处理流程（保留元数据）：**

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. 加载文档
loader = PyPDFLoader("document.pdf")
documents = loader.load()

# 2. 创建分块器（NVIDIA 2025 推荐）
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,  # 15% overlap
    separators=["\n\n", "\n", "。", ".", "！", "？", " ", ""]
)

# 3. 分块（保留元数据）
chunks = splitter.split_documents(documents)

# 4. 查看结果
for chunk in chunks[:3]:
    print(f"内容: {chunk.page_content[:100]}...")
    print(f"元数据: {chunk.metadata}")
    print(f"来源: {chunk.metadata.get('source', 'unknown')}")
    print(f"页码: {chunk.metadata.get('page', 'unknown')}")
    print("---")
```

**注意：** 使用 `split_documents()` 而不是 `split_text()`，可以保留文档的元数据（如页码、来源等），这对检索结果的可追溯性非常重要。

---

## 4.5 上下文感知分块（Anthropic 2024-2025）⭐ NEW

**Anthropic 研究：为每个 chunk 添加上下文，减少 49-67% 检索失败**

### 基础实现

```python
from openai import OpenAI

client = OpenAI()

def add_context_to_chunk(chunk: str, document: str, max_context_tokens: int = 100) -> str:
    """
    为 chunk 添加文档级上下文（Anthropic 方法）

    效果：
    - 单独使用：减少 49% 检索失败
    - 结合 reranking：减少 67% 检索失败
    """
    prompt = f"""
    为以下文档片段生成 50-100 token 的上下文说明。

    完整文档：
    {document[:2000]}  # 只传入文档开头，节省成本

    文档片段：
    {chunk}

    要求：
    1. 说明这个片段在文档中的位置（哪个章节/主题）
    2. 概括片段的核心内容
    3. 保持简洁（50-100 tokens）

    格式：
    [上下文说明]
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",  # 使用便宜的模型
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150
    )

    context = response.choices[0].message.content.strip()
    return f"{context}\n\n{chunk}"

# 使用示例
document = "完整文档内容..."
chunks = splitter.split_text(document)

# 为每个 chunk 添加上下文
contextual_chunks = [
    add_context_to_chunk(chunk, document)
    for chunk in chunks
]

# 查看效果
print("原始 chunk:")
print(chunks[0][:200])
print("\n上下文感知 chunk:")
print(contextual_chunks[0][:300])
```

**研究来源**: [Anthropic Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)

**成本优化：**
- 使用 `gpt-4o-mini` 而非 `gpt-4`（成本降低 90%）
- 只传入文档开头 2000 字符（足够生成上下文）
- 批量处理（一次生成多个 chunk 的上下文）

---

## 4.6 快速验证分块效果

**分块后检查这三点：**

```python
def validate_chunks(chunks: list, min_size: int = 100, max_size: int = 600):
    """验证分块效果（2025-2026 标准）"""

    # 1. 检查块数量
    print(f"总块数: {len(chunks)}")

    # 2. 检查块大小分布
    sizes = [len(c) if isinstance(c, str) else len(c.page_content) for c in chunks]
    print(f"块大小: 最小={min(sizes)}, 最大={max(sizes)}, 平均={sum(sizes)//len(sizes)}")

    # 3. 检查是否有异常小/大的块
    too_small = sum(1 for s in sizes if s < min_size)
    too_large = sum(1 for s in sizes if s > max_size)
    print(f"异常块: {too_small} 个太小, {too_large} 个太大")

    # 4. 检查重叠率（NVIDIA 推荐 15%）
    if len(chunks) > 1:
        # 简单估算：检查相邻块的重叠
        sample_overlap = 0
        for i in range(min(5, len(chunks)-1)):
            c1 = chunks[i] if isinstance(chunks[i], str) else chunks[i].page_content
            c2 = chunks[i+1] if isinstance(chunks[i+1], str) else chunks[i+1].page_content
            # 查找重叠部分
            overlap_len = 0
            for j in range(min(len(c1), len(c2))):
                if c1[-j:] == c2[:j]:
                    overlap_len = j
            sample_overlap += overlap_len / len(c1) if len(c1) > 0 else 0

        avg_overlap = sample_overlap / min(5, len(chunks)-1)
        print(f"平均重叠率: {avg_overlap:.1%} (NVIDIA 推荐: 15%)")

    # 5. 抽样查看内容
    print("\n--- 抽样查看 ---")
    sample = chunks[len(chunks)//2]  # 取中间的块
    content = sample if isinstance(sample, str) else sample.page_content
    print(f"示例块: {content[:200]}...")

# 使用
validate_chunks(chunks)
```

**健康的分块应该：**
- ✅ 块大小在目标范围内（±20%）
- ✅ 重叠率接近 15%（NVIDIA 最优）
- ✅ 没有太多异常小的块（<100 字符）
- ✅ 内容在语义边界处切分（段落、句子）

---

## 4.7 生产环境推荐配置（2025-2026）

### 配置1：快速原型（成本优先）

```python
# 适合：快速验证、成本敏感场景
from langchain.text_splitter import RecursiveCharacterTextSplitter

prototype_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,  # 15% overlap
    separators=["\n\n", "\n", "。", ".", "！", "？", " ", ""]
)
```

### 配置2：生产环境（质量优先）

```python
# 适合：生产环境、高质量要求
from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import OpenAI

client = OpenAI()

# 1. 基础分块
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,
)

# 2. 上下文感知（Anthropic 方法）
def production_chunking(document: str) -> list:
    """生产级分块：基础分块 + 上下文感知"""
    # 基础分块
    chunks = splitter.split_text(document)

    # 添加上下文（减少 49% 检索失败）
    contextual_chunks = []
    for chunk in chunks:
        context = add_context_to_chunk(chunk, document)
        contextual_chunks.append(context)

    return contextual_chunks
```

### 配置3：查询类型自适应（NVIDIA 2025）

```python
# 适合：多种查询类型的场景
def adaptive_chunking(document: str, query_type: str = "mixed") -> list:
    """根据查询类型自适应分块"""
    splitter = create_splitter_by_query_type(query_type)
    return splitter.split_text(document)

# 使用
factual_chunks = adaptive_chunking(doc, "factual")      # 事实查询
analytical_chunks = adaptive_chunking(doc, "analytical") # 分析查询
```

---

## 这些知识足以：

- ✅ 为任何文档创建合理的分块
- ✅ 根据查询类型调整分块参数（NVIDIA 2025 研究）
- ✅ 应用上下文感知分块（Anthropic 方法，减少 49-67% 失败）
- ✅ 正确处理中文文档
- ✅ 验证分块效果
- ✅ 与 LangChain 生态无缝集成
- ✅ 在生产环境中部署高质量分块系统

---

## 速查表（2025-2026 标准）

### 通用配置（复制即用）

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# NVIDIA 2025 推荐配置
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,        # 适合事实查询
    chunk_overlap=77,      # 15% overlap（最优）
    separators=["\n\n", "\n", "。", ".", "！", "？", " ", ""]
)

# 分块文本
chunks = splitter.split_text(text)

# 分块文档（保留元数据）
chunks = splitter.split_documents(documents)
```

### 查询类型自适应

```python
# 事实查询（精准匹配）
factual_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512, chunk_overlap=77
)

# 分析查询（更多上下文）
analytical_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1024, chunk_overlap=154
)
```

### 上下文感知（Anthropic）

```python
from openai import OpenAI

def add_context(chunk: str, doc: str) -> str:
    """为 chunk 添加上下文（减少 49% 失败）"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"为片段生成 50-100 token 上下文：\n\n文档：{doc[:2000]}\n\n片段：{chunk}"
        }],
        max_tokens=150
    )
    context = response.choices[0].message.content
    return f"{context}\n\n{chunk}"
```

---

## 核心研究来源

1. **NVIDIA 2025**: [Finding the Best Chunking Strategy](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses)
   - 页面级分块最优（准确率 0.648）
   - 查询类型决定块大小
   - 15% 重叠率最佳

2. **Anthropic 2024-2025**: [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)
   - 上下文感知分块
   - 减少 49-67% 检索失败

3. **IBM 2025-2026**: [Agentic Chunking](https://www.ibm.com/think/topics/agentic-chunking)
   - LLM 驱动的智能分块
   - 提升 15-20% 准确率

---

## 下一步学习

**进阶技术：**
- → [03_核心概念_04_代理式分块Agentic](./03_核心概念_04_代理式分块Agentic.md) - IBM 2025-2026 最新技术
- → [03_核心概念_05_上下文感知分块Contextual](./03_核心概念_05_上下文感知分块Contextual.md) - Anthropic 深度解析

**实战代码：**
- → [07_实战代码_05_上下文感知实现](./07_实战代码_05_上下文感知实现.md) - 完整实现代码
- → [07_实战代码_06_NVIDIA基准测试](./07_实战代码_06_NVIDIA基准测试.md) - 性能对比

**后续流程：**
- → [05_双重类比](./05_双重类比.md) - 用熟悉的概念理解文本分块
- → [04_向量存储](../04_向量存储/) - 分块后的向量如何存储
