# 最小可用知识：链式调试与日志

## 什么是最小可用知识？

**最小可用知识**是指掌握20%的核心技能，就能解决80%的实际问题。

对于链式调试与日志，你需要掌握三个核心技能：

1. **使用 astream_events v2 调试** - 开发阶段快速定位问题
2. **实现自定义回调处理器** - 集成阶段记录关键信息
3. **配置生产监控** - 生产阶段全链路追踪

---

## 核心技能1：使用 astream_events v2 调试

### 为什么需要？

LCEL链是黑盒，不知道内部发生了什么。`astream_events()` 让你实时查看每个步骤的执行过程。

### 最简使用

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import asyncio

load_dotenv()

# 构建链
prompt = ChatPromptTemplate.from_template("回答问题: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model | StrOutputParser()

# 调试链
async def debug_chain():
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2"  # 必须指定 v2
    ):
        kind = event["event"]
        name = event["name"]
        print(f"[{kind}] {name}")

asyncio.run(debug_chain())
```

**输出示例**：
```
[on_chain_start] RunnableSequence
[on_prompt_start] ChatPromptTemplate
[on_prompt_end] ChatPromptTemplate
[on_chat_model_start] ChatOpenAI
[on_chat_model_stream] ChatOpenAI
[on_chat_model_stream] ChatOpenAI
...
[on_chat_model_end] ChatOpenAI
[on_parser_start] StrOutputParser
[on_parser_stream] StrOutputParser
[on_parser_end] StrOutputParser
[on_chain_end] RunnableSequence
```

---

### 常用事件类型

| 事件类型 | 触发时机 | 用途 |
|----------|----------|------|
| `on_chain_start` | 链开始执行 | 查看输入数据 |
| `on_chain_end` | 链执行完成 | 查看最终输出 |
| `on_llm_start` | LLM开始调用 | 查看prompt |
| `on_llm_stream` | LLM流式输出 | 实时查看生成内容 |
| `on_llm_end` | LLM调用完成 | 查看完整响应 |
| `on_tool_start` | 工具开始执行 | 查看工具输入 |
| `on_tool_end` | 工具执行完成 | 查看工具输出 |
| `on_retriever_start` | 检索器开始 | 查看检索查询 |
| `on_retriever_end` | 检索器完成 | 查看检索结果 |

---

### 过滤事件

只查看特定类型的事件：

```python
async def debug_with_filter():
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2",
        include_types=["chat_model"]  # 只看LLM相关事件
    ):
        if event["event"] == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            print(content, end="", flush=True)

asyncio.run(debug_with_filter())
```

---

### 查看中间结果

```python
async def debug_intermediate():
    async for event in chain.astream_events(
        {"question": "什么是LCEL?"},
        version="v2"
    ):
        kind = event["event"]

        # 查看prompt
        if kind == "on_prompt_end":
            messages = event["data"]["output"]
            print(f"Prompt: {messages}")

        # 查看LLM输出
        if kind == "on_chat_model_end":
            output = event["data"]["output"]
            print(f"LLM Output: {output.content}")

asyncio.run(debug_intermediate())
```

---

### 快速上手清单

- [ ] 安装依赖：`uv add langchain-openai`
- [ ] 配置API密钥：`.env` 文件中添加 `OPENAI_API_KEY`
- [ ] 使用 `astream_events(input, version="v2")`
- [ ] 检查 `event["event"]` 判断事件类型
- [ ] 使用 `include_types` 过滤事件
- [ ] 查看 `event["data"]` 获取详细信息

---

## 核心技能2：实现自定义回调处理器

### 为什么需要？

`astream_events()` 适合开发调试，但生产环境需要：
- 记录到文件或数据库
- 自定义日志格式
- 统计性能指标
- 错误分类和处理

### 最简使用

```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import logging

load_dotenv()

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# 自定义回调处理器
class MyLogHandler(BaseCallbackHandler):
    """记录链执行的关键信息"""

    def on_chain_start(self, serialized, inputs, **kwargs):
        """链开始时记录输入"""
        logger.info(f"Chain started with inputs: {inputs}")

    def on_chain_end(self, outputs, **kwargs):
        """链结束时记录输出"""
        logger.info(f"Chain completed with outputs: {outputs}")

    def on_chain_error(self, error, **kwargs):
        """链出错时记录错误"""
        logger.error(f"Chain error: {error}")

# 使用回调处理器
prompt = ChatPromptTemplate.from_template("回答问题: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model

result = chain.invoke(
    {"question": "什么是LCEL?"},
    config={"callbacks": [MyLogHandler()]}
)

print(result.content)
```

**输出示例**：
```
2026-02-20 14:30:00 - __main__ - INFO - Chain started with inputs: {'question': '什么是LCEL?'}
2026-02-20 14:30:02 - __main__ - INFO - Chain completed with outputs: content='LCEL是...'
LCEL是LangChain Expression Language的缩写...
```

---

### 常用回调方法

| 方法 | 触发时机 | 参数 |
|------|----------|------|
| `on_chain_start` | 链开始 | `serialized, inputs` |
| `on_chain_end` | 链结束 | `outputs` |
| `on_chain_error` | 链出错 | `error` |
| `on_llm_start` | LLM开始 | `serialized, prompts` |
| `on_llm_end` | LLM结束 | `response` |
| `on_llm_error` | LLM出错 | `error` |
| `on_tool_start` | 工具开始 | `serialized, input_str` |
| `on_tool_end` | 工具结束 | `output` |
| `on_tool_error` | 工具出错 | `error` |

---

### 记录到文件

```python
import json
from datetime import datetime

class FileLogHandler(BaseCallbackHandler):
    """记录到JSON文件"""

    def __init__(self, log_file="chain_logs.jsonl"):
        self.log_file = log_file

    def _write_log(self, event_type, data):
        """写入日志"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "event": event_type,
            "data": data
        }
        with open(self.log_file, "a") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

    def on_chain_start(self, serialized, inputs, **kwargs):
        self._write_log("chain_start", {"inputs": inputs})

    def on_chain_end(self, outputs, **kwargs):
        self._write_log("chain_end", {"outputs": str(outputs)})

# 使用
result = chain.invoke(
    {"question": "什么是LCEL?"},
    config={"callbacks": [FileLogHandler("my_logs.jsonl")]}
)
```

---

### 性能追踪

```python
import time

class PerformanceHandler(BaseCallbackHandler):
    """追踪性能指标"""

    def __init__(self):
        self.start_time = None
        self.llm_start_time = None

    def on_chain_start(self, serialized, inputs, **kwargs):
        self.start_time = time.time()
        logger.info("Chain started")

    def on_chain_end(self, outputs, **kwargs):
        duration = time.time() - self.start_time
        logger.info(f"Chain completed in {duration:.2f}s")

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.llm_start_time = time.time()

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.llm_start_time
        tokens = response.llm_output.get("token_usage", {})
        logger.info(f"LLM call: {duration:.2f}s, tokens: {tokens}")

# 使用
result = chain.invoke(
    {"question": "什么是LCEL?"},
    config={"callbacks": [PerformanceHandler()]}
)
```

---

### 快速上手清单

- [ ] 继承 `BaseCallbackHandler`
- [ ] 实现需要的回调方法（`on_chain_start`, `on_chain_end` 等）
- [ ] 在 `invoke()` 或 `stream()` 中传入 `config={"callbacks": [handler]}`
- [ ] 使用 `logging` 模块记录日志
- [ ] 考虑异步场景使用 `AsyncCallbackHandler`

---

## 核心技能3：配置生产监控

### 为什么需要？

生产环境需要：
- 全链路追踪（分布式系统）
- 可视化仪表板
- 告警和通知
- 成本追踪
- 性能分析

### 方案1：LangSmith（最简单）

LangSmith是LangChain官方的监控平台，零代码集成。

**步骤1：注册并获取API密钥**

访问 https://smith.langchain.com/ 注册账号，获取API密钥。

**步骤2：配置环境变量**

```bash
# .env 文件
OPENAI_API_KEY=your_openai_key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_key
LANGCHAIN_PROJECT=my_project  # 可选，项目名称
```

**步骤3：运行代码（无需修改）**

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

# 正常使用，自动追踪
prompt = ChatPromptTemplate.from_template("回答问题: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model

result = chain.invoke({"question": "什么是LCEL?"})
print(result.content)
```

**步骤4：查看追踪**

访问 https://smith.langchain.com/ 查看：
- 完整的执行流程
- 每个步骤的输入输出
- 延迟和Token使用
- 成本统计

---

### 方案2：OpenTelemetry（开源方案）

OpenTelemetry是云原生标准，支持导出到多种监控系统。

**步骤1：安装依赖**

```bash
uv add opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp
```

**步骤2：配置OpenTelemetry**

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource

# 配置资源
resource = Resource.create({"service.name": "my-langchain-app"})

# 配置TracerProvider
trace.set_tracer_provider(TracerProvider(resource=resource))

# 配置导出器（导出到Uptrace/Jaeger/Zipkin等）
exporter = OTLPSpanExporter(
    endpoint="http://localhost:4318/v1/traces"  # 根据实际情况修改
)

# 添加处理器
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(exporter)
)

# 获取tracer
tracer = trace.get_tracer(__name__)
```

**步骤3：在链中使用**

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

prompt = ChatPromptTemplate.from_template("回答问题: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model

# 使用span追踪
with tracer.start_as_current_span("my_chain") as span:
    span.set_attribute("question", "什么是LCEL?")
    result = chain.invoke({"question": "什么是LCEL?"})
    span.set_attribute("response_length", len(result.content))

print(result.content)
```

---

### 快速上手清单（LangSmith）

- [ ] 注册 LangSmith 账号
- [ ] 获取 API 密钥
- [ ] 配置环境变量：`LANGCHAIN_TRACING_V2=true`
- [ ] 配置环境变量：`LANGCHAIN_API_KEY=your_key`
- [ ] 运行代码（无需修改）
- [ ] 访问 LangSmith 查看追踪

---

### 快速上手清单（OpenTelemetry）

- [ ] 安装依赖：`uv add opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp`
- [ ] 配置 TracerProvider 和 Exporter
- [ ] 使用 `tracer.start_as_current_span()` 创建span
- [ ] 设置 span 属性：`span.set_attribute()`
- [ ] 配置监控后端（Uptrace/Jaeger/Zipkin）

---

## 常见场景速查表

### 场景1：调试链执行流程

**问题**：不知道链的执行顺序

**解决**：使用 `astream_events()` 查看所有事件

```python
async for event in chain.astream_events(input, version="v2"):
    print(f"[{event['event']}] {event['name']}")
```

---

### 场景2：查看LLM的prompt

**问题**：不知道发送给LLM的实际prompt是什么

**解决**：过滤 `on_chat_model_start` 事件

```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_start":
        messages = event["data"]["input"]["messages"]
        print(f"Prompt: {messages}")
```

---

### 场景3：实时查看LLM输出

**问题**：想要流式查看LLM生成的内容

**解决**：过滤 `on_chat_model_stream` 事件

```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_chat_model_stream":
        print(event["data"]["chunk"].content, end="", flush=True)
```

---

### 场景4：记录所有链执行到文件

**问题**：需要持久化日志

**解决**：使用自定义回调处理器

```python
class FileLogHandler(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        with open("logs.txt", "a") as f:
            f.write(f"Started: {inputs}\n")

chain.invoke(input, config={"callbacks": [FileLogHandler()]})
```

---

### 场景5：统计Token使用和成本

**问题**：不知道每次调用消耗了多少Token

**解决**：在回调中记录 `token_usage`

```python
class TokenHandler(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        tokens = response.llm_output.get("token_usage", {})
        print(f"Tokens: {tokens}")

chain.invoke(input, config={"callbacks": [TokenHandler()]})
```

---

### 场景6：生产环境全链路追踪

**问题**：需要监控生产系统的性能和成本

**解决**：使用 LangSmith

```bash
# .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_key
```

---

### 场景7：调试RAG链的检索结果

**问题**：不知道检索器返回了什么文档

**解决**：过滤 `on_retriever_end` 事件

```python
async for event in chain.astream_events(input, version="v2"):
    if event["event"] == "on_retriever_end":
        docs = event["data"]["output"]
        print(f"Retrieved {len(docs)} documents")
        for doc in docs:
            print(f"- {doc.page_content[:100]}")
```

---

### 场景8：错误处理和重试

**问题**：链执行失败，需要记录错误

**解决**：实现 `on_chain_error` 回调

```python
class ErrorHandler(BaseCallbackHandler):
    def on_chain_error(self, error, **kwargs):
        logger.error(f"Chain failed: {error}")
        # 可以在这里实现重试逻辑

chain.invoke(input, config={"callbacks": [ErrorHandler()]})
```

---

## 三个技能的使用时机

| 阶段 | 技能 | 工具 | 目的 |
|------|------|------|------|
| **开发** | 技能1 | astream_events | 理解流程、快速调试 |
| **集成** | 技能2 | BaseCallbackHandler | 记录日志、性能追踪 |
| **生产** | 技能3 | LangSmith/OpenTelemetry | 监控、告警、优化 |

---

## 学习路径建议

### 第一步：掌握 astream_events（30分钟）

1. 阅读：03_核心概念_01_LCEL内置调试工具.md
2. 实践：07_实战代码_01_基础调试与事件流.md
3. 练习：调试一个简单的链，查看所有事件

---

### 第二步：实现自定义回调（30分钟）

1. 阅读：03_核心概念_02_自定义日志策略.md
2. 实践：07_实战代码_02_自定义回调处理器.md
3. 练习：实现一个记录到文件的回调处理器

---

### 第三步：配置生产监控（30分钟）

1. 阅读：03_核心概念_03_生产环境监控.md
2. 实践：07_实战代码_04_LangSmith追踪.md
3. 练习：配置LangSmith，查看追踪结果

---

## 最后的话

掌握这三个核心技能，你就能：

- ✅ 快速调试LCEL链的执行流程
- ✅ 记录关键信息到文件或数据库
- ✅ 监控生产环境的性能和成本

**记住**：调试和日志不是可选项，而是生产系统的必需品。

---

**版本信息**
- LangChain: v0.3+ (2025-2026)
- Python: 3.13+
- 最后更新: 2026-02-20
