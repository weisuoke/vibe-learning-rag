# 核心概念 03：LLM 令牌流式

## 概述

深入理解 `stream_mode="messages"` 模式，掌握 LLM token 流式输出的原理和实践。

---

## Token 流式原理

### 生成流程

```
用户输入
↓
LLM 开始推理
↓
生成第一个 token → 触发回调 → yield (token, metadata)
↓
生成第二个 token → 触发回调 → yield (token, metadata)
↓
...
↓
生成完成
```

### 回调机制

```python
# langchain_core/tracers/_streaming.py（简化）
class _StreamingCallbackHandler(AsyncCallbackHandler):
    async def on_llm_new_token(
        self,
        token: str,
        *,
        chunk: Optional[BaseMessageChunk] = None,
        **kwargs: Any,
    ) -> None:
        # 创建消息块
        if chunk is None:
            chunk = AIMessageChunk(content=token)

        # 获取元数据
        metadata = {
            "langgraph_node": kwargs.get("node_name"),
            "langgraph_step": kwargs.get("step"),
            "lc_agent_name": kwargs.get("agent_name"),  # 2026 新增
        }

        # 放入流式队列
        await self.queue.put((chunk, metadata))
```

---

## Token 类型详解

### 1. 文本 Token

```python
AIMessageChunk(
    content="Hello",
    chunk_position="first"  # first, middle, last
)
```

**使用示例**：
```python
for token, metadata in agent.stream(input, stream_mode="messages"):
    if isinstance(token, AIMessageChunk) and token.content:
        print(token.content, end="", flush=True)
```

---

### 2. 工具调用 Token

```python
AIMessageChunk(
    content="",
    tool_call_chunks=[
        {
            'name': 'get_weather',
            'args': '{"ci',  # 部分 JSON
            'id': 'call_123',
            'index': 0,
            'type': 'tool_call_chunk'
        }
    ]
)
```

**聚合工具调用**：
```python
tool_call_buffer = {}

for token, metadata in agent.stream(input, stream_mode="messages"):
    if hasattr(token, 'tool_call_chunks'):
        for chunk in token.tool_call_chunks:
            call_id = chunk.get('id')
            if call_id not in tool_call_buffer:
                tool_call_buffer[call_id] = {
                    'name': chunk.get('name'),
                    'args': '',
                    'id': call_id
                }
            tool_call_buffer[call_id]['args'] += chunk.get('args', '')

# 解析完整的工具调用
import json
for call_id, call_data in tool_call_buffer.items():
    try:
        args = json.loads(call_data['args'])
        print(f"工具: {call_data['name']}, 参数: {args}")
    except json.JSONDecodeError:
        print(f"工具调用未完成: {call_data}")
```

---

### 3. 工具结果

```python
ToolMessage(
    content="北京的天气是晴天",
    name="get_weather",
    tool_call_id="call_123"
)
```

---

## 元数据使用

### 元数据字段

```python
metadata = {
    "langgraph_node": "model",      # 节点名称
    "langgraph_step": 1,            # 执行步骤
    "lc_agent_name": "main_agent"  # Agent 名称（2026）
}
```

### 按节点过滤

```python
for token, metadata in agent.stream(input, stream_mode="messages"):
    node = metadata.get('langgraph_node')

    if node == 'model':
        # 只处理模型节点的输出
        if hasattr(token, 'content'):
            print(token.content, end="")
    elif node == 'tools':
        # 只处理工具节点的输出
        print(f"\n[工具结果] {token.content}")
```

### 按 Agent 过滤（2026）

```python
for token, metadata in agent.stream(input, stream_mode="messages"):
    agent_name = metadata.get('lc_agent_name')

    if agent_name == 'main_agent':
        print(f"[主Agent] {token.content}", end="")
    elif agent_name == 'sub_agent':
        print(f"\n[子Agent] {token.content}", end="")
```

---

## 实战场景

### 场景 1：ChatGPT 式对话

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

model = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个友好的助手"),
    ("user", "{input}")
])

chain = prompt | model

async def chat_stream(user_input: str):
    """ChatGPT 式流式对话"""
    async for chunk in chain.astream({"input": user_input}):
        print(chunk.content, end="", flush=True)
    print()  # 换行

# 使用
import asyncio
asyncio.run(chat_stream("介绍一下 Python"))
```

---

### 场景 2：打字机效果

```python
import time

async def typewriter_effect(chain, input, delay=0.05):
    """打字机效果"""
    async for chunk in chain.astream(input):
        if chunk.content:
            print(chunk.content, end="", flush=True)
            await asyncio.sleep(delay)  # 延迟显示
    print()

# 使用
asyncio.run(typewriter_effect(chain, {"input": "讲个笑话"}, delay=0.05))
```

---

### 场景 3：实时翻译

```python
async def streaming_translation(text: str, target_lang: str):
    """实时翻译"""
    prompt = ChatPromptTemplate.from_template(
        "将以下文本翻译成{target_lang}：\n{text}"
    )
    chain = prompt | model

    print(f"翻译中... ", end="")
    async for chunk in chain.astream({"text": text, "target_lang": target_lang}):
        print(chunk.content, end="", flush=True)
    print()

# 使用
asyncio.run(streaming_translation("Hello, world!", "中文"))
```

---

### 场景 4：多 Agent 对话（2026）

```python
from langchain.agents import create_agent

# 创建多个 Agent
agent1 = create_agent(model="gpt-4o-mini", tools=[...], name="agent1")
agent2 = create_agent(model="gpt-4o-mini", tools=[...], name="agent2")

def call_agent2(query: str) -> str:
    result = agent2.invoke({"messages": [{"role": "user", "content": query}]})
    return result["messages"][-1].content

main_agent = create_agent(
    model="gpt-4o-mini",
    tools=[call_agent2],
    name="main_agent"
)

# 区分不同 Agent 的输出
current_agent = None

for token, metadata in main_agent.stream(
    input,
    stream_mode="messages",
    subgraphs=True
):
    agent_name = metadata.get('lc_agent_name')

    if agent_name != current_agent:
        print(f"\n\n[{agent_name}]:")
        current_agent = agent_name

    if hasattr(token, 'content') and token.content:
        print(token.content, end="", flush=True)
```

---

## 高级技巧

### 技巧 1：消息聚合

```python
from langchain_core.messages import AIMessageChunk

full_message = None

for token, metadata in agent.stream(input, stream_mode="messages"):
    if isinstance(token, AIMessageChunk):
        if full_message is None:
            full_message = token
        else:
            full_message = full_message + token  # 累加

        # 检查是否是最后一个 chunk
        if hasattr(token, 'chunk_position') and token.chunk_position == "last":
            print(f"\n完整消息: {full_message.content}")
            if full_message.tool_calls:
                print(f"工具调用: {full_message.tool_calls}")
            full_message = None
```

---

### 技巧 2：Token 计数

```python
token_count = 0
word_count = 0

async for chunk in chain.astream(input):
    if chunk.content:
        print(chunk.content, end="", flush=True)
        token_count += 1
        word_count += len(chunk.content.split())

print(f"\n\nToken 数: {token_count}")
print(f"单词数: {word_count}")
```

---

### 技巧 3：实时保存

```python
output_file = "output.txt"

with open(output_file, "w", encoding="utf-8") as f:
    async for chunk in chain.astream(input):
        if chunk.content:
            print(chunk.content, end="", flush=True)
            f.write(chunk.content)
            f.flush()  # 立即写入磁盘

print(f"\n输出已保存到 {output_file}")
```

---

### 技巧 4：流式 + 结构化输出

```python
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel

class WeatherInfo(BaseModel):
    city: str
    temperature: int
    condition: str

parser = JsonOutputParser(pydantic_object=WeatherInfo)

prompt = ChatPromptTemplate.from_template(
    "以 JSON 格式返回{city}的天气信息\n{format_instructions}"
)

chain = prompt | model | parser

# 流式输出 JSON
json_buffer = ""

async for chunk in chain.astream({
    "city": "北京",
    "format_instructions": parser.get_format_instructions()
}):
    json_buffer += str(chunk)
    print(chunk, end="", flush=True)

print(f"\n解析结果: {json_buffer}")
```

---

## 性能优化

### 优化 1：禁用不需要的流式

```python
# 某些场景不需要流式
model = ChatOpenAI(
    model="gpt-4o-mini",
    streaming=False  # 禁用流式
)

# messages 模式不会返回 token 流
for token, metadata in agent.stream(input, stream_mode="messages"):
    # 只会收到完整消息，没有逐 token 流式
    print(token.content)
```

---

### 优化 2：批量处理

```python
# 批量输入，每个输入流式输出
inputs = [
    {"input": "介绍 Python"},
    {"input": "介绍 JavaScript"},
    {"input": "介绍 Go"}
]

async def process_batch():
    for input_data in inputs:
        print(f"\n{'='*50}")
        print(f"处理: {input_data['input']}")
        print('='*50)

        async for chunk in chain.astream(input_data):
            print(chunk.content, end="", flush=True)
        print()

asyncio.run(process_batch())
```

---

### 优化 3：并发流式

```python
async def concurrent_streaming():
    """并发处理多个流式请求"""
    tasks = [
        chain.astream({"input": "介绍 Python"}),
        chain.astream({"input": "介绍 JavaScript"}),
        chain.astream({"input": "介绍 Go"})
    ]

    # 并发执行
    results = await asyncio.gather(*[
        collect_stream(stream) for stream in tasks
    ])

    return results

async def collect_stream(stream):
    """收集流式输出"""
    content = ""
    async for chunk in stream:
        content += chunk.content
    return content

# 使用
results = asyncio.run(concurrent_streaming())
for i, result in enumerate(results):
    print(f"\n结果 {i+1}: {result[:100]}...")
```

---

## 错误处理

### 处理连接中断

```python
async def stream_with_retry(chain, input, max_retries=3):
    """带重试的流式执行"""
    for attempt in range(max_retries):
        try:
            async for chunk in chain.astream(input):
                print(chunk.content, end="", flush=True)
            break  # 成功完成

        except ConnectionError as e:
            print(f"\n连接中断: {e}")
            if attempt < max_retries - 1:
                print(f"重试 {attempt + 1}/{max_retries}...")
                await asyncio.sleep(2 ** attempt)  # 指数退避
            else:
                print("达到最大重试次数")
                raise

# 使用
asyncio.run(stream_with_retry(chain, input))
```

---

### 处理超时

```python
async def stream_with_timeout(chain, input, timeout=30):
    """带超时的流式执行"""
    try:
        async with asyncio.timeout(timeout):
            async for chunk in chain.astream(input):
                print(chunk.content, end="", flush=True)

    except asyncio.TimeoutError:
        print(f"\n超时 ({timeout}秒)")
        # 降级到批量模式
        result = chain.invoke(input)
        print(result.content)

# 使用
asyncio.run(stream_with_timeout(chain, input, timeout=30))
```

---

### 处理部分失败

```python
async def stream_with_fallback(chain, input):
    """带降级的流式执行"""
    buffer = ""

    try:
        async for chunk in chain.astream(input):
            buffer += chunk.content
            print(chunk.content, end="", flush=True)

    except Exception as e:
        print(f"\n流式执行失败: {e}")

        if buffer:
            print(f"已接收部分内容: {len(buffer)} 字符")
            print("尝试继续...")
            # 可以基于已接收的内容继续处理
        else:
            print("降级到批量模式")
            result = chain.invoke(input)
            print(result.content)

# 使用
asyncio.run(stream_with_fallback(chain, input))
```

---

## 监控与调试

### 监控 Token 速率

```python
import time

class TokenRateMonitor:
    def __init__(self):
        self.start_time = None
        self.token_count = 0
        self.first_token_time = None

    async def monitor_stream(self, chain, input):
        """监控流式执行"""
        self.start_time = time.time()

        async for chunk in chain.astream(input):
            if chunk.content:
                self.token_count += 1

                if self.first_token_time is None:
                    self.first_token_time = time.time()

                print(chunk.content, end="", flush=True)

        self.print_stats()

    def print_stats(self):
        """打印统计信息"""
        total_time = time.time() - self.start_time
        ttfb = self.first_token_time - self.start_time if self.first_token_time else 0
        tokens_per_sec = self.token_count / total_time if total_time > 0 else 0

        print(f"\n\n统计信息:")
        print(f"  TTFB: {ttfb:.3f}s")
        print(f"  总时间: {total_time:.3f}s")
        print(f"  Token 数: {self.token_count}")
        print(f"  Token/秒: {tokens_per_sec:.2f}")

# 使用
monitor = TokenRateMonitor()
asyncio.run(monitor.monitor_stream(chain, input))
```

---

### 调试元数据

```python
def debug_stream(agent, input):
    """调试流式执行"""
    print("开始流式执行...\n")

    for i, (token, metadata) in enumerate(agent.stream(input, stream_mode="messages")):
        print(f"\n[Token {i+1}]")
        print(f"  类型: {type(token).__name__}")
        print(f"  内容: {token.content if hasattr(token, 'content') else 'N/A'}")
        print(f"  元数据: {metadata}")

        if hasattr(token, 'tool_call_chunks'):
            print(f"  工具调用块: {token.tool_call_chunks}")

# 使用
debug_stream(agent, input)
```

---

## 最佳实践

### 1. 总是使用 flush

```python
# ✅ 推荐
async for chunk in chain.astream(input):
    print(chunk.content, end="", flush=True)

# ❌ 不推荐
async for chunk in chain.astream(input):
    print(chunk.content, end="")  # 可能被缓冲
```

---

### 2. 正确解构返回值

```python
# ✅ 推荐
for token, metadata in agent.stream(input, stream_mode="messages"):
    if hasattr(token, 'content'):
        print(token.content, end="")

# ❌ 不推荐
for chunk in agent.stream(input, stream_mode="messages"):
    print(chunk.content)  # chunk 是元组，没有 content
```

---

### 3. 使用异步流式

```python
# ✅ 推荐
async for chunk in chain.astream(input):
    print(chunk.content, end="", flush=True)

# ❌ 不推荐（阻塞）
for chunk in chain.stream(input):
    print(chunk.content, end="", flush=True)
```

---

### 4. 合理使用元数据过滤

```python
# ✅ 推荐：只处理需要的节点
for token, metadata in agent.stream(input, stream_mode="messages"):
    if metadata.get('langgraph_node') == 'model':
        print(token.content, end="")

# ❌ 不推荐：处理所有 token
for token, metadata in agent.stream(input, stream_mode="messages"):
    print(token.content, end="")  # 包含工具结果等
```

---

## 总结

### 核心要点

1. **messages 模式返回 `(token, metadata)` 元组**
2. **Token 类型包括文本、工具调用、工具结果**
3. **元数据可用于过滤节点和 Agent**
4. **适用于 ChatGPT 式对话、实时翻译等场景**
5. **性能开销 ~5%，但用户体验提升显著**

### 使用场景

- ✅ ChatGPT 式对话
- ✅ 长文本生成
- ✅ 实时翻译
- ✅ 打字机效果
- ✅ 多 Agent 对话

### 避免误区

- ❌ 忘记 flush（输出被缓冲）
- ❌ 混淆返回值类型（元组 vs 对象）
- ❌ 不处理工具调用 token
- ❌ 忽略元数据过滤

---

## 参考资源

- **官方文档**: https://docs.langchain.com/oss/python/langchain/streaming/overview
- **相关知识点**:
  - 03_核心概念_01 - Stream 模式详解
  - 03_核心概念_02 - Agent 进度流式
  - 07_实战代码 - 完整代码示例

---

**版本**: LangChain 0.3.x (2025-2026)
**最后更新**: 2026-02-21
