# 最小可用知识

> 掌握以下内容，就能开始使用 RunnableSequence 构建 AI 应用

---

## 核心理念

**20% 的核心知识解决 80% 的问题**

RunnableSequence 看似复杂，但实际上只需要掌握 5 个核心要点，就能应对大部分 AI Agent 开发场景。

---

## 4.1 基础语法：用 `|` 连接组件

**一句话**：用管道操作符 `|` 将多个 Runnable 按顺序连接。

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 创建三个组件
prompt = ChatPromptTemplate.from_template("请用一句话解释：{topic}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# 用 | 连接成序列链
chain = prompt | llm | parser

# 执行
result = chain.invoke({"topic": "量子计算"})
print(result)  # "量子计算是利用量子力学原理进行信息处理的计算方式。"
```

**关键点**：
- `|` 左边的输出自动成为右边的输入
- 数据从左到右依次流转
- 最终返回最右边组件的输出

**在 AI Agent 开发中**：
- RAG 应用：`retriever | context_builder | llm | parser`
- 对话系统：`history_loader | prompt | llm | parser`
- 数据处理：`loader | transformer | analyzer | saver`

---

## 4.2 三种执行方式：invoke/batch/stream

**一句话**：所有 RunnableSequence 都支持三种执行方式。

### invoke：单次执行

```python
# 处理单个输入
result = chain.invoke({"topic": "人工智能"})
```

### batch：批量执行

```python
# 批量处理多个输入（自动并行）
results = chain.batch([
    {"topic": "机器学习"},
    {"topic": "深度学习"},
    {"topic": "强化学习"}
])
# 返回 3 个结果的列表
```

### stream：流式输出

```python
# 实时流式输出（适合对话场景）
for chunk in chain.stream({"topic": "神经网络"}):
    print(chunk, end="", flush=True)
```

**选择标准**：
- **invoke**：单次查询，等待完整结果
- **batch**：批量评估、数据标注（提升吞吐量）
- **stream**：实时对话、进度显示（降低首字延迟）

**在 AI Agent 开发中**：
- 对话机器人：用 `stream` 实时显示回复
- 批量评估：用 `batch` 并行处理测试集
- API 服务：用 `invoke` 返回完整结果

---

## 4.3 错误处理：with_retry 和 with_fallbacks

**一句话**：用 `with_retry` 重试失败请求，用 `with_fallbacks` 添加备用方案。

### with_retry：自动重试

```python
from langchain_core.runnables import RunnableRetry

# 添加重试逻辑（最多重试 3 次）
chain_with_retry = chain.with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True  # 指数退避
)

# 如果失败会自动重试
result = chain_with_retry.invoke({"topic": "AI"})
```

### with_fallbacks：降级方案

```python
# 主链：使用 GPT-4
main_chain = prompt | ChatOpenAI(model="gpt-4") | parser

# 备用链：使用 GPT-3.5（更便宜）
fallback_chain = prompt | ChatOpenAI(model="gpt-3.5-turbo") | parser

# 组合：主链失败时自动切换到备用链
chain_with_fallback = main_chain.with_fallbacks([fallback_chain])

result = chain_with_fallback.invoke({"topic": "AI"})
```

**在 AI Agent 开发中**：
- 生产环境：必须添加重试和降级
- API 限流：重试避免偶发失败
- 成本优化：主模型失败时降级到便宜模型

**2025-2026 最佳实践**（来源：Medium - 7 Retry & Timeout Policies）：
- 使用指数退避避免雪崩
- 设置最大重试次数（通常 3 次）
- 为不同错误类型设置不同策略

---

## 4.4 调试技巧：查看中间结果

**一句话**：用 `astream_events` 查看每个步骤的输入输出。

```python
import asyncio

async def debug_chain():
    # 使用 astream_events 追踪执行过程
    async for event in chain.astream_events(
        {"topic": "机器学习"},
        version="v2"
    ):
        kind = event["event"]

        # 打印每个步骤的输入输出
        if kind == "on_chain_start":
            print(f"\n开始: {event['name']}")
        elif kind == "on_chain_end":
            print(f"结束: {event['name']}")
            print(f"输出: {event['data']['output']}")

asyncio.run(debug_chain())
```

**输出示例**：
```
开始: ChatPromptTemplate
结束: ChatPromptTemplate
输出: [HumanMessage(content='请用一句话解释：机器学习')]

开始: ChatOpenAI
结束: ChatOpenAI
输出: AIMessage(content='机器学习是让计算机从数据中学习规律的技术。')

开始: StrOutputParser
结束: StrOutputParser
输出: 机器学习是让计算机从数据中学习规律的技术。
```

**在 AI Agent 开发中**：
- 调试链：找出哪个步骤出错
- 性能分析：查看每个步骤的耗时
- 可观测性：集成 LangSmith 追踪

**2025-2026 增强**（来源：LangChain 官方文档）：
- `astream_events` v2 提供更详细的事件
- 与 LangSmith 深度集成
- 支持自定义事件过滤

---

## 4.5 实战模式：RAG 问答链

**一句话**：最常见的应用模式是 RAG 问答链。

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ===== 1. 准备组件 =====

# 向量存储（假设已有数据）
vectorstore = Chroma(
    embedding_function=OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Prompt 模板
prompt = ChatPromptTemplate.from_template("""
根据以下上下文回答问题：

上下文：
{context}

问题：{question}

答案：
""")

# LLM 和解析器
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# ===== 2. 构建 RAG 链 =====

rag_chain = (
    {
        "context": retriever,  # 检索相关文档
        "question": RunnablePassthrough()  # 透传问题
    }
    | prompt  # 注入上下文
    | llm     # 生成答案
    | parser  # 解析输出
)

# ===== 3. 使用 =====

# 单次查询
answer = rag_chain.invoke("什么是向量数据库？")
print(answer)

# 流式输出
for chunk in rag_chain.stream("RAG 的优势是什么？"):
    print(chunk, end="", flush=True)
```

**关键模式**：
1. **检索**：`retriever` 根据问题检索相关文档
2. **上下文注入**：`RunnablePassthrough` 透传问题，与检索结果组合
3. **生成**：`llm` 基于上下文生成答案
4. **解析**：`parser` 提取文本

**在 AI Agent 开发中**：
- 文档问答：企业知识库、技术文档
- 客服机器人：基于历史对话和知识库
- 代码助手：基于代码库和文档

**2025-2026 优化**（来源：Towards AI - Master LangChain in 2025）：
- 使用 `RunnableParallel` 并行检索多个源
- 使用 `with_fallbacks` 添加备用检索器
- 使用 `langasync` 批处理降低成本 50%

---

## 这些知识足以

掌握以上 5 个核心要点，你就能：

✅ **构建基础 AI 应用**
- RAG 问答系统
- 对话机器人
- 文本处理管道

✅ **处理常见场景**
- 单次查询和批量处理
- 流式输出和实时响应
- 错误重试和降级

✅ **调试和优化**
- 查看中间结果
- 追踪执行过程
- 性能分析

✅ **为后续学习打基础**
- 理解更复杂的组合模式
- 学习 LangGraph 工作流
- 构建生产级应用

---

## 快速参考卡

| 场景 | 代码模式 | 说明 |
|------|----------|------|
| **基础链** | `prompt \| llm \| parser` | 最简单的序列链 |
| **单次执行** | `chain.invoke(input)` | 处理单个输入 |
| **批量执行** | `chain.batch([input1, input2])` | 并行处理多个输入 |
| **流式输出** | `chain.stream(input)` | 实时流式响应 |
| **添加重试** | `chain.with_retry(stop_after_attempt=3)` | 自动重试失败请求 |
| **添加降级** | `chain.with_fallbacks([fallback_chain])` | 主链失败时切换 |
| **查看中间结果** | `chain.astream_events(input, version="v2")` | 调试每个步骤 |
| **RAG 模式** | `{context: retriever, q: pass} \| prompt \| llm` | 检索增强生成 |

---

## 常见问题

### Q1: 为什么用 `|` 而不是函数调用？

**A**: `|` 操作符提供统一接口（invoke/batch/stream），而函数调用需要手动实现这些特性。

```python
# ❌ 手动调用（需要自己实现 batch/stream）
prompt_result = prompt.invoke(input)
llm_result = llm.invoke(prompt_result)
final_result = parser.invoke(llm_result)

# ✅ 使用 | 操作符（自动支持 batch/stream）
chain = prompt | llm | parser
final_result = chain.invoke(input)
# 还能用 chain.batch() 和 chain.stream()
```

### Q2: batch 和多次 invoke 有什么区别？

**A**: `batch` 会自动并行执行，性能更好。

```python
# ❌ 多次 invoke（串行执行，慢）
results = []
for input in inputs:
    results.append(chain.invoke(input))

# ✅ 使用 batch（并行执行，快）
results = chain.batch(inputs)
```

### Q3: 什么时候用 with_retry，什么时候用 with_fallbacks？

**A**:
- **with_retry**：临时性错误（网络抖动、API 限流）
- **with_fallbacks**：永久性错误（模型不可用、成本过高）

### Q4: 如何在生产环境使用？

**A**: 必须添加错误处理和可观测性。

```python
# 生产级链
production_chain = (
    chain
    .with_retry(stop_after_attempt=3)  # 重试
    .with_fallbacks([fallback_chain])  # 降级
    .with_config({"run_name": "production"})  # 标记
)
```

---

## 下一步学习

### 如果你想深入理解原理

阅读：
- `02_第一性原理.md` - 理解为什么需要 RunnableSequence
- `03_核心概念_02_管道操作符实现.md` - 理解 `|` 的实现

### 如果你想学习更多用法

阅读：
- `03_核心概念_06_流式执行.md` - 深入理解流式输出
- `03_核心概念_07_批处理优化.md` - 性能优化技巧

### 如果你想看更多实战案例

阅读：
- `07_实战代码_01_基础序列链构建.md` - RAG 问答链
- `07_实战代码_04_流式输出实现.md` - 实时对话机器人

---

## 学习检查清单

完成本节学习后，你应该能够：

- [ ] 用 `|` 操作符构建基础序列链
- [ ] 使用 invoke/batch/stream 执行链
- [ ] 添加 with_retry 重试逻辑
- [ ] 添加 with_fallbacks 降级方案
- [ ] 使用 astream_events 调试链
- [ ] 构建 RAG 问答链

---

**版本**: v1.0
**最后更新**: 2026-02-18
**参考来源**:
- LangChain 官方文档
- Towards AI - Master LangChain in 2025
- Medium - 7 Retry & Timeout Policies for Flaky LangChain Tools
