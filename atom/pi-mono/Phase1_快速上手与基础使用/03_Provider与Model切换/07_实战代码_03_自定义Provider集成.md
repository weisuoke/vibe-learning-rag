# å®æˆ˜ä»£ç  03ï¼šè‡ªå®šä¹‰ Provider é›†æˆ

> **é›†æˆ Ollamaã€LM Studioã€OpenRouter ç­‰è‡ªå®šä¹‰ Provider**

---

## Ollama æœ¬åœ°éƒ¨ç½²

### å®‰è£… Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# å¯åŠ¨æœåŠ¡
ollama serve
```

### ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½ Llama 3.1 8B
ollama pull llama3.1:8b

# ä¸‹è½½ Code Llama 13B
ollama pull codellama:13b

# éªŒè¯
ollama list
```

### Pi é…ç½®

```json
// ~/.pi/agent/models.json
{
  "providers": {
    "ollama": {
      "apiType": "openai-compatible",
      "baseUrl": "http://localhost:11434",
      "models": {
        "llama3.1:8b": {
          "id": "llama3.1:8b",
          "name": "Llama 3.1 8B (Local)",
          "contextWindow": 131072,
          "maxOutput": 32768,
          "cost": {
            "input": 0.0,
            "output": 0.0
          },
          "tags": ["local", "free", "offline"]
        },
        "codellama:13b": {
          "id": "codellama:13b",
          "name": "Code Llama 13B (Local)",
          "contextWindow": 16384,
          "maxOutput": 4096,
          "cost": {
            "input": 0.0,
            "output": 0.0
          },
          "tags": ["local", "free", "coding"]
        }
      }
    }
  }
}
```

### éªŒè¯è„šæœ¬

```bash
#!/bin/bash
# verify-ollama.sh

echo "Verifying Ollama setup..."

# æ£€æŸ¥ Ollama æœåŠ¡
if ! curl -s http://localhost:11434/api/tags > /dev/null; then
  echo "âŒ Ollama service not running"
  echo "Run: ollama serve"
  exit 1
fi

# æ£€æŸ¥æ¨¡å‹
MODELS=$(curl -s http://localhost:11434/api/tags | jq -r '.models[].name')

if [ -z "$MODELS" ]; then
  echo "âŒ No models found"
  echo "Run: ollama pull llama3.1:8b"
  exit 1
fi

echo "âœ… Ollama is running"
echo "Available models:"
echo "$MODELS" | sed 's/^/  - /'

# æµ‹è¯• Pi é›†æˆ
echo ""
echo "Testing Pi integration..."
pi --provider ollama --model llama3.1:8b <<EOF
Say "Ollama OK"
EOF
```

---

## LM Studio é›†æˆ

### å®‰è£… LM Studio

1. ä¸‹è½½ï¼šhttps://lmstudio.ai/
2. å®‰è£…å¹¶å¯åŠ¨
3. ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚ Llama 3.1 8Bï¼‰
4. å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨ï¼ˆç«¯å£ 1234ï¼‰

### Pi é…ç½®

```json
// ~/.pi/agent/models.json
{
  "providers": {
    "lmstudio": {
      "apiType": "openai-compatible",
      "baseUrl": "http://localhost:1234",
      "models": {
        "llama-3.1-8b": {
          "id": "llama-3.1-8b",
          "name": "Llama 3.1 8B (LM Studio)",
          "contextWindow": 131072,
          "maxOutput": 32768,
          "cost": {
            "input": 0.0,
            "output": 0.0
          },
          "tags": ["local", "free", "gui"]
        }
      }
    }
  }
}
```

### éªŒè¯è„šæœ¬

```bash
#!/bin/bash
# verify-lmstudio.sh

echo "Verifying LM Studio setup..."

# æ£€æŸ¥æœåŠ¡
if ! curl -s http://localhost:1234/v1/models > /dev/null; then
  echo "âŒ LM Studio server not running"
  echo "Start server in LM Studio app"
  exit 1
fi

# è·å–æ¨¡å‹åˆ—è¡¨
MODELS=$(curl -s http://localhost:1234/v1/models | jq -r '.data[].id')

echo "âœ… LM Studio is running"
echo "Available models:"
echo "$MODELS" | sed 's/^/  - /'
```

---

## OpenRouter é›†æˆ

### è·å– API Key

1. è®¿é—®ï¼šhttps://openrouter.ai/
2. æ³¨å†Œè´¦å·
3. è·å– API Key

### Pi é…ç½®

```json
// ~/.pi/agent/models.json
{
  "providers": {
    "openrouter": {
      "apiType": "openai-compatible",
      "baseUrl": "https://openrouter.ai/api/v1",
      "models": {
        "anthropic/claude-3.5-sonnet": {
          "id": "anthropic/claude-3.5-sonnet",
          "name": "Claude 3.5 Sonnet (OpenRouter)",
          "contextWindow": 200000,
          "maxOutput": 8192,
          "cost": {
            "input": 3.0,
            "output": 15.0
          }
        },
        "openai/gpt-4o": {
          "id": "openai/gpt-4o",
          "name": "GPT-4o (OpenRouter)",
          "contextWindow": 128000,
          "maxOutput": 16384,
          "cost": {
            "input": 2.5,
            "output": 10.0
          }
        }
      }
    }
  }
}

// ~/.pi/agent/auth.json
{
  "openrouter": {
    "apiKey": "sk-or-v1-YOUR_KEY_HERE"
  }
}
```

---

## Azure OpenAI é›†æˆ

### é…ç½®

```json
// ~/.pi/agent/models.json
{
  "providers": {
    "azure-openai": {
      "apiType": "openai-completions",
      "baseUrl": "https://YOUR_RESOURCE.openai.azure.com",
      "headers": {
        "api-key": "${AZURE_OPENAI_API_KEY}"
      },
      "models": {
        "gpt-4o": {
          "id": "gpt-4o",
          "name": "GPT-4o (Azure)",
          "contextWindow": 128000,
          "maxOutput": 16384,
          "cost": {
            "input": 2.5,
            "output": 10.0
          }
        }
      }
    }
  }
}
```

### ç¯å¢ƒå˜é‡

```bash
export AZURE_OPENAI_API_KEY="your-key"
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com"
```

---

## å®Œæ•´é›†æˆè„šæœ¬

### è‡ªåŠ¨åŒ–è®¾ç½®

```bash
#!/bin/bash
# setup-custom-providers.sh

set -e

echo "ğŸš€ Setting up custom providers..."

# 1. Ollama
echo ""
echo "1. Setting up Ollama..."

if command -v ollama &> /dev/null; then
  echo "âœ… Ollama installed"

  # å¯åŠ¨æœåŠ¡ï¼ˆåå°ï¼‰
  if ! pgrep -x "ollama" > /dev/null; then
    echo "Starting Ollama service..."
    ollama serve &
    sleep 2
  fi

  # ä¸‹è½½æ¨¡å‹
  if ! ollama list | grep -q "llama3.1:8b"; then
    echo "Downloading llama3.1:8b..."
    ollama pull llama3.1:8b
  fi

  echo "âœ… Ollama configured"
else
  echo "âš ï¸  Ollama not installed"
  echo "Install: brew install ollama"
fi

# 2. é…ç½® Pi
echo ""
echo "2. Configuring Pi..."

mkdir -p ~/.pi/agent

# æ·»åŠ  Ollama åˆ° models.json
cat >> ~/.pi/agent/models.json <<'EOF'
{
  "providers": {
    "ollama": {
      "apiType": "openai-compatible",
      "baseUrl": "http://localhost:11434",
      "models": {
        "llama3.1:8b": {
          "id": "llama3.1:8b",
          "name": "Llama 3.1 8B (Local)",
          "contextWindow": 131072,
          "maxOutput": 32768,
          "cost": { "input": 0.0, "output": 0.0 },
          "tags": ["local", "free"]
        }
      }
    }
  }
}
EOF

echo "âœ… Pi configured"

# 3. éªŒè¯
echo ""
echo "3. Verifying setup..."

if curl -s http://localhost:11434/api/tags > /dev/null; then
  echo "âœ… Ollama service is running"
else
  echo "âŒ Ollama service not running"
fi

echo ""
echo "âœ… Setup complete!"
echo ""
echo "Test with:"
echo "  pi --provider ollama --model llama3.1:8b"
```

---

## å¥åº·æ£€æŸ¥è„šæœ¬

```typescript
// health-check.ts
import fetch from 'node-fetch';

interface ProviderHealth {
  name: string;
  url: string;
  status: 'healthy' | 'unhealthy' | 'unknown';
  latency?: number;
  error?: string;
}

async function checkProvider(
  name: string,
  url: string
): Promise<ProviderHealth> {
  const start = Date.now();

  try {
    const response = await fetch(url, {
      method: 'GET',
      timeout: 5000
    });

    const latency = Date.now() - start;

    if (response.ok) {
      return {
        name,
        url,
        status: 'healthy',
        latency
      };
    } else {
      return {
        name,
        url,
        status: 'unhealthy',
        error: `HTTP ${response.status}`
      };
    }
  } catch (error) {
    return {
      name,
      url,
      status: 'unhealthy',
      error: error.message
    };
  }
}

async function checkAllProviders() {
  console.log('Checking provider health...\n');

  const providers = [
    { name: 'Ollama', url: 'http://localhost:11434/api/tags' },
    { name: 'LM Studio', url: 'http://localhost:1234/v1/models' },
    { name: 'Anthropic', url: 'https://api.anthropic.com' },
    { name: 'OpenAI', url: 'https://api.openai.com' }
  ];

  const results = await Promise.all(
    providers.map(p => checkProvider(p.name, p.url))
  );

  results.forEach(result => {
    const icon = result.status === 'healthy' ? 'âœ…' : 'âŒ';
    const latency = result.latency ? ` (${result.latency}ms)` : '';
    const error = result.error ? ` - ${result.error}` : '';

    console.log(`${icon} ${result.name}${latency}${error}`);
  });

  const healthyCount = results.filter(r => r.status === 'healthy').length;
  console.log(`\n${healthyCount}/${results.length} providers healthy`);
}

checkAllProviders();
```

---

## é¡¹ç›®æ¨¡æ¿

### æœ¬åœ°å¼€å‘æ¨¡æ¿

```json
// project-local/.pi/settings.json
{
  "defaultModel": "llama3.1:8b",
  "scopedModels": [
    "llama3.1:8b",
    "claude-3-5-haiku-20241022",
    "claude-3-5-sonnet-20241022"
  ]
}
```

### æ··åˆæ¨¡å¼æ¨¡æ¿

```json
// project-hybrid/.pi/settings.json
{
  "defaultModel": "llama3.1:8b",
  "scopedModels": [
    "llama3.1:8b",
    "claude-3-5-haiku-20241022",
    "gpt-4o-mini"
  ]
}
```

---

## æ•…éšœæ’æŸ¥

### Ollama é—®é¢˜

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
pgrep -x "ollama"

# é‡å¯æœåŠ¡
pkill ollama
ollama serve

# æ£€æŸ¥ç«¯å£
lsof -i :11434

# æŸ¥çœ‹æ—¥å¿—
ollama logs
```

### LM Studio é—®é¢˜

```bash
# æ£€æŸ¥ç«¯å£
lsof -i :1234

# æµ‹è¯•è¿æ¥
curl http://localhost:1234/v1/models
```

---

**è®°ä½**ï¼šæœ¬åœ°æ¨¡å‹å…è´¹ä½†æ€§èƒ½æœ‰é™ï¼Œé€‚åˆå¼€å‘æµ‹è¯•ï¼›äº‘ç«¯æ¨¡å‹ä»˜è´¹ä½†è´¨é‡æ›´é«˜ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚
