# 一句话总结

**相似度度量是向量检索的数学基础，L2适合未归一化向量，IP适合推荐排序，COSINE适合语义搜索，选择度量方式要根据Embedding模型的归一化特性和业务场景决定。**

---

## 核心要点回顾

### 三种度量的本质区别

```
L2距离 (欧氏距离)
├─ 计算：√[(x₁-y₁)² + (x₂-y₂)² + ...]
├─ 本质：两点间的直线距离
├─ 特点：受向量幅度影响
└─ 适用：未归一化向量、物理距离

内积 IP (Inner Product)
├─ 计算：x₁y₁ + x₂y₂ + ...
├─ 本质：方向一致性 × 幅度
├─ 特点：值越大越相似
└─ 适用：推荐系统、相关性排序

余弦相似度 COSINE
├─ 计算：(x·y) / (|x||y|)
├─ 本质：纯方向一致性（归一化后的内积）
├─ 特点：忽略向量幅度
└─ 适用：语义搜索、归一化向量
```

---

## 选型决策树

```
开始
  ↓
你的Embedding模型是否归一化？
  ├─ 是 → 使用 COSINE 或 IP（等价）
  │        ↓
  │        业务场景是什么？
  │        ├─ 语义搜索 → COSINE（语义更直观）
  │        └─ 推荐排序 → IP（计算更快）
  │
  └─ 否 → 业务场景是什么？
           ├─ 需要考虑向量幅度 → IP
           ├─ 只关注方向 → COSINE
           └─ 物理距离概念 → L2
```

---

## 常见Embedding模型的度量选择

| Embedding模型 | 是否归一化 | 推荐度量 | 原因 |
|--------------|-----------|---------|------|
| OpenAI text-embedding-3 | 是 | COSINE | 官方推荐，语义搜索标准 |
| sentence-transformers | 通常是 | COSINE | 大多数模型默认归一化 |
| Word2Vec | 否 | L2 或 COSINE | 未归一化，看业务需求 |
| BERT (原始) | 否 | COSINE | 通常需要归一化后使用 |
| BGE系列 | 是 | COSINE | 专为语义搜索设计 |

---

## 三个关键误区

### ❌ 误区1："度量方式对结果影响不大"
**真相：** 度量方式直接决定排序结果，选错度量可能导致完全不同的Top-K结果。

### ❌ 误区2："COSINE一定比L2好"
**真相：** 对于未归一化向量，COSINE会丢失幅度信息，可能不如L2准确。

### ❌ 误区3："可以随时切换度量方式"
**真相：** 切换度量需要重建索引（某些索引类型），且结果排序会完全改变。

---

## 实战建议

### 1. 默认选择
**如果不确定，优先选择 COSINE**
- 适用于90%的语义搜索场景
- 对向量幅度不敏感，更稳定
- 与大多数Embedding模型兼容

### 2. 性能优化
**归一化向量 + IP = 最快的COSINE**
```python
# 如果向量已归一化，IP和COSINE等价，但IP更快
# COSINE: (x·y) / (|x||y|) = x·y  (当|x|=|y|=1时)
# IP: x·y

# 推荐做法：
# 1. 在插入前归一化向量
# 2. 使用IP度量（比COSINE快10-20%）
# 3. 效果与COSINE完全相同
```

### 3. 调试技巧
**如何验证度量选择是否正确？**
```python
# 1. 手动计算前3个结果的度量值
# 2. 检查排序是否符合直觉
# 3. 对比不同度量方式的Top-10结果

# 示例：
query = [0.1, 0.2, 0.3]
results = collection.search(query, limit=10)

for result in results[:3]:
    vector = result.entity.get('embedding')
    l2 = np.linalg.norm(query - vector)
    ip = np.dot(query, vector)
    cosine = ip / (np.linalg.norm(query) * np.linalg.norm(vector))
    print(f"L2={l2:.4f}, IP={ip:.4f}, COSINE={cosine:.4f}")
```

---

## 记忆口诀

**归一化用COSINE，推荐用IP，物理距离用L2。**

**不确定就COSINE，90%场景都适用。**

---

## 延伸学习

### 下一步学习
- 📚 **L2_核心功能/03_标量过滤** - 结合度量与过滤条件
- 📚 **L2_核心功能/04_高级索引类型** - 不同索引对度量的支持
- 📚 **L4_性能优化/01_索引参数调优** - 度量方式对性能的影响

### 深入阅读
- Milvus官方文档：Metric Types
- 论文：Efficient and Robust Approximate Nearest Neighbor Search
- 博客：Understanding Vector Similarity Metrics

---

**恭喜！** 你已经掌握了Milvus相似度度量的核心知识。

**记住：** 度量方式是向量检索的灵魂，选对度量，事半功倍！
