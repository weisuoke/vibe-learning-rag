# 核心概念 02 - 常用组件

> 掌握 LCEL 的三大核心组件：PromptTemplate、ChatModel、OutputParser

---

## 核心概念1：PromptTemplate - 格式化输入

### 一句话定义

**PromptTemplate 是用于格式化 LLM 输入的模板组件，支持变量占位符、系统消息、用户消息等多种格式。**

---

### 基础用法

```python
from langchain_core.prompts import ChatPromptTemplate

# 创建简单模板
prompt = ChatPromptTemplate.from_template(
    "你是一个{role}，请回答：{question}"
)

# 格式化输入
formatted = prompt.invoke({
    "role": "Python 专家",
    "question": "什么是装饰器？"
})

print(formatted)
```

**输出**：
```python
[HumanMessage(content='你是一个Python 专家，请回答：什么是装饰器？')]
```

---

### 多消息模板

```python
# 创建包含系统消息和用户消息的模板
prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个{role}，擅长{skill}。"),
    ("human", "请回答：{question}"),
])

# 格式化
formatted = prompt.invoke({
    "role": "Python 专家",
    "skill": "异步编程",
    "question": "什么是 asyncio？"
})

print(formatted)
```

**输出**：
```python
[
    SystemMessage(content='你是一个Python 专家，擅长异步编程。'),
    HumanMessage(content='请回答：什么是 asyncio？')
]
```

---

### 手写实现：简化版 PromptTemplate

```python
from typing import Dict, Any, List
from dataclasses import dataclass


@dataclass
class Message:
    """消息基类"""
    role: str
    content: str

    def __repr__(self):
        return f"{self.__class__.__name__}(content='{self.content}')"


class SystemMessage(Message):
    def __init__(self, content: str):
        super().__init__(role="system", content=content)


class HumanMessage(Message):
    def __init__(self, content: str):
        super().__init__(role="user", content=content)


class SimplePromptTemplate:
    """简化版 PromptTemplate"""

    def __init__(self, template: str):
        self.template = template

    def invoke(self, variables: Dict[str, Any]) -> List[Message]:
        """格式化模板"""
        # 替换变量
        content = self.template.format(**variables)
        return [HumanMessage(content=content)]

    @classmethod
    def from_template(cls, template: str):
        """工厂方法"""
        return cls(template)


class SimpleChatPromptTemplate:
    """支持多消息的 PromptTemplate"""

    def __init__(self, messages: List[tuple]):
        self.messages = messages

    def invoke(self, variables: Dict[str, Any]) -> List[Message]:
        """格式化所有消息"""
        result = []
        for role, template in self.messages:
            content = template.format(**variables)
            if role == "system":
                result.append(SystemMessage(content))
            elif role == "human":
                result.append(HumanMessage(content))
        return result

    @classmethod
    def from_messages(cls, messages: List[tuple]):
        """工厂方法"""
        return cls(messages)


# 测试
prompt = SimpleChatPromptTemplate.from_messages([
    ("system", "你是一个{role}。"),
    ("human", "请回答：{question}")
])

formatted = prompt.invoke({
    "role": "Python 专家",
    "question": "什么是 LCEL？"
})

for msg in formatted:
    print(msg)
```

**输出**：
```
SystemMessage(content='你是一个Python 专家。')
HumanMessage(content='请回答：什么是 LCEL？')
```

---

### 高级用法：部分变量

```python
# 创建模板
prompt = ChatPromptTemplate.from_template(
    "你是一个{role}，请用{language}回答：{question}"
)

# 部分填充变量（固定 role 和 language）
partial_prompt = prompt.partial(
    role="Python 专家",
    language="中文"
)

# 只需要提供 question
result = partial_prompt.invoke({"question": "什么是 LCEL？"})
```

**应用场景**：
- 固定系统角色，只改变问题
- 固定输出语言，只改变输入
- 复用相同的 Prompt 配置

---

### 在实际应用中

**场景1：RAG Prompt**

```python
# RAG 场景的 Prompt 模板
rag_prompt = ChatPromptTemplate.from_template("""
基于以下上下文回答问题。如果上下文中没有相关信息，请说"我不知道"。

上下文：
{context}

问题：{question}

回答：
""")

# 使用
chain = {
    "context": retriever | format_docs,
    "question": RunnablePassthrough()
} | rag_prompt | llm | parser
```

**场景2：多轮对话 Prompt**

```python
# 多轮对话模板
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个友好的AI助手。"),
    ("human", "{history}"),
    ("human", "{question}")
])

# 使用
chain = chat_prompt | llm | parser
result = chain.invoke({
    "history": "用户：你好\nAI：你好！有什么可以帮你的吗？",
    "question": "什么是 LCEL？"
})
```

**场景3：结构化输出 Prompt**

```python
# 要求 LLM 输出 JSON 格式
json_prompt = ChatPromptTemplate.from_template("""
请分析以下文本的情感，并以 JSON 格式返回结果。

文本：{text}

返回格式：
{{
    "sentiment": "positive/negative/neutral",
    "confidence": 0.0-1.0,
    "keywords": ["关键词1", "关键词2"]
}}
""")

# 使用
chain = json_prompt | llm | JsonOutputParser()
```

---

## 核心概念2：ChatModel - LLM 调用

### 一句话定义

**ChatModel 是 LCEL 中用于调用大语言模型的组件，支持同步/异步调用、流式输出、批量处理等功能。**

---

### 基础用法

```python
from langchain_openai import ChatOpenAI

# 创建 ChatModel
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=1000
)

# 调用
response = llm.invoke([
    {"role": "user", "content": "什么是 LCEL？"}
])

print(response.content)
```

---

### 配置参数

```python
llm = ChatOpenAI(
    model="gpt-3.5-turbo",      # 模型名称
    temperature=0.7,             # 温度（0-2，越高越随机）
    max_tokens=1000,             # 最大生成 token 数
    top_p=1.0,                   # 核采样参数
    frequency_penalty=0.0,       # 频率惩罚（-2.0 到 2.0）
    presence_penalty=0.0,        # 存在惩罚（-2.0 到 2.0）
    n=1,                         # 生成几个回复
    timeout=30,                  # 超时时间（秒）
    max_retries=2,               # 最大重试次数
)
```

**参数说明**：

| 参数 | 作用 | 推荐值 |
|------|------|--------|
| **temperature** | 控制随机性 | 0.0（精确）- 1.0（创意） |
| **max_tokens** | 限制输出长度 | 根据需求设置 |
| **top_p** | 核采样 | 1.0（默认） |
| **frequency_penalty** | 减少重复词 | 0.0-0.5 |
| **presence_penalty** | 鼓励新话题 | 0.0-0.5 |

---

### 手写实现：简化版 ChatModel

```python
import os
from typing import List, Dict, Any
import httpx


class SimpleChatModel:
    """简化版 ChatModel（调用 OpenAI API）"""

    def __init__(
        self,
        model: str = "gpt-3.5-turbo",
        temperature: float = 0.7,
        max_tokens: int = 1000
    ):
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")

    def invoke(self, messages: List[Dict[str, str]]) -> str:
        """同步调用"""
        # 构建请求
        response = httpx.post(
            f"{self.base_url}/chat/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": self.model,
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens
            },
            timeout=30.0
        )

        # 解析响应
        data = response.json()
        return data["choices"][0]["message"]["content"]

    async def ainvoke(self, messages: List[Dict[str, str]]) -> str:
        """异步调用"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": messages,
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens
                },
                timeout=30.0
            )

            data = response.json()
            return data["choices"][0]["message"]["content"]

    async def astream(self, messages: List[Dict[str, str]]):
        """流式调用"""
        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                f"{self.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": messages,
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens,
                    "stream": True  # 启用流式
                },
                timeout=30.0
            ) as response:
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        import json
                        chunk = json.loads(data)
                        if "choices" in chunk:
                            delta = chunk["choices"][0].get("delta", {})
                            if "content" in delta:
                                yield delta["content"]


# 测试
llm = SimpleChatModel()

# 同步调用
result = llm.invoke([
    {"role": "user", "content": "什么是 LCEL？"}
])
print(result)

# 异步流式调用
import asyncio

async def test_stream():
    async for chunk in llm.astream([
        {"role": "user", "content": "讲一个笑话"}
    ]):
        print(chunk, end="", flush=True)

asyncio.run(test_stream())
```

---

### 多模型支持

```python
# OpenAI
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4")

# Anthropic Claude
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-3-sonnet-20240229")

# 本地模型（Ollama）
from langchain_community.chat_models import ChatOllama
llm = ChatOllama(model="llama2")

# 所有模型都有相同的接口
response = llm.invoke([{"role": "user", "content": "Hello"}])
```

---

### 在实际应用中

**场景1：简单问答**

```python
# 创建问答链
chain = ChatPromptTemplate.from_template("回答：{question}") | llm | parser

# 使用
answer = chain.invoke({"question": "什么是 LCEL？"})
```

**场景2：流式输出**

```python
# FastAPI 流式端点
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.post("/chat/stream")
async def chat_stream(message: str):
    async def generate():
        async for chunk in llm.astream([
            {"role": "user", "content": message}
        ]):
            yield f"data: {chunk}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

**场景3：批量处理**

```python
# 批量调用（并行）
questions = [
    "什么是 LCEL？",
    "什么是 Runnable？",
    "什么是管道操作符？"
]

answers = await llm.abatch([
    [{"role": "user", "content": q}] for q in questions
])
```

---

## 核心概念3：OutputParser - 解析输出

### 一句话定义

**OutputParser 是用于解析 LLM 输出的组件，支持字符串、JSON、结构化数据等多种格式。**

---

### 基础用法：StrOutputParser

```python
from langchain_core.output_parsers import StrOutputParser

# 创建解析器
parser = StrOutputParser()

# 解析 LLM 输出
from langchain_core.messages import AIMessage
response = AIMessage(content="这是 LLM 的回复")

result = parser.invoke(response)
print(result)  # "这是 LLM 的回复"
print(type(result))  # <class 'str'>
```

**作用**：将 `AIMessage` 对象转换为纯字符串。

---

### JsonOutputParser - 解析 JSON

```python
from langchain_core.output_parsers import JsonOutputParser

# 创建 JSON 解析器
parser = JsonOutputParser()

# LLM 输出的 JSON 字符串
response = AIMessage(content='{"name": "Alice", "age": 30}')

# 解析为 Python 字典
result = parser.invoke(response)
print(result)  # {'name': 'Alice', 'age': 30}
print(type(result))  # <class 'dict'>
```

---

### PydanticOutputParser - 结构化输出

```python
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# 定义输出结构
class Person(BaseModel):
    name: str = Field(description="人名")
    age: int = Field(description="年龄")
    occupation: str = Field(description="职业")

# 创建解析器
parser = PydanticOutputParser(pydantic_object=Person)

# 获取格式说明（用于 Prompt）
format_instructions = parser.get_format_instructions()
print(format_instructions)

# 创建 Prompt
prompt = ChatPromptTemplate.from_template("""
提取以下文本中的人物信息：

{text}

{format_instructions}
""")

# 完整链
chain = prompt | llm | parser

# 使用
result = chain.invoke({
    "text": "Alice 今年30岁，是一名软件工程师。",
    "format_instructions": format_instructions
})

print(result)  # Person(name='Alice', age=30, occupation='软件工程师')
print(type(result))  # <class 'Person'>
```

---

### 手写实现：简化版 OutputParser

```python
import json
from typing import Any
from pydantic import BaseModel


class SimpleStrOutputParser:
    """简化版字符串解析器"""

    def invoke(self, input: Any) -> str:
        """提取字符串内容"""
        if hasattr(input, "content"):
            return input.content
        return str(input)


class SimpleJsonOutputParser:
    """简化版 JSON 解析器"""

    def invoke(self, input: Any) -> dict:
        """解析 JSON 字符串"""
        if hasattr(input, "content"):
            content = input.content
        else:
            content = str(input)

        # 解析 JSON
        return json.loads(content)


class SimplePydanticOutputParser:
    """简化版 Pydantic 解析器"""

    def __init__(self, pydantic_object: type[BaseModel]):
        self.pydantic_object = pydantic_object

    def invoke(self, input: Any) -> BaseModel:
        """解析为 Pydantic 对象"""
        if hasattr(input, "content"):
            content = input.content
        else:
            content = str(input)

        # 解析 JSON 并创建 Pydantic 对象
        data = json.loads(content)
        return self.pydantic_object(**data)

    def get_format_instructions(self) -> str:
        """生成格式说明"""
        schema = self.pydantic_object.model_json_schema()
        properties = schema.get("properties", {})

        instructions = "请以 JSON 格式返回，包含以下字段：\n"
        for field_name, field_info in properties.items():
            field_type = field_info.get("type", "string")
            field_desc = field_info.get("description", "")
            instructions += f"- {field_name} ({field_type}): {field_desc}\n"

        return instructions


# 测试
class AIMessage:
    def __init__(self, content: str):
        self.content = content


# 测试字符串解析器
str_parser = SimpleStrOutputParser()
result = str_parser.invoke(AIMessage("Hello"))
print(result)  # "Hello"

# 测试 JSON 解析器
json_parser = SimpleJsonOutputParser()
result = json_parser.invoke(AIMessage('{"name": "Alice", "age": 30}'))
print(result)  # {'name': 'Alice', 'age': 30}

# 测试 Pydantic 解析器
class Person(BaseModel):
    name: str
    age: int

pydantic_parser = SimplePydanticOutputParser(pydantic_object=Person)
print(pydantic_parser.get_format_instructions())
result = pydantic_parser.invoke(AIMessage('{"name": "Alice", "age": 30}'))
print(result)  # Person(name='Alice', age=30)
```

---

### 在实际应用中

**场景1：简单问答（字符串输出）**

```python
chain = prompt | llm | StrOutputParser()
answer = chain.invoke({"question": "什么是 LCEL？"})
# answer 是纯字符串，可以直接使用
```

**场景2：结构化数据提取（JSON 输出）**

```python
# 提取文本中的实体
prompt = ChatPromptTemplate.from_template("""
从以下文本中提取人名、地点、时间，以 JSON 格式返回。

文本：{text}

返回格式：{{"person": "人名", "location": "地点", "time": "时间"}}
""")

chain = prompt | llm | JsonOutputParser()

result = chain.invoke({
    "text": "Alice 昨天在北京参加了会议。"
})
# result = {"person": "Alice", "location": "北京", "time": "昨天"}
```

**场景3：类型安全的数据提取（Pydantic 输出）**

```python
from pydantic import BaseModel, Field

class SentimentAnalysis(BaseModel):
    sentiment: str = Field(description="情感：positive/negative/neutral")
    confidence: float = Field(description="置信度：0.0-1.0")
    keywords: list[str] = Field(description="关键词列表")

parser = PydanticOutputParser(pydantic_object=SentimentAnalysis)

prompt = ChatPromptTemplate.from_template("""
分析以下文本的情感：

{text}

{format_instructions}
""")

chain = prompt | llm | parser

result = chain.invoke({
    "text": "这个产品太棒了！",
    "format_instructions": parser.get_format_instructions()
})

# result 是 SentimentAnalysis 对象，类型安全
print(result.sentiment)  # "positive"
print(result.confidence)  # 0.95
print(result.keywords)  # ["产品", "棒"]
```

---

## 三大组件的组合使用

### 完整的问答链

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. PromptTemplate：格式化输入
prompt = ChatPromptTemplate.from_template("回答：{question}")

# 2. ChatModel：调用 LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

# 3. OutputParser：解析输出
parser = StrOutputParser()

# 组合成链
chain = prompt | llm | parser

# 使用
answer = chain.invoke({"question": "什么是 LCEL？"})
print(answer)
```

---

### 完整的 RAG 链

```python
# RAG 链：检索 + 问答
rag_chain = (
    # 步骤1：并行执行检索和问题保留
    {
        "context": vectorstore.as_retriever() | format_docs,
        "question": RunnablePassthrough()
    }
    # 步骤2：PromptTemplate 格式化
    | ChatPromptTemplate.from_template(
        "基于以下上下文回答问题：\n\n{context}\n\n问题：{question}"
    )
    # 步骤3：ChatModel 生成
    | ChatOpenAI()
    # 步骤4：OutputParser 解析
    | StrOutputParser()
)

# 使用
answer = rag_chain.invoke("什么是向量检索？")
```

---

### 完整的结构化输出链

```python
from pydantic import BaseModel, Field

# 定义输出结构
class MovieReview(BaseModel):
    title: str = Field(description="电影名称")
    rating: int = Field(description="评分：1-10")
    summary: str = Field(description="一句话总结")
    pros: list[str] = Field(description="优点列表")
    cons: list[str] = Field(description="缺点列表")

# 创建解析器
parser = PydanticOutputParser(pydantic_object=MovieReview)

# 创建 Prompt
prompt = ChatPromptTemplate.from_template("""
分析以下电影评论：

{review}

{format_instructions}
""")

# 完整链
chain = prompt | llm | parser

# 使用
result = chain.invoke({
    "review": "这部电影剧情紧凑，演员演技出色，但特效略显粗糙。",
    "format_instructions": parser.get_format_instructions()
})

# result 是 MovieReview 对象
print(result.title)
print(result.rating)
print(result.pros)
print(result.cons)
```

---

## 组件总结表

| 组件 | 作用 | 输入 | 输出 | 常用场景 |
|------|------|------|------|---------|
| **PromptTemplate** | 格式化输入 | 变量字典 | 消息列表 | 构建 Prompt |
| **ChatModel** | 调用 LLM | 消息列表 | AI 消息 | 生成文本 |
| **OutputParser** | 解析输出 | AI 消息 | 字符串/JSON/对象 | 提取结构化数据 |

---

## 学习检查清单

掌握常用组件后，你应该能够：

- [ ] 使用 `ChatPromptTemplate.from_template()` 创建简单模板
- [ ] 使用 `ChatPromptTemplate.from_messages()` 创建多消息模板
- [ ] 理解 PromptTemplate 的变量占位符机制
- [ ] 使用 `partial()` 部分填充变量
- [ ] 创建 ChatModel 并配置参数（temperature、max_tokens 等）
- [ ] 理解 ChatModel 的同步/异步/流式调用
- [ ] 使用 StrOutputParser 解析字符串输出
- [ ] 使用 JsonOutputParser 解析 JSON 输出
- [ ] 使用 PydanticOutputParser 解析结构化输出
- [ ] 组合三大组件构建完整的问答链
- [ ] 组合三大组件构建 RAG 链
- [ ] 组合三大组件构建结构化输出链

---

## 下一步

掌握了常用组件后，继续学习：

- **03_核心概念_03_高级特性.md** - 并行执行、条件分支、错误处理等高级特性

---

**版本**: v1.0
**最后更新**: 2026-02-12
