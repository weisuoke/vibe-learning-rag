# 实战代码 01：基础配置

> **RunnableConfig 的基础使用场景和配置传递方式**

---

## 场景 1：最简单的配置

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

# 创建 LLM
llm = ChatOpenAI(model="gpt-3.5-turbo")

# 创建配置
config = RunnableConfig(
    tags=["production"],
    metadata={"user_id": "user_123"}
)

# 使用配置
result = llm.invoke("你好", config=config)
print(result.content)
```

---

## 场景 2：配置传递的三种方式

### 方式 1：构造时配置（静态）

```python
from langchain_openai import ChatOpenAI

# 构造时传入配置
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    tags=["default"],
    metadata={"env": "dev"}
)

# 直接使用
result = llm.invoke("你好")
```

**优点**：简单直接
**缺点**：配置固定，无法动态调整

---

### 方式 2：with_config（链级配置）

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

# 创建基础 LLM
llm = ChatOpenAI(model="gpt-3.5-turbo")

# 使用 with_config 添加配置
llm_with_config = llm.with_config(
    tags=["production"],
    metadata={"env": "prod"}
)

# 使用配置后的 LLM
result = llm_with_config.invoke("你好")
```

**优点**：可以创建多个配置版本
**缺点**：每次都需要创建新的包装器

---

### 方式 3：invoke 时配置（动态，推荐）

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

# 创建基础 LLM
llm = ChatOpenAI(model="gpt-3.5-turbo")

# 每次调用时传入不同配置
config1 = RunnableConfig(
    tags=["user_query"],
    metadata={"user_id": "user_001"}
)

config2 = RunnableConfig(
    tags=["admin_query"],
    metadata={"user_id": "admin_001"}
)

result1 = llm.invoke("你好", config=config1)
result2 = llm.invoke("你好", config=config2)
```

**优点**：最灵活，可以动态调整
**缺点**：需要每次传入配置

---

## 场景 3：配置优先级

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

# 1. 构造时配置（优先级 3，最低）
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    tags=["default"],
    metadata={"env": "dev"}
)

# 2. with_config 配置（优先级 2）
llm_with_config = llm.with_config(
    tags=["production"],
    metadata={"env": "prod", "version": "1.0"}
)

# 3. invoke 时配置（优先级 1，最高）
config = RunnableConfig(
    tags=["critical"],
    metadata={"user_id": "user_123"}
)

result = llm_with_config.invoke("你好", config=config)

# 最终配置：
# tags: ["critical"]  # 来自 invoke（优先级最高）
# metadata: {"user_id": "user_123"}  # 来自 invoke（完全覆盖）
```

**关键点**：配置是完全覆盖，不是合并！

---

## 场景 4：Tags 的使用

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

llm = ChatOpenAI()

# 环境标签
config_prod = RunnableConfig(tags=["production"])
config_dev = RunnableConfig(tags=["development"])

# 功能标签
config_rag = RunnableConfig(tags=["rag", "document-qa"])
config_chat = RunnableConfig(tags=["chat", "conversation"])

# 优先级标签
config_critical = RunnableConfig(tags=["critical", "high-priority"])
config_normal = RunnableConfig(tags=["normal", "low-priority"])

# 组合使用
config = RunnableConfig(
    tags=["production", "rag", "critical"]
)

result = llm.invoke("你好", config=config)
```

**在 LangSmith 中可以按 tags 筛选追踪**

---

## 场景 5：Metadata 的使用

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

llm = ChatOpenAI()

# 用户归因
config = RunnableConfig(
    metadata={
        "user_id": "user_123",
        "user_name": "Alice",
        "user_tier": "premium"
    }
)

# 请求追踪
config = RunnableConfig(
    metadata={
        "request_id": "req_abc123",
        "session_id": "session_xyz",
        "trace_id": "trace_456"
    }
)

# 业务上下文
config = RunnableConfig(
    metadata={
        "department": "sales",
        "region": "us-west",
        "cost_center": "marketing"
    }
)

# 完整示例
config = RunnableConfig(
    metadata={
        "user_id": "user_123",
        "session_id": "session_abc",
        "request_id": "req_xyz",
        "env": "production",
        "version": "1.0.0"
    }
)

result = llm.invoke("你好", config=config)
```

**注意**：metadata 必须是 JSON 可序列化的！

---

## 场景 6：Run Name 的使用

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig
import uuid

llm = ChatOpenAI()

# 简单命名
config = RunnableConfig(
    run_name="user_query_123"
)

# 动态生成
def create_config(user_id: str, query_type: str):
    run_id = str(uuid.uuid4())[:8]
    return RunnableConfig(
        run_name=f"{user_id}_{query_type}_{run_id}",
        metadata={
            "user_id": user_id,
            "query_type": query_type
        }
    )

config = create_config("user_123", "rag_search")
# run_name: "user_123_rag_search_a1b2c3d4"

result = llm.invoke("你好", config=config)
```

**在 LangSmith 中可以按 run_name 搜索**

---

## 场景 7：Max Concurrency 控制

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

llm = ChatOpenAI()

# 批量调用时限制并发
config = RunnableConfig(max_concurrency=10)

inputs = [{"input": f"问题{i}"} for i in range(100)]

# 最多同时执行 10 个，其余排队等待
results = llm.batch(
    ["你好"] * 100,
    config=config
)

print(f"处理了 {len(results)} 个请求")
```

**避免触发 API 限流**

---

## 场景 8：Recursion Limit 保护

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

llm = ChatOpenAI()

# 限制递归深度（用于 Agent）
config = RunnableConfig(recursion_limit=10)

# Agent 最多执行 10 次决策
try:
    result = agent.invoke({"input": "查询天气"}, config=config)
except RecursionError:
    print("Agent 超过递归限制，已终止")
```

**防止 Agent 陷入无限循环**

---

## 场景 9：完整配置示例

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig
import uuid

def create_production_config(user_id: str, query_type: str):
    """创建生产环境配置"""
    return RunnableConfig(
        # 分类标签
        tags=["production", "critical", query_type],

        # 追踪信息
        metadata={
            "user_id": user_id,
            "query_type": query_type,
            "env": "production",
            "timestamp": "2025-01-15T10:30:00Z"
        },

        # 可读标识
        run_name=f"{user_id}_{query_type}_{uuid.uuid4().hex[:8]}",

        # 并发控制
        max_concurrency=10,

        # 递归限制
        recursion_limit=25
    )

# 使用
llm = ChatOpenAI()
config = create_production_config("user_123", "rag_search")
result = llm.invoke("什么是量子纠缠？", config=config)
```

---

## 场景 10：链式传递配置

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableConfig

# 创建链
chain = ChatOpenAI() | StrOutputParser()

# 配置会自动传递到链中的所有组件
config = RunnableConfig(
    tags=["production"],
    metadata={"user_id": "user_123"}
)

result = chain.invoke("你好", config=config)

# ChatOpenAI 和 StrOutputParser 都会收到配置
```

**配置自动传播到所有子组件**

---

## 完整生产示例

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks import BaseCallbackHandler
import uuid
import logging

logger = logging.getLogger(__name__)

class SimpleLogger(BaseCallbackHandler):
    """简单日志回调"""
    def on_llm_start(self, serialized, prompts, **kwargs):
        metadata = kwargs.get("metadata", {})
        logger.info(f"LLM 开始: user_id={metadata.get('user_id')}")

    def on_llm_end(self, response, **kwargs):
        tokens = response.llm_output["token_usage"]["total_tokens"]
        logger.info(f"LLM 完成: tokens={tokens}")

def query_with_config(
    query: str,
    user_id: str,
    env: str = "production"
):
    """带配置的查询函数"""
    # 创建 LLM
    llm = ChatOpenAI(model="gpt-3.5-turbo")

    # 创建配置
    config = RunnableConfig(
        # 回调
        callbacks=[SimpleLogger()],

        # 标签
        tags=[env, "query"],

        # 元数据
        metadata={
            "user_id": user_id,
            "env": env,
            "timestamp": "2025-01-15T10:30:00Z"
        },

        # 运行名称
        run_name=f"query_{user_id}_{uuid.uuid4().hex[:8]}"
    )

    # 执行查询
    result = llm.invoke(query, config=config)

    return result.content

# 使用
result = query_with_config(
    query="什么是量子纠缠？",
    user_id="user_123",
    env="production"
)

print(result)
```

---

## 常见错误

### 错误 1：Metadata 不可序列化

```python
from datetime import datetime

# ❌ 错误
config = RunnableConfig(
    metadata={
        "timestamp": datetime.now(),  # 不可序列化
        "user": User(id=123)  # 对象实例
    }
)

# ✅ 正确
config = RunnableConfig(
    metadata={
        "timestamp": "2025-01-15T10:30:00Z",  # ISO 8601 字符串
        "user_id": 123  # 基本类型
    }
)
```

---

### 错误 2：配置合并误解

```python
# ❌ 错误理解：配置会合并
llm = ChatOpenAI().with_config(
    metadata={"env": "prod", "version": "1.0"}
)

result = llm.invoke(
    "你好",
    config=RunnableConfig(metadata={"user_id": "123"})
)

# 最终 metadata: {"user_id": "123"}  # 完全覆盖，不是合并！

# ✅ 正确：手动合并
base_metadata = {"env": "prod", "version": "1.0"}
user_metadata = {"user_id": "123"}
merged_metadata = {**base_metadata, **user_metadata}

config = RunnableConfig(metadata=merged_metadata)
```

---

### 错误 3：Tags 编码数据

```python
# ❌ 错误：在 tags 中编码数据
config = RunnableConfig(
    tags=["user_id:123", "session:abc"]
)

# ✅ 正确：使用 metadata
config = RunnableConfig(
    tags=["production", "query"],
    metadata={"user_id": "123", "session": "abc"}
)
```

---

## 总结

### 配置传递方式

| 方式 | 优先级 | 使用场景 |
|------|--------|----------|
| 构造时 | 3（最低） | 静态配置 |
| with_config | 2 | 链级配置 |
| invoke 时 | 1（最高） | 动态配置（推荐） |

### 核心字段

| 字段 | 用途 | 示例 |
|------|------|------|
| `tags` | 过滤分类 | `["production", "rag"]` |
| `metadata` | 追踪归因 | `{"user_id": "123"}` |
| `run_name` | 可读标识 | `"user_query_123"` |
| `max_concurrency` | 并发控制 | `10` |
| `recursion_limit` | 递归限制 | `25` |

---

## 参考资料

- [RunnableConfig API](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.config.RunnableConfig.html)
- [LangChain Configuration Guide](https://python.langchain.com/docs/how_to/configure/)
