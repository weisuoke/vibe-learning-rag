# 实战代码6：元数据提取保留

> 完整的元数据提取和保留策略示例

---

## 场景描述

演示如何从各种文档中提取完整的元数据，并在整个RAG流程中保留这些信息。

**适用场景：**
- 企业文档管理
- 多租户系统
- 可追溯的知识库

---

## 依赖库

```bash
# 安装依赖
uv add pypdf python-docx openpyxl langchain langchain-openai chromadb python-dotenv pydantic
```

---

## 完整代码

```python
"""
元数据提取保留示例
演示：完整的元数据提取、验证和保留策略

依赖库：
- pypdf, python-docx, openpyxl: 文档解析
- pydantic: 数据验证
- langchain: 文档处理框架

参考来源：
- LangChain Document Schema (2025): https://python.langchain.com/docs/modules/data_connection/document_loaders/#document
- Pydantic Documentation (2025): https://docs.pydantic.dev/
- Chroma Metadata Filtering (2025): https://docs.trychroma.com/usage-guide#filtering-by-metadata
"""

import os
from typing import List, Dict, Optional, Any
from datetime import datetime
from dotenv import load_dotenv
from langchain.schema import Document
from pydantic import BaseModel, Field, validator

load_dotenv()

# ===== 1. 标准化元数据Schema =====
print("=== 1. 标准化元数据Schema ===")

class StandardMetadata(BaseModel):
    """标准化的元数据Schema"""

    # 必需字段
    source: str = Field(..., description="文件路径或URL")

    # 文档元数据
    title: Optional[str] = Field(None, description="文档标题")
    author: Optional[str] = Field(None, description="作者")
    created_date: Optional[str] = Field(None, description="创建日期(ISO 8601)")
    modified_date: Optional[str] = Field(None, description="修改日期(ISO 8601)")
    language: Optional[str] = Field("zh", description="语言代码")

    # 结构元数据
    page: Optional[int] = Field(None, description="页码")
    page_count: Optional[int] = Field(None, description="总页数")
    section: Optional[str] = Field(None, description="章节")
    heading: Optional[str] = Field(None, description="标题")

    # 文件元数据
    file_name: Optional[str] = Field(None, description="文件名")
    file_size: Optional[int] = Field(None, description="文件大小(字节)")
    file_type: Optional[str] = Field(None, description="文件类型")

    # 语义元数据
    keywords: Optional[List[str]] = Field(None, description="关键词")
    word_count: Optional[int] = Field(None, description="词数")

    # 业务元数据
    department: Optional[str] = Field(None, description="部门")
    category: Optional[str] = Field(None, description="分类")
    tags: Optional[List[str]] = Field(None, description="标签")
    access_level: Optional[str] = Field("public", description="访问级别")

    # 系统元数据
    loaded_at: Optional[str] = Field(None, description="加载时间(ISO 8601)")

    class Config:
        json_schema_extra = {
            "example": {
                "source": "reports/rag_guide.pdf",
                "title": "RAG技术指南",
                "author": "张三",
                "created_date": "2025-01-15T10:30:00",
                "page": 15,
                "department": "技术部"
            }
        }

# 测试Schema
example_metadata = StandardMetadata(
    source="report.pdf",
    title="技术报告",
    author="张三",
    department="技术部"
)
print(f"标准元数据: {example_metadata.model_dump()}\n")

# ===== 2. PDF元数据提取器 =====
print("=== 2. PDF元数据提取器 ===")

from pypdf import PdfReader

class PDFMetadataExtractor:
    """PDF元数据提取器"""

    def extract(self, file_path: str) -> Dict[str, Any]:
        """提取PDF的完整元数据"""
        metadata = {}

        # 1. 文件系统元数据
        stat = os.stat(file_path)
        metadata.update({
            "source": file_path,
            "file_name": os.path.basename(file_path),
            "file_size": stat.st_size,
            "file_type": "pdf",
            "created_date": datetime.fromtimestamp(stat.st_ctime).isoformat(),
            "modified_date": datetime.fromtimestamp(stat.st_mtime).isoformat()
        })

        # 2. PDF内部元数据
        try:
            reader = PdfReader(file_path)
            pdf_meta = reader.metadata

            if pdf_meta:
                metadata.update({
                    "title": pdf_meta.get("/Title", ""),
                    "author": pdf_meta.get("/Author", ""),
                    "subject": pdf_meta.get("/Subject", ""),
                    "creator": pdf_meta.get("/Creator", ""),
                    "producer": pdf_meta.get("/Producer", "")
                })

                # 解析创建日期
                if "/CreationDate" in pdf_meta:
                    metadata["pdf_created_date"] = pdf_meta["/CreationDate"]

            metadata["page_count"] = len(reader.pages)

        except Exception as e:
            print(f"提取PDF元数据失败: {e}")

        # 3. 添加加载时间
        metadata["loaded_at"] = datetime.now().isoformat()

        return metadata

    def extract_with_validation(self, file_path: str) -> StandardMetadata:
        """提取并验证元数据"""
        raw_metadata = self.extract(file_path)

        # 使用Pydantic验证
        try:
            validated_metadata = StandardMetadata(**raw_metadata)
            print(f"✅ 元数据验证通过: {file_path}")
            return validated_metadata
        except Exception as e:
            print(f"❌ 元数据验证失败: {e}")
            # 返回最小元数据
            return StandardMetadata(source=file_path)

# 测试PDF元数据提取
# extractor = PDFMetadataExtractor()
# metadata = extractor.extract("sample.pdf")
# print(f"PDF元数据: {metadata}\n")

# ===== 3. Office文档元数据提取器 =====
print("\n=== 3. Office文档元数据提取器 ===")

from docx import Document as DocxDocument

class OfficeMetadataExtractor:
    """Office文档元数据提取器"""

    def extract_docx(self, file_path: str) -> Dict[str, Any]:
        """提取Word文档元数据"""
        metadata = {
            "source": file_path,
            "file_name": os.path.basename(file_path),
            "file_size": os.path.getsize(file_path),
            "file_type": "docx"
        }

        try:
            doc = DocxDocument(file_path)

            # 核心属性
            core_props = doc.core_properties
            metadata.update({
                "title": core_props.title or "",
                "author": core_props.author or "",
                "subject": core_props.subject or "",
                "keywords": core_props.keywords or "",
                "created_date": core_props.created.isoformat() if core_props.created else "",
                "modified_date": core_props.modified.isoformat() if core_props.modified else "",
                "last_modified_by": core_props.last_modified_by or ""
            })

            # 文档统计
            metadata.update({
                "paragraph_count": len(doc.paragraphs),
                "table_count": len(doc.tables),
                "section_count": len(doc.sections)
            })

        except Exception as e:
            print(f"提取Word元数据失败: {e}")

        metadata["loaded_at"] = datetime.now().isoformat()
        return metadata

    def extract_xlsx(self, file_path: str) -> Dict[str, Any]:
        """提取Excel文档元数据"""
        import openpyxl

        metadata = {
            "source": file_path,
            "file_name": os.path.basename(file_path),
            "file_size": os.path.getsize(file_path),
            "file_type": "xlsx"
        }

        try:
            workbook = openpyxl.load_workbook(file_path, data_only=True)

            # 工作簿属性
            props = workbook.properties
            metadata.update({
                "title": props.title or "",
                "author": props.creator or "",
                "subject": props.subject or "",
                "keywords": props.keywords or "",
                "created_date": props.created.isoformat() if props.created else "",
                "modified_date": props.modified.isoformat() if props.modified else ""
            })

            # 工作表统计
            metadata.update({
                "sheet_count": len(workbook.sheetnames),
                "sheet_names": workbook.sheetnames
            })

        except Exception as e:
            print(f"提取Excel元数据失败: {e}")

        metadata["loaded_at"] = datetime.now().isoformat()
        return metadata

# 测试Office元数据提取
# extractor = OfficeMetadataExtractor()
# docx_meta = extractor.extract_docx("report.docx")
# xlsx_meta = extractor.extract_xlsx("data.xlsx")

# ===== 4. 元数据增强器 =====
print("\n=== 4. 元数据增强器 ===")

class MetadataEnhancer:
    """元数据增强器"""

    def enhance(self, document: Document, custom_metadata: Dict = None) -> Document:
        """增强文档元数据"""

        # 1. 内容统计
        document.metadata["content_length"] = len(document.page_content)
        document.metadata["word_count"] = len(document.page_content.split())

        # 2. 语言检测
        chinese_chars = sum(1 for c in document.page_content if '\u4e00' <= c <= '\u9fff')
        total_chars = len(document.page_content)
        if total_chars > 0:
            document.metadata["language"] = "zh" if chinese_chars / total_chars > 0.1 else "en"

        # 3. 关键词提取（简单版本）
        words = document.page_content.split()
        word_freq = {}
        for word in words:
            if len(word) > 1:
                word_freq[word] = word_freq.get(word, 0) + 1

        # 取频率最高的5个词
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        document.metadata["keywords"] = [word for word, freq in top_keywords]

        # 4. 添加自定义元数据
        if custom_metadata:
            document.metadata.update(custom_metadata)

        # 5. 添加处理时间戳
        if "loaded_at" not in document.metadata:
            document.metadata["loaded_at"] = datetime.now().isoformat()

        return document

# 测试元数据增强
# doc = Document(page_content="RAG系统通过检索增强生成...", metadata={"source": "test.txt"})
# enhancer = MetadataEnhancer()
# enhanced_doc = enhancer.enhance(doc, custom_metadata={"department": "技术部"})
# print(f"增强后的元数据: {enhanced_doc.metadata}")

# ===== 5. 元数据验证器 =====
print("\n=== 5. 元数据验证器 ===")

class MetadataValidator:
    """元数据验证器"""

    def __init__(self, required_fields: List[str] = None):
        self.required_fields = required_fields or ["source"]

    def validate(self, document: Document) -> tuple[bool, List[str]]:
        """
        验证文档元数据
        返回: (是否有效, 错误列表)
        """
        errors = []

        # 1. 检查必需字段
        for field in self.required_fields:
            if field not in document.metadata:
                errors.append(f"缺少必需字段: {field}")

        # 2. 检查数据类型
        if "page" in document.metadata:
            if not isinstance(document.metadata["page"], int):
                errors.append("page字段必须是整数")

        if "word_count" in document.metadata:
            if not isinstance(document.metadata["word_count"], int):
                errors.append("word_count字段必须是整数")

        # 3. 检查日期格式
        date_fields = ["created_date", "modified_date", "loaded_at"]
        for field in date_fields:
            if field in document.metadata and document.metadata[field]:
                try:
                    datetime.fromisoformat(document.metadata[field])
                except ValueError:
                    errors.append(f"{field}字段日期格式无效")

        is_valid = len(errors) == 0
        return is_valid, errors

    def validate_batch(self, documents: List[Document]) -> Dict:
        """批量验证文档"""
        results = {
            "total": len(documents),
            "valid": 0,
            "invalid": 0,
            "errors": []
        }

        for i, doc in enumerate(documents):
            is_valid, errors = self.validate(doc)
            if is_valid:
                results["valid"] += 1
            else:
                results["invalid"] += 1
                results["errors"].append({
                    "document_index": i,
                    "source": doc.metadata.get("source", "unknown"),
                    "errors": errors
                })

        return results

# 测试元数据验证
# validator = MetadataValidator(required_fields=["source", "title"])
# is_valid, errors = validator.validate(document)
# if not is_valid:
#     print(f"验证失败: {errors}")

# ===== 6. RAG应用：元数据在检索中的使用 =====
print("\n=== 6. RAG应用：元数据在检索中的使用 ===")

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

def build_metadata_rich_kb(file_paths: List[str], custom_metadata: Dict[str, Dict] = None):
    """构建元数据丰富的知识库"""

    print("=== 构建元数据丰富的知识库 ===\n")

    # 1. 加载文档并提取元数据
    print("步骤1: 加载文档并提取元数据")
    all_documents = []

    pdf_extractor = PDFMetadataExtractor()
    office_extractor = OfficeMetadataExtractor()
    enhancer = MetadataEnhancer()

    for file_path in file_paths:
        try:
            # 根据文件类型选择提取器
            if file_path.endswith('.pdf'):
                metadata = pdf_extractor.extract(file_path)
                # 简化：这里应该实际加载PDF内容
                content = f"PDF内容来自 {file_path}"
            elif file_path.endswith('.docx'):
                metadata = office_extractor.extract_docx(file_path)
                content = f"Word内容来自 {file_path}"
            elif file_path.endswith('.xlsx'):
                metadata = office_extractor.extract_xlsx(file_path)
                content = f"Excel内容来自 {file_path}"
            else:
                continue

            # 创建Document
            doc = Document(page_content=content, metadata=metadata)

            # 增强元数据
            custom_meta = custom_metadata.get(file_path, {}) if custom_metadata else {}
            doc = enhancer.enhance(doc, custom_meta)

            all_documents.append(doc)
            print(f"  ✅ {file_path}")

        except Exception as e:
            print(f"  ❌ {file_path}: {e}")

    # 2. 验证元数据
    print("\n步骤2: 验证元数据")
    validator = MetadataValidator(required_fields=["source", "file_type"])
    validation_results = validator.validate_batch(all_documents)

    print(f"  总文档数: {validation_results['total']}")
    print(f"  有效: {validation_results['valid']}")
    print(f"  无效: {validation_results['invalid']}")

    if validation_results['invalid'] > 0:
        print("\n  验证错误:")
        for error in validation_results['errors']:
            print(f"    - {error['source']}: {error['errors']}")

    # 3. 分块（保留元数据）
    print("\n步骤3: 文本分块")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(all_documents)
    print(f"  ✅ 分块完成: {len(chunks)} 个文本块")

    # 验证：分块后元数据仍然保留
    print(f"  第一个chunk的元数据: {list(chunks[0].metadata.keys())}")

    # 4. 向量化并存储
    print("\n步骤4: 向量化并存储")
    try:
        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma.from_documents(
            chunks,
            embeddings,
            persist_directory="./metadata_rich_kb"
        )
        print(f"  ✅ 知识库构建成功")

        # 5. 测试元数据过滤检索
        print("\n步骤5: 测试元数据过滤检索")

        # 测试1：按文件类型过滤
        results = vectorstore.similarity_search(
            "技术文档",
            k=3,
            filter={"file_type": "pdf"}
        )
        print(f"\n  只检索PDF文档: {len(results)} 个结果")

        # 测试2：按部门过滤
        results = vectorstore.similarity_search(
            "项目报告",
            k=3,
            filter={"department": "技术部"}
        )
        print(f"  只检索技术部文档: {len(results)} 个结果")

        # 测试3：组合过滤
        results = vectorstore.similarity_search(
            "数据分析",
            k=3,
            filter={
                "file_type": "xlsx",
                "access_level": "public"
            }
        )
        print(f"  组合过滤: {len(results)} 个结果")

        # 6. 显示检索结果的元数据
        print("\n步骤6: 检索结果元数据")
        results = vectorstore.similarity_search("技术", k=2)
        for i, doc in enumerate(results, 1):
            print(f"\n  结果 {i}:")
            print(f"    来源: {doc.metadata.get('source')}")
            print(f"    标题: {doc.metadata.get('title')}")
            print(f"    作者: {doc.metadata.get('author')}")
            print(f"    部门: {doc.metadata.get('department')}")
            print(f"    文件类型: {doc.metadata.get('file_type')}")

        return vectorstore

    except Exception as e:
        print(f"  ❌ 向量化失败: {e}")
        return None

# 测试构建知识库
# file_paths = ["report1.pdf", "report2.docx", "data.xlsx"]
# custom_metadata = {
#     "report1.pdf": {"department": "技术部", "category": "技术报告"},
#     "report2.docx": {"department": "产品部", "category": "产品文档"},
#     "data.xlsx": {"department": "数据部", "category": "数据分析"}
# }
# vectorstore = build_metadata_rich_kb(file_paths, custom_metadata)

# ===== 7. 元数据导出和分析 =====
print("\n=== 7. 元数据导出和分析 ===")

import json

def export_metadata_report(documents: List[Document], output_file: str):
    """导出元数据报告"""

    # 1. 收集所有元数据
    metadata_list = []
    for doc in documents:
        metadata_list.append(doc.metadata)

    # 2. 统计分析
    stats = {
        "total_documents": len(documents),
        "by_file_type": {},
        "by_department": {},
        "by_language": {},
        "by_author": {}
    }

    for meta in metadata_list:
        # 按文件类型统计
        file_type = meta.get("file_type", "unknown")
        stats["by_file_type"][file_type] = stats["by_file_type"].get(file_type, 0) + 1

        # 按部门统计
        department = meta.get("department", "unknown")
        stats["by_department"][department] = stats["by_department"].get(department, 0) + 1

        # 按语言统计
        language = meta.get("language", "unknown")
        stats["by_language"][language] = stats["by_language"].get(language, 0) + 1

        # 按作者统计
        author = meta.get("author", "unknown")
        if author:
            stats["by_author"][author] = stats["by_author"].get(author, 0) + 1

    # 3. 生成报告
    report = {
        "generated_at": datetime.now().isoformat(),
        "statistics": stats,
        "documents": metadata_list
    }

    # 4. 保存报告
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"✅ 元数据报告已导出: {output_file}")

    # 5. 打印统计摘要
    print("\n元数据统计摘要:")
    print(f"  总文档数: {stats['total_documents']}")
    print(f"\n  按文件类型:")
    for file_type, count in stats['by_file_type'].items():
        print(f"    {file_type}: {count}")
    print(f"\n  按部门:")
    for dept, count in stats['by_department'].items():
        print(f"    {dept}: {count}")

# 测试导出报告
# export_metadata_report(all_documents, "metadata_report.json")

print("\n=== 示例代码执行完成 ===")
print("\n使用说明:")
print("1. 准备要处理的文档")
print("2. 取消注释测试代码")
print("3. 运行脚本查看效果")
print("\n功能:")
print("- 标准化元数据Schema")
print("- 完整的元数据提取")
print("- 元数据验证")
print("- 元数据增强")
print("- 元数据过滤检索")
print("- 元数据报告导出")
```

---

## 运行输出示例

```
=== 构建元数据丰富的知识库 ===

步骤1: 加载文档并提取元数据
  ✅ report1.pdf
  ✅ report2.docx
  ✅ data.xlsx

步骤2: 验证元数据
  总文档数: 3
  有效: 3
  无效: 0

步骤3: 文本分块
  ✅ 分块完成: 15 个文本块
  第一个chunk的元数据: ['source', 'file_name', 'file_type', 'title', 'author', 'department', 'loaded_at']

步骤4: 向量化并存储
  ✅ 知识库构建成功

步骤5: 测试元数据过滤检索
  只检索PDF文档: 3 个结果
  只检索技术部文档: 2 个结果
  组合过滤: 1 个结果

步骤6: 检索结果元数据
  结果 1:
    来源: report1.pdf
    标题: 技术报告
    作者: 张三
    部门: 技术部
    文件类型: pdf
```

---

## 关键要点

### 1. 元数据Schema设计

```python
# 使用Pydantic定义标准Schema
class StandardMetadata(BaseModel):
    # 必需字段
    source: str

    # 可选字段
    title: Optional[str] = None
    author: Optional[str] = None

    # 带默认值
    language: str = "zh"
    access_level: str = "public"
```

### 2. 元数据在RAG中的三层作用

1. **检索增强**：过滤、排序、权重
2. **可解释性**：溯源、引用、可信度
3. **系统管理**：权限、版本、统计

### 3. 元数据过滤语法

```python
# Chroma元数据过滤
filter = {
    "department": "技术部",  # 等于
    "year": {"$gte": 2025},  # 大于等于
    "file_type": {"$in": ["pdf", "docx"]}  # 包含
}
```

---

## 常见问题

### Q1: 如何处理元数据缺失？

```python
# 提供默认值
metadata = {
    "source": file_path,
    "title": metadata.get("title") or os.path.basename(file_path),
    "author": metadata.get("author") or "未知",
    "department": metadata.get("department") or "未分类"
}
```

### Q2: 如何在分块后保留元数据？

LangChain的TextSplitter会自动保留元数据：
```python
splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
chunks = splitter.split_documents(documents)
# 每个chunk都保留原始文档的元数据
```

### Q3: 如何更新已存储文档的元数据？

```python
# 需要重新加载并更新
new_docs = load_documents_with_updated_metadata()
vectorstore.delete(filter={"source": "old_doc.pdf"})
vectorstore.add_documents(new_docs)
```

---

## 扩展阅读

- [LangChain Document Schema](https://python.langchain.com/docs/modules/data_connection/document_loaders/#document) (2025)
- [Pydantic Documentation](https://docs.pydantic.dev/) (2025)
- [Chroma Metadata Filtering](https://docs.trychroma.com/usage-guide#filtering-by-metadata) (2025)

---

**版本：** v1.0
**最后更新：** 2026-02-15
**下一步：** 阅读 [07_实战代码_07_错误处理容错.md](./07_实战代码_07_错误处理容错.md)
