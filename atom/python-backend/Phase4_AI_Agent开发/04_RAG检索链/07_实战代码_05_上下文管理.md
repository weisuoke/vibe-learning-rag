# RAG检索链 - 实战代码5：上下文管理

> 管理上下文窗口、引用来源追踪、多文档合并

---

## 完整代码

```python
"""
上下文管理完整示例
演示：上下文窗口管理、引用来源、多文档合并
"""

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import tiktoken

load_dotenv()

# ===== 1. Token 计数 =====
print("=== 1. Token 计数 ===")

def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    """计算文本的 token 数量"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

documents = [
    "Python 是一门编程语言，由 Guido van Rossum 于 1991 年创建。",
    "FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。",
    "RAG 是检索增强生成技术，结合了检索和生成。"
]

for i, doc in enumerate(documents, 1):
    tokens = count_tokens(doc)
    print(f"文档{i}: {tokens} tokens - {doc[:40]}...")

# ===== 2. 上下文窗口限制 =====
print("\n=== 2. 上下文窗口限制 ===")

MAX_CONTEXT_TOKENS = 3000  # GPT-3.5-turbo 的上下文限制约 4K

def truncate_context(docs: list[str], max_tokens: int) -> list[str]:
    """截断上下文以适应窗口限制"""
    selected_docs = []
    total_tokens = 0

    for doc in docs:
        doc_tokens = count_tokens(doc)
        if total_tokens + doc_tokens <= max_tokens:
            selected_docs.append(doc)
            total_tokens += doc_tokens
        else:
            print(f"⚠️  达到 token 限制，跳过剩余文档")
            break

    print(f"✓ 选择了 {len(selected_docs)}/{len(docs)} 个文档")
    print(f"✓ 总 tokens: {total_tokens}/{max_tokens}")
    return selected_docs

# 测试
long_docs = documents * 500  # 模拟大量文档
selected = truncate_context(long_docs, MAX_CONTEXT_TOKENS)

# ===== 3. 引用来源追踪 =====
print("\n=== 3. 引用来源追踪 ===")

embeddings = OpenAIEmbeddings()

# 创建带元数据的向量库
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    metadatas=[
        {"source": "python_intro.pdf", "page": 1},
        {"source": "fastapi_guide.md", "page": 3},
        {"source": "rag_tutorial.txt", "page": 1}
    ]
)

# 定义带引用的 Prompt
prompt_with_citation = PromptTemplate(
    template="""
基于以下文档回答问题，并引用来源。

格式要求：
- 在答案中标注来源，如"根据 [文档1]..."
- 在答案末尾列出所有引用的文档

文档：
{context}

问题：{question}

答案（包含引用）：
""",
    input_variables=["context", "question"]
)

# 创建 RAG 链
llm = ChatOpenAI(temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
    chain_type_kwargs={"prompt": prompt_with_citation},
    return_source_documents=True
)

# 测试
result = qa_chain.invoke({"query": "什么是 FastAPI？"})
print(f"问题: {result['query']}")
print(f"答案: {result['result']}")
print(f"\n引用来源:")
for i, doc in enumerate(result['source_documents'], 1):
    print(f"  [文档{i}] {doc.metadata['source']} (第{doc.metadata['page']}页)")
    print(f"    内容: {doc.page_content[:60]}...")

# ===== 4. 多文档合并策略 =====
print("\n=== 4. 多文档合并策略 ===")

def merge_documents_simple(docs: list[str]) -> str:
    """简单合并：直接拼接"""
    return "\n\n".join(docs)

def merge_documents_numbered(docs: list[str]) -> str:
    """编号合并：添加文档编号"""
    return "\n\n".join([
        f"【文档{i}】\n{doc}"
        for i, doc in enumerate(docs, 1)
    ])

def merge_documents_hierarchical(docs: list[dict]) -> str:
    """分层合并：按相关性分组"""
    high_relevance = [
        doc['content'] for doc in docs
        if doc.get('score', 0) > 0.8
    ]
    medium_relevance = [
        doc['content'] for doc in docs
        if 0.6 < doc.get('score', 0) <= 0.8
    ]

    result = ""
    if high_relevance:
        result += "【高相关性文档】\n" + "\n\n".join(high_relevance)
    if medium_relevance:
        result += "\n\n【中等相关性文档】\n" + "\n\n".join(medium_relevance)
    return result

# 测试不同合并策略
test_docs = documents[:2]

print("简单合并:")
print(merge_documents_simple(test_docs)[:100] + "...")

print("\n编号合并:")
print(merge_documents_numbered(test_docs)[:100] + "...")

print("\n分层合并:")
test_docs_with_score = [
    {"content": documents[0], "score": 0.9},
    {"content": documents[1], "score": 0.7}
]
print(merge_documents_hierarchical(test_docs_with_score)[:100] + "...")

# ===== 5. 动态上下文调整 =====
print("\n=== 5. 动态上下文调整 ===")

def dynamic_context_selection(
    query: str,
    all_docs: list[str],
    max_tokens: int = 2000
) -> list[str]:
    """根据查询动态选择上下文"""
    # 简单策略：根据查询长度调整文档数量
    query_tokens = count_tokens(query)

    if query_tokens < 20:
        # 简单问题：少量文档
        target_docs = 2
    elif query_tokens < 50:
        # 中等问题：中等文档
        target_docs = 3
    else:
        # 复杂问题：更多文档
        target_docs = 5

    print(f"查询 tokens: {query_tokens} → 目标文档数: {target_docs}")

    # 选择文档直到达到 token 限制
    selected = []
    total_tokens = query_tokens

    for doc in all_docs[:target_docs]:
        doc_tokens = count_tokens(doc)
        if total_tokens + doc_tokens <= max_tokens:
            selected.append(doc)
            total_tokens += doc_tokens
        else:
            break

    print(f"✓ 选择了 {len(selected)} 个文档，总 tokens: {total_tokens}")
    return selected

# 测试
queries = [
    "Python",
    "什么是 FastAPI？",
    "详细对比 Python、FastAPI 和 RAG 的特点和应用场景"
]

for query in queries:
    print(f"\n查询: {query}")
    selected = dynamic_context_selection(query, documents)

# ===== 6. 上下文压缩 =====
print("\n=== 6. 上下文压缩 ===")

def compress_context(docs: list[str], query: str) -> list[str]:
    """压缩上下文：只保留与查询相关的句子"""
    compressed = []

    for doc in docs:
        # 简单策略：保留包含查询关键词的句子
        sentences = doc.split("。")
        relevant_sentences = [
            s for s in sentences
            if any(word in s for word in query.split())
        ]

        if relevant_sentences:
            compressed.append("。".join(relevant_sentences) + "。")

    return compressed

# 测试
query = "FastAPI 框架"
original_docs = documents
compressed_docs = compress_context(original_docs, query)

print(f"查询: {query}")
print(f"原始文档数: {len(original_docs)}")
print(f"压缩后文档数: {len(compressed_docs)}")
print(f"\n压缩后内容:")
for doc in compressed_docs:
    print(f"  - {doc}")

# ===== 7. 完整的上下文管理流程 =====
print("\n=== 7. 完整的上下文管理流程 ===")

class ContextManager:
    """上下文管理器"""

    def __init__(self, max_tokens: int = 3000):
        self.max_tokens = max_tokens

    def manage_context(
        self,
        query: str,
        documents: list[dict]  # {"content": str, "metadata": dict, "score": float}
    ) -> dict:
        """完整的上下文管理流程"""
        # 1. 计算查询 tokens
        query_tokens = count_tokens(query)
        available_tokens = self.max_tokens - query_tokens - 200  # 预留 200 给 Prompt

        # 2. 按相似度排序
        sorted_docs = sorted(documents, key=lambda x: x['score'], reverse=True)

        # 3. 选择文档直到达到 token 限制
        selected_docs = []
        total_tokens = 0

        for doc in sorted_docs:
            doc_tokens = count_tokens(doc['content'])
            if total_tokens + doc_tokens <= available_tokens:
                selected_docs.append(doc)
                total_tokens += doc_tokens
            else:
                break

        # 4. 构造上下文
        context = self._build_context(selected_docs)

        return {
            "context": context,
            "selected_count": len(selected_docs),
            "total_count": len(documents),
            "context_tokens": total_tokens,
            "sources": [doc['metadata'] for doc in selected_docs]
        }

    def _build_context(self, docs: list[dict]) -> str:
        """构造带编号和元数据的上下文"""
        parts = []
        for i, doc in enumerate(docs, 1):
            meta = doc['metadata']
            parts.append(
                f"【文档{i}】来源: {meta.get('source', 'unknown')}\n"
                f"{doc['content']}"
            )
        return "\n\n".join(parts)

# 测试
manager = ContextManager(max_tokens=3000)

test_documents = [
    {
        "content": documents[0],
        "metadata": {"source": "python_intro.pdf", "page": 1},
        "score": 0.95
    },
    {
        "content": documents[1],
        "metadata": {"source": "fastapi_guide.md", "page": 3},
        "score": 0.85
    },
    {
        "content": documents[2],
        "metadata": {"source": "rag_tutorial.txt", "page": 1},
        "score": 0.75
    }
]

result = manager.manage_context("什么是 Python？", test_documents)

print(f"选择了 {result['selected_count']}/{result['total_count']} 个文档")
print(f"上下文 tokens: {result['context_tokens']}")
print(f"\n上下文内容:")
print(result['context'][:200] + "...")
print(f"\n引用来源:")
for source in result['sources']:
    print(f"  - {source['source']} (第{source['page']}页)")

print("\n✓ 上下文管理演示完成！")
```

---

## 运行输出示例

```
=== 1. Token 计数 ===
文档1: 28 tokens - Python 是一门编程语言，由 Guido van Rossum 于 1991 年创建。...
文档2: 26 tokens - FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。...
文档3: 18 tokens - RAG 是检索增强生成技术，结合了检索和生成。...

=== 2. 上下文窗口限制 ===
⚠️  达到 token 限制，跳过剩余文档
✓ 选择了 41/1500 个文档
✓ 总 tokens: 2996/3000

=== 3. 引用来源追踪 ===
问题: 什么是 FastAPI？
答案: 根据 [文档1]，FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。

引用来源:
  [文档1] fastapi_guide.md (第3页)
    内容: FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。...
  [文档2] python_intro.pdf (第1页)
    内容: Python 是一门编程语言，由 Guido van Rossum 于 1991 年创建。...

=== 4. 多文档合并策略 ===
简单合并:
Python 是一门编程语言，由 Guido van Rossum 于 1991 年创建。

FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。...

编号合并:
【文档1】
Python 是一门编程语言，由 Guido van Rossum 于 1991 年创建。

【文档2】
FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。...

分层合并:
【高相关性文档】
Python 是一门编程语言，由 Guido van Rossum 于 1991 年创建。

【中等相关性文档】
FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。...

=== 5. 动态上下文调整 ===

查询: Python
查询 tokens: 2 → 目标文档数: 2
✓ 选择了 2 个文档，总 tokens: 56

查询: 什么是 FastAPI？
查询 tokens: 8 → 目标文档数: 2
✓ 选择了 2 个文档，总 tokens: 62

查询: 详细对比 Python、FastAPI 和 RAG 的特点和应用场景
查询 tokens: 24 → 目标文档数: 3
✓ 选择了 3 个文档，总 tokens: 96

=== 6. 上下文压缩 ===
查询: FastAPI 框架
原始文档数: 3
压缩后文档数: 1

压缩后内容:
  - FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。

=== 7. 完整的上下文管理流程 ===
选择了 3/3 个文档
上下文 tokens: 72

上下文内容:
【文档1】来源: python_intro.pdf
Python 是一门编程语言，由 Guido van Rossum 于 1991 年创建。

【文档2】来源: fastapi_guide.md
FastAPI 是一个现代、快速的 Web 框架，基于 Python 3.6+。

【文档3】来源: rag_tutorial.txt
RAG 是检索增强生成技术，结合了检索和生成。...

引用来源:
  - python_intro.pdf (第1页)
  - fastapi_guide.md (第3页)
  - rag_tutorial.txt (第1页)

✓ 上下文管理演示完成！
```

---

## 关键技术点

### 1. Token 计数

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))
```

### 2. 上下文窗口限制

| 模型 | 上下文窗口 | 推荐上下文 |
|------|-----------|----------|
| GPT-3.5-turbo | 4K tokens | 3K tokens |
| GPT-4 | 8K tokens | 6K tokens |
| GPT-4-32K | 32K tokens | 28K tokens |
| Claude-3 | 100K tokens | 90K tokens |

### 3. 引用来源格式

```python
# 在 Prompt 中要求引用
prompt = """
请在答案中标注来源，如"根据 [文档1]..."
"""

# 返回引用来源
return {
    "answer": "...",
    "sources": [
        {"source": "file.pdf", "page": 1},
        ...
    ]
}
```

---

## 最佳实践

1. **预留 token**：为 Prompt 和输出预留 20-30% token
2. **优先级排序**：按相似度选择文档
3. **引用来源**：始终追踪文档来源
4. **动态调整**：根据查询复杂度调整上下文
5. **压缩策略**：必要时压缩上下文

---

**下一步**：学习 [07_实战代码_06_FastAPI集成](./07_实战代码_06_FastAPI集成.md)
