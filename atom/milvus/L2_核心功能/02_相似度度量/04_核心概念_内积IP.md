# 核心概念：内积IP（Inner Product）

## 一句话定义

**内积IP是计算两个向量点积的度量方式，同时考虑方向一致性和向量幅度，值越大表示越相似。**

---

## 1. 数学原理

### 1.1 公式定义

对于两个n维向量 x = [x₁, x₂, ..., xₙ] 和 y = [y₁, y₂, ..., yₙ]：

```
IP(x, y) = x₁y₁ + x₂y₂ + ... + xₙyₙ = Σ(xᵢ × yᵢ)
```

**向量形式：**
```
IP(x, y) = x · y = ‖x‖ ‖y‖ cos(θ)
```

其中θ是两向量的夹角。

### 1.2 手写实现

```python
import numpy as np

def inner_product(x, y):
    """
    计算内积（点积）

    参数:
        x: 向量1
        y: 向量2

    返回:
        内积值（标量）
    """
    # 方法1：逐元素相乘后求和
    result = 0
    for i in range(len(x)):
        result += x[i] * y[i]
    return result

# 方法2：使用NumPy（推荐）
def inner_product_fast(x, y):
    return np.dot(x, y)

# 示例
x = np.array([1.0, 2.0, 3.0])
y = np.array([4.0, 5.0, 6.0])

ip = inner_product(x, y)
print(f"内积: {ip:.4f}")  # 32.0000
```

---

## 2. 几何意义

### 2.1 投影解释

内积 = 一个向量在另一个向量方向上的投影长度 × 另一个向量的长度

```
IP(x, y) = ‖x‖ × (‖y‖ cos(θ))
         = ‖x‖ × 投影长度
```

**可视化：**
```
      y
     /|
    / |
   /  | ‖y‖cos(θ) (投影)
  /   |
 / θ  |
x-----+

IP(x, y) = ‖x‖ × ‖y‖cos(θ)
```

### 2.2 物理意义

在物理学中，内积表示"做功"：

```
功 = 力 · 位移
   = ‖力‖ × ‖位移‖ × cos(夹角)
```

**示例：**
```python
# 力向量
force = np.array([3.0, 4.0])  # 5N的力，方向(3,4)

# 位移向量
displacement = np.array([1.0, 0.0])  # 沿x轴移动1m

# 做功
work = np.dot(force, displacement)
print(f"做功: {work:.2f} J")  # 3.00 J
```

---

## 3. 在Milvus中的使用

### 3.1 配置IP度量

```python
from pymilvus import Collection, FieldSchema, CollectionSchema, DataType

# 定义Schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
]
schema = CollectionSchema(fields=fields, description="IP度量示例")

# 创建Collection
collection = Collection(name="ip_collection", schema=schema)

# 创建索引，指定IP度量
index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "IP",  # 使用内积
    "params": {"nlist": 128}
}
collection.create_index(field_name="embedding", index_params=index_params)
```

### 3.2 搜索时使用IP

```python
# 加载Collection
collection.load()

# 查询向量
query_vector = [[0.1, 0.2, 0.3, ...]]  # 768维

# 搜索参数
search_params = {
    "metric_type": "IP",
    "params": {"nprobe": 10}
}

# 执行搜索
results = collection.search(
    data=query_vector,
    anns_field="embedding",
    param=search_params,
    limit=10,
    output_fields=["id"]
)

# 结果按IP值从大到小排序
for hits in results:
    for hit in hits:
        print(f"ID: {hit.id}, IP值: {hit.distance:.4f}")
```

### 3.3 IP的排序规则

**重要：IP值越大越相似**

```python
# 示例结果
# ID: 123, IP值: 0.9876  ← 最相似（IP最大）
# ID: 456, IP值: 0.8765
# ID: 789, IP值: 0.7654
# ...
# ID: 999, IP值: 0.1234  ← 最不相似（IP最小）
```

---

## 4. RAG应用场景

### 4.1 场景1：推荐系统

**适用情况：**
- 需要同时考虑匹配度和热度
- 向量幅度表示重要性或热度

**示例：商品推荐**

```python
import numpy as np
from pymilvus import connections, Collection

# 连接Milvus
connections.connect(host="localhost", port="19530")

# 用户兴趣向量（幅度表示兴趣强度）
user_interest = np.array([0.8, 0.6, 0.3])  # [电子产品, 图书, 服装]

# 商品特征向量（幅度表示热度）
products = {
    "iPhone": np.array([0.9, 0.1, 0.0]),   # 热门电子产品
    "Python书": np.array([0.2, 0.8, 0.1]),  # 热门图书
    "小众书": np.array([0.1, 0.4, 0.05]),  # 冷门图书
}

# 计算推荐分数（内积）
for name, product in products.items():
    score = np.dot(user_interest, product)
    print(f"{name}: {score:.4f}")

# 输出：
# iPhone: 0.7800（高分：匹配+热门）
# Python书: 0.5100（中分：匹配但不是最热门）
# 小众书: 0.2650（低分：匹配但冷门）
```

**关键洞察：** IP能同时考虑"用户喜欢"和"商品热门"。

### 4.2 场景2：归一化向量的快速检索

**适用情况：**
- Embedding已归一化
- 需要最快的COSINE等价计算

**示例：语义搜索优化**

```python
# 归一化Embedding
def normalize(vector):
    return vector / np.linalg.norm(vector)

# 文档Embedding（归一化）
documents = [
    normalize(np.random.randn(768)) for _ in range(1000)
]

# 查询Embedding（归一化）
query = normalize(np.random.randn(768))

# 使用IP代替COSINE（更快）
collection = Collection("normalized_docs")

# 创建IP索引
index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "IP",  # IP = COSINE（当归一化时）
    "params": {"nlist": 128}
}
collection.create_index(field_name="embedding", index_params=index_params)

# 搜索（比COSINE快10-20%）
results = collection.search(
    data=[query],
    anns_field="embedding",
    param={"metric_type": "IP", "params": {"nprobe": 10}},
    limit=10
)
```

**性能对比：**
```
COSINE: 需要计算 (x·y) / (‖x‖‖y‖)
IP: 只需要计算 x·y（归一化后等价）

性能提升：10-20%
```

---

## 5. 完整实战代码

```python
"""
内积IP检索系统完整示例
演示：推荐系统中使用IP度量
"""

import numpy as np
from pymilvus import (
    connections,
    Collection,
    FieldSchema,
    CollectionSchema,
    DataType,
    utility
)

# ===== 1. 连接Milvus =====
print("=== 连接Milvus ===")
connections.connect(alias="default", host="localhost", port="19530")
print("✓ 连接成功")

# ===== 2. 创建Collection =====
print("\n=== 创建Collection ===")

collection_name = "ip_recommendation"
if utility.has_collection(collection_name):
    utility.drop_collection(collection_name)

fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="item_name", dtype=DataType.VARCHAR, max_length=200),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)
]
schema = CollectionSchema(fields=fields, description="IP推荐示例")
collection = Collection(name=collection_name, schema=schema)
print(f"✓ 创建Collection: {collection_name}")

# ===== 3. 插入商品数据 =====
print("\n=== 插入商品数据 ===")

items = [
    "热门电子产品A",
    "热门电子产品B",
    "冷门电子产品C",
    "热门图书D",
    "冷门图书E",
]

# 生成商品Embedding（幅度表示热度）
def generate_item_embedding(item_name, dim=128):
    np.random.seed(hash(item_name) % 2**32)
    base = np.random.randn(dim)
    # 热门商品幅度大，冷门商品幅度小
    if "热门" in item_name:
        return base * 2.0  # 热门商品
    else:
        return base * 0.5  # 冷门商品

embeddings = [generate_item_embedding(item) for item in items]

entities = [items, embeddings]
insert_result = collection.insert(entities)
print(f"✓ 插入 {len(items)} 个商品")

# ===== 4. 创建IP索引 =====
print("\n=== 创建IP索引 ===")

index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "IP",
    "params": {"nlist": 16}
}
collection.create_index(field_name="embedding", index_params=index_params)
print("✓ 创建IP索引")

# ===== 5. 加载Collection =====
collection.load()
print("✓ Collection已加载")

# ===== 6. 用户查询 =====
print("\n=== 用户推荐 ===")

# 用户兴趣（偏好电子产品）
user_interest = generate_item_embedding("电子产品爱好者", dim=128)
query_embedding = [user_interest]

print("用户兴趣: 电子产品")

# 搜索推荐
search_params = {"metric_type": "IP", "params": {"nprobe": 10}}
results = collection.search(
    data=query_embedding,
    anns_field="embedding",
    param=search_params,
    limit=5,
    output_fields=["item_name"]
)

# ===== 7. 显示推荐结果 =====
print("\n=== 推荐结果（按IP值排序）===")
for i, hits in enumerate(results):
    print(f"\n用户 {i+1} 的推荐:")
    for rank, hit in enumerate(hits, 1):
        print(f"  {rank}. [IP值: {hit.distance:.4f}] {hit.entity.get('item_name')}")

# ===== 8. 对比COSINE =====
print("\n=== 对比：如果用COSINE ===")

# 重建索引为COSINE
collection.release()
collection.drop_index()

cosine_index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "COSINE",
    "params": {"nlist": 16}
}
collection.create_index(field_name="embedding", index_params=cosine_index_params)
collection.load()

# 搜索
cosine_results = collection.search(
    data=query_embedding,
    anns_field="embedding",
    param={"metric_type": "COSINE", "params": {"nprobe": 10}},
    limit=5,
    output_fields=["item_name"]
)

print("\n用COSINE的推荐结果:")
for hits in cosine_results:
    for rank, hit in enumerate(hits, 1):
        print(f"  {rank}. [COSINE: {hit.distance:.4f}] {hit.entity.get('item_name')}")

print("\n对比分析:")
print("- IP: 热门商品排名更高（考虑热度）")
print("- COSINE: 只看匹配度（忽略热度）")

# ===== 9. 清理 =====
collection.release()
print("\n✓ 完成")
```

---

## 6. IP的特点

### 6.1 优点

✅ **计算最快**
- 只需乘法和加法
- 无需计算范数或开方

✅ **考虑幅度**
- 向量长度参与计算
- 适合需要"强度"信息的场景

✅ **适合推荐**
- 同时考虑匹配度和热度
- 符合推荐系统的需求

### 6.2 缺点

❌ **受幅度影响**
- 长向量天然得分高
- 可能不适合只关注方向的场景

❌ **值域无界**
- IP值可以是任意实数
- 不如COSINE直观（[-1, 1]）

---

## 7. 何时使用IP

### 7.1 推荐使用的场景

✅ **推荐系统**
```python
# 用户-物品匹配
user_vector = [兴趣1, 兴趣2, ...]
item_vector = [特征1, 特征2, ...]
score = np.dot(user_vector, item_vector)
# 高分 = 匹配 + 热门
```

✅ **归一化向量的性能优化**
```python
# 替代COSINE，性能提升10-20%
normalized_vectors = [v / np.linalg.norm(v) for v in vectors]
# 使用IP索引，效果与COSINE相同
```

✅ **需要考虑向量幅度的场景**
- 幅度表示重要性
- 幅度表示置信度
- 幅度表示热度

### 7.2 不推荐使用的场景

❌ **纯语义搜索**
- 语义搜索只关注方向
- 应该使用COSINE

❌ **未归一化且不关注幅度**
- 如果幅度无意义，用COSINE
- 如果需要距离概念，用L2

---

## 8. IP vs 其他度量

### 8.1 IP vs COSINE

| 维度 | 内积IP | 余弦COSINE |
|------|--------|-----------|
| 计算内容 | x·y | (x·y)/(‖x‖‖y‖) |
| 受幅度影响 | 是 | 否 |
| 归一化向量 | 等价 | 标准选择 |
| 计算速度 | 最快 | 较快 |
| 适用场景 | 推荐系统 | 语义搜索 |

### 8.2 归一化后的等价性

```python
# 归一化向量
x_norm = x / np.linalg.norm(x)
y_norm = y / np.linalg.norm(y)

# IP = COSINE
ip = np.dot(x_norm, y_norm)
cosine = np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))

assert np.isclose(ip, cosine)  # True
```

---

## 9. 性能优化

### 9.1 批量计算

```python
# 批量计算内积
queries = np.random.randn(10, 128)
candidates = np.random.randn(1000, 128)

# 使用矩阵乘法（高效）
scores = np.dot(queries, candidates.T)
# scores.shape = (10, 1000)

# 找到每个查询的Top-K
k = 5
top_k_indices = np.argsort(scores, axis=1)[:, -k:][:, ::-1]
```

### 9.2 归一化优化

```python
# 预先归一化，使用IP代替COSINE
def normalize_batch(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / norms

# 归一化
normalized_vectors = normalize_batch(vectors)

# 使用IP（比COSINE快10-20%）
scores = np.dot(query_norm, normalized_vectors.T)
```

---

## 10. 总结

### 核心要点

1. **IP = 点积 = 方向 × 幅度**
2. **值越大越相似**
3. **计算最快**
4. **适合推荐系统**
5. **归一化后 IP = COSINE**

### 记忆口诀

**IP测投影，考虑方向和强度，推荐系统首选，归一化后等于COSINE。**

---

**下一步：** [05_核心概念_余弦相似度COSINE.md](./05_核心概念_余弦相似度COSINE.md)
