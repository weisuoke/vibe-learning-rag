# 核心概念8：检索评估指标

> 量化检索质量，持续优化的基础

---

## 一句话定义

**检索评估指标是量化检索系统质量的标准体系，主要包括Recall@k（召回率）、MRR（平均倒数排名）、NDCG（归一化折损累积增益）等，是RAG系统优化和监控的核心依据。**

---

## 为什么需要评估指标？

### 问题场景

```python
"""
没有评估指标的问题
"""
# 场景：优化检索器

# 方案A：增加chunk_size到1000
# 方案B：使用混合检索
# 方案C：调整HNSW参数

# 问题：哪个方案更好？
# - 凭感觉？❌
# - 看几个例子？❌
# - 用户反馈？❌（滞后且主观）

# 解决方案：用评估指标量化
# - Recall@5: 0.65 → 0.78 → 0.85
# - MRR: 0.58 → 0.72 → 0.78
# - 数据驱动决策 ✅
```

**参考**: [RAG Evaluation Guide 2025 - Maxim AI](https://www.getmaxim.ai/articles/rag-evaluation-a-complete-guide-for-2025)

---

## 核心指标

### 1. Recall@k (召回率)

**定义**：在Top-K结果中找到了多少个相关文档

```
Recall@k = 找到的相关文档数 / 总相关文档数
```

**Python实现**：

```python
"""
Recall@k计算
"""
def recall_at_k(retrieved_docs: list, relevant_docs: set, k: int) -> float:
    """
    计算Recall@k

    Args:
        retrieved_docs: 检索返回的文档ID列表
        relevant_docs: 真正相关的文档ID集合
        k: 考察前k个结果

    Returns:
        召回率 [0, 1]
    """
    # 取前k个结果
    top_k = set(retrieved_docs[:k])

    # 计算交集
    hits = len(top_k & relevant_docs)

    # 召回率
    if len(relevant_docs) == 0:
        return 0.0

    return hits / len(relevant_docs)


# 示例
retrieved = ["doc1", "doc5", "doc3", "doc8", "doc2"]
relevant = {"doc1", "doc3", "doc7"}

print(f"Recall@3: {recall_at_k(retrieved, relevant, 3):.3f}")
# 输出: 0.667 (找到了doc1和doc3，漏了doc7)

print(f"Recall@5: {recall_at_k(retrieved, relevant, 5):.3f}")
# 输出: 0.667 (前5个结果中仍然只有doc1和doc3)
```

**目标值参考**：

| 场景 | Recall@5 | Recall@10 |
|------|----------|-----------|
| 精准问答 | >0.8 | >0.9 |
| 通用检索 | >0.6 | >0.8 |
| 探索性搜索 | >0.4 | >0.7 |

**参考**: [7 Retrieval Metrics RAG Teams Must Track](https://medium.com/@bhagyarana80/7-retrieval-metrics-rag-teams-must-track-8961c12fff92)

---

### 2. MRR (Mean Reciprocal Rank)

**定义**：第一个相关文档的排名倒数的平均值

```
MRR = 1/N × Σ(1/rank_i)

其中：
- N: 查询数量
- rank_i: 第i个查询的第一个相关文档的排名
```

**Python实现**：

```python
"""
MRR计算
"""
def mean_reciprocal_rank(results: list) -> float:
    """
    计算MRR

    Args:
        results: 每个查询的检索结果
                 [(retrieved_docs, relevant_docs), ...]

    Returns:
        MRR值 [0, 1]
    """
    reciprocal_ranks = []

    for retrieved, relevant in results:
        # 找到第一个相关文档的排名
        for rank, doc_id in enumerate(retrieved, 1):
            if doc_id in relevant:
                reciprocal_ranks.append(1.0 / rank)
                break
        else:
            # 没有找到相关文档
            reciprocal_ranks.append(0.0)

    # 平均倒数排名
    return sum(reciprocal_ranks) / len(reciprocal_ranks)


# 示例
results = [
    (["doc1", "doc5", "doc3"], {"doc1", "doc3"}),  # 第1个就是相关的
    (["doc2", "doc4", "doc6"], {"doc6"}),          # 第3个才是相关的
    (["doc7", "doc8", "doc9"], {"doc10"}),         # 没有相关的
]

mrr = mean_reciprocal_rank(results)
print(f"MRR: {mrr:.3f}")
# 输出: 0.444 (1/1 + 1/3 + 0) / 3 = 0.444
```

**目标值参考**：

| 场景 | MRR目标 |
|------|---------|
| 精准问答 | >0.7 |
| 通用检索 | >0.6 |
| 探索性搜索 | >0.5 |

---

### 3. NDCG@k (Normalized Discounted Cumulative Gain)

**定义**：考虑排名位置和相关性程度的评估指标

```
DCG@k = Σ (rel_i / log2(i+1))

NDCG@k = DCG@k / IDCG@k

其中：
- rel_i: 第i个文档的相关性得分
- IDCG: 理想情况下的DCG（完美排序）
```

**Python实现**：

```python
"""
NDCG@k计算
"""
import numpy as np

def dcg_at_k(relevances: list, k: int) -> float:
    """
    计算DCG@k

    Args:
        relevances: 相关性得分列表（按检索顺序）
        k: 考察前k个结果

    Returns:
        DCG值
    """
    relevances = np.array(relevances[:k])
    if relevances.size == 0:
        return 0.0

    # DCG公式：rel_i / log2(i+1)
    discounts = np.log2(np.arange(2, relevances.size + 2))
    return np.sum(relevances / discounts)


def ndcg_at_k(relevances: list, k: int) -> float:
    """
    计算NDCG@k

    Args:
        relevances: 相关性得分列表（按检索顺序）
        k: 考察前k个结果

    Returns:
        NDCG值 [0, 1]
    """
    # 实际DCG
    dcg = dcg_at_k(relevances, k)

    # 理想DCG（完美排序）
    ideal_relevances = sorted(relevances, reverse=True)
    idcg = dcg_at_k(ideal_relevances, k)

    if idcg == 0:
        return 0.0

    return dcg / idcg


# 示例
# 相关性得分：3=高度相关, 2=相关, 1=略相关, 0=不相关
relevances = [3, 2, 0, 1, 2]  # 检索结果的相关性

print(f"NDCG@3: {ndcg_at_k(relevances, 3):.3f}")
# 输出: 0.918

print(f"NDCG@5: {ndcg_at_k(relevances, 5):.3f}")
# 输出: 0.892
```

**目标值参考**：

| 场景 | NDCG@10目标 |
|------|-------------|
| 精准问答 | >0.8 |
| 通用检索 | >0.7 |
| 探索性搜索 | >0.6 |

**参考**: [RAG Evaluation Guide 2025 - Maxim AI](https://www.getmaxim.ai/articles/rag-evaluation-a-complete-guide-for-2025)

---

## 评估流程

### 1. 构建测试集

```python
"""
构建测试集
"""
# 测试集格式
test_set = [
    {
        "query": "Python异步编程",
        "relevant_docs": ["doc1", "doc5", "doc12"],  # 相关文档ID
        "relevance_scores": {  # 可选：多级相关性
            "doc1": 3,  # 高度相关
            "doc5": 2,  # 相关
            "doc12": 1  # 略相关
        }
    },
    {
        "query": "FastAPI性能优化",
        "relevant_docs": ["doc3", "doc8"],
        "relevance_scores": {
            "doc3": 3,
            "doc8": 2
        }
    },
    # ... 100-500个查询
]

# 测试集大小建议
test_set_sizes = {
    "快速验证": 50,
    "标准评估": 100-200,
    "全面评估": 500-1000
}
```

### 2. 执行评估

```python
"""
完整评估流程
"""
def evaluate_retriever(retriever, test_set, k=10):
    """
    评估检索器

    Args:
        retriever: 检索器实例
        test_set: 测试集
        k: Top-K

    Returns:
        评估结果字典
    """
    recalls = []
    mrrs = []
    ndcgs = []

    for item in test_set:
        query = item["query"]
        relevant_docs = set(item["relevant_docs"])

        # 检索
        results = retriever.get_relevant_documents(query)
        retrieved_ids = [doc.metadata['id'] for doc in results[:k]]

        # Recall@k
        recall = recall_at_k(retrieved_ids, relevant_docs, k)
        recalls.append(recall)

        # MRR
        for rank, doc_id in enumerate(retrieved_ids, 1):
            if doc_id in relevant_docs:
                mrrs.append(1.0 / rank)
                break
        else:
            mrrs.append(0.0)

        # NDCG@k（如果有多级相关性）
        if "relevance_scores" in item:
            relevances = [
                item["relevance_scores"].get(doc_id, 0)
                for doc_id in retrieved_ids
            ]
            ndcg = ndcg_at_k(relevances, k)
            ndcgs.append(ndcg)

    # 汇总结果
    results = {
        f"Recall@{k}": np.mean(recalls),
        "MRR": np.mean(mrrs),
    }

    if ndcgs:
        results[f"NDCG@{k}"] = np.mean(ndcgs)

    return results


# 使用
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(documents, embeddings)
retriever = vectorstore.as_retriever()

# 评估
metrics = evaluate_retriever(retriever, test_set, k=10)

print("=== 评估结果 ===")
for metric, value in metrics.items():
    print(f"{metric}: {value:.3f}")

# 输出示例:
# Recall@10: 0.782
# MRR: 0.654
# NDCG@10: 0.723
```

---

## 对比实验

### 实验1：Dense vs Sparse vs Hybrid

```python
"""
对比不同检索方式
"""
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# 1. Dense检索
dense_retriever = vectorstore.as_retriever()
dense_metrics = evaluate_retriever(dense_retriever, test_set)

# 2. Sparse检索
sparse_retriever = BM25Retriever.from_documents(documents)
sparse_metrics = evaluate_retriever(sparse_retriever, test_set)

# 3. Hybrid检索
hybrid_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, sparse_retriever],
    weights=[0.6, 0.4]
)
hybrid_metrics = evaluate_retriever(hybrid_retriever, test_set)

# 对比
print("=== 检索方式对比 ===")
print(f"Dense:  Recall@10={dense_metrics['Recall@10']:.3f}, MRR={dense_metrics['MRR']:.3f}")
print(f"Sparse: Recall@10={sparse_metrics['Recall@10']:.3f}, MRR={sparse_metrics['MRR']:.3f}")
print(f"Hybrid: Recall@10={hybrid_metrics['Recall@10']:.3f}, MRR={hybrid_metrics['MRR']:.3f}")

# 输出示例:
# Dense:  Recall@10=0.752, MRR=0.634
# Sparse: Recall@10=0.698, MRR=0.612
# Hybrid: Recall@10=0.852, MRR=0.723  ← 最优
```

### 实验2：参数调优

```python
"""
HNSW参数调优实验
"""
ef_search_values = [50, 100, 200, 500]
results = []

for ef_search in ef_search_values:
    # 设置参数
    vectorstore.collection.metadata["hnsw:search_ef"] = ef_search

    # 评估
    metrics = evaluate_retriever(vectorstore.as_retriever(), test_set)

    results.append({
        "efSearch": ef_search,
        **metrics
    })

    print(f"efSearch={ef_search}: Recall@10={metrics['Recall@10']:.3f}")

# 输出:
# efSearch=50:  Recall@10=0.745
# efSearch=100: Recall@10=0.782
# efSearch=200: Recall@10=0.798
# efSearch=500: Recall@10=0.805
```

---

## 在RAG中的应用

### 应用1：持续监控

```python
"""
生产环境监控
"""
import time
from datetime import datetime

class RetrievalMonitor:
    def __init__(self, retriever, test_set, alert_threshold=0.7):
        self.retriever = retriever
        self.test_set = test_set
        self.alert_threshold = alert_threshold
        self.history = []

    def monitor(self):
        """执行监控"""
        # 评估
        metrics = evaluate_retriever(self.retriever, self.test_set)

        # 记录
        self.history.append({
            "timestamp": datetime.now(),
            **metrics
        })

        # 告警
        if metrics["Recall@10"] < self.alert_threshold:
            self.alert(f"⚠️ Recall@10下降到{metrics['Recall@10']:.3f}")

        return metrics

    def alert(self, message):
        """发送告警"""
        print(f"[{datetime.now()}] {message}")
        # 实际应用中可以发送邮件、Slack通知等

# 使用
monitor = RetrievalMonitor(retriever, test_set, alert_threshold=0.75)

# 定期监控（如每小时）
while True:
    metrics = monitor.monitor()
    print(f"Recall@10: {metrics['Recall@10']:.3f}")
    time.sleep(3600)  # 1小时
```

### 应用2：A/B测试

```python
"""
A/B测试框架
"""
def ab_test(retriever_a, retriever_b, test_set, name_a="A", name_b="B"):
    """
    对比两个检索器

    Args:
        retriever_a: 检索器A
        retriever_b: 检索器B
        test_set: 测试集
        name_a: 检索器A名称
        name_b: 检索器B名称
    """
    # 评估A
    metrics_a = evaluate_retriever(retriever_a, test_set)

    # 评估B
    metrics_b = evaluate_retriever(retriever_b, test_set)

    # 对比
    print(f"=== A/B测试结果 ===")
    print(f"\n{name_a}:")
    for metric, value in metrics_a.items():
        print(f"  {metric}: {value:.3f}")

    print(f"\n{name_b}:")
    for metric, value in metrics_b.items():
        print(f"  {metric}: {value:.3f}")

    print(f"\n提升:")
    for metric in metrics_a.keys():
        improvement = (metrics_b[metric] - metrics_a[metric]) / metrics_a[metric] * 100
        print(f"  {metric}: {improvement:+.1f}%")

    # 统计显著性检验（简化版）
    if metrics_b["Recall@10"] > metrics_a["Recall@10"] * 1.05:
        print(f"\n✅ {name_b}显著优于{name_a}")
    else:
        print(f"\n⚠️ 差异不显著")

# 使用
ab_test(
    retriever_a=dense_retriever,
    retriever_b=hybrid_retriever,
    test_set=test_set,
    name_a="Dense Only",
    name_b="Hybrid (0.6/0.4)"
)
```

### 应用3：基准测试

```python
"""
使用公开基准数据集
"""
# MS MARCO数据集示例
from datasets import load_dataset

# 加载MS MARCO
dataset = load_dataset("ms_marco", "v2.1")

# 转换为测试集格式
test_set = []
for item in dataset["validation"][:100]:
    test_set.append({
        "query": item["query"],
        "relevant_docs": item["passages"]["is_selected"]
    })

# 评估
metrics = evaluate_retriever(retriever, test_set)

print("=== MS MARCO基准测试 ===")
print(f"Recall@10: {metrics['Recall@10']:.3f}")
print(f"MRR: {metrics['MRR']:.3f}")
```

---

## 2025-2026最佳实践

### 1. 建立评估基线

```python
"""
建立baseline
"""
# 1. 构建测试集（100-200个查询）
test_set = build_test_set(documents, n_queries=150)

# 2. 评估baseline（Dense检索）
baseline_retriever = vectorstore.as_retriever()
baseline_metrics = evaluate_retriever(baseline_retriever, test_set)

print("=== Baseline ===")
print(f"Recall@10: {baseline_metrics['Recall@10']:.3f}")
print(f"MRR: {baseline_metrics['MRR']:.3f}")

# 3. 设定优化目标
target_metrics = {
    "Recall@10": baseline_metrics["Recall@10"] * 1.2,  # 提升20%
    "MRR": baseline_metrics["MRR"] * 1.15              # 提升15%
}

print("\n=== 优化目标 ===")
for metric, target in target_metrics.items():
    print(f"{metric}: {target:.3f}")
```

### 2. 迭代优化

```python
"""
迭代优化流程
"""
# 迭代1：混合检索
hybrid_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, sparse_retriever],
    weights=[0.6, 0.4]
)
metrics_v1 = evaluate_retriever(hybrid_retriever, test_set)
print(f"V1 (Hybrid): Recall@10={metrics_v1['Recall@10']:.3f}")

# 迭代2：调整权重
hybrid_retriever.weights = [0.7, 0.3]
metrics_v2 = evaluate_retriever(hybrid_retriever, test_set)
print(f"V2 (0.7/0.3): Recall@10={metrics_v2['Recall@10']:.3f}")

# 迭代3：增加MMR
# ... 继续优化

# 选择最优版本
best_version = max(
    [("V1", metrics_v1), ("V2", metrics_v2)],
    key=lambda x: x[1]["Recall@10"]
)
print(f"\n最优版本: {best_version[0]}")
```

### 3. 自动化评估

```python
"""
CI/CD集成
"""
def run_evaluation_pipeline():
    """
    自动化评估流程
    """
    # 1. 加载测试集
    test_set = load_test_set("test_set.json")

    # 2. 评估当前版本
    metrics = evaluate_retriever(retriever, test_set)

    # 3. 与baseline对比
    baseline = load_baseline_metrics("baseline.json")

    # 4. 检查是否退化
    if metrics["Recall@10"] < baseline["Recall@10"] * 0.95:
        raise ValueError("⚠️ 性能退化！Recall@10下降超过5%")

    # 5. 保存结果
    save_metrics(metrics, "current_metrics.json")

    print("✅ 评估通过")
    return metrics

# 在CI/CD中运行
if __name__ == "__main__":
    run_evaluation_pipeline()
```

---

## 常见问题

### Q1: 测试集需要多大？

**A**:
- 快速验证：50个查询
- 标准评估：100-200个查询
- 全面评估：500-1000个查询
- 建议：从100个开始，逐步扩充

### Q2: 如何标注测试集？

**A**:
1. 人工标注（最准确但成本高）
2. 使用LLM辅助标注
3. 利用用户行为数据（点击、停留时间）
4. 混合方法：LLM初标 + 人工审核

### Q3: Recall和Precision哪个更重要？

**A**:
- RAG系统中Recall更重要（不能漏掉相关文档）
- Precision可以通过ReRank优化
- 检索阶段优先保证Recall，生成阶段优化Precision

### Q4: 如何设定目标值？

**A**:
1. 参考行业标准（见上文目标值表格）
2. 基于baseline设定提升目标（如提升20%）
3. 根据业务需求调整（精准问答要求更高）
4. 持续迭代优化

---

## 总结

**检索评估指标的核心价值**：
1. ✅ 量化质量：将主观的"好坏"转化为客观数值
2. ✅ 指导优化：数据驱动的优化决策
3. ✅ 持续监控：及时发现性能退化

**核心要点**：
- Recall@k：最重要的指标（不能漏）
- MRR：关注第一个相关文档的排名
- NDCG@k：考虑多级相关性
- 测试集：100-200个查询是标准

**2025-2026标准实践**：
- 建立baseline和测试集
- 每次优化都要评估
- 生产环境持续监控
- CI/CD集成自动化评估

**完成Phase 3**：8个核心概念全部生成完毕！

**下一步**：进入Phase 4，生成7个实战代码文件。
