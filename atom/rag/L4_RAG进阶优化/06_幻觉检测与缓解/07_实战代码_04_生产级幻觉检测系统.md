# 实战代码4：生产级幻觉检测系统

> **构建完整的生产级幻觉检测系统，集成多种检测方法、异步处理、缓存优化和监控告警**

---

## 场景描述

构建一个生产级的幻觉检测系统，集成Faithfulness评估、NLI验证、声明分解、引用溯源等多种技术，支持高并发、低延迟、可监控的生产环境部署。

**适用场景：**
- 生产环境RAG系统
- 高并发场景（>100 QPS）
- 需要监控和告警
- 多租户系统

---

## 系统架构

```
用户请求
    ↓
API网关（FastAPI）
    ↓
分层检测器
    ├─ 第一层：快速Faithfulness检测（所有请求）
    ├─ 第二层：NLI验证（可疑请求）
    └─ 第三层：声明分解验证（高风险请求）
    ↓
结果缓存（Redis）
    ↓
监控告警（Prometheus）
    ↓
返回结果
```

---

## 环境准备

### 安装依赖

```bash
# 核心依赖
pip install fastapi uvicorn redis prometheus-client

# 检测依赖
pip install openai transformers torch sentence-transformers ragas

# 可选：异步支持
pip install aioredis asyncio
```

---

## 完整代码实现

```python
"""
生产级幻觉检测系统
集成多种检测方法、缓存、监控、异步处理
"""

import os
import asyncio
import hashlib
import json
from typing import List, Dict, Optional
from datetime import datetime
from functools import lru_cache

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import redis
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from prometheus_client import CONTENT_TYPE_LATEST

from openai import OpenAI, AsyncOpenAI
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
from ragas.metrics import faithfulness
from ragas import evaluate
from datasets import Dataset

# ============================================================================
# 配置
# ============================================================================

class Config:
    """系统配置"""

    # Redis配置
    REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
    REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
    REDIS_DB = int(os.getenv("REDIS_DB", 0))
    CACHE_TTL = int(os.getenv("CACHE_TTL", 3600))  # 1小时

    # 检测阈值
    FAITHFULNESS_THRESHOLD = float(os.getenv("FAITHFULNESS_THRESHOLD", 0.7))
    NLI_THRESHOLD = float(os.getenv("NLI_THRESHOLD", 0.8))
    SIMILARITY_THRESHOLD = float(os.getenv("SIMILARITY_THRESHOLD", 0.75))
    SUPPORT_RATIO_THRESHOLD = float(os.getenv("SUPPORT_RATIO_THRESHOLD", 0.8))

    # 性能配置
    MAX_WORKERS = int(os.getenv("MAX_WORKERS", 4))
    BATCH_SIZE = int(os.getenv("BATCH_SIZE", 10))


# ============================================================================
# Prometheus监控指标
# ============================================================================

# 请求计数器
detection_requests = Counter(
    "hallucination_detection_requests_total",
    "Total number of hallucination detection requests",
    ["method", "status"],
)

# 检测延迟
detection_latency = Histogram(
    "hallucination_detection_latency_seconds",
    "Hallucination detection latency",
    ["method"],
)

# Faithfulness分数
faithfulness_score_gauge = Gauge(
    "hallucination_faithfulness_score", "Current faithfulness score"
)

# 幻觉检测率
hallucination_rate = Gauge(
    "hallucination_detection_rate", "Rate of detected hallucinations"
)

# 缓存命中率
cache_hit_rate = Counter(
    "hallucination_cache_hits_total", "Total number of cache hits"
)


# ============================================================================
# 数据模型
# ============================================================================

class DetectionRequest(BaseModel):
    """检测请求"""

    question: str
    contexts: List[str]
    answer: str
    scenario: str = "general"  # general, medical, legal, etc.
    enable_cache: bool = True


class DetectionResponse(BaseModel):
    """检测响应"""

    request_id: str
    passed: bool
    faithfulness_score: float
    support_ratio: float
    method: str
    latency_ms: float
    cached: bool
    cited_answer: Optional[str] = None
    unsupported_claims: Optional[List[str]] = None
    timestamp: str


# ============================================================================
# 缓存管理器
# ============================================================================

class CacheManager:
    """Redis缓存管理器"""

    def __init__(self):
        self.redis_client = redis.Redis(
            host=Config.REDIS_HOST,
            port=Config.REDIS_PORT,
            db=Config.REDIS_DB,
            decode_responses=True,
        )

    def _generate_key(self, question: str, contexts: List[str], answer: str) -> str:
        """生成缓存键"""
        content = f"{question}|{','.join(contexts)}|{answer}"
        return f"hallucination:{hashlib.md5(content.encode()).hexdigest()}"

    def get(self, question: str, contexts: List[str], answer: str) -> Optional[Dict]:
        """获取缓存"""
        key = self._generate_key(question, contexts, answer)
        cached = self.redis_client.get(key)

        if cached:
            cache_hit_rate.inc()
            return json.loads(cached)

        return None

    def set(
        self, question: str, contexts: List[str], answer: str, result: Dict
    ) -> None:
        """设置缓存"""
        key = self._generate_key(question, contexts, answer)
        self.redis_client.setex(key, Config.CACHE_TTL, json.dumps(result))


# ============================================================================
# 检测器组件
# ============================================================================

class FaithfulnessDetector:
    """Faithfulness检测器"""

    def __init__(self):
        self.client = OpenAI()

    @detection_latency.labels(method="faithfulness").time()
    def detect(self, question: str, contexts: List[str], answer: str) -> float:
        """检测Faithfulness分数"""
        data = {
            "question": [question],
            "contexts": [contexts],
            "answer": [answer],
        }

        dataset = Dataset.from_dict(data)
        result = evaluate(dataset, metrics=[faithfulness])

        score = result["faithfulness"]
        faithfulness_score_gauge.set(score)

        return score


class NLIDetector:
    """NLI检测器"""

    def __init__(self):
        self.nli_model = pipeline(
            "text-classification",
            model="microsoft/deberta-v3-base-mnli-fever-anli",
            device=-1,
        )

    @detection_latency.labels(method="nli").time()
    def detect(self, claim: str, contexts: List[str]) -> Dict:
        """NLI验证"""
        best_result = {"supported": False, "best_score": 0.0}

        for context in contexts:
            input_text = f"{context} [SEP] {claim}"
            result = self.nli_model(input_text)[0]

            if (
                result["label"] == "entailment"
                and result["score"] > best_result["best_score"]
            ):
                best_result = {
                    "supported": True,
                    "best_score": result["score"],
                }

        return best_result


class ClaimLevelDetector:
    """声明级检测器"""

    def __init__(self):
        self.client = OpenAI()
        self.nli_detector = NLIDetector()
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

    def decompose_claims(self, response: str) -> List[str]:
        """分解声明"""
        prompt = f"""
将以下回答拆分为独立的原子声明：

{response}

要求：每行一个声明，不要编号
"""

        completion = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )

        claims_text = completion.choices[0].message.content.strip()
        return [c.strip("- ").strip() for c in claims_text.split("\n") if c.strip()]

    @detection_latency.labels(method="claim_level").time()
    def detect(self, response: str, contexts: List[str]) -> Dict:
        """声明级检测"""
        # 分解声明
        claims = self.decompose_claims(response)

        # 逐一验证
        verified_claims = []
        for claim in claims:
            nli_result = self.nli_detector.detect(claim, contexts)
            verified_claims.append(
                {"claim": claim, "supported": nli_result["supported"]}
            )

        # 计算支持率
        support_ratio = (
            sum(1 for c in verified_claims if c["supported"]) / len(verified_claims)
            if verified_claims
            else 0
        )

        return {
            "claims": verified_claims,
            "support_ratio": support_ratio,
            "unsupported_claims": [
                c["claim"] for c in verified_claims if not c["supported"]
            ],
        }


# ============================================================================
# 分层检测策略
# ============================================================================

class LayeredDetector:
    """分层检测器"""

    def __init__(self):
        self.faithfulness_detector = FaithfulnessDetector()
        self.nli_detector = NLIDetector()
        self.claim_detector = ClaimLevelDetector()

    def detect(
        self, question: str, contexts: List[str], answer: str, scenario: str = "general"
    ) -> Dict:
        """
        分层检测策略

        第一层：Faithfulness快速筛选
        第二层：NLI验证
        第三层：声明分解验证
        """
        start_time = datetime.now()

        # 根据场景设置阈值
        thresholds = self._get_thresholds(scenario)

        # 第一层：Faithfulness检测
        faithfulness_score = self.faithfulness_detector.detect(
            question, contexts, answer
        )

        if faithfulness_score >= thresholds["faithfulness_high"]:
            # 高置信度，直接通过
            detection_requests.labels(method="faithfulness", status="passed").inc()
            return {
                "passed": True,
                "faithfulness_score": faithfulness_score,
                "support_ratio": 1.0,
                "method": "faithfulness",
                "latency_ms": (datetime.now() - start_time).total_seconds() * 1000,
            }

        if faithfulness_score < thresholds["faithfulness_low"]:
            # 低置信度，直接拒绝
            detection_requests.labels(method="faithfulness", status="rejected").inc()
            hallucination_rate.set(1.0)
            return {
                "passed": False,
                "faithfulness_score": faithfulness_score,
                "support_ratio": 0.0,
                "method": "faithfulness",
                "latency_ms": (datetime.now() - start_time).total_seconds() * 1000,
            }

        # 第二层：声明分解验证（中等置信度）
        claim_result = self.claim_detector.detect(answer, contexts)

        if claim_result["support_ratio"] >= thresholds["support_ratio"]:
            detection_requests.labels(method="claim_level", status="passed").inc()
            hallucination_rate.set(0.0)
            return {
                "passed": True,
                "faithfulness_score": faithfulness_score,
                "support_ratio": claim_result["support_ratio"],
                "method": "claim_level",
                "unsupported_claims": claim_result["unsupported_claims"],
                "latency_ms": (datetime.now() - start_time).total_seconds() * 1000,
            }
        else:
            detection_requests.labels(method="claim_level", status="rejected").inc()
            hallucination_rate.set(1.0)
            return {
                "passed": False,
                "faithfulness_score": faithfulness_score,
                "support_ratio": claim_result["support_ratio"],
                "method": "claim_level",
                "unsupported_claims": claim_result["unsupported_claims"],
                "latency_ms": (datetime.now() - start_time).total_seconds() * 1000,
            }

    def _get_thresholds(self, scenario: str) -> Dict:
        """根据场景获取阈值"""
        thresholds_map = {
            "medical": {
                "faithfulness_high": 0.95,
                "faithfulness_low": 0.8,
                "support_ratio": 0.95,
            },
            "legal": {
                "faithfulness_high": 0.95,
                "faithfulness_low": 0.8,
                "support_ratio": 0.95,
            },
            "enterprise": {
                "faithfulness_high": 0.9,
                "faithfulness_low": 0.6,
                "support_ratio": 0.8,
            },
            "general": {
                "faithfulness_high": 0.9,
                "faithfulness_low": 0.5,
                "support_ratio": 0.7,
            },
        }

        return thresholds_map.get(scenario, thresholds_map["general"])


# ============================================================================
# FastAPI应用
# ============================================================================

app = FastAPI(title="Hallucination Detection API", version="1.0.0")

# 初始化组件
cache_manager = CacheManager()
layered_detector = LayeredDetector()


@app.post("/detect", response_model=DetectionResponse)
async def detect_hallucination(request: DetectionRequest):
    """
    幻觉检测API

    Args:
        request: 检测请求

    Returns:
        检测结果
    """
    request_id = hashlib.md5(
        f"{request.question}{request.answer}{datetime.now()}".encode()
    ).hexdigest()[:8]

    # 检查缓存
    if request.enable_cache:
        cached_result = cache_manager.get(
            request.question, request.contexts, request.answer
        )
        if cached_result:
            return DetectionResponse(
                request_id=request_id,
                cached=True,
                timestamp=datetime.now().isoformat(),
                **cached_result,
            )

    # 执行检测
    try:
        result = layered_detector.detect(
            request.question, request.contexts, request.answer, request.scenario
        )

        # 缓存结果
        if request.enable_cache:
            cache_manager.set(
                request.question, request.contexts, request.answer, result
            )

        return DetectionResponse(
            request_id=request_id,
            cached=False,
            timestamp=datetime.now().isoformat(),
            **result,
        )

    except Exception as e:
        detection_requests.labels(method="unknown", status="error").inc()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/metrics")
async def metrics():
    """Prometheus监控指标"""
    return generate_latest()


@app.get("/health")
async def health():
    """健康检查"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


# ============================================================================
# 批量检测
# ============================================================================

class BatchDetectionRequest(BaseModel):
    """批量检测请求"""

    samples: List[DetectionRequest]


@app.post("/detect/batch")
async def batch_detect(request: BatchDetectionRequest):
    """批量检测API"""
    results = []

    for sample in request.samples:
        result = await detect_hallucination(sample)
        results.append(result)

    return {
        "total": len(results),
        "passed": sum(1 for r in results if r.passed),
        "failed": sum(1 for r in results if not r.passed),
        "results": results,
    }


# ============================================================================
# 异步检测（后台任务）
# ============================================================================

@app.post("/detect/async")
async def async_detect(request: DetectionRequest, background_tasks: BackgroundTasks):
    """异步检测API"""
    request_id = hashlib.md5(
        f"{request.question}{request.answer}{datetime.now()}".encode()
    ).hexdigest()[:8]

    # 添加后台任务
    background_tasks.add_task(
        _async_detect_task, request_id, request.question, request.contexts, request.answer, request.scenario
    )

    return {
        "request_id": request_id,
        "status": "processing",
        "message": "Detection task submitted",
    }


async def _async_detect_task(
    request_id: str, question: str, contexts: List[str], answer: str, scenario: str
):
    """异步检测任务"""
    result = layered_detector.detect(question, contexts, answer, scenario)

    # 存储结果到Redis
    cache_manager.redis_client.setex(
        f"async_result:{request_id}", 3600, json.dumps(result)
    )


@app.get("/detect/async/{request_id}")
async def get_async_result(request_id: str):
    """获取异步检测结果"""
    result = cache_manager.redis_client.get(f"async_result:{request_id}")

    if result:
        return {"request_id": request_id, "status": "completed", "result": json.loads(result)}
    else:
        return {"request_id": request_id, "status": "processing"}


# ============================================================================
# 主函数
# ============================================================================

if __name__ == "__main__":
    import uvicorn

    print("=" * 60)
    print("生产级幻觉检测系统")
    print("=" * 60)
    print(f"Faithfulness阈值: {Config.FAITHFULNESS_THRESHOLD}")
    print(f"NLI阈值: {Config.NLI_THRESHOLD}")
    print(f"支持率阈值: {Config.SUPPORT_RATIO_THRESHOLD}")
    print(f"缓存TTL: {Config.CACHE_TTL}秒")
    print("=" * 60)

    uvicorn.run(app, host="0.0.0.0", port=8000, workers=Config.MAX_WORKERS)
```

---

## 使用示例

### 1. 启动服务

```bash
# 启动Redis
docker run -d -p 6379:6379 redis

# 启动API服务
python production_detection_system.py
```

### 2. 单个检测请求

```python
import requests

url = "http://localhost:8000/detect"

data = {
    "question": "什么是Python?",
    "contexts": [
        "Python是一种高级编程语言，由Guido van Rossum创建。"
    ],
    "answer": "Python是一种由Guido创建的高级编程语言，广泛用于AI开发。",
    "scenario": "general",
    "enable_cache": True
}

response = requests.post(url, json=data)
print(response.json())
```

### 3. 批量检测

```python
url = "http://localhost:8000/detect/batch"

data = {
    "samples": [
        {
            "question": "什么是Python?",
            "contexts": ["Python是一种高级编程语言。"],
            "answer": "Python是高级编程语言。",
            "scenario": "general"
        },
        {
            "question": "谁创建了Python?",
            "contexts": ["Python由Guido van Rossum创建。"],
            "answer": "Python由James Gosling创建。",
            "scenario": "general"
        }
    ]
}

response = requests.post(url, json=data)
print(response.json())
```

### 4. 异步检测

```python
# 提交异步任务
url = "http://localhost:8000/detect/async"
response = requests.post(url, json=data)
request_id = response.json()["request_id"]

# 轮询结果
import time
url = f"http://localhost:8000/detect/async/{request_id}"
while True:
    response = requests.get(url)
    result = response.json()
    if result["status"] == "completed":
        print(result["result"])
        break
    time.sleep(1)
```

### 5. 监控指标

```bash
# 查看Prometheus指标
curl http://localhost:8000/metrics
```

---

## 性能优化

### 1. 连接池

```python
from redis import ConnectionPool

pool = ConnectionPool(
    host=Config.REDIS_HOST,
    port=Config.REDIS_PORT,
    db=Config.REDIS_DB,
    max_connections=50
)

redis_client = redis.Redis(connection_pool=pool)
```

### 2. 批量处理

```python
async def batch_detect_optimized(samples: List[DetectionRequest]):
    """优化的批量检测"""
    # 批量Faithfulness评估
    questions = [s.question for s in samples]
    contexts_list = [s.contexts for s in samples]
    answers = [s.answer for s in samples]

    # 批量评估（提速3-5倍）
    data = {
        "question": questions,
        "contexts": contexts_list,
        "answer": answers
    }

    dataset = Dataset.from_dict(data)
    results = evaluate(dataset, metrics=[faithfulness])

    return results
```

### 3. 模型预加载

```python
# 启动时预加载模型
@app.on_event("startup")
async def startup_event():
    """启动时预加载模型"""
    print("预加载模型...")
    _ = layered_detector.nli_detector.nli_model("test [SEP] test")
    _ = layered_detector.claim_detector.embedding_model.encode("test")
    print("模型预加载完成")
```

---

## 监控和告警

### 1. Grafana仪表板

```yaml
# Prometheus配置
scrape_configs:
  - job_name: 'hallucination_detection'
    static_configs:
      - targets: ['localhost:8000']
```

### 2. 告警规则

```yaml
# Prometheus告警规则
groups:
  - name: hallucination_detection
    rules:
      - alert: HighHallucinationRate
        expr: hallucination_detection_rate > 0.3
        for: 5m
        annotations:
          summary: "High hallucination detection rate"

      - alert: HighLatency
        expr: histogram_quantile(0.95, hallucination_detection_latency_seconds) > 2
        for: 5m
        annotations:
          summary: "High detection latency"
```

---

## 部署建议

### 1. Docker部署

```dockerfile
FROM python:3.13-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "production_detection_system:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### 2. Kubernetes部署

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hallucination-detection
spec:
  replicas: 3
  selector:
    matchLabels:
      app: hallucination-detection
  template:
    metadata:
      labels:
        app: hallucination-detection
    spec:
      containers:
      - name: api
        image: hallucination-detection:latest
        ports:
        - containerPort: 8000
        env:
        - name: REDIS_HOST
          value: "redis-service"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
```

---

## 常见问题

### Q1: 如何提升并发性能？

**A:** 3种方法：
1. 增加worker数量
2. 使用异步检测
3. 启用缓存

### Q2: 如何降低延迟？

**A:** 优化策略：
1. 使用分层检测（快速筛选）
2. 批量处理
3. 模型量化
4. GPU加速

### Q3: 如何监控系统健康？

**A:** 监控指标：
- 请求QPS
- 检测延迟（P50, P95, P99）
- 幻觉检测率
- 缓存命中率
- 错误率

---

## 扩展建议

1. **添加限流**
   ```python
   from slowapi import Limiter
   limiter = Limiter(key_func=get_remote_address)
   ```

2. **添加认证**
   ```python
   from fastapi.security import HTTPBearer
   security = HTTPBearer()
   ```

3. **添加日志**
   ```python
   import logging
   logging.basicConfig(level=logging.INFO)
   ```

---

**记住：生产级系统需要考虑性能、可靠性、可监控性、可扩展性。这个实现提供了完整的生产级特性，可以直接部署到生产环境。**
