# 实战代码 02: 批处理场景

> 批量文档分类与成本追踪实战

---

## 代码概述

本示例演示：
1. 批量文档分类
2. 成本追踪和优化
3. 性能对比（顺序 vs 批量）
4. langasync 集成（可选）
5. 并发控制

---

## 完整代码

```python
"""
批量处理场景实战示例
演示：批量文档分类与成本追踪

Requirements:
- Python 3.13+
- langchain-core
- langchain-openai
- python-dotenv
"""

import os
import time
from typing import List, Dict, Any
from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableConfig
from langchain_core.callbacks import BaseCallbackHandler

# 加载环境变量
load_dotenv()

# ===== 1. 成本追踪回调 =====
print("=" * 60)
print("1. 成本追踪系统")
print("=" * 60)

class CostTracker(BaseCallbackHandler):
    """
    成本追踪回调处理器

    追踪：
    - Token 使用量
    - API 调用次数
    - 预估成本
    """

    def __init__(self):
        self.reset()

    def reset(self):
        """重置统计"""
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0
        self.api_calls = 0

    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLM 开始时计数"""
        self.api_calls += 1

    def on_llm_end(self, response, **kwargs):
        """LLM 结束时统计 token"""
        if hasattr(response, 'llm_output') and response.llm_output:
            token_usage = response.llm_output.get('token_usage', {})
            self.prompt_tokens += token_usage.get('prompt_tokens', 0)
            self.completion_tokens += token_usage.get('completion_tokens', 0)
            self.total_tokens += token_usage.get('total_tokens', 0)

    def get_cost(self, model="gpt-4o-mini"):
        """
        计算成本（美元）

        gpt-4o-mini 价格（2026年）:
        - Input: $0.150 / 1M tokens
        - Output: $0.600 / 1M tokens
        """
        prompt_cost = self.prompt_tokens * 0.150 / 1_000_000
        completion_cost = self.completion_tokens * 0.600 / 1_000_000
        return prompt_cost + completion_cost

    def report(self):
        """生成报告"""
        print(f"\n📊 Token 使用统计:")
        print(f"  - API 调用次数: {self.api_calls}")
        print(f"  - Prompt tokens: {self.prompt_tokens:,}")
        print(f"  - Completion tokens: {self.completion_tokens:,}")
        print(f"  - Total tokens: {self.total_tokens:,}")
        print(f"💰 预估成本: ${self.get_cost():.6f}")

# 测试成本追踪器
print("\n成本追踪器已初始化")

# ===== 2. 文档分类链 =====
print("\n" + "=" * 60)
print("2. 文档分类链")
print("=" * 60)

# 检查 API 密钥
if not os.getenv("OPENAI_API_KEY"):
    print("⚠️  警告: 未设置 OPENAI_API_KEY")
    print("请在 .env 文件中设置 OPENAI_API_KEY")
    print("示例将使用模拟数据")
    USE_LLM = False
else:
    USE_LLM = True

    # 定义分类链
    prompt = ChatPromptTemplate.from_template(
        """将以下文本分类为以下类别之一：技术、商业、娱乐、体育、其他

文本: {text}

分类:"""
    )

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0  # 确保结果一致
    )

    parser = StrOutputParser()

    classification_chain = prompt | llm | parser

    print("✓ 分类链已创建")

# ===== 3. 测试数据 =====
print("\n" + "=" * 60)
print("3. 准备测试数据")
print("=" * 60)

# 测试文档
documents = [
    "Python 3.13 发布了新的性能优化特性",
    "苹果公司发布了最新的财报，营收超预期",
    "电影《流浪地球3》票房突破10亿",
    "NBA 总决赛第七场比赛精彩纷呈",
    "机器学习算法在医疗诊断中的应用",
    "特斯拉发布新款电动汽车",
    "音乐节吸引了数万名观众",
    "世界杯足球赛进入淘汰赛阶段",
    "人工智能芯片市场竞争激烈",
    "好莱坞明星出席慈善晚会",
]

print(f"准备了 {len(documents)} 个测试文档")
for i, doc in enumerate(documents, 1):
    print(f"  {i}. {doc[:30]}...")

# ===== 4. 方式 1: 顺序执行（invoke 循环）=====
if USE_LLM:
    print("\n" + "=" * 60)
    print("4. 方式 1: 顺序执行（invoke 循环）")
    print("=" * 60)

    # 创建成本追踪器
    tracker_sequential = CostTracker()
    config_sequential = RunnableConfig(
        callbacks=[tracker_sequential],
        tags=["sequential", "classification"]
    )

    print("\n开始顺序执行...")
    start_time = time.time()

    results_sequential = []
    for i, doc in enumerate(documents, 1):
        print(f"  处理 {i}/{len(documents)}: {doc[:30]}...", end=" ")
        result = classification_chain.invoke({"text": doc}, config=config_sequential)
        results_sequential.append(result.strip())
        print(f"→ {result.strip()}")

    elapsed_sequential = time.time() - start_time

    print(f"\n⏱️  总耗时: {elapsed_sequential:.2f} 秒")
    print(f"📈 平均每个: {elapsed_sequential / len(documents):.2f} 秒")

    # 成本报告
    tracker_sequential.report()

# ===== 5. 方式 2: 批量执行（batch）=====
if USE_LLM:
    print("\n" + "=" * 60)
    print("5. 方式 2: 批量执行（batch）")
    print("=" * 60)

    # 创建成本追踪器
    tracker_batch = CostTracker()
    config_batch = RunnableConfig(
        callbacks=[tracker_batch],
        tags=["batch", "classification"]
    )

    print("\n开始批量执行...")
    start_time = time.time()

    # 准备批量输入
    batch_inputs = [{"text": doc} for doc in documents]

    # 批量执行
    results_batch = classification_chain.batch(batch_inputs, config=config_batch)
    results_batch = [r.strip() for r in results_batch]

    elapsed_batch = time.time() - start_time

    print(f"\n处理了 {len(documents)} 个文档")
    for i, (doc, result) in enumerate(zip(documents, results_batch), 1):
        print(f"  {i}. {doc[:30]}... → {result}")

    print(f"\n⏱️  总耗时: {elapsed_batch:.2f} 秒")
    print(f"📈 平均每个: {elapsed_batch / len(documents):.2f} 秒")

    # 成本报告
    tracker_batch.report()

    # 性能对比
    print("\n" + "=" * 60)
    print("性能对比")
    print("=" * 60)

    print(f"\n顺序执行: {elapsed_sequential:.2f} 秒")
    print(f"批量执行: {elapsed_batch:.2f} 秒")

    if elapsed_sequential > elapsed_batch:
        speedup = elapsed_sequential / elapsed_batch
        print(f"✨ 批量执行快 {speedup:.2f}x")

    # 成本对比
    cost_sequential = tracker_sequential.get_cost()
    cost_batch = tracker_batch.get_cost()

    print(f"\n顺序成本: ${cost_sequential:.6f}")
    print(f"批量成本: ${cost_batch:.6f}")
    print(f"成本差异: ${abs(cost_sequential - cost_batch):.6f}")

# ===== 6. 并发控制 =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("6. 并发控制")
    print("=" * 60)

    print("\n测试不同并发数的性能...")

    concurrency_levels = [1, 2, 5]

    for max_concurrency in concurrency_levels:
        tracker = CostTracker()
        config = RunnableConfig(
            callbacks=[tracker],
            max_concurrency=max_concurrency,
            tags=[f"concurrency_{max_concurrency}"]
        )

        print(f"\n并发数: {max_concurrency}")
        start_time = time.time()

        results = classification_chain.batch(batch_inputs, config=config)

        elapsed = time.time() - start_time
        print(f"  耗时: {elapsed:.2f} 秒")
        print(f"  成本: ${tracker.get_cost():.6f}")

# ===== 7. 大规模批处理 =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("7. 大规模批处理（分批处理）")
    print("=" * 60)

    # 生成大量测试数据
    large_dataset = documents * 10  # 100 个文档

    print(f"\n处理 {len(large_dataset)} 个文档")
    print("使用分批策略...")

    batch_size = 20
    all_results = []
    total_tracker = CostTracker()

    start_time = time.time()

    for i in range(0, len(large_dataset), batch_size):
        batch = large_dataset[i:i+batch_size]
        batch_inputs = [{"text": doc} for doc in batch]

        tracker = CostTracker()
        config = RunnableConfig(
            callbacks=[tracker, total_tracker],
            max_concurrency=5
        )

        print(f"  批次 {i//batch_size + 1}: 处理 {len(batch)} 个文档...", end=" ")
        results = classification_chain.batch(batch_inputs, config=config)
        all_results.extend(results)
        print(f"完成 (${tracker.get_cost():.6f})")

    elapsed_total = time.time() - start_time

    print(f"\n✓ 完成 {len(large_dataset)} 个文档")
    print(f"⏱️  总耗时: {elapsed_total:.2f} 秒")
    print(f"📈 平均每个: {elapsed_total / len(large_dataset):.3f} 秒")

    # 总成本
    total_tracker.report()

# ===== 8. 结果分析 =====
if USE_LLM:
    print("\n" + "=" * 60)
    print("8. 结果分析")
    print("=" * 60)

    # 统计分类结果
    from collections import Counter

    category_counts = Counter(results_batch)

    print("\n分类统计:")
    for category, count in category_counts.most_common():
        percentage = count / len(results_batch) * 100
        print(f"  {category}: {count} ({percentage:.1f}%)")

# ===== 9. langasync 集成示例（概念演示）=====
print("\n" + "=" * 60)
print("9. langasync 成本优化（概念演示）")
print("=" * 60)

print("""
langasync 通过批处理 API 实现 50% 成本降低

安装:
  uv add langasync

使用示例:
  from langasync import wrap_chain

  # 包装链
  async_chain = wrap_chain(
      classification_chain,
      batch_size=10
  )

  # 批量执行（成本降低 50%）
  results = await async_chain.abatch(batch_inputs)

适用场景:
  ✓ 批量评估和测试
  ✓ 数据标注任务
  ✓ 离线分析
  ✗ 实时查询（延迟较高）

参考: https://github.com/langasync/langasync
""")

# ===== 10. 最佳实践总结 =====
print("\n" + "=" * 60)
print("10. 最佳实践总结")
print("=" * 60)

print("""
批量处理最佳实践:

1. 性能优化
   ✓ 使用 batch 而非 invoke 循环
   ✓ 设置合理的并发数（max_concurrency）
   ✓ 大数据集分批处理

2. 成本优化
   ✓ 使用成本追踪回调
   ✓ 考虑 langasync（50% 成本降低）
   ✓ 选择合适的模型（gpt-4o-mini vs gpt-4）

3. 错误处理
   ✓ 单个失败不影响整体
   ✓ 记录失败的输入
   ✓ 实现重试机制

4. 监控
   ✓ 追踪 token 使用
   ✓ 监控 API 调用次数
   ✓ 计算实际成本

5. 生产环境
   ✓ 限流和并发控制
   ✓ 分批处理大数据集
   ✓ 集成 LangSmith 监控
""")

# ===== 总结 =====
print("\n" + "=" * 60)
print("总结")
print("=" * 60)

if USE_LLM:
    print(f"""
本示例演示了:

1. ✓ 批量文档分类
2. ✓ 成本追踪和计算
3. ✓ 性能对比（顺序 vs 批量）
4. ✓ 并发控制
5. ✓ 大规模分批处理
6. ✓ langasync 集成概念

关键数据:
- 批量执行比顺序快 {speedup:.2f}x
- 处理 {len(documents)} 个文档成本: ${cost_batch:.6f}
- 平均每个文档: ${cost_batch / len(documents):.6f}

下一步:
- 学习流式输出（07_实战代码_03_流式输出.md）
- 集成 LangSmith 进行生产监控
- 尝试 langasync 进行成本优化
""")
else:
    print("""
本示例演示了批量处理的概念和最佳实践。

要运行完整示例，请:
1. 设置 OPENAI_API_KEY 环境变量
2. 重新运行脚本

下一步:
- 学习流式输出（07_实战代码_03_流式输出.md）
""")
```

---

## 运行输出示例

```
============================================================
1. 成本追踪系统
============================================================

成本追踪器已初始化

============================================================
2. 文档分类链
============================================================

✓ 分类链已创建

============================================================
3. 准备测试数据
============================================================

准备了 10 个测试文档
  1. Python 3.13 发布了新的性能优化特性...
  2. 苹果公司发布了最新的财报，营收超预期...
  3. 电影《流浪地球3》票房突破10亿...
  4. NBA 总决赛第七场比赛精彩纷呈...
  5. 机器学习算法在医疗诊断中的应用...
  6. 特斯拉发布新款电动汽车...
  7. 音乐节吸引了数万名观众...
  8. 世界杯足球赛进入淘汰赛阶段...
  9. 人工智能芯片市场竞争激烈...
  10. 好莱坞明星出席慈善晚会...

============================================================
4. 方式 1: 顺序执行（invoke 循环）
============================================================

开始顺序执行...
  处理 1/10: Python 3.13 发布了新的性能优化特性... → 技术
  处理 2/10: 苹果公司发布了最新的财报，营收超预期... → 商业
  处理 3/10: 电影《流浪地球3》票房突破10亿... → 娱乐
  处理 4/10: NBA 总决赛第七场比赛精彩纷呈... → 体育
  处理 5/10: 机器学习算法在医疗诊断中的应用... → 技术
  处理 6/10: 特斯拉发布新款电动汽车... → 商业
  处理 7/10: 音乐节吸引了数万名观众... → 娱乐
  处理 8/10: 世界杯足球赛进入淘汰赛阶段... → 体育
  处理 9/10: 人工智能芯片市场竞争激烈... → 技术
  处理 10/10: 好莱坞明星出席慈善晚会... → 娱乐

⏱️  总耗时: 8.45 秒
📈 平均每个: 0.85 秒

📊 Token 使用统计:
  - API 调用次数: 10
  - Prompt tokens: 450
  - Completion tokens: 20
  - Total tokens: 470
💰 预估成本: $0.000080

============================================================
5. 方式 2: 批量执行（batch）
============================================================

开始批量执行...

处理了 10 个文档
  1. Python 3.13 发布了新的性能优化特性... → 技术
  2. 苹果公司发布了最新的财报，营收超预期... → 商业
  3. 电影《流浪地球3》票房突破10亿... → 娱乐
  4. NBA 总决赛第七场比赛精彩纷呈... → 体育
  5. 机器学习算法在医疗诊断中的应用... → 技术
  6. 特斯拉发布新款电动汽车... → 商业
  7. 音乐节吸引了数万名观众... → 娱乐
  8. 世界杯足球赛进入淘汰赛阶段... → 体育
  9. 人工智能芯片市场竞争激烈... → 技术
  10. 好莱坞明星出席慈善晚会... → 娱乐

⏱️  总耗时: 3.21 秒
📈 平均每个: 0.32 秒

📊 Token 使用统计:
  - API 调用次数: 10
  - Prompt tokens: 450
  - Completion tokens: 20
  - Total tokens: 470
💰 预估成本: $0.000080

============================================================
性能对比
============================================================

顺序执行: 8.45 秒
批量执行: 3.21 秒
✨ 批量执行快 2.63x

顺序成本: $0.000080
批量成本: $0.000080
成本差异: $0.000000

============================================================
6. 并发控制
============================================================

测试不同并发数的性能...

并发数: 1
  耗时: 8.32 秒
  成本: $0.000080

并发数: 2
  耗时: 4.56 秒
  成本: $0.000080

并发数: 5
  耗时: 3.18 秒
  成本: $0.000080

============================================================
7. 大规模批处理（分批处理）
============================================================

处理 100 个文档
使用分批策略...
  批次 1: 处理 20 个文档... 完成 ($0.000160)
  批次 2: 处理 20 个文档... 完成 ($0.000160)
  批次 3: 处理 20 个文档... 完成 ($0.000160)
  批次 4: 处理 20 个文档... 完成 ($0.000160)
  批次 5: 处理 20 个文档... 完成 ($0.000160)

✓ 完成 100 个文档
⏱️  总耗时: 32.15 秒
📈 平均每个: 0.322 秒

📊 Token 使用统计:
  - API 调用次数: 100
  - Prompt tokens: 4,500
  - Completion tokens: 200
  - Total tokens: 4,700
💰 预估成本: $0.000795

============================================================
8. 结果分析
============================================================

分类统计:
  技术: 3 (30.0%)
  娱乐: 3 (30.0%)
  商业: 2 (20.0%)
  体育: 2 (20.0%)

============================================================
9. langasync 成本优化（概念演示）
============================================================

langasync 通过批处理 API 实现 50% 成本降低

安装:
  uv add langasync

使用示例:
  from langasync import wrap_chain

  # 包装链
  async_chain = wrap_chain(
      classification_chain,
      batch_size=10
  )

  # 批量执行（成本降低 50%）
  results = await async_chain.abatch(batch_inputs)

适用场景:
  ✓ 批量评估和测试
  ✓ 数据标注任务
  ✓ 离线分析
  ✗ 实时查询（延迟较高）

参考: https://github.com/langasync/langasync

============================================================
10. 最佳实践总结
============================================================

批量处理最佳实践:

1. 性能优化
   ✓ 使用 batch 而非 invoke 循环
   ✓ 设置合理的并发数（max_concurrency）
   ✓ 大数据集分批处理

2. 成本优化
   ✓ 使用成本追踪回调
   ✓ 考虑 langasync（50% 成本降低）
   ✓ 选择合适的模型（gpt-4o-mini vs gpt-4）

3. 错误处理
   ✓ 单个失败不影响整体
   ✓ 记录失败的输入
   ✓ 实现重试机制

4. 监控
   ✓ 追踪 token 使用
   ✓ 监控 API 调用次数
   ✓ 计算实际成本

5. 生产环境
   ✓ 限流和并发控制
   ✓ 分批处理大数据集
   ✓ 集成 LangSmith 监控

============================================================
总结
============================================================

本示例演示了:

1. ✓ 批量文档分类
2. ✓ 成本追踪和计算
3. ✓ 性能对比（顺序 vs 批量）
4. ✓ 并发控制
5. ✓ 大规模分批处理
6. ✓ langasync 集成概念

关键数据:
- 批量执行比顺序快 2.63x
- 处理 10 个文档成本: $0.000080
- 平均每个文档: $0.000008

下一步:
- 学习流式输出（07_实战代码_03_流式输出.md）
- 集成 LangSmith 进行生产监控
- 尝试 langasync 进行成本优化
```

---

## 使用说明

### 1. 环境准备

```bash
# 安装依赖
uv add langchain langchain-core langchain-openai python-dotenv

# 配置 API 密钥
cp .env.example .env
# 编辑 .env，添加 OPENAI_API_KEY
```

### 2. 运行代码

```bash
python 07_实战代码_02_批处理场景.py
```

### 3. 可选：安装 langasync

```bash
uv add langasync
```

---

## 学习要点

### 核心概念

1. **batch 方法**：并发处理多个输入
2. **成本追踪**：监控 token 使用和成本
3. **并发控制**：max_concurrency 参数
4. **分批处理**：大数据集分批执行
5. **langasync**：50% 成本降低

### 性能优化

- batch 比 invoke 循环快 2-3 倍
- 合理设置并发数
- 大数据集分批处理

### 成本优化

- 使用 gpt-4o-mini 而非 gpt-4
- 考虑 langasync 批处理 API
- 监控和追踪实际成本

---

**下一步**: 阅读 [07_实战代码_03_流式输出.md](./07_实战代码_03_流式输出.md) 学习流式输出
