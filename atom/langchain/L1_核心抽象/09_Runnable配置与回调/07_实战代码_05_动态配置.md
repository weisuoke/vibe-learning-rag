# 实战代码 05：动态配置

> **使用 configurable_fields() 实现运行时参数调整、A/B 测试和智能模型选择**

---

## 场景 1：基础动态参数调整

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig

# 声明可配置字段
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
).configurable_fields(
    temperature=ConfigurableField(id="temp")
)

# 运行时动态调整 temperature
config_low = RunnableConfig(configurable={"temp": 0.3})
config_high = RunnableConfig(configurable={"temp": 0.9})

# 低温度：更确定性的输出
result1 = llm.invoke("写一首诗", config=config_low)
print("低温度:", result1.content[:50])

# 高温度：更有创意的输出
result2 = llm.invoke("写一首诗", config=config_high)
print("高温度:", result2.content[:50])
```

---

## 场景 2：动态模型切换

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig

# 声明可配置模型
llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_fields(
    model=ConfigurableField(id="model")
)

# 简单查询用 GPT-3.5
config_cheap = RunnableConfig(
    configurable={"model": "gpt-3.5-turbo"},
    metadata={"cost_tier": "low"}
)

# 复杂查询用 GPT-4
config_quality = RunnableConfig(
    configurable={"model": "gpt-4"},
    metadata={"cost_tier": "high"}
)

# 使用
result1 = llm.invoke("1+1=?", config=config_cheap)
result2 = llm.invoke("解释量子纠缠", config=config_quality)
```

---

## 场景 3：A/B 测试框架

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig
from langchain_core.callbacks import BaseCallbackHandler
import random

class ABTestTracker(BaseCallbackHandler):
    """A/B 测试追踪"""
    def __init__(self):
        self.results = []

    def on_llm_end(self, response, **kwargs):
        metadata = kwargs.get("metadata", {})
        variant = metadata.get("variant")
        text = response.generations[0][0].text

        self.results.append({
            "variant": variant,
            "output": text,
            "tokens": response.llm_output["token_usage"]["total_tokens"]
        })

def ab_test(query: str, num_samples: int = 10):
    """A/B 测试：对比不同 temperature"""
    llm = ChatOpenAI(temperature=0.7).configurable_fields(
        temperature=ConfigurableField(id="temp")
    )

    tracker = ABTestTracker()

    # 变体 A: temperature=0.3
    # 变体 B: temperature=0.9
    for i in range(num_samples):
        variant = "A" if i % 2 == 0 else "B"
        temp = 0.3 if variant == "A" else 0.9

        config = RunnableConfig(
            configurable={"temp": temp},
            callbacks=[tracker],
            metadata={"variant": variant, "sample_id": i}
        )

        llm.invoke(query, config=config)

    # 分析结果
    variant_a = [r for r in tracker.results if r["variant"] == "A"]
    variant_b = [r for r in tracker.results if r["variant"] == "B"]

    print(f"\n=== A/B 测试结果 ===")
    print(f"变体 A (temp=0.3): {len(variant_a)} 样本")
    print(f"  平均 tokens: {sum(r['tokens'] for r in variant_a) / len(variant_a):.0f}")
    print(f"\n变体 B (temp=0.9): {len(variant_b)} 样本")
    print(f"  平均 tokens: {sum(r['tokens'] for r in variant_b) / len(variant_b):.0f}")

    return tracker.results

# 使用
results = ab_test("写一首关于春天的诗", num_samples=10)
```

---

## 场景 4：智能模型选择器

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig

class SmartModelSelector:
    """智能模型选择器"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_fields(
            model=ConfigurableField(id="model"),
            temperature=ConfigurableField(id="temp")
        )

    def select_config(self, query: str, user_tier: str = "free"):
        """根据查询和用户等级选择配置"""
        query_length = len(query)
        query_lower = query.lower()

        # 检测查询复杂度
        is_complex = any(keyword in query_lower for keyword in [
            "解释", "分析", "详细", "原理", "为什么", "如何"
        ])

        # 选择模型和参数
        if user_tier == "premium":
            if is_complex or query_length > 200:
                model = "gpt-4"
                temp = 0.3
            else:
                model = "gpt-3.5-turbo"
                temp = 0.5
        else:
            # 免费用户只用便宜模型
            model = "gpt-3.5-turbo"
            temp = 0.7

        return RunnableConfig(
            configurable={"model": model, "temp": temp},
            metadata={
                "selected_model": model,
                "selected_temp": temp,
                "user_tier": user_tier,
                "is_complex": is_complex
            }
        )

    def invoke(self, query: str, user_tier: str = "free"):
        """执行查询"""
        config = self.select_config(query, user_tier)
        result = self.llm.invoke(query, config=config)

        # 显示选择的配置
        metadata = config["metadata"]
        print(f"使用模型: {metadata['selected_model']}")
        print(f"Temperature: {metadata['selected_temp']}")

        return result.content

# 使用
selector = SmartModelSelector()

# 免费用户 - 简单查询
result1 = selector.invoke("你好", user_tier="free")

# 付费用户 - 复杂查询
result2 = selector.invoke("详细解释量子纠缠的原理", user_tier="premium")
```

---

## 场景 5：多模型对比

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import ConfigurableField, RunnableConfig

# 配置多个模型选项
llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_alternatives(
    ConfigurableField(id="model"),
    default_key="gpt35",
    gpt4=ChatOpenAI(model="gpt-4"),
    claude_opus=ChatAnthropic(model="claude-3-opus-20240229"),
    claude_sonnet=ChatAnthropic(model="claude-3-sonnet-20240229")
)

def compare_models(query: str):
    """对比不同模型的输出"""
    models = ["gpt35", "gpt4", "claude_opus", "claude_sonnet"]
    results = []

    for model_key in models:
        config = RunnableConfig(
            configurable={"model": model_key},
            metadata={"model": model_key}
        )

        try:
            result = llm.invoke(query, config=config)
            results.append({
                "model": model_key,
                "output": result.content,
                "success": True
            })
        except Exception as e:
            results.append({
                "model": model_key,
                "error": str(e),
                "success": False
            })

    # 显示结果
    print("\n=== 模型对比 ===")
    for r in results:
        print(f"\n{r['model']}:")
        if r["success"]:
            print(f"  {r['output'][:100]}...")
        else:
            print(f"  错误: {r['error']}")

    return results

# 使用
results = compare_models("解释量子纠缠")
```

---

## 场景 6：成本优化器

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig
from langchain_core.callbacks import BaseCallbackHandler

class CostOptimizer(BaseCallbackHandler):
    """成本优化追踪"""
    def __init__(self):
        self.total_cost = 0.0

    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})
        model = response.llm_output.get("model_name")

        # 计算成本
        if model == "gpt-4":
            cost = (
                usage.get("prompt_tokens", 0) * 0.00003 +
                usage.get("completion_tokens", 0) * 0.00006
            )
        else:
            cost = (
                usage.get("prompt_tokens", 0) * 0.0005 / 1000 +
                usage.get("completion_tokens", 0) * 0.0015 / 1000
            )

        self.total_cost += cost

class SmartCostOptimizer:
    """智能成本优化器"""

    def __init__(self, daily_budget: float = 10.0):
        self.daily_budget = daily_budget
        self.cost_tracker = CostOptimizer()

        self.llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_fields(
            model=ConfigurableField(id="model")
        )

    def select_model(self, query: str):
        """根据预算和查询选择模型"""
        remaining_budget = self.daily_budget - self.cost_tracker.total_cost

        # 预算不足，只用便宜模型
        if remaining_budget < 0.01:
            return "gpt-3.5-turbo"

        # 预算充足，根据查询复杂度选择
        query_length = len(query)
        if query_length > 200:
            return "gpt-4"
        else:
            return "gpt-3.5-turbo"

    def invoke(self, query: str):
        """执行查询"""
        model = self.select_model(query)

        config = RunnableConfig(
            configurable={"model": model},
            callbacks=[self.cost_tracker],
            metadata={"selected_model": model}
        )

        result = self.llm.invoke(query, config=config)

        print(f"使用模型: {model}")
        print(f"累计成本: ${self.cost_tracker.total_cost:.4f}")
        print(f"剩余预算: ${self.daily_budget - self.cost_tracker.total_cost:.4f}")

        return result.content

# 使用
optimizer = SmartCostOptimizer(daily_budget=1.0)

# 多次查询
for i in range(5):
    optimizer.invoke(f"问题{i}")
```

---

## 场景 7：降级策略

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig

class FallbackChain:
    """降级策略链"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4").configurable_fields(
            model=ConfigurableField(id="model")
        )

        # 降级顺序
        self.fallback_models = [
            "gpt-4",
            "gpt-3.5-turbo",
            "gpt-3.5-turbo-16k"
        ]

    def invoke_with_fallback(self, query: str):
        """带降级的调用"""
        for model in self.fallback_models:
            try:
                config = RunnableConfig(
                    configurable={"model": model},
                    metadata={"model": model}
                )

                result = self.llm.invoke(query, config=config)
                print(f"✅ 成功使用模型: {model}")
                return result.content

            except Exception as e:
                print(f"❌ 模型 {model} 失败: {e}")
                continue

        raise Exception("所有模型都失败了")

# 使用
chain = FallbackChain()
result = chain.invoke_with_fallback("你好")
```

---

## 场景 8：动态 max_tokens 调整

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig

# 声明可配置字段
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    max_tokens=100
).configurable_fields(
    max_tokens=ConfigurableField(id="max_tokens")
)

# 根据需求调整 max_tokens
def query_with_length_control(query: str, output_type: str = "short"):
    """根据输出类型调整 max_tokens"""
    if output_type == "short":
        max_tokens = 50
    elif output_type == "medium":
        max_tokens = 200
    else:  # long
        max_tokens = 1000

    config = RunnableConfig(
        configurable={"max_tokens": max_tokens},
        metadata={"output_type": output_type}
    )

    result = llm.invoke(query, config=config)
    print(f"输出类型: {output_type}, max_tokens: {max_tokens}")
    return result.content

# 使用
result1 = query_with_length_control("解释机器学习", output_type="short")
result2 = query_with_length_control("解释机器学习", output_type="long")
```

---

## 场景 9：用户偏好配置

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField, RunnableConfig

class UserPreferenceManager:
    """用户偏好管理器"""

    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7
        ).configurable_fields(
            model=ConfigurableField(id="model"),
            temperature=ConfigurableField(id="temp")
        )

        # 用户偏好存储
        self.user_preferences = {
            "user_001": {"model": "gpt-4", "temp": 0.3},
            "user_002": {"model": "gpt-3.5-turbo", "temp": 0.9},
            "user_003": {"model": "gpt-3.5-turbo", "temp": 0.5}
        }

    def get_user_config(self, user_id: str):
        """获取用户配置"""
        prefs = self.user_preferences.get(user_id, {
            "model": "gpt-3.5-turbo",
            "temp": 0.7
        })

        return RunnableConfig(
            configurable=prefs,
            metadata={"user_id": user_id, **prefs}
        )

    def invoke(self, query: str, user_id: str):
        """使用用户偏好执行查询"""
        config = self.get_user_config(user_id)
        result = self.llm.invoke(query, config=config)

        prefs = config["metadata"]
        print(f"用户 {user_id} 偏好: model={prefs['model']}, temp={prefs['temp']}")

        return result.content

# 使用
manager = UserPreferenceManager()

# 不同用户使用各自偏好
result1 = manager.invoke("你好", "user_001")
result2 = manager.invoke("你好", "user_002")
```

---

## 场景 10：完整生产配置系统

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import ConfigurableField, RunnableConfig
from langchain_core.callbacks import BaseCallbackHandler
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProductionConfigManager:
    """生产环境配置管理器"""

    # 模型价格表
    PRICES = {
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
        "gpt-4": {"input": 0.03, "output": 0.06},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015}
    }

    def __init__(self, daily_budget: float = 100.0):
        self.daily_budget = daily_budget
        self.daily_cost = 0.0

        # 配置多模型
        self.llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_alternatives(
            ConfigurableField(id="model"),
            default_key="gpt35",
            gpt4=ChatOpenAI(model="gpt-4"),
            claude_opus=ChatAnthropic(model="claude-3-opus-20240229"),
            claude_sonnet=ChatAnthropic(model="claude-3-sonnet-20240229")
        ).configurable_fields(
            temperature=ConfigurableField(id="temp")
        )

    def select_config(
        self,
        query: str,
        user_tier: str = "free",
        query_type: str = "general"
    ):
        """智能选择配置"""
        query_length = len(query)
        is_complex = any(kw in query.lower() for kw in [
            "解释", "分析", "详细", "原理"
        ])

        # 检查预算
        remaining_budget = self.daily_budget - self.daily_cost

        # 选择模型
        if remaining_budget < 0.01:
            model = "gpt35"
            temp = 0.7
        elif user_tier == "premium":
            if is_complex or query_length > 200:
                model = "gpt4"
                temp = 0.3
            else:
                model = "claude_sonnet"
                temp = 0.5
        else:
            model = "gpt35"
            temp = 0.7

        return RunnableConfig(
            configurable={"model": model, "temp": temp},
            tags=["production", query_type, user_tier],
            metadata={
                "user_tier": user_tier,
                "query_type": query_type,
                "selected_model": model,
                "selected_temp": temp,
                "is_complex": is_complex,
                "remaining_budget": remaining_budget
            }
        )

    def invoke(
        self,
        query: str,
        user_id: str,
        user_tier: str = "free",
        query_type: str = "general"
    ):
        """执行查询"""
        config = self.select_config(query, user_tier, query_type)

        try:
            result = self.llm.invoke(query, config=config)

            # 记录日志
            metadata = config["metadata"]
            logger.info(
                f"查询成功: user={user_id}, "
                f"model={metadata['selected_model']}, "
                f"temp={metadata['selected_temp']}"
            )

            return result.content

        except Exception as e:
            logger.error(f"查询失败: {e}")
            raise

# 使用
manager = ProductionConfigManager(daily_budget=100.0)

# 免费用户 - 简单查询
result1 = manager.invoke(
    query="你好",
    user_id="user_001",
    user_tier="free",
    query_type="chat"
)

# 付费用户 - 复杂查询
result2 = manager.invoke(
    query="详细解释量子纠缠的原理",
    user_id="user_002",
    user_tier="premium",
    query_type="rag_search"
)
```

---

## 总结

### 动态配置应用场景

| 场景 | 配置字段 | 用途 |
|------|---------|------|
| **参数调整** | temperature, max_tokens | 控制输出质量 |
| **模型切换** | model | 成本优化、质量控制 |
| **A/B 测试** | temperature, model | 对比效果 |
| **成本优化** | model | 预算控制 |
| **降级策略** | model | 容错处理 |
| **用户偏好** | model, temperature | 个性化 |

### 最佳实践

1. **声明式配置**：使用 `configurable_fields()` 声明可配置字段
2. **运行时调整**：在 `invoke()` 时传入配置
3. **智能选择**：根据查询复杂度、用户等级、预算自动选择
4. **降级策略**：主模型失败时自动切换到备用模型
5. **成本控制**：追踪成本，预算不足时降级

---

## 参考资料

- [LangChain Configurable Runnables](https://python.langchain.com/docs/how_to/configure/)
- [ConfigurableField API](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.config.ConfigurableField.html)
