# 核心概念4：超时控制

**超时控制是为异步操作设置最大等待时间，防止请求无限等待导致资源泄漏和用户体验下降。**

---

## 为什么需要超时控制？

**问题：** 外部服务响应缓慢或无响应，导致请求一直挂起

```python
# ❌ 没有超时控制
async def call_llm(prompt: str):
    response = await openai.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# 场景：OpenAI API 无响应
# 结果：请求一直挂起，占用连接，用户无限等待
```

**解决方案：** 超时控制

```python
# ✅ 有超时控制
import asyncio

async def call_llm(prompt: str):
    try:
        async with asyncio.timeout(30):  # 30秒超时
            response = await openai.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
    except asyncio.TimeoutError:
        raise LLMTimeoutError("LLM 调用超时（30s）")

# 场景：OpenAI API 无响应
# 结果：30秒后超时，释放连接，返回错误
```

---

## Python 中的超时控制

### 1. asyncio.timeout（Python 3.11+）

**推荐方式**：使用 `asyncio.timeout` 上下文管理器

```python
import asyncio

async def call_with_timeout(timeout: float):
    """带超时的异步操作"""
    try:
        async with asyncio.timeout(timeout):
            # 异步操作
            await asyncio.sleep(5)
            return "成功"
    except asyncio.TimeoutError:
        print(f"操作超时（{timeout}s）")
        raise

# 使用示例
try:
    result = await call_with_timeout(3.0)  # 3秒超时
except asyncio.TimeoutError:
    print("超时了")
```

---

### 2. asyncio.wait_for（Python 3.7+）

**兼容方式**：使用 `asyncio.wait_for` 函数

```python
import asyncio

async def slow_operation():
    """慢操作"""
    await asyncio.sleep(5)
    return "成功"

# 使用示例
try:
    result = await asyncio.wait_for(slow_operation(), timeout=3.0)
except asyncio.TimeoutError:
    print("操作超时（3s）")
```

---

### 3. 对比：asyncio.timeout vs asyncio.wait_for

| 特性 | asyncio.timeout | asyncio.wait_for |
|------|----------------|------------------|
| **Python 版本** | 3.11+ | 3.7+ |
| **语法** | 上下文管理器 | 函数调用 |
| **可读性** | 更好（with 语句） | 一般 |
| **嵌套使用** | 更方便 | 需要包装 |
| **推荐度** | ✅ 推荐 | 兼容旧版本时使用 |

```python
# asyncio.timeout（推荐）
async with asyncio.timeout(3.0):
    result = await slow_operation()

# asyncio.wait_for（兼容）
result = await asyncio.wait_for(slow_operation(), timeout=3.0)
```

---

## 超时控制的最佳实践

### 1. 根据服务类型设置超时

**不同服务的推荐超时时间：**

| 服务类型 | 推荐超时时间 | 理由 |
|---------|------------|------|
| **数据库查询** | 5-10s | 应该很快，超过说明有问题 |
| **缓存操作** | 1-3s | 内存操作，应该极快 |
| **向量检索** | 10-20s | 取决于数据量和索引 |
| **LLM 生成（短文本）** | 30s | 生成时间较短 |
| **LLM 生成（长文本）** | 60s | 生成时间较长 |
| **文档解析** | 30-60s | 取决于文档大小 |
| **HTTP 请求** | 10-30s | 取决于网络和服务 |

```python
# 数据库查询：5秒超时
async def query_database(query: str):
    try:
        async with asyncio.timeout(5):
            return await db.execute(query)
    except asyncio.TimeoutError:
        raise DatabaseTimeoutError("数据库查询超时（5s）")

# LLM 生成：30秒超时
async def call_llm(prompt: str):
    try:
        async with asyncio.timeout(30):
            return await openai.chat.completions.create(...)
    except asyncio.TimeoutError:
        raise LLMTimeoutError("LLM 调用超时（30s）")

# 向量检索：10秒超时
async def vector_search(query: str):
    try:
        async with asyncio.timeout(10):
            return await chroma.query(query)
    except asyncio.TimeoutError:
        raise VectorSearchTimeoutError("向量检索超时（10s）")
```

---

### 2. 级联超时：上游 > 下游

**原则：** 上游超时应该比下游超时更长（留出处理时间）

```python
# ❌ 错误：上游超时 = 下游超时
@app.post("/chat")
async def chat(message: str):
    async with asyncio.timeout(30):  # 上游：30秒
        response = await call_llm_with_timeout(message, timeout=30)  # 下游：30秒
        return {"response": response}

# 问题：下游超时后，上游没有时间处理错误

# ✅ 正确：上游超时 > 下游超时
@app.post("/chat")
async def chat(message: str):
    async with asyncio.timeout(35):  # 上游：35秒
        response = await call_llm_with_timeout(message, timeout=30)  # 下游：30秒
        return {"response": response}

# 优点：下游超时后，上游还有 5 秒处理错误
```

**级联超时示例：**
```
用户请求 → API 端点（35s）
  ↓
LLM 调用（30s）
  ↓
OpenAI API（25s）
```

---

### 3. 超时与重试的协作

**原则：** 超时时间应该考虑重试次数

```python
# ❌ 错误：超时时间没有考虑重试
@retry(stop=stop_after_attempt(3))
async def call_llm(prompt: str):
    async with asyncio.timeout(30):  # 单次超时：30秒
        return await openai.chat.completions.create(...)

# 问题：3次重试 = 90秒总等待时间

# ✅ 正确：超时时间考虑重试
@retry(stop=stop_after_attempt(3))
async def call_llm(prompt: str):
    async with asyncio.timeout(10):  # 单次超时：10秒
        return await openai.chat.completions.create(...)

# 优点：3次重试 = 30秒总等待时间（可接受）
```

**计算公式：**
```
单次超时 = 总超时 / 重试次数
单次超时 = 30s / 3 = 10s
```

---

### 4. 超时错误的友好提示

```python
async def call_llm_with_friendly_timeout(prompt: str):
    """LLM 调用（友好的超时提示）"""
    try:
        async with asyncio.timeout(30):
            response = await openai.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
    except asyncio.TimeoutError:
        # 友好的错误提示
        raise HTTPException(
            status_code=504,
            detail={
                "error": "AI 服务响应超时，请稍后重试",
                "error_code": "LLM_TIMEOUT",
                "timeout": 30,
                "suggestion": "您可以尝试缩短输入文本或稍后再试"
            }
        )
```

---

## 超时装饰器

### 基础版本

```python
import asyncio
from typing import TypeVar, Callable, Awaitable
from functools import wraps

T = TypeVar('T')

def timeout(seconds: float):
    """超时装饰器"""
    def decorator(func: Callable[..., Awaitable[T]]) -> Callable[..., Awaitable[T]]:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> T:
            try:
                async with asyncio.timeout(seconds):
                    return await func(*args, **kwargs)
            except asyncio.TimeoutError:
                raise TimeoutError(f"{func.__name__} 超时（{seconds}s）")
        return wrapper
    return decorator

# 使用示例
@timeout(30)
async def call_llm(prompt: str):
    response = await openai.chat.completions.create(...)
    return response.choices[0].message.content
```

---

### 完整版本：自定义超时异常

```python
"""
完整的超时装饰器
支持：自定义超时异常、超时回调、动态超时
"""

import asyncio
from typing import TypeVar, Callable, Awaitable, Optional, Type
from functools import wraps
import structlog

T = TypeVar('T')
logger = structlog.get_logger()

def timeout(
    seconds: float,
    error_class: Type[Exception] = TimeoutError,
    error_message: Optional[str] = None,
    on_timeout: Optional[Callable] = None
):
    """
    超时装饰器

    参数：
        seconds: 超时时间（秒）
        error_class: 超时异常类
        error_message: 自定义错误消息
        on_timeout: 超时回调函数
    """
    def decorator(func: Callable[..., Awaitable[T]]) -> Callable[..., Awaitable[T]]:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> T:
            try:
                async with asyncio.timeout(seconds):
                    return await func(*args, **kwargs)
            except asyncio.TimeoutError:
                # 记录超时日志
                logger.warning(
                    "function_timeout",
                    function=func.__name__,
                    timeout=seconds
                )

                # 执行超时回调
                if on_timeout:
                    await on_timeout(func.__name__, seconds)

                # 抛出自定义异常
                message = error_message or f"{func.__name__} 超时（{seconds}s）"
                raise error_class(message)
        return wrapper
    return decorator

# 使用示例
class LLMTimeoutError(Exception):
    """LLM 超时异常"""
    pass

async def on_llm_timeout(func_name: str, timeout: float):
    """LLM 超时回调"""
    logger.error("llm_timeout", function=func_name, timeout=timeout)
    # 可以在这里发送告警、记录指标等

@timeout(
    seconds=30,
    error_class=LLMTimeoutError,
    error_message="LLM 服务响应超时，请稍后重试",
    on_timeout=on_llm_timeout
)
async def call_llm(prompt: str):
    response = await openai.chat.completions.create(...)
    return response.choices[0].message.content
```

---

## 在 AI Agent API 中的应用

### 场景1：LLM 调用超时

```python
from openai import AsyncOpenAI
import asyncio

client = AsyncOpenAI()

async def call_llm(prompt: str, timeout: float = 30) -> str:
    """调用 LLM（带超时）"""
    try:
        async with asyncio.timeout(timeout):
            response = await client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                timeout=timeout - 5  # OpenAI SDK 超时比外层超时短 5 秒
            )
            return response.choices[0].message.content
    except asyncio.TimeoutError:
        logger.error("llm_timeout", prompt_length=len(prompt), timeout=timeout)
        raise LLMTimeoutError(f"LLM 调用超时（{timeout}s）")

# 使用示例
@app.post("/chat")
async def chat(message: str):
    try:
        response = await call_llm(message, timeout=30)
        return {"response": response}
    except LLMTimeoutError:
        raise HTTPException(
            status_code=504,
            detail="AI 服务响应超时，请稍后重试"
        )
```

---

### 场景2：数据库查询超时

```python
from sqlalchemy.ext.asyncio import AsyncSession
import asyncio

async def query_user(user_id: str, session: AsyncSession, timeout: float = 5):
    """查询用户（带超时）"""
    try:
        async with asyncio.timeout(timeout):
            result = await session.execute(
                select(User).where(User.id == user_id)
            )
            return result.scalar_one_or_none()
    except asyncio.TimeoutError:
        logger.error("db_query_timeout", user_id=user_id, timeout=timeout)
        raise DatabaseTimeoutError(f"数据库查询超时（{timeout}s）")

# 使用示例
@app.get("/users/{user_id}")
async def get_user(user_id: str, session: AsyncSession = Depends(get_session)):
    try:
        user = await query_user(user_id, session, timeout=5)
        if not user:
            raise HTTPException(status_code=404, detail="用户不存在")
        return user
    except DatabaseTimeoutError:
        raise HTTPException(
            status_code=504,
            detail="数据库查询超时，请稍后重试"
        )
```

---

### 场景3：向量检索超时

```python
import chromadb
import asyncio

async def vector_search(query: str, top_k: int = 5, timeout: float = 10):
    """向量检索（带超时）"""
    try:
        async with asyncio.timeout(timeout):
            client = chromadb.Client()
            collection = client.get_collection("documents")

            results = collection.query(
                query_texts=[query],
                n_results=top_k
            )
            return results
    except asyncio.TimeoutError:
        logger.error("vector_search_timeout", query=query, timeout=timeout)
        raise VectorSearchTimeoutError(f"向量检索超时（{timeout}s）")

# 使用示例
@app.post("/search")
async def search(query: str):
    try:
        results = await vector_search(query, timeout=10)
        return {"results": results}
    except VectorSearchTimeoutError:
        raise HTTPException(
            status_code=504,
            detail="检索超时，请稍后重试"
        )
```

---

### 场景4：多服务协作超时

```python
@app.post("/rag-chat")
async def rag_chat(message: str):
    """RAG 对话（多服务协作，级联超时）"""
    try:
        # 总超时：35秒
        async with asyncio.timeout(35):
            # 1. 向量检索（10秒超时）
            results = await vector_search(message, timeout=10)

            # 2. 构建上下文
            context = "\n".join(results["documents"][0])

            # 3. LLM 生成（20秒超时）
            prompt = f"上下文：{context}\n\n问题：{message}"
            response = await call_llm(prompt, timeout=20)

            # 4. 保存对话（5秒超时）
            await save_conversation(
                {"message": message, "response": response},
                timeout=5
            )

            return {"response": response}
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=504,
            detail="请求超时，请稍后重试"
        )
```

---

## 超时控制的高级技巧

### 1. 动态超时

根据输入大小动态调整超时时间：

```python
def calculate_timeout(text: str, base_timeout: float = 30) -> float:
    """根据文本长度计算超时时间"""
    # 每 1000 字符增加 5 秒
    extra_time = (len(text) // 1000) * 5
    return min(base_timeout + extra_time, 120)  # 最大 120 秒

async def call_llm_dynamic_timeout(prompt: str):
    """LLM 调用（动态超时）"""
    timeout = calculate_timeout(prompt)
    logger.info("llm_call", prompt_length=len(prompt), timeout=timeout)

    try:
        async with asyncio.timeout(timeout):
            response = await openai.chat.completions.create(...)
            return response.choices[0].message.content
    except asyncio.TimeoutError:
        raise LLMTimeoutError(f"LLM 调用超时（{timeout}s）")
```

---

### 2. 超时重试

超时后自动重试（减少超时时间）：

```python
async def call_llm_with_timeout_retry(prompt: str, max_attempts: int = 3):
    """LLM 调用（超时重试，逐步减少超时时间）"""
    timeouts = [30, 20, 10]  # 第1次30s，第2次20s，第3次10s

    for attempt, timeout in enumerate(timeouts[:max_attempts]):
        try:
            async with asyncio.timeout(timeout):
                response = await openai.chat.completions.create(...)
                return response.choices[0].message.content
        except asyncio.TimeoutError:
            if attempt == max_attempts - 1:
                raise LLMTimeoutError("LLM 调用超时（多次重试失败）")
            logger.warning(
                "llm_timeout_retry",
                attempt=attempt + 1,
                timeout=timeout
            )
```

---

### 3. 超时取消

超时后取消正在执行的任务：

```python
async def call_llm_with_cancellation(prompt: str, timeout: float = 30):
    """LLM 调用（超时取消）"""
    task = asyncio.create_task(
        openai.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
    )

    try:
        async with asyncio.timeout(timeout):
            response = await task
            return response.choices[0].message.content
    except asyncio.TimeoutError:
        # 取消任务
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass
        raise LLMTimeoutError(f"LLM 调用超时（{timeout}s）")
```

---

## 超时控制的监控

### 1. 超时指标收集

```python
from prometheus_client import Counter, Histogram

# 超时计数器
timeout_counter = Counter(
    "api_timeout_total",
    "Total number of timeouts",
    ["service", "endpoint"]
)

# 响应时间直方图
response_time_histogram = Histogram(
    "api_response_time_seconds",
    "API response time in seconds",
    ["service", "endpoint"]
)

async def call_llm_with_metrics(prompt: str, timeout: float = 30):
    """LLM 调用（带指标收集）"""
    start_time = time.time()

    try:
        async with asyncio.timeout(timeout):
            response = await openai.chat.completions.create(...)
            return response.choices[0].message.content
    except asyncio.TimeoutError:
        # 记录超时
        timeout_counter.labels(service="llm", endpoint="chat").inc()
        raise
    finally:
        # 记录响应时间
        duration = time.time() - start_time
        response_time_histogram.labels(service="llm", endpoint="chat").observe(duration)
```

---

### 2. 超时告警

```python
async def call_llm_with_alert(prompt: str, timeout: float = 30):
    """LLM 调用（带超时告警）"""
    try:
        async with asyncio.timeout(timeout):
            response = await openai.chat.completions.create(...)
            return response.choices[0].message.content
    except asyncio.TimeoutError:
        # 发送告警
        await send_alert(
            title="LLM 调用超时",
            message=f"LLM 调用超时（{timeout}s）",
            severity="warning",
            tags=["llm", "timeout"]
        )
        raise
```

---

## 超时控制的最佳实践总结

### 1. 超时时间设置

| 服务类型 | 推荐超时 | 理由 |
|---------|---------|------|
| 数据库查询 | 5-10s | 应该很快 |
| 缓存操作 | 1-3s | 内存操作 |
| 向量检索 | 10-20s | 取决于数据量 |
| LLM 生成 | 30-60s | 生成时间较长 |
| HTTP 请求 | 10-30s | 取决于网络 |

### 2. 级联超时

```
上游超时 > 下游超时 + 处理时间
API 端点（35s） > LLM 调用（30s） + 5s
```

### 3. 超时与重试

```
单次超时 = 总超时 / 重试次数
10s = 30s / 3
```

### 4. 超时错误处理

- 记录详细日志
- 返回友好提示
- 收集超时指标
- 发送告警通知

---

## 总结

**超时控制的核心价值：**
1. **防止资源泄漏**：请求不会无限等待，及时释放连接
2. **用户体验**：快速失败，而非无限等待
3. **系统稳定性**：防止慢请求堆积，导致系统崩溃

**实现方式：**
- Python 3.11+：`asyncio.timeout`（推荐）
- Python 3.7+：`asyncio.wait_for`（兼容）

**最佳实践：**
- 根据服务类型设置超时
- 级联超时：上游 > 下游
- 超时与重试协作
- 友好的超时提示
- 超时指标收集

**在 AI Agent API 中的应用：**
- LLM 调用超时（30-60s）
- 数据库查询超时（5-10s）
- 向量检索超时（10-20s）
- 多服务协作超时（级联）
