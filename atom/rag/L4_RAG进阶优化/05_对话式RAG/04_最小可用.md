# 最小可用知识

> 20%的核心知识解决80%的对话式RAG问题

---

## 核心理念

对话式RAG的本质是**让RAG系统记住历史、理解指代、智能压缩**，实现连续对话。掌握以下20%的核心知识，就能解决80%的实际问题。

---

## 一、三大核心技术（必须掌握）

### 1. 上下文管理

**核心原理**：维护对话历史，让系统理解"之前说了什么"

**最小可用实现**：
```python
# 使用LangChain的ConversationBufferMemory
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# 自动记录对话历史
memory.save_context(
    {"input": "Python装饰器是什么？"},
    {"output": "装饰器是一种设计模式..."}
)

# 获取历史
history = memory.load_memory_variables({})
```

**关键点**：
- Memory自动管理历史
- 每轮对话后调用`save_context`
- 检索时自动注入历史

### 2. 历史压缩

**核心原理**：智能压缩长对话，节省token同时保留关键信息

**最小可用实现**：
```python
# 滑动窗口：只保留最近N轮
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(
    k=5,  # 只保留最近5轮
    memory_key="chat_history",
    return_messages=True
)
```

**关键点**：
- 窗口大小根据场景调整（3-10轮）
- 超出窗口的历史自动丢弃
- 适合大多数对话场景

### 3. 指代消解

**核心原理**：识别和解析指代词（"它"、"这个"），将其还原为具体实体

**最小可用实现**：
```python
# 简单的查询改写
def rewrite_query(query, chat_history):
    if has_pronoun(query):  # 检测指代词
        # 使用LLM改写查询
        prompt = f"""
        对话历史：{chat_history}
        当前查询：{query}

        将查询中的指代词替换为具体实体，输出改写后的查询。
        """
        rewritten = llm.invoke(prompt)
        return rewritten
    return query

# 使用
rewritten_query = rewrite_query("它有什么用？", chat_history)
# 输出："Python装饰器有什么用？"
```

**关键点**：
- 检测常见指代词（它、这个、那个、他们）
- 使用LLM改写查询
- 改写后的查询用于检索

---

## 二、最小可用架构

### 基础对话RAG系统（5个组件）

```python
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# 1. LLM
llm = ChatOpenAI(temperature=0)

# 2. Embeddings
embeddings = OpenAIEmbeddings()

# 3. 向量存储
vectorstore = Chroma(embedding_function=embeddings)

# 4. Memory（滑动窗口）
memory = ConversationBufferWindowMemory(
    k=5,
    memory_key="chat_history",
    return_messages=True
)

# 5. 对话链
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory
)

# 使用
response = chain({"question": "Python装饰器是什么？"})
response = chain({"question": "它有什么用？"})  # 自动理解"它"
```

**这5个组件就是最小可用系统**，可以处理80%的对话场景。

---

## 三、关键决策点（3个）

### 决策1：选择Memory策略

| 策略 | 适用场景 | 代码 |
|------|---------|------|
| **Buffer** | 短对话（<10轮） | `ConversationBufferMemory()` |
| **Window** | 中等对话（10-50轮） | `ConversationBufferWindowMemory(k=5)` |
| **Summary** | 长对话（>50轮） | `ConversationSummaryMemory(llm=llm)` |

**推荐**：大多数场景使用Window（k=5）

### 决策2：何时压缩历史

```python
# 简单规则：超过阈值就压缩
def should_compress(chat_history):
    total_tokens = count_tokens(chat_history)
    return total_tokens > 2000  # 阈值：2000 tokens

if should_compress(chat_history):
    # 压缩最早的消息
    chat_history = compress_old_messages(chat_history)
```

**推荐阈值**：
- GPT-3.5: 2000 tokens
- GPT-4: 4000 tokens
- Claude: 6000 tokens

### 决策3：是否需要查询改写

```python
# 简单规则：检测指代词
pronouns = ["它", "这个", "那个", "他们", "她们", "它们"]

def needs_rewrite(query):
    return any(p in query for p in pronouns)

if needs_rewrite(query):
    query = rewrite_query(query, chat_history)
```

**推荐**：始终启用查询改写（成本低，效果好）

---

## 四、常见场景速查

### 场景1：技术文档问答

**需求**：用户连续追问技术细节

**配置**：
```python
memory = ConversationBufferWindowMemory(k=5)  # 保留5轮
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True  # 返回来源文档
)
```

### 场景2：客服对话

**需求**：长对话，需要压缩历史

**配置**：
```python
memory = ConversationSummaryMemory(
    llm=llm,
    memory_key="chat_history",
    return_messages=True
)
```

### 场景3：教育辅导

**需求**：支持追问和澄清

**配置**：
```python
memory = ConversationBufferWindowMemory(k=10)  # 保留更多历史
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    verbose=True  # 显示中间步骤
)
```

---

## 五、核心API速查

### LangChain Memory API

```python
# 1. 创建Memory
memory = ConversationBufferWindowMemory(
    k=5,                          # 窗口大小
    memory_key="chat_history",    # 键名
    return_messages=True          # 返回消息对象
)

# 2. 保存对话
memory.save_context(
    {"input": "用户问题"},
    {"output": "系统回答"}
)

# 3. 获取历史
history = memory.load_memory_variables({})

# 4. 清空历史
memory.clear()
```

### ConversationalRetrievalChain API

```python
# 1. 创建链
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,                      # LLM
    retriever=retriever,          # 检索器
    memory=memory,                # Memory
    return_source_documents=True, # 返回来源
    verbose=True                  # 显示日志
)

# 2. 调用
response = chain({"question": "用户问题"})

# 3. 获取结果
answer = response["answer"]
sources = response["source_documents"]
```

---

## 六、性能优化速查

### 优化1：减少Token消耗

```python
# 使用滑动窗口而非Buffer
memory = ConversationBufferWindowMemory(k=5)  # 而非ConversationBufferMemory()
```

**效果**：Token消耗减少60-80%

### 优化2：提升检索精度

```python
# 查询改写
rewritten_query = rewrite_query(query, chat_history)
results = retriever.get_relevant_documents(rewritten_query)
```

**效果**：检索准确率提升20-30%

### 优化3：加速响应

```python
# 缓存常见查询
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_retrieve(query):
    return retriever.get_relevant_documents(query)
```

**效果**：响应时间减少50-70%

---

## 七、调试技巧

### 技巧1：查看历史

```python
# 打印当前历史
history = memory.load_memory_variables({})
print(history["chat_history"])
```

### 技巧2：查看改写后的查询

```python
# 启用verbose模式
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    verbose=True  # 显示中间步骤
)
```

### 技巧3：统计Token使用

```python
import tiktoken

def count_tokens(text):
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(text))

# 统计历史token
history_text = str(memory.load_memory_variables({}))
print(f"历史token数：{count_tokens(history_text)}")
```

---

## 八、常见错误与解决

### 错误1：历史过长导致token超限

**症状**：API报错"context_length_exceeded"

**解决**：
```python
# 使用滑动窗口
memory = ConversationBufferWindowMemory(k=5)
```

### 错误2：指代词无法理解

**症状**：系统回答"请问您指的是什么？"

**解决**：
```python
# 启用查询改写
rewritten_query = rewrite_query(query, chat_history)
```

### 错误3：检索到无关文档

**症状**：回答与问题不相关

**解决**：
```python
# 增加检索数量，使用ReRank
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 10}  # 检索更多文档
)
```

---

## 九、快速检查清单

在实现对话式RAG时，检查以下要点：

- [ ] **Memory已配置**：选择合适的Memory策略
- [ ] **窗口大小合理**：根据场景设置k值（3-10）
- [ ] **查询改写启用**：处理指代词
- [ ] **Token监控**：避免超出context window
- [ ] **历史清理**：提供清空历史的功能
- [ ] **错误处理**：捕获API错误
- [ ] **日志记录**：记录对话历史和检索结果
- [ ] **性能测试**：测试响应时间和准确率

---

## 十、学习路径

### 第1天：基础实现（2小时）

1. 安装依赖：`pip install langchain openai chromadb`
2. 实现基础对话RAG（使用ConversationBufferMemory）
3. 测试多轮对话

### 第2天：优化策略（2小时）

1. 实现滑动窗口（ConversationBufferWindowMemory）
2. 实现查询改写
3. 对比优化前后的效果

### 第3天：生产部署（2小时）

1. 添加错误处理
2. 实现会话持久化
3. 添加性能监控

**总计6小时**，掌握对话式RAG的核心技能。

---

## 十一、最小可用代码模板

```python
"""
对话式RAG最小可用模板
功能：支持多轮对话、历史压缩、指代消解
"""

from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader

# 1. 加载文档
loader = TextLoader("your_document.txt")
documents = loader.load()

# 2. 分块
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = text_splitter.split_documents(documents)

# 3. 创建向量存储
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# 4. 创建LLM
llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")

# 5. 创建Memory（滑动窗口）
memory = ConversationBufferWindowMemory(
    k=5,
    memory_key="chat_history",
    return_messages=True
)

# 6. 创建对话链
chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory,
    return_source_documents=True
)

# 7. 对话循环
print("对话式RAG系统已启动（输入'quit'退出）")
while True:
    query = input("\n用户: ")
    if query.lower() == "quit":
        break

    response = chain({"question": query})
    print(f"系统: {response['answer']}")

    # 可选：显示来源
    if response.get("source_documents"):
        print(f"\n来源：{len(response['source_documents'])}个文档")
```

**这个模板包含了对话式RAG的所有核心功能，可以直接运行。**

---

## 十二、关键数字记忆

| 指标 | 推荐值 | 说明 |
|------|--------|------|
| **窗口大小** | 5 | 保留最近5轮对话 |
| **Token阈值** | 2000 | 超过2000 tokens开始压缩 |
| **检索数量** | 3-5 | 每次检索3-5个文档 |
| **Chunk大小** | 500 | 文档分块大小500字符 |
| **Overlap** | 50 | 分块重叠50字符 |
| **Temperature** | 0 | 生成温度设为0（确定性） |

---

## 总结

掌握以下20%的核心知识，就能解决80%的对话式RAG问题：

1. **三大核心技术**：上下文管理、历史压缩、指代消解
2. **最小可用架构**：5个组件（LLM、Embeddings、向量存储、Memory、对话链）
3. **关键决策点**：Memory策略、压缩时机、查询改写
4. **常见场景配置**：技术文档、客服、教育
5. **核心API**：Memory API、ConversationalRetrievalChain API
6. **性能优化**：减少Token、提升检索、加速响应
7. **调试技巧**：查看历史、查看改写、统计Token
8. **常见错误**：历史过长、指代不理解、检索无关
9. **检查清单**：8个要点
10. **学习路径**：3天6小时

**记住**：对话式RAG的核心是**记住历史、理解指代、智能压缩**，掌握这三点就掌握了80%。

---

**版本**：v1.0
**最后更新**：2026-02-17
**维护者**：Claude Code
**基于**：2025-2026年最新研究和生产实践
