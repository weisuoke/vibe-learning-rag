# 核心概念：并发与批处理优化

> **本节目标**：掌握LCEL的并发和批处理机制，实现10-20x性能提升。

---

## 一、性能优化的必要性

### 1.1 性能瓶颈

**LLM API调用特点**：
- 延迟：1-3秒/请求
- 成本：$0.001-0.01/请求
- 限流：10-100 RPM

**串行处理问题**：
```python
# ❌ 串行：100个请求需要100-300秒
for input in inputs:
    result = chain.invoke(input)
```

### 1.2 优化收益

| 优化方式 | 性能提升 | 适用场景 |
|----------|----------|----------|
| 并发执行 | 10-20x | IO密集型 |
| 批处理 | 5-10x | 批量请求 |
| 异步处理 | 10-20x | 高并发 |

---

## 二、batch()：批量处理

### 2.1 基础用法

```python
# 串行处理
results = [chain.invoke(input) for input in inputs]
# 时间：N × T

# 批量处理
results = chain.batch(inputs)
# 时间：N × T / concurrency
```

### 2.2 并发控制

```python
# 默认并发（无限制）
results = chain.batch(inputs)

# 控制并发数
results = chain.batch(
    inputs,
    config={"max_concurrency": 10}
)
```

### 2.3 实战示例

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("总结：{text}")
llm = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | llm

# 批量处理100个文档
documents = ["文档1", "文档2", ..., "文档100"]

# 方式1：串行（慢）
results = [chain.invoke({"text": doc}) for doc in documents]
# 时间：~200秒

# 方式2：批处理（快）
results = chain.batch([{"text": doc} for doc in documents])
# 时间：~20秒（10x提升）

# 方式3：控制并发
results = chain.batch(
    [{"text": doc} for doc in documents],
    config={"max_concurrency": 20}
)
# 时间：~10秒（20x提升）
```

---

## 三、abatch()：异步批处理

### 3.1 基础用法

```python
import asyncio

# 异步批处理
results = await chain.abatch(inputs)
```

### 3.2 性能对比

```python
import time

# 同步批处理
start = time.time()
results = chain.batch(inputs)
print(f"同步：{time.time() - start}秒")

# 异步批处理
start = time.time()
results = await chain.abatch(inputs)
print(f"异步：{time.time() - start}秒")
# 异步通常快20-30%
```

### 3.3 实战示例

```python
async def process_documents_async(documents: list[str]):
    """异步批量处理文档"""
    chain = prompt | llm | parser

    # 异步批处理
    results = await chain.abatch(
        [{"text": doc} for doc in documents],
        config={"max_concurrency": 20}
    )

    return results

# 使用
documents = load_documents()
results = asyncio.run(process_documents_async(documents))
```

---

## 四、gather_with_concurrency()：精细并发控制

### 4.1 核心原理

**gather_with_concurrency** 使用信号量控制并发数。

```python
from langchain_core.runnables import gather_with_concurrency

# 控制并发数为10
results = await gather_with_concurrency(
    max_concurrency=10,
    *[chain.ainvoke(input) for input in inputs]
)
```

### 4.2 实战场景：高吞吐量处理

```python
async def process_high_volume(inputs: list, max_concurrency: int = 10):
    """高吞吐量处理"""
    tasks = [chain.ainvoke(input) for input in inputs]

    results = await gather_with_concurrency(
        max_concurrency,
        *tasks
    )

    return results

# 处理1000个请求
inputs = range(1000)
results = await process_high_volume(inputs, max_concurrency=50)
```

### 4.3 动态并发调整

```python
class AdaptiveConcurrency:
    """自适应并发控制"""

    def __init__(self, initial: int = 10):
        self.concurrency = initial
        self.error_rate = 0.0

    async def process(self, inputs: list):
        while inputs:
            batch = inputs[:self.concurrency]
            inputs = inputs[self.concurrency:]

            try:
                results = await gather_with_concurrency(
                    self.concurrency,
                    *[chain.ainvoke(input) for input in batch]
                )

                # 成功：增加并发
                if self.error_rate < 0.01:
                    self.concurrency = min(100, self.concurrency + 5)

            except Exception as e:
                # 失败：降低并发
                self.error_rate += 0.1
                self.concurrency = max(1, self.concurrency - 5)
                raise

        return results
```

---

## 五、batch_as_completed()：流式批处理

### 5.1 基础用法

```python
# 等待所有完成
results = chain.batch(inputs)

# 流式返回（先完成先返回）
for result in chain.batch_as_completed(inputs):
    print(f"完成：{result}")
```

### 5.2 实战场景：实时反馈

```python
def process_with_progress(inputs: list):
    """带进度的批处理"""
    total = len(inputs)
    completed = 0

    for result in chain.batch_as_completed(inputs):
        completed += 1
        progress = completed / total * 100
        print(f"进度：{progress:.1f}% ({completed}/{total})")
        yield result

# 使用
for result in process_with_progress(documents):
    save_result(result)
```

---

## 六、RunnableParallel：并发执行

### 6.1 基础用法

```python
from langchain_core.runnables import RunnableParallel

# 并发执行多个链
parallel = RunnableParallel({
    "summary": summary_chain,
    "keywords": keyword_chain,
    "sentiment": sentiment_chain
})

# 一次调用，获取所有结果
results = parallel.invoke({"text": "..."})
# {
#   "summary": "...",
#   "keywords": [...],
#   "sentiment": "positive"
# }
```

### 6.2 性能提升

```python
# 串行：T1 + T2 + T3
summary = summary_chain.invoke(text)
keywords = keyword_chain.invoke(text)
sentiment = sentiment_chain.invoke(text)
# 总时间：6秒

# 并行：max(T1, T2, T3)
results = RunnableParallel({
    "summary": summary_chain,
    "keywords": keyword_chain,
    "sentiment": sentiment_chain
}).invoke({"text": text})
# 总时间：2秒（3x提升）
```

---

## 七、性能优化最佳实践

### 7.1 选择合适的方法

| 场景 | 推荐方法 | 原因 |
|------|----------|------|
| 批量处理 | batch() | 简单高效 |
| 高并发 | abatch() | 异步性能更好 |
| 精细控制 | gather_with_concurrency() | 可控并发数 |
| 实时反馈 | batch_as_completed() | 流式返回 |
| 独立操作 | RunnableParallel | 并发执行 |

### 7.2 并发数设置

```python
# 经验值
CONCURRENCY_LIMITS = {
    "openai": 50,        # OpenAI API
    "anthropic": 20,     # Anthropic API
    "local_model": 4,    # 本地模型（CPU核心数）
    "database": 10       # 数据库连接池
}

# 使用
max_concurrency = CONCURRENCY_LIMITS["openai"]
results = chain.batch(inputs, config={"max_concurrency": max_concurrency})
```

### 7.3 错误处理

```python
# 批处理中的错误处理
results = []
for input in inputs:
    try:
        result = chain.invoke(input)
        results.append(result)
    except Exception as e:
        logger.error(f"处理失败：{e}")
        results.append(None)

# 或使用return_exceptions
results = await asyncio.gather(
    *[chain.ainvoke(input) for input in inputs],
    return_exceptions=True
)
```

---

## 八、综合实战案例

### 场景：大规模文档处理系统

```python
import asyncio
from typing import List
from langchain_core.runnables import gather_with_concurrency

class DocumentProcessor:
    """大规模文档处理器"""

    def __init__(self, max_concurrency: int = 20):
        self.max_concurrency = max_concurrency
        self.chain = self._build_chain()

    def _build_chain(self):
        """构建处理链"""
        return (
            prompt
            | llm.with_retry(stop_after_attempt=3)
            | parser
        )

    async def process_batch(
        self,
        documents: List[str],
        batch_size: int = 100
    ) -> List[str]:
        """批量处理文档"""
        results = []

        # 分批处理
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]

            # 异步批处理
            batch_results = await self.chain.abatch(
                [{"text": doc} for doc in batch],
                config={"max_concurrency": self.max_concurrency}
            )

            results.extend(batch_results)

            # 进度反馈
            progress = min(i + batch_size, len(documents))
            print(f"已处理：{progress}/{len(documents)}")

        return results

    async def process_with_priority(
        self,
        high_priority: List[str],
        low_priority: List[str]
    ):
        """优先级处理"""
        # 先处理高优先级
        high_results = await self.process_batch(high_priority)

        # 再处理低优先级
        low_results = await self.process_batch(low_priority)

        return {
            "high": high_results,
            "low": low_results
        }

# 使用
processor = DocumentProcessor(max_concurrency=30)

# 处理10000个文档
documents = load_documents(10000)
results = asyncio.run(processor.process_batch(documents))
```

---

## 九、总结

### 核心要点

1. **batch()**：批量处理，10x提升
2. **abatch()**：异步批处理，20x提升
3. **gather_with_concurrency()**：精细并发控制
4. **RunnableParallel**：独立操作并发

### 性能优化路径

```
串行处理（基准）
  ↓ batch()
批量处理（10x）
  ↓ abatch()
异步批处理（20x）
  ↓ gather_with_concurrency()
精细并发控制（最优）
```

### 实施建议

- ✅ 优先使用batch()进行批量处理
- ✅ 高并发场景使用abatch()
- ✅ 根据下游服务能力设置并发数
- ✅ 使用RunnableParallel并发独立操作
- ✅ 监控性能指标持续优化

---

**下一步**：学习【缓存策略与实现】，进一步降低成本和延迟。
