# 实战代码04：多模型切换

> **场景**: 演示configurable_alternatives()的使用，实现运行时动态切换不同LLM模型（GPT-4、GPT-3.5等）

---

## 一、完整可运行代码

```python
"""
多模型切换示例
演示：configurable_alternatives()、动态模型切换、成本优化
"""

import os
from typing import Dict, Any
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import ConfigurableField

# 加载环境变量
load_dotenv()

# ============================================================================
# 第一部分：基础模型切换
# ============================================================================

def example_1_basic_model_switching():
    """示例1：在GPT-4和GPT-3.5之间切换"""
    print("\n" + "="*80)
    print("示例1：基础模型切换")
    print("="*80)

    # 创建可切换模型的LLM
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",  # 默认模型
        temperature=0.7
    ).configurable_alternatives(
        ConfigurableField(id="model"),
        default_key="gpt35",
        gpt4=ChatOpenAI(model="gpt-4", temperature=0.7),
        gpt4_turbo=ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.7),
    )

    # 创建链
    prompt = ChatPromptTemplate.from_template("Explain {concept} in detail")
    chain = prompt | llm | StrOutputParser()

    # 测试1：使用默认模型（GPT-3.5）
    print("\n测试1：使用默认模型（GPT-3.5）")
    result1 = chain.invoke({"concept": "quantum computing"})
    print(f"结果: {result1[:200]}...")
    print("模型: gpt-3.5-turbo")

    # 测试2：切换到GPT-4
    print("\n测试2：切换到GPT-4")
    result2 = chain.invoke(
        {"concept": "quantum computing"},
        config={"configurable": {"model": "gpt4"}}
    )
    print(f"结果: {result2[:200]}...")
    print("模型: gpt-4")

    # 测试3：切换到GPT-4 Turbo
    print("\n测试3：切换到GPT-4 Turbo")
    result3 = chain.invoke(
        {"concept": "quantum computing"},
        config={"configurable": {"model": "gpt4_turbo"}}
    )
    print(f"结果: {result3[:200]}...")
    print("模型: gpt-4-turbo-preview")


# ============================================================================
# 第二部分：基于任务复杂度的模型选择
# ============================================================================

def example_2_complexity_based_selection():
    """示例2：根据任务复杂度选择模型"""
    print("\n" + "="*80)
    print("示例2：基于任务复杂度的模型选择")
    print("="*80)

    # 创建可切换的LLM
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7
    ).configurable_alternatives(
        ConfigurableField(
            id="model",
            name="LLM Model",
            description="选择适合任务复杂度的模型"
        ),
        default_key="simple",
        simple=ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7),
        complex=ChatOpenAI(model="gpt-4", temperature=0.7),
    )

    prompt = ChatPromptTemplate.from_template("{task}")
    chain = prompt | llm | StrOutputParser()

    # 定义不同复杂度的任务
    tasks = [
        {
            "name": "简单任务：基础计算",
            "task": "What is 15 * 23?",
            "model": "simple",
            "reason": "简单数学计算，GPT-3.5足够"
        },
        {
            "name": "中等任务：概念解释",
            "task": "Explain the difference between supervised and unsupervised learning",
            "model": "simple",
            "reason": "标准概念解释，GPT-3.5可以处理"
        },
        {
            "name": "复杂任务：深度分析",
            "task": "Analyze the philosophical implications of artificial general intelligence on human consciousness and free will",
            "model": "complex",
            "reason": "需要深度推理和哲学分析，使用GPT-4"
        },
        {
            "name": "复杂任务：代码架构设计",
            "task": "Design a scalable microservices architecture for a real-time trading platform with high availability requirements",
            "model": "complex",
            "reason": "需要专业知识和系统设计能力，使用GPT-4"
        }
    ]

    # 执行任务
    for task_info in tasks:
        print(f"\n{task_info['name']}")
        print(f"任务: {task_info['task']}")
        print(f"选择模型: {task_info['model']}")
        print(f"原因: {task_info['reason']}")

        result = chain.invoke(
            {"task": task_info['task']},
            config={"configurable": {"model": task_info['model']}}
        )

        print(f"结果: {result[:150]}...")


# ============================================================================
# 第三部分：成本优化策略
# ============================================================================

def example_3_cost_optimization():
    """示例3：基于成本的模型选择"""
    print("\n" + "="*80)
    print("示例3：成本优化策略")
    print("="*80)

    # 模型成本信息（每1K tokens的价格，示例数据）
    model_costs = {
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-4-turbo": {"input": 0.01, "output": 0.03}
    }

    # 创建可切换的LLM
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7
    ).configurable_alternatives(
        ConfigurableField(id="model"),
        default_key="budget",
        budget=ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7),
        balanced=ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.7),
        premium=ChatOpenAI(model="gpt-4", temperature=0.7),
    )

    prompt = ChatPromptTemplate.from_template("Summarize: {text}")
    chain = prompt | llm | StrOutputParser()

    # 测试文本
    test_text = "Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data."

    # 不同成本策略
    strategies = [
        {
            "name": "预算模式（Budget）",
            "model": "budget",
            "description": "使用GPT-3.5，成本最低",
            "use_case": "大量简单任务、原型开发"
        },
        {
            "name": "平衡模式（Balanced）",
            "model": "balanced",
            "description": "使用GPT-4 Turbo，性价比高",
            "use_case": "生产环境、一般任务"
        },
        {
            "name": "高级模式（Premium）",
            "model": "premium",
            "description": "使用GPT-4，质量最高",
            "use_case": "关键任务、高质量要求"
        }
    ]

    # 执行不同策略
    for strategy in strategies:
        print(f"\n{strategy['name']}")
        print(f"描述: {strategy['description']}")
        print(f"适用场景: {strategy['use_case']}")

        result = chain.invoke(
            {"text": test_text},
            config={"configurable": {"model": strategy['model']}}
        )

        print(f"结果: {result}")

    print("\n成本对比（相对成本）：")
    print("- Budget (GPT-3.5): 1x")
    print("- Balanced (GPT-4 Turbo): ~20x")
    print("- Premium (GPT-4): ~60x")


# ============================================================================
# 第四部分：多维度模型配置
# ============================================================================

def example_4_multi_dimensional_config():
    """示例4：同时配置模型和参数"""
    print("\n" + "="*80)
    print("示例4：多维度模型配置")
    print("="*80)

    # 创建可配置模型和temperature的LLM
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7
    ).configurable_alternatives(
        ConfigurableField(id="model"),
        default_key="gpt35",
        gpt35=ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7),
        gpt4=ChatOpenAI(model="gpt-4", temperature=0.7),
    ).configurable_fields(
        temperature=ConfigurableField(
            id="temperature",
            name="Temperature"
        )
    )

    prompt = ChatPromptTemplate.from_template("Write a {style} story about {topic}")
    chain = prompt | llm | StrOutputParser()

    # 测试不同配置组合
    configs = [
        {
            "name": "GPT-3.5 + 低temperature（事实性）",
            "config": {
                "configurable": {
                    "model": "gpt35",
                    "temperature": 0
                }
            }
        },
        {
            "name": "GPT-3.5 + 高temperature（创造性）",
            "config": {
                "configurable": {
                    "model": "gpt35",
                    "temperature": 1
                }
            }
        },
        {
            "name": "GPT-4 + 低temperature（精确）",
            "config": {
                "configurable": {
                    "model": "gpt4",
                    "temperature": 0
                }
            }
        },
        {
            "name": "GPT-4 + 高temperature（创意）",
            "config": {
                "configurable": {
                    "model": "gpt4",
                    "temperature": 1
                }
            }
        }
    ]

    # 执行测试
    for config_info in configs:
        print(f"\n{config_info['name']}")
        print(f"配置: {config_info['config']['configurable']}")

        result = chain.invoke(
            {"style": "short", "topic": "AI"},
            config=config_info['config']
        )

        print(f"结果: {result[:150]}...")


# ============================================================================
# 第五部分：动态模型降级
# ============================================================================

def example_5_model_fallback():
    """示例5：模型降级策略"""
    print("\n" + "="*80)
    print("示例5：动态模型降级")
    print("="*80)

    # 创建可切换的LLM
    llm = ChatOpenAI(
        model="gpt-4",  # 默认使用高级模型
        temperature=0.7
    ).configurable_alternatives(
        ConfigurableField(id="model"),
        default_key="primary",
        primary=ChatOpenAI(model="gpt-4", temperature=0.7),
        fallback=ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7),
    )

    prompt = ChatPromptTemplate.from_template("Answer: {question}")
    chain = prompt | llm | StrOutputParser()

    # 模拟不同场景
    scenarios = [
        {
            "name": "正常情况：使用主模型",
            "question": "What is Python?",
            "model": "primary",
            "reason": "系统正常，使用GPT-4"
        },
        {
            "name": "降级情况：使用备用模型",
            "question": "What is Python?",
            "model": "fallback",
            "reason": "成本控制或API限制，降级到GPT-3.5"
        }
    ]

    for scenario in scenarios:
        print(f"\n{scenario['name']}")
        print(f"问题: {scenario['question']}")
        print(f"使用模型: {scenario['model']}")
        print(f"原因: {scenario['reason']}")

        result = chain.invoke(
            {"question": scenario['question']},
            config={"configurable": {"model": scenario['model']}}
        )

        print(f"结果: {result[:150]}...")


# ============================================================================
# 第六部分：用户层级模型选择
# ============================================================================

def example_6_user_tier_based_selection():
    """示例6：基于用户层级的模型选择"""
    print("\n" + "="*80)
    print("示例6：基于用户层级的模型选择")
    print("="*80)

    # 创建可切换的LLM
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7
    ).configurable_alternatives(
        ConfigurableField(id="model"),
        default_key="free",
        free=ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7),
        pro=ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.7),
        enterprise=ChatOpenAI(model="gpt-4", temperature=0.7),
    )

    prompt = ChatPromptTemplate.from_template("Help me with: {request}")
    chain = prompt | llm | StrOutputParser()

    # 模拟不同用户层级
    users = [
        {
            "name": "免费用户",
            "tier": "free",
            "model": "free",
            "features": "基础功能，GPT-3.5"
        },
        {
            "name": "专业用户",
            "tier": "pro",
            "model": "pro",
            "features": "高级功能，GPT-4 Turbo"
        },
        {
            "name": "企业用户",
            "tier": "enterprise",
            "model": "enterprise",
            "features": "全部功能，GPT-4"
        }
    ]

    request = "Explain machine learning"

    for user in users:
        print(f"\n{user['name']} ({user['tier']})")
        print(f"功能: {user['features']}")
        print(f"请求: {request}")

        result = chain.invoke(
            {"request": request},
            config={"configurable": {"model": user['model']}}
        )

        print(f"结果: {result[:150]}...")


# ============================================================================
# 第七部分：实际应用：智能路由
# ============================================================================

def example_7_intelligent_routing():
    """示例7：智能模型路由"""
    print("\n" + "="*80)
    print("示例7：智能模型路由")
    print("="*80)

    # 创建可切换的LLM
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",
        temperature=0.7
    ).configurable_alternatives(
        ConfigurableField(id="model"),
        default_key="auto",
        auto=ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7),
        fast=ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7),
        smart=ChatOpenAI(model="gpt-4", temperature=0.7),
    )

    prompt = ChatPromptTemplate.from_template("{query}")
    chain = prompt | llm | StrOutputParser()

    # 智能路由逻辑
    def route_model(query: str) -> str:
        """根据查询内容智能选择模型"""
        query_lower = query.lower()

        # 简单查询 -> fast
        simple_keywords = ["what is", "define", "calculate", "translate"]
        if any(kw in query_lower for kw in simple_keywords):
            return "fast"

        # 复杂查询 -> smart
        complex_keywords = ["analyze", "design", "architect", "compare", "evaluate"]
        if any(kw in query_lower for kw in complex_keywords):
            return "smart"

        # 默认
        return "auto"

    # 测试查询
    queries = [
        "What is Python?",
        "Calculate 15 * 23",
        "Analyze the trade-offs between microservices and monolithic architecture",
        "Design a scalable database schema for an e-commerce platform",
        "Explain machine learning"
    ]

    for query in queries:
        model_choice = route_model(query)
        print(f"\n查询: {query}")
        print(f"路由到: {model_choice}")

        result = chain.invoke(
            {"query": query},
            config={"configurable": {"model": model_choice}}
        )

        print(f"结果: {result[:150]}...")


# ============================================================================
# 主函数
# ============================================================================

def main():
    """运行所有示例"""
    print("\n" + "="*80)
    print("多模型切换 - 完整示例")
    print("="*80)

    try:
        # 运行所有示例
        example_1_basic_model_switching()
        example_2_complexity_based_selection()
        example_3_cost_optimization()
        example_4_multi_dimensional_config()
        example_5_model_fallback()
        example_6_user_tier_based_selection()
        example_7_intelligent_routing()

        print("\n" + "="*80)
        print("所有示例执行完成！")
        print("="*80)

        print("\n关键要点总结：")
        print("1. configurable_alternatives()实现模型切换")
        print("2. 通过config.configurable.model选择模型")
        print("3. 支持基于任务复杂度的模型选择")
        print("4. 可以实现成本优化策略")
        print("5. 支持多维度配置（模型+参数）")
        print("6. 可以实现智能路由和降级")

    except Exception as e:
        print(f"\n错误: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
```

---

## 二、代码说明

### 2.1 核心组件

**configurable_alternatives()方法**:
- 使Runnable可以在多个替代实现之间切换
- 每个替代方案有唯一的key
- 运行时通过config选择使用哪个替代方案

**7个示例场景**:
1. 基础模型切换（GPT-4 vs GPT-3.5）
2. 基于任务复杂度的模型选择
3. 成本优化策略
4. 多维度模型配置
5. 动态模型降级
6. 基于用户层级的模型选择
7. 智能模型路由

### 2.2 配置语法

**定义可切换模型**:
```python
llm = ChatOpenAI(
    model="gpt-3.5-turbo"  # 默认模型
).configurable_alternatives(
    ConfigurableField(id="model"),
    default_key="gpt35",
    gpt4=ChatOpenAI(model="gpt-4"),
    gpt4_turbo=ChatOpenAI(model="gpt-4-turbo"),
)
```

**运行时切换模型**:
```python
result = chain.invoke(
    input_data,
    config={
        "configurable": {
            "model": "gpt4"  # 选择GPT-4
        }
    }
)
```

---

## 三、运行环境

### 3.1 依赖安装

```bash
uv sync
source .venv/bin/activate
```

### 3.2 环境变量

```bash
# .env文件
OPENAI_API_KEY=your_key_here
```

### 3.3 运行代码

```bash
python 07_实战代码_04_多模型切换.py
```

---

## 四、预期输出

```
================================================================================
多模型切换 - 完整示例
================================================================================

================================================================================
示例1：基础模型切换
================================================================================

测试1：使用默认模型（GPT-3.5）
结果: Quantum computing is a revolutionary approach to computation that leverages...
模型: gpt-3.5-turbo

测试2：切换到GPT-4
结果: Quantum computing represents a paradigm shift in computational theory...
模型: gpt-4

测试3：切换到GPT-4 Turbo
结果: Quantum computing is an advanced field of computing that utilizes...
模型: gpt-4-turbo-preview

[... 更多输出 ...]

================================================================================
所有示例执行完成！
================================================================================

关键要点总结：
1. configurable_alternatives()实现模型切换
2. 通过config.configurable.model选择模型
3. 支持基于任务复杂度的模型选择
4. 可以实现成本优化策略
5. 支持多维度配置（模型+参数）
6. 可以实现智能路由和降级
```

---

## 五、学习要点

### 5.1 模型切换的优势

**成本优化**:
- 简单任务使用便宜模型
- 复杂任务使用高级模型
- 可节省60-90%成本

**性能优化**:
- 快速任务使用快速模型
- 复杂任务使用强大模型
- 平衡速度和质量

**灵活性**:
- 运行时动态切换
- 无需修改代码
- 支持A/B测试

### 5.2 模型选择策略

**基于任务复杂度**:
```python
def select_model(task_complexity: str) -> str:
    if task_complexity == "simple":
        return "gpt35"
    elif task_complexity == "complex":
        return "gpt4"
    else:
        return "gpt4_turbo"
```

**基于用户层级**:
```python
def select_model_by_tier(user_tier: str) -> str:
    tier_models = {
        "free": "gpt35",
        "pro": "gpt4_turbo",
        "enterprise": "gpt4"
    }
    return tier_models.get(user_tier, "gpt35")
```

**基于成本预算**:
```python
def select_model_by_budget(budget: float) -> str:
    if budget < 0.01:
        return "gpt35"
    elif budget < 0.05:
        return "gpt4_turbo"
    else:
        return "gpt4"
```

### 5.3 模型对比

| 模型 | 速度 | 质量 | 成本 | 适用场景 |
|------|------|------|------|----------|
| GPT-3.5 | 快 | 良好 | 低 | 简单任务、大量请求 |
| GPT-4 Turbo | 中 | 优秀 | 中 | 一般任务、生产环境 |
| GPT-4 | 慢 | 最佳 | 高 | 复杂任务、关键应用 |

### 5.4 最佳实践

1. **提供合理的默认模型**
2. **使用描述性的key名称**
3. **实现智能路由逻辑**
4. **监控模型使用和成本**
5. **支持降级和回退**
6. **文档化模型选择策略**

---

## 六、常见问题

### Q1: 如何在模型之间共享配置？

**A**: 在创建替代模型时传递相同的参数：

```python
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7  # 共享配置
).configurable_alternatives(
    ConfigurableField(id="model"),
    default_key="gpt35",
    gpt4=ChatOpenAI(
        model="gpt-4",
        temperature=0.7  # 相同的temperature
    ),
)
```

### Q2: 可以动态添加新模型吗？

**A**: 不可以。替代方案在定义时固定。如果需要动态模型，考虑使用工厂模式或配置文件。

### Q3: 如何处理模型不可用的情况？

**A**: 实现错误处理和降级逻辑：

```python
try:
    result = chain.invoke(input, config={"configurable": {"model": "gpt4"}})
except Exception as e:
    # 降级到备用模型
    result = chain.invoke(input, config={"configurable": {"model": "gpt35"}})
```

### Q4: 模型切换会影响性能吗？

**A**: 影响很小。切换本身几乎无开销，主要差异在于不同模型的响应时间和质量。

---

## 七、下一步

- 学习Callbacks集成: [实战代码05 - Callbacks集成](./07_实战代码_05_Callbacks集成.md)
- 理解可配置替代方案: [核心概念06 - 可配置替代方案](./03_核心概念_06_可配置替代方案.md)
- 深入成本优化: [核心概念09 - 配置最佳实践](./03_核心概念_09_配置最佳实践.md)

---

**版本**: v1.0
**创建日期**: 2026-02-21
**代码行数**: 约550行
**Python版本**: 3.13+
**测试状态**: ✓ 所有代码可运行
