# 实战代码2：RAG管道构建

## 概述

本节提供完整可运行的 RAG（检索增强生成）管道示例，展示如何使用管道操作符构建生产级 RAG 系统。

**学习目标**：
- 掌握 RAG 管道的构建方法
- 理解检索器与生成器的组合
- 能够处理多源检索和重排序

---

## 1. 基础 RAG 管道

### 1.1 完整示例

```python
"""
基础 RAG 管道
演示：retriever | prompt | model | parser
"""

import os
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# 加载环境变量
load_dotenv()

# ===== 1. 准备文档 =====
print("=== 准备文档 ===")

documents = [
    "LangChain 是一个用于构建 AI 应用的框架，提供了丰富的组件和工具。",
    "LCEL（LangChain Expression Language）是 LangChain 的表达式语言，使用管道操作符 | 组合组件。",
    "Runnable 是 LangChain 的核心抽象，所有组件都实现了 Runnable 接口。",
    "管道操作符通过 Python 的 __or__ 方法重载实现，返回 RunnableSequence。",
    "RAG（检索增强生成）结合了检索和生成，可以基于外部知识回答问题。"
]

print(f"文档数量: {len(documents)}")

# ===== 2. 创建向量存储 =====
print("\n=== 创建向量存储 ===")

vector_store = Chroma.from_texts(
    documents,
    embedding=OpenAIEmbeddings(),
    collection_name="langchain_docs"
)

print("向量存储创建完成")

# ===== 3. 创建检索器 =====
print("\n=== 创建检索器 ===")

retriever = vector_store.as_retriever(
    search_kwargs={"k": 2}  # 返回最相关的2个文档
)

print(f"检索器类型: {type(retriever)}")

# ===== 4. 构建 RAG 管道 =====
print("\n=== 构建 RAG 管道 ===")

# 提示模板
prompt = ChatPromptTemplate.from_template(
    """基于以下上下文回答问题：

上下文：
{context}

问题：{question}

答案："""
)

# 语言模型
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 输出解析器
parser = StrOutputParser()

# 组合成 RAG 管道
rag_chain = (
    {
        "context": retriever,  # 检索相关文档
        "question": RunnablePassthrough()  # 传递原始问题
    }
    | prompt
    | model
    | parser
)

print(f"RAG 管道类型: {type(rag_chain)}")

# ===== 5. 使用 RAG 管道 =====
print("\n=== 使用 RAG 管道 ===")

questions = [
    "什么是 LangChain？",
    "什么是 LCEL？",
    "什么是 RAG？"
]

for question in questions:
    print(f"\n问题: {question}")
    answer = rag_chain.invoke(question)
    print(f"答案: {answer}")

# ===== 6. 查看检索到的文档 =====
print("\n=== 查看检索到的文档 ===")

question = "什么是管道操作符？"
docs = retriever.invoke(question)

print(f"问题: {question}")
print(f"检索到 {len(docs)} 个文档:")
for i, doc in enumerate(docs):
    print(f"\n文档 {i+1}:")
    print(doc.page_content)
```

**运行输出示例**：
```
=== 准备文档 ===
文档数量: 5

=== 创建向量存储 ===
向量存储创建完成

=== 创建检索器 ===
检索器类型: <class 'langchain_core.vectorstores.VectorStoreRetriever'>

=== 构建 RAG 管道 ===
RAG 管道类型: <class 'langchain_core.runnables.base.RunnableSequence'>

=== 使用 RAG 管道 ===

问题: 什么是 LangChain？
答案: LangChain 是一个用于构建 AI 应用的框架，提供了丰富的组件和工具。

问题: 什么是 LCEL？
答案: LCEL（LangChain Expression Language）是 LangChain 的表达式语言，使用管道操作符 | 组合组件。

问题: 什么是 RAG？
答案: RAG（检索增强生成）结合了检索和生成，可以基于外部知识回答问题。
```

---

## 2. 格式化上下文的 RAG 管道

### 2.1 完整示例

```python
"""
格式化上下文的 RAG 管道
演示：自定义上下文格式化
"""

from langchain_core.runnables import RunnableLambda

# ===== 1. 定义格式化函数 =====

def format_docs(docs):
    """格式化检索到的文档"""
    formatted = []
    for i, doc in enumerate(docs):
        formatted.append(f"[文档 {i+1}]\n{doc.page_content}")
    return "\n\n".join(formatted)

# ===== 2. 构建带格式化的 RAG 管道 =====

rag_chain = (
    {
        "context": retriever | RunnableLambda(format_docs),  # 检索并格式化
        "question": RunnablePassthrough()
    }
    | prompt
    | model
    | parser
)

# ===== 3. 使用 =====

question = "什么是 Runnable？"
answer = rag_chain.invoke(question)

print(f"问题: {question}")
print(f"答案: {answer}")

# ===== 4. 查看格式化后的上下文 =====

docs = retriever.invoke(question)
formatted_context = format_docs(docs)

print(f"\n格式化后的上下文:")
print(formatted_context)
```

---

## 3. 多源检索 RAG 管道

### 3.1 完整示例

```python
"""
多源检索 RAG 管道
演示：从多个来源检索并合并结果
"""

from langchain_core.runnables import RunnableParallel, RunnableLambda

# ===== 1. 创建多个向量存储 =====
print("=== 创建多个向量存储 ===")

# 向量存储1: 技术文档
tech_docs = [
    "LangChain 使用 Python 开发，支持异步和批处理。",
    "LCEL 提供了声明式的组件组合方式。"
]

tech_store = Chroma.from_texts(
    tech_docs,
    embedding=OpenAIEmbeddings(),
    collection_name="tech_docs"
)

tech_retriever = tech_store.as_retriever(search_kwargs={"k": 1})

# 向量存储2: 概念文档
concept_docs = [
    "Runnable 是所有组件的基类，定义了统一的执行接口。",
    "管道操作符 | 是 LCEL 的核心语法。"
]

concept_store = Chroma.from_texts(
    concept_docs,
    embedding=OpenAIEmbeddings(),
    collection_name="concept_docs"
)

concept_retriever = concept_store.as_retriever(search_kwargs={"k": 1})

print("多个向量存储创建完成")

# ===== 2. 定义合并函数 =====

def merge_docs(doc_dict):
    """合并多个来源的文档"""
    all_docs = []

    # 从技术文档中获取
    if "tech" in doc_dict:
        all_docs.extend(doc_dict["tech"])

    # 从概念文档中获取
    if "concept" in doc_dict:
        all_docs.extend(doc_dict["concept"])

    # 格式化
    formatted = []
    for i, doc in enumerate(all_docs):
        formatted.append(f"[来源 {i+1}]\n{doc.page_content}")

    return "\n\n".join(formatted)

# ===== 3. 构建多源 RAG 管道 =====
print("\n=== 构建多源 RAG 管道 ===")

multi_source_rag = (
    {
        "context": (
            RunnableParallel(
                tech=tech_retriever,      # 并行检索技术文档
                concept=concept_retriever  # 并行检索概念文档
            )
            | RunnableLambda(merge_docs)  # 合并结果
        ),
        "question": RunnablePassthrough()
    }
    | prompt
    | model
    | parser
)

print("多源 RAG 管道构建完成")

# ===== 4. 使用多源 RAG =====
print("\n=== 使用多源 RAG ===")

question = "什么是 LangChain 的核心概念？"
answer = multi_source_rag.invoke(question)

print(f"问题: {question}")
print(f"答案: {answer}")
```

---

## 4. 带重排序的 RAG 管道

### 4.1 完整示例

```python
"""
带重排序的 RAG 管道
演示：检索后重新排序文档
"""

from langchain_core.runnables import RunnableLambda

# ===== 1. 定义重排序函数 =====

def rerank_docs(docs_and_query):
    """重新排序文档（简化版）"""
    docs = docs_and_query["docs"]
    query = docs_and_query["query"]

    # 简单的重排序：按文档长度排序（实际应用中使用专门的重排序模型）
    scored_docs = []
    for doc in docs:
        # 计算相关性分数（这里简化为文档长度）
        score = len(doc.page_content)
        scored_docs.append((score, doc))

    # 按分数排序
    scored_docs.sort(reverse=True, key=lambda x: x[0])

    # 返回排序后的文档
    return [doc for score, doc in scored_docs]

# ===== 2. 构建带重排序的 RAG 管道 =====

rag_with_rerank = (
    {
        "context": (
            {
                "docs": retriever,
                "query": RunnablePassthrough()
            }
            | RunnableLambda(rerank_docs)  # 重排序
            | RunnableLambda(format_docs)  # 格式化
        ),
        "question": RunnablePassthrough()
    }
    | prompt
    | model
    | parser
)

# ===== 3. 使用 =====

question = "什么是 LCEL？"
answer = rag_with_rerank.invoke(question)

print(f"问题: {question}")
print(f"答案: {answer}")
```

---

## 5. 对话式 RAG 管道

### 5.1 完整示例

```python
"""
对话式 RAG 管道
演示：带历史记录的 RAG 系统
"""

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# ===== 1. 创建对话提示模板 =====

conversational_prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个友好的助手，基于以下上下文回答问题：\n\n{context}"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}")
])

# ===== 2. 构建对话式 RAG 管道 =====

conversational_rag = (
    {
        "context": retriever | RunnableLambda(format_docs),
        "history": lambda x: x.get("history", []),
        "question": lambda x: x["question"]
    }
    | conversational_prompt
    | model
    | parser
)

# ===== 3. 使用对话式 RAG =====
print("=== 对话式 RAG ===")

# 对话历史
history = []

# 第一轮对话
question1 = "什么是 LangChain？"
answer1 = conversational_rag.invoke({
    "question": question1,
    "history": history
})

print(f"\n用户: {question1}")
print(f"助手: {answer1}")

# 更新历史
history.append(HumanMessage(content=question1))
history.append(AIMessage(content=answer1))

# 第二轮对话（引用上下文）
question2 = "它有什么特点？"
answer2 = conversational_rag.invoke({
    "question": question2,
    "history": history
})

print(f"\n用户: {question2}")
print(f"助手: {answer2}")

# 更新历史
history.append(HumanMessage(content=question2))
history.append(AIMessage(content=answer2))

# 第三轮对话
question3 = "如何使用它？"
answer3 = conversational_rag.invoke({
    "question": question3,
    "history": history
})

print(f"\n用户: {question3}")
print(f"助手: {answer3}")
```

---

## 6. 流式 RAG 管道

### 6.1 完整示例

```python
"""
流式 RAG 管道
演示：实时流式输出 RAG 答案
"""

# ===== 1. 构建流式 RAG 管道 =====

streaming_rag = (
    {
        "context": retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | prompt
    | model
    | parser
)

# ===== 2. 流式输出 =====
print("=== 流式 RAG ===")

question = "什么是管道操作符？"
print(f"\n问题: {question}")
print("答案: ", end="", flush=True)

for chunk in streaming_rag.stream(question):
    print(chunk, end="", flush=True)

print()  # 换行
```

---

## 7. 批量 RAG 处理

### 7.1 完整示例

```python
"""
批量 RAG 处理
演示：批量处理多个问题
"""

import time

# ===== 1. 准备批量问题 =====

questions = [
    "什么是 LangChain？",
    "什么是 LCEL？",
    "什么是 Runnable？",
    "什么是 RAG？",
    "什么是管道操作符？"
]

# ===== 2. 方式1: 循环处理（慢） =====
print("=== 循环处理 ===")

start = time.time()
answers_loop = []
for question in questions:
    answer = rag_chain.invoke(question)
    answers_loop.append(answer)
loop_time = time.time() - start

print(f"循环处理耗时: {loop_time:.2f}秒")

# ===== 3. 方式2: 批量处理（快） =====
print("\n=== 批量处理 ===")

start = time.time()
answers_batch = rag_chain.batch(questions)
batch_time = time.time() - start

print(f"批量处理耗时: {batch_time:.2f}秒")

# ===== 4. 显示结果 =====
print("\n=== 批量处理结果 ===")

for i, (question, answer) in enumerate(zip(questions, answers_batch)):
    print(f"\n{i+1}. {question}")
    print(f"   {answer}")

# ===== 5. 性能对比 =====
print(f"\n=== 性能对比 ===")
print(f"循环处理: {loop_time:.2f}秒")
print(f"批量处理: {batch_time:.2f}秒")
print(f"性能提升: {loop_time / batch_time:.2f}x")
```

---

## 8. 异步 RAG 管道

### 8.1 完整示例

```python
"""
异步 RAG 管道
演示：异步并发处理多个 RAG 查询
"""

import asyncio
import time

# ===== 1. 异步 RAG 函数 =====

async def async_rag_query(question):
    """异步 RAG 查询"""
    answer = await rag_chain.ainvoke(question)
    return answer

# ===== 2. 并发处理多个查询 =====

async def concurrent_rag():
    """并发处理多个 RAG 查询"""
    questions = [
        "什么是 LangChain？",
        "什么是 LCEL？",
        "什么是 Runnable？"
    ]

    print("=== 异步并发 RAG ===")
    start = time.time()

    # 创建并发任务
    tasks = [async_rag_query(q) for q in questions]

    # 并发执行
    answers = await asyncio.gather(*tasks)

    elapsed = time.time() - start

    # 显示结果
    for question, answer in zip(questions, answers):
        print(f"\n问题: {question}")
        print(f"答案: {answer}")

    print(f"\n并发处理耗时: {elapsed:.2f}秒")

# ===== 3. 运行 =====

asyncio.run(concurrent_rag())
```

---

## 9. 完整的 RAG 系统

### 9.1 完整示例

```python
"""
完整的 RAG 系统
演示：一个生产级的 RAG 系统
"""

from typing import List, Dict
from langchain_core.documents import Document

class RAGSystem:
    """完整的 RAG 系统"""

    def __init__(self, documents: List[str]):
        """初始化 RAG 系统"""
        # 创建向量存储
        self.vector_store = Chroma.from_texts(
            documents,
            embedding=OpenAIEmbeddings(),
            collection_name="rag_system"
        )

        # 创建检索器
        self.retriever = self.vector_store.as_retriever(
            search_kwargs={"k": 3}
        )

        # 创建 RAG 管道
        self.rag_chain = self._build_chain()

    def _build_chain(self):
        """构建 RAG 管道"""
        prompt = ChatPromptTemplate.from_template(
            """基于以下上下文回答问题：

上下文：
{context}

问题：{question}

答案："""
        )

        model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        parser = StrOutputParser()

        return (
            {
                "context": self.retriever | RunnableLambda(self._format_docs),
                "question": RunnablePassthrough()
            }
            | prompt
            | model
            | parser
        )

    def _format_docs(self, docs: List[Document]) -> str:
        """格式化文档"""
        formatted = []
        for i, doc in enumerate(docs):
            formatted.append(f"[文档 {i+1}]\n{doc.page_content}")
        return "\n\n".join(formatted)

    def query(self, question: str) -> str:
        """查询"""
        return self.rag_chain.invoke(question)

    def query_with_sources(self, question: str) -> Dict:
        """查询并返回来源"""
        # 检索文档
        docs = self.retriever.invoke(question)

        # 生成答案
        answer = self.rag_chain.invoke(question)

        return {
            "question": question,
            "answer": answer,
            "sources": [doc.page_content for doc in docs]
        }

    def batch_query(self, questions: List[str]) -> List[str]:
        """批量查询"""
        return self.rag_chain.batch(questions)

    async def async_query(self, question: str) -> str:
        """异步查询"""
        return await self.rag_chain.ainvoke(question)

    def stream_query(self, question: str):
        """流式查询"""
        for chunk in self.rag_chain.stream(question):
            yield chunk

# ===== 使用 RAG 系统 =====

# 初始化
documents = [
    "LangChain 是一个用于构建 AI 应用的框架。",
    "LCEL 是 LangChain 的表达式语言。",
    "Runnable 是 LangChain 的核心抽象。",
    "管道操作符 | 用于组合组件。",
    "RAG 结合了检索和生成。"
]

rag_system = RAGSystem(documents)

# 1. 简单查询
print("=== 简单查询 ===")
answer = rag_system.query("什么是 LangChain？")
print(f"答案: {answer}")

# 2. 带来源的查询
print("\n=== 带来源的查询 ===")
result = rag_system.query_with_sources("什么是 LCEL？")
print(f"问题: {result['question']}")
print(f"答案: {result['answer']}")
print(f"来源:")
for i, source in enumerate(result['sources']):
    print(f"  {i+1}. {source}")

# 3. 批量查询
print("\n=== 批量查询 ===")
questions = ["什么是 Runnable？", "什么是 RAG？"]
answers = rag_system.batch_query(questions)
for q, a in zip(questions, answers):
    print(f"\n问题: {q}")
    print(f"答案: {a}")

# 4. 流式查询
print("\n=== 流式查询 ===")
print("问题: 什么是管道操作符？")
print("答案: ", end="", flush=True)
for chunk in rag_system.stream_query("什么是管道操作符？"):
    print(chunk, end="", flush=True)
print()
```

---

## 10. RAG 评估

### 10.1 完整示例

```python
"""
RAG 评估
演示：评估 RAG 系统的性能
"""

from typing import List, Tuple

# ===== 1. 准备测试数据 =====

test_cases = [
    ("什么是 LangChain？", "LangChain 是一个用于构建 AI 应用的框架"),
    ("什么是 LCEL？", "LCEL 是 LangChain 的表达式语言"),
    ("什么是 Runnable？", "Runnable 是 LangChain 的核心抽象")
]

# ===== 2. 评估函数 =====

def evaluate_answer(answer: str, expected: str) -> float:
    """评估答案质量（简化版）"""
    # 简单的关键词匹配评分
    keywords = expected.lower().split()
    answer_lower = answer.lower()

    matches = sum(1 for keyword in keywords if keyword in answer_lower)
    score = matches / len(keywords)

    return score

# ===== 3. 批量评估 =====

def evaluate_rag(rag_system, test_cases: List[Tuple[str, str]]):
    """评估 RAG 系统"""
    print("=== RAG 评估 ===")

    scores = []
    for question, expected in test_cases:
        # 生成答案
        answer = rag_system.query(question)

        # 评估
        score = evaluate_answer(answer, expected)
        scores.append(score)

        print(f"\n问题: {question}")
        print(f"答案: {answer}")
        print(f"期望: {expected}")
        print(f"得分: {score:.2f}")

    # 平均分
    avg_score = sum(scores) / len(scores)
    print(f"\n平均得分: {avg_score:.2f}")

    return avg_score

# ===== 4. 运行评估 =====

avg_score = evaluate_rag(rag_system, test_cases)
```

---

## 学习检查

完成本节后，检查是否掌握：

- [ ] 能构建基础的 RAG 管道
- [ ] 能格式化检索到的上下文
- [ ] 能实现多源检索
- [ ] 能添加重排序功能
- [ ] 能构建对话式 RAG
- [ ] 能实现流式 RAG 输出
- [ ] 能批量处理 RAG 查询
- [ ] 能使用异步 RAG
- [ ] 能构建完整的 RAG 系统
- [ ] 能评估 RAG 性能

---

[Source: LangChain Official Docs - https://python.langchain.com/docs/concepts/]
[Source: Pinecone LCEL Tutorial - https://www.pinecone.io/learn/series/langchain/langchain-expression-language]
