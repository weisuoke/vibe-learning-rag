# 核心概念03：请求链路追踪

## 学习目标

深入理解请求链路追踪的原理和实现，掌握在AI Agent API中实现完整的可观测性。

---

## 第一部分：请求链路追踪基础

### 1.1 什么是请求链路追踪？

**定义：** 请求链路追踪（Request Tracing）是追踪一个请求从开始到结束的完整流程，包括所有中间步骤。

**核心概念：**
- **请求ID（Request ID）**：每个请求的唯一标识符
- **链路（Trace）**：一个请求的完整生命周期
- **跨度（Span）**：链路中的一个操作单元

### 1.2 为什么需要请求链路追踪？

#### 场景1：并发请求混淆

**问题：** 多个用户同时请求，日志混在一起

```
10:30:45.100 用户A请求开始
10:30:45.150 用户B请求开始
10:30:45.200 RAG检索完成  ← 是A还是B？
10:30:45.250 用户C请求开始
10:30:45.300 LLM调用完成  ← 是A、B还是C？
```

**解决：** 每个请求分配唯一ID

```
10:30:45.100 [req_A] 用户A请求开始
10:30:45.150 [req_B] 用户B请求开始
10:30:45.200 [req_A] RAG检索完成
10:30:45.250 [req_C] 用户C请求开始
10:30:45.300 [req_B] LLM调用完成
```

#### 场景2：复杂调用链路

**AI Agent请求的典型链路：**

```
用户请求
  ├─ RAG检索
  │   ├─ Embedding生成
  │   ├─ 向量检索
  │   └─ 结果重排序
  ├─ LLM调用
  │   ├─ Prompt构建
  │   ├─ API调用
  │   └─ 响应解析
  └─ 工具调用（可选）
      ├─ 工具1
      └─ 工具2
```

**问题：** 如何知道哪一步最慢？

**解决：** 记录每一步的耗时

```json
{
  "request_id": "req_123",
  "total_duration_ms": 2500,
  "steps": [
    {"step": "rag_search", "duration_ms": 300},
    {"step": "llm_call", "duration_ms": 2000},
    {"step": "tool_call", "duration_ms": 200}
  ]
}
```

### 1.3 请求ID的生成和传播

#### 生成请求ID

```python
import uuid

# 方法1：UUID4（推荐）
request_id = str(uuid.uuid4())
# 输出：'550e8400-e29b-41d4-a716-446655440000'

# 方法2：短ID（更易读）
import secrets
request_id = secrets.token_urlsafe(16)
# 输出：'Xq3jK9mN2pL5rT8w'

# 方法3：带时间戳的ID
import time
request_id = f"req_{int(time.time())}_{secrets.token_hex(4)}"
# 输出：'req_1705318245_a3b2c1d4'
```

#### 在FastAPI中生成和传播

```python
import uuid
from fastapi import FastAPI, Request
import structlog

app = FastAPI()
logger = structlog.get_logger()

@app.middleware("http")
async def add_request_id(request: Request, call_next):
    # 1. 优先使用客户端提供的请求ID
    request_id = request.headers.get("X-Request-ID")

    # 2. 如果没有，生成新的请求ID
    if not request_id:
        request_id = str(uuid.uuid4())

    # 3. 绑定到日志上下文
    structlog.contextvars.bind_contextvars(request_id=request_id)

    # 4. 记录请求开始
    logger.info("request_start",
        method=request.method,
        path=request.url.path,
        client_ip=request.client.host
    )

    # 5. 处理请求
    response = await call_next(request)

    # 6. 记录请求结束
    logger.info("request_end",
        status_code=response.status_code
    )

    # 7. 在响应头中返回请求ID
    response.headers["X-Request-ID"] = request_id

    # 8. 清理上下文
    structlog.contextvars.clear_contextvars()

    return response
```

---

## 第二部分：contextvars深度解析

### 2.1 为什么需要contextvars？

#### 问题：全局变量在异步环境中不安全

```python
# ❌ 错误：使用全局变量
current_request_id = None

@app.get("/api/chat")
async def chat():
    global current_request_id
    current_request_id = str(uuid.uuid4())

    # 问题：如果有并发请求，会互相覆盖
    await process_chat()

async def process_chat():
    # 这里的 current_request_id 可能已经被其他请求覆盖了
    logger.info("processing", request_id=current_request_id)
```

#### 解决：contextvars提供异步安全的上下文

```python
from contextvars import ContextVar

# ✅ 正确：使用contextvars
request_id_var: ContextVar[str] = ContextVar("request_id")

@app.get("/api/chat")
async def chat():
    request_id = str(uuid.uuid4())
    request_id_var.set(request_id)

    # 即使有并发请求，每个请求的上下文都是独立的
    await process_chat()

async def process_chat():
    # 自动获取当前请求的request_id
    request_id = request_id_var.get()
    logger.info("processing", request_id=request_id)
```

### 2.2 contextvars的工作原理

**核心机制：** 每个异步任务有独立的上下文副本

```python
import asyncio
from contextvars import ContextVar

counter_var: ContextVar[int] = ContextVar("counter", default=0)

async def task(name: str):
    # 每个任务设置自己的值
    counter_var.set(int(name))

    # 模拟异步操作
    await asyncio.sleep(0.1)

    # 获取的是自己设置的值，不会被其他任务影响
    value = counter_var.get()
    print(f"Task {name}: {value}")

# 并发运行3个任务
async def main():
    await asyncio.gather(
        task("1"),
        task("2"),
        task("3")
    )

asyncio.run(main())

# 输出：
# Task 1: 1
# Task 2: 2
# Task 3: 3
```

### 2.3 structlog与contextvars集成

#### 配置structlog使用contextvars

```python
import structlog

structlog.configure(
    processors=[
        # 关键：合并contextvars中的变量
        structlog.contextvars.merge_contextvars,

        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)

logger = structlog.get_logger()
```

#### 绑定和清理上下文

```python
# 绑定单个变量
structlog.contextvars.bind_contextvars(request_id="req_123")

# 绑定多个变量
structlog.contextvars.bind_contextvars(
    request_id="req_123",
    user_id="user_456",
    session_id="session_789"
)

# 获取当前上下文
context = structlog.contextvars.get_contextvars()
# 返回：{"request_id": "req_123", "user_id": "user_456", ...}

# 清理上下文
structlog.contextvars.clear_contextvars()

# 清理特定变量
structlog.contextvars.unbind_contextvars("session_id")
```

### 2.4 上下文的生命周期管理

#### 请求级别上下文

```python
@app.middleware("http")
async def request_context_middleware(request: Request, call_next):
    # 请求开始：绑定上下文
    structlog.contextvars.bind_contextvars(
        request_id=str(uuid.uuid4()),
        method=request.method,
        path=request.url.path
    )

    try:
        response = await call_next(request)
        return response
    finally:
        # 请求结束：清理上下文
        structlog.contextvars.clear_contextvars()
```

#### 操作级别上下文

```python
async def call_llm(prompt: str, model: str):
    # 临时绑定操作级别的上下文
    structlog.contextvars.bind_contextvars(
        operation="llm_call",
        model=model
    )

    try:
        logger.info("llm_call_start")
        response = await client.chat.completions.create(...)
        logger.info("llm_call_success")
        return response
    finally:
        # 清理操作级别的上下文
        structlog.contextvars.unbind_contextvars("operation", "model")
```

---

## 第三部分：分布式追踪

### 3.1 什么是分布式追踪？

**定义：** 在微服务架构中，一个请求可能跨越多个服务，分布式追踪用于追踪跨服务的请求链路。

**概念：**
- **Trace ID**：整个请求链路的唯一标识
- **Span ID**：单个操作的唯一标识
- **Parent Span ID**：父操作的Span ID

### 3.2 单体应用中的"分布式"追踪

**即使是单体应用，也可能有"分布式"场景：**

1. **调用外部API**（OpenAI、数据库）
2. **后台任务**
3. **异步操作**

#### 示例：追踪LLM调用链路

```python
import time
import structlog

logger = structlog.get_logger()

async def handle_chat(message: str):
    """处理聊天请求"""
    # 主请求
    structlog.contextvars.bind_contextvars(
        trace_id=str(uuid.uuid4()),
        span_id="span_main"
    )

    logger.info("chat_start")

    # 子操作1：RAG检索
    await rag_search(message)

    # 子操作2：LLM调用
    await call_llm(message)

    logger.info("chat_end")

async def rag_search(query: str):
    """RAG检索"""
    # 创建子span
    parent_span = structlog.contextvars.get_contextvars().get("span_id")
    structlog.contextvars.bind_contextvars(
        span_id="span_rag",
        parent_span_id=parent_span
    )

    start = time.time()
    logger.info("rag_search_start")

    # 模拟检索
    await asyncio.sleep(0.1)

    duration_ms = (time.time() - start) * 1000
    logger.info("rag_search_end", duration_ms=duration_ms)

    # 恢复父span
    structlog.contextvars.bind_contextvars(span_id=parent_span)

async def call_llm(prompt: str):
    """LLM调用"""
    parent_span = structlog.contextvars.get_contextvars().get("span_id")
    structlog.contextvars.bind_contextvars(
        span_id="span_llm",
        parent_span_id=parent_span
    )

    start = time.time()
    logger.info("llm_call_start")

    # 模拟LLM调用
    await asyncio.sleep(0.5)

    duration_ms = (time.time() - start) * 1000
    logger.info("llm_call_end", duration_ms=duration_ms)

    structlog.contextvars.bind_contextvars(span_id=parent_span)
```

**日志输出：**
```json
{"trace_id": "trace_123", "span_id": "span_main", "event": "chat_start"}
{"trace_id": "trace_123", "span_id": "span_rag", "parent_span_id": "span_main", "event": "rag_search_start"}
{"trace_id": "trace_123", "span_id": "span_rag", "parent_span_id": "span_main", "event": "rag_search_end", "duration_ms": 100}
{"trace_id": "trace_123", "span_id": "span_llm", "parent_span_id": "span_main", "event": "llm_call_start"}
{"trace_id": "trace_123", "span_id": "span_llm", "parent_span_id": "span_main", "event": "llm_call_end", "duration_ms": 500}
{"trace_id": "trace_123", "span_id": "span_main", "event": "chat_end"}
```

### 3.3 跨服务传播Trace ID

#### 通过HTTP头传播

```python
import httpx

async def call_downstream_service(data: dict):
    """调用下游服务"""
    # 获取当前的trace_id
    trace_id = structlog.contextvars.get_contextvars().get("trace_id")

    # 通过HTTP头传递
    headers = {
        "X-Trace-ID": trace_id,
        "X-Request-ID": structlog.contextvars.get_contextvars().get("request_id")
    }

    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://downstream-service/api",
            json=data,
            headers=headers
        )

    return response.json()
```

#### 下游服务接收Trace ID

```python
@app.middleware("http")
async def trace_middleware(request: Request, call_next):
    # 从请求头中获取trace_id
    trace_id = request.headers.get("X-Trace-ID")
    request_id = request.headers.get("X-Request-ID")

    # 如果没有，生成新的
    if not trace_id:
        trace_id = str(uuid.uuid4())
    if not request_id:
        request_id = str(uuid.uuid4())

    # 绑定到上下文
    structlog.contextvars.bind_contextvars(
        trace_id=trace_id,
        request_id=request_id
    )

    response = await call_next(request)

    # 在响应头中返回
    response.headers["X-Trace-ID"] = trace_id
    response.headers["X-Request-ID"] = request_id

    structlog.contextvars.clear_contextvars()

    return response
```

---

## 第四部分：错误追踪和堆栈信息

### 4.1 记录错误上下文

```python
import traceback
import structlog

logger = structlog.get_logger()

async def call_llm(prompt: str):
    try:
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        return response
    except Exception as e:
        # 记录完整的错误上下文
        logger.error("llm_call_failed",
            error=str(e),
            error_type=type(e).__name__,
            error_module=type(e).__module__,
            stack_trace=traceback.format_exc(),

            # 请求上下文
            model="gpt-4",
            prompt_length=len(prompt),

            # 当前状态
            retry_count=getattr(e, "retry_count", 0)
        )
        raise
```

### 4.2 异常链追踪

```python
async def handle_chat(message: str):
    try:
        # 第一层：业务逻辑
        await process_chat(message)
    except ValueError as e:
        # 第二层：业务异常处理
        logger.error("chat_processing_failed",
            error=str(e),
            message_length=len(message)
        )
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        # 第三层：未预期的异常
        logger.critical("unexpected_error",
            error=str(e),
            error_type=type(e).__name__,
            stack_trace=traceback.format_exc()
        )
        raise HTTPException(status_code=500, detail="Internal server error")
```

---

## 第五部分：性能监控和瓶颈分析

### 5.1 记录每个步骤的耗时

```python
import time
from contextlib import asynccontextmanager

@asynccontextmanager
async def log_duration(operation: str):
    """上下文管理器：自动记录操作耗时"""
    start = time.time()
    logger.info(f"{operation}_start")

    try:
        yield
    finally:
        duration_ms = (time.time() - start) * 1000
        logger.info(f"{operation}_end", duration_ms=duration_ms)

# 使用
async def handle_chat(message: str):
    async with log_duration("rag_search"):
        docs = await rag_search(message)

    async with log_duration("llm_call"):
        response = await call_llm(message, docs)

    return response
```

### 5.2 分析性能瓶颈

```python
async def handle_chat_with_metrics(message: str):
    """带性能指标的聊天处理"""
    total_start = time.time()
    metrics = {}

    # RAG检索
    rag_start = time.time()
    docs = await rag_search(message)
    metrics["rag_duration_ms"] = (time.time() - rag_start) * 1000

    # LLM调用
    llm_start = time.time()
    response = await call_llm(message, docs)
    metrics["llm_duration_ms"] = (time.time() - llm_start) * 1000

    # 总耗时
    metrics["total_duration_ms"] = (time.time() - total_start) * 1000

    # 计算占比
    metrics["rag_percentage"] = (metrics["rag_duration_ms"] / metrics["total_duration_ms"]) * 100
    metrics["llm_percentage"] = (metrics["llm_duration_ms"] / metrics["total_duration_ms"]) * 100

    # 记录性能指标
    logger.info("chat_metrics", **metrics)

    # 如果某个步骤太慢，记录警告
    if metrics["rag_duration_ms"] > 500:
        logger.warning("rag_slow", duration_ms=metrics["rag_duration_ms"])

    if metrics["llm_duration_ms"] > 3000:
        logger.warning("llm_slow", duration_ms=metrics["llm_duration_ms"])

    return response
```

---

## 第六部分：AI Agent特定的链路追踪

### 6.1 完整的AI Agent请求链路

```python
@app.post("/api/chat")
async def chat(message: str):
    """完整的AI Agent请求链路追踪"""

    # 1. 请求开始
    logger.info("chat_request",
        message_length=len(message),
        timestamp=time.time()
    )

    # 2. RAG检索
    rag_start = time.time()
    docs = await rag_search(message)
    rag_duration = (time.time() - rag_start) * 1000

    logger.info("rag_complete",
        results_count=len(docs),
        top_score=docs[0].metadata.get("score", 0) if docs else 0,
        duration_ms=rag_duration
    )

    # 3. Prompt构建
    prompt_start = time.time()
    prompt = build_prompt(message, docs)
    prompt_duration = (time.time() - prompt_start) * 1000

    logger.info("prompt_built",
        prompt_length=len(prompt),
        duration_ms=prompt_duration
    )

    # 4. LLM调用
    llm_start = time.time()
    response = await call_llm(prompt)
    llm_duration = (time.time() - llm_start) * 1000

    logger.info("llm_complete",
        tokens=response.usage.total_tokens,
        duration_ms=llm_duration
    )

    # 5. 工具调用（如果需要）
    if needs_tool_call(response):
        tool_start = time.time()
        tool_result = await call_tool(response)
        tool_duration = (time.time() - tool_start) * 1000

        logger.info("tool_complete",
            tool_name=tool_result.tool_name,
            duration_ms=tool_duration
        )

    # 6. 请求完成
    total_duration = (time.time() - rag_start) * 1000

    logger.info("chat_complete",
        total_duration_ms=total_duration,
        breakdown={
            "rag_ms": rag_duration,
            "prompt_ms": prompt_duration,
            "llm_ms": llm_duration,
            "tool_ms": tool_duration if needs_tool_call(response) else 0
        }
    )

    return response
```

### 6.2 流式响应的链路追踪

```python
@app.post("/api/chat/stream")
async def chat_stream(message: str):
    """流式响应的链路追踪"""

    logger.info("stream_start")

    async def generate():
        # RAG检索
        docs = await rag_search(message)
        logger.info("rag_complete", results_count=len(docs))

        # 流式LLM调用
        chunk_count = 0
        first_chunk_time = None

        async for chunk in call_llm_stream(message, docs):
            if chunk_count == 0:
                first_chunk_time = time.time()
                logger.info("first_chunk_received")

            chunk_count += 1
            yield chunk

        # 流式完成
        logger.info("stream_complete",
            chunk_count=chunk_count,
            time_to_first_chunk_ms=(first_chunk_time - time.time()) * 1000 if first_chunk_time else 0
        )

    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

## 总结

### 核心要点

1. **请求ID**
   - 每个请求的唯一标识
   - 关联请求的所有日志
   - 在响应头中返回

2. **contextvars**
   - 异步安全的上下文管理
   - 自动传递上下文信息
   - 请求结束时清理

3. **分布式追踪**
   - Trace ID：整个链路的标识
   - Span ID：单个操作的标识
   - 通过HTTP头传播

4. **性能监控**
   - 记录每个步骤的耗时
   - 分析性能瓶颈
   - 设置性能告警

### 最佳实践

1. 始终使用请求ID
2. 使用contextvars管理上下文
3. 记录完整的错误堆栈
4. 监控关键操作的耗时
5. 在响应头中返回追踪信息

### 下一步

- 【实战代码】：完整的链路追踪实现
- 【生产级日志系统】：日志聚合和分析
