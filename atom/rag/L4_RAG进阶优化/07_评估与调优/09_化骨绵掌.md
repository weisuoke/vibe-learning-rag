# 化骨绵掌

## RAG评估与调优知识速查卡

本文档提供10张知识速查卡，涵盖RAG评估与调优的核心知识点，方便快速查阅和记忆。

---

## 卡片1: 检索指标速查

### 核心指标公式

```
Precision@K = 检索到的相关文档数 / K
Recall@K = 检索到的相关文档数 / 总相关文档数
MRR = 1 / 第一个相关文档的排名
NDCG@K = DCG@K / IDCG@K
Hit Rate@K = 命中查询数 / 总查询数
```

### 使用场景

| 指标 | 适用场景 | 目标值 |
|------|----------|--------|
| Precision@K | 成本敏感，控制噪音 | >0.80 |
| Recall@K | 信息完整性要求高 | >0.85 |
| MRR | 单答案场景，关注首个结果 | >0.80 |
| NDCG@K | 多级相关性，排序重要 | >0.85 |
| Hit Rate | 快速评估基础可用性 | >0.90 |

### 快速实现

```python
from ranx import Qrels, Run, evaluate

qrels = Qrels({'q1': {'doc1': 3, 'doc3': 2}})
run = Run({'q1': {'doc1': 0.9, 'doc2': 0.8, 'doc3': 0.7}})

results = evaluate(qrels, run, ['precision@5', 'recall@5', 'mrr', 'ndcg@5'])
```

---

## 卡片2: 生成指标速查

### 核心指标定义

| 指标 | 定义 | 评估方法 | 目标值 |
|------|------|----------|--------|
| **Faithfulness** | 答案忠实于上下文 | NLI模型/LLM-as-judge | >0.95 |
| **Answer Relevancy** | 答案回答了问题 | 语义相似度/LLM-as-judge | >0.90 |
| **Answer Correctness** | 答案与标准答案匹配 | F1分数/语义相似度 | >0.85 |
| **Fluency** | 文本流畅自然 | 困惑度/LLM-as-judge | >0.90 |
| **Groundedness** | 严格基于上下文 | NLI模型（严格版） | >0.95 |

### 评估优先级

1. **必选**: Faithfulness + Answer Relevancy
2. **可选**: Answer Correctness（如果有ground truth）
3. **高风险场景**: Groundedness（医疗、法律）
4. **内容生成**: Fluency

### 快速实现

```python
from openai import OpenAI

def evaluate_faithfulness(answer, context):
    client = OpenAI()
    prompt = f"判断答案是否完全基于上下文。上下文:{context} 答案:{answer}"
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return response.choices[0].message.content
```

---

## 卡片3: RAGAS核心指标

### 四大核心指标

```
检索阶段:
├── Context Precision: 检索上下文的精确度 (目标: >0.85)
└── Context Recall: 检索上下文的召回率 (目标: >0.85)

生成阶段:
├── Faithfulness: 生成内容的忠实度 (目标: >0.95)
└── Answer Relevancy: 答案的相关性 (目标: >0.90)
```

### 快速使用

```python
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy
)
from datasets import Dataset

data = {
    'question': ["问题"],
    'answer': ["答案"],
    'contexts': [["上下文"]],
    'ground_truth': ["标准答案"]
}

dataset = Dataset.from_dict(data)
result = evaluate(dataset, metrics=[
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy
])
```

### 优缺点

**优势**:
- 端到端评估
- 标准化指标
- 易于集成

**局限**:
- 需要ground truth
- 评估成本较高
- 不适合所有场景（对话式、多模态）

---

## 卡片4: LLM-as-judge设计

### 核心原则

```
1. 明确评估标准
2. 使用结构化输出（JSON）
3. 设置temperature=0
4. 提供评分理由
5. 多次评估取平均
```

### Prompt模板

```python
prompt_template = """
你是严格的评估者。评估答案的{dimension}。

问题: {question}
答案: {answer}
上下文: {context}

评估标准:
{criteria}

返回格式:
{{
    "score": <分数0-10>,
    "reasoning": "<评分理由>"
}}

只返回JSON，不要解释。
"""
```

### 一致性优化

```python
# 方法1: 多次评估取平均
scores = [judge.evaluate(q, a, c) for _ in range(3)]
avg_score = sum(scores) / len(scores)

# 方法2: 多judge ensemble
judges = [LLMJudge("gpt-4o"), LLMJudge("gpt-4o-mini")]
scores = [judge.evaluate(q, a, c) for judge in judges]
avg_score = sum(scores) / len(scores)

# 方法3: 人工校准
calibration_factor = sum(human_scores) / sum(llm_scores)
calibrated_score = llm_score * calibration_factor
```

### 成本优化

| 模型 | 成本/1K tokens | 适用场景 |
|------|----------------|----------|
| gpt-4o | $0.015 | 开发阶段，高质量评估 |
| gpt-4o-mini | $0.0006 | 生产阶段，成本敏感 |
| 采样评估 | 降低90% | 大规模数据 |
| 缓存 | 降低100% | 重复评估 |

---

## 卡片5: 性能优化技术

### 延迟优化

```
缓存策略 (效果最大):
├── 语义缓存: 70%命中率 → 延迟降低68%
└── 精确缓存: 30%命中率 → 延迟降低100%

并行处理:
├── 检索与生成并行 → 延迟降低30-40%
└── 批量Embedding → 延迟降低20-30%

模型优化:
└── 动态模型路由 → 延迟降低10-20%
```

### 成本优化

```
模型选择 (效果最大):
├── 动态路由: 70%简单问题用mini → 成本降低67%
└── 模型降级: 全部用mini → 成本降低97%

Token优化:
├── 上下文压缩: 2000→500 tokens → 成本降低75%
└── Prompt优化: 精简提示词 → 成本降低10-15%

缓存策略:
└── 语义缓存: 70%命中率 → 成本降低70%
```

### 吞吐量优化

```
负载均衡:
└── 3个实例 → 吞吐量提升3倍

异步处理:
└── 同步→异步 → 吞吐量提升10倍

批处理:
└── 批量Embedding → 吞吐量提升5倍
```

### 快速实现

```python
from sentence_transformers import SentenceTransformer, util

class SemanticCache:
    def __init__(self, threshold=0.95):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.cache = {}
        self.threshold = threshold
    
    def get(self, question):
        q_emb = self.model.encode(question, convert_to_tensor=True)
        for cached_emb, (_, answer) in self.cache.items():
            if util.cos_sim(q_emb, cached_emb).item() >= self.threshold:
                return answer
        return None
    
    def set(self, question, answer):
        q_emb = self.model.encode(question, convert_to_tensor=True)
        self.cache[q_emb] = (question, answer)
```

---

## 卡片6: 成本优化策略

### 成本分解

```
RAG系统成本 = Embedding成本 + 检索成本 + 生成成本

典型分布:
├── Embedding: 5% ($0.00002/1K tokens)
├── 检索: 5% (向量数据库)
└── 生成: 90% ($0.0006-0.015/1K tokens)

优化重点: 生成成本
```

### 优化策略对比

| 策略 | 成本降低 | 质量影响 | 实施难度 |
|------|----------|----------|----------|
| 动态模型路由 | 60-70% | 几乎无 | 低 |
| 语义缓存 | 60-70% | 无 | 低 |
| Token压缩 | 30-50% | 轻微 | 中 |
| 请求去重 | 20-30% | 无 | 低 |
| 批处理 | 10-15% | 无 | 中 |
| 模型降级 | 97% | 中等 | 低 |

### 成本追踪

```python
class CostTracker:
    PRICES = {
        'gpt-4o': {'input': 0.005, 'output': 0.015},
        'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006}
    }
    
    def track(self, model, input_tokens, output_tokens):
        prices = self.PRICES[model]
        cost = (input_tokens/1000) * prices['input'] + \
               (output_tokens/1000) * prices['output']
        self.total_cost += cost
        return cost
```

---

## 卡片7: 评估工具对比

### 主流工具对比

| 工具 | 类型 | 优势 | 劣势 | 适用场景 |
|------|------|------|------|----------|
| **RAGAS** | 开源框架 | 标准化、易用 | 需要ground truth | 通用RAG评估 |
| **DeepEval** | 开源框架 | 快速、丰富指标 | 文档较少 | 快速原型 |
| **TruLens** | 开源框架 | 实时监控、可解释性 | 学习曲线陡 | 生产监控 |
| **LangSmith** | 商业平台 | 完整observability | 需要付费 | LangChain用户 |
| **Arize AI** | 商业平台 | 企业级、异常检测 | 价格高 | 大型企业 |
| **ranx** | 开源库 | 高性能、标准化 | 只评估检索 | 检索评估 |

### 选择建议

```
开发阶段:
└── RAGAS (标准化、易用)

测试阶段:
└── RAGAS + ranx (全面评估)

生产阶段:
├── 小团队: TruLens (开源、实时监控)
└── 大团队: LangSmith/Arize (企业级)

检索专项:
└── ranx (高性能、标准化)
```

---

## 卡片8: 生产监控指标

### 三层监控体系

```
质量层 (采样评估):
├── Faithfulness: >0.95
├── Answer Relevancy: >0.90
└── 采样率: 10%

性能层 (全量监控):
├── P50延迟: <1.5秒
├── P95延迟: <2.0秒
├── P99延迟: <5.0秒
├── 错误率: <1%
└── 可用性: >99.9%

成本层 (全量监控):
├── 每次查询成本: <$0.05
├── 日均成本: <$100
├── Token利用率: >80%
└── 缓存命中率: >70%
```

### 告警阈值

```python
alert_thresholds = {
    # 质量告警
    'faithfulness': 0.80,
    'answer_relevancy': 0.80,
    
    # 性能告警
    'p95_latency': 2.0,  # 秒
    'error_rate': 0.01,  # 1%
    
    # 成本告警
    'daily_cost': 100,  # 美元
    'cost_per_query': 0.05  # 美元
}
```

### 监控实现

```python
class ProductionMonitor:
    def __init__(self):
        self.query_buffer = deque(maxlen=100)
    
    def monitor_query(self, question, answer, context):
        self.query_buffer.append({
            'question': question,
            'answer': answer,
            'context': context,
            'timestamp': time.time()
        })
        
        if len(self.query_buffer) >= 100:
            self._evaluate_and_alert()
    
    def _evaluate_and_alert(self):
        # 采样评估
        sample = random.sample(self.query_buffer, 10)
        scores = evaluate_batch(sample)
        
        # 检查阈值
        if scores['faithfulness'] < 0.80:
            self._send_alert('Faithfulness低于阈值')
```

---

## 卡片9: 评估最佳实践

### 开发阶段

```
目标: 建立评估基线

步骤:
1. 准备测试集 (100-500条)
2. 选择核心指标 (3-5个)
3. 使用RAGAS进行端到端评估
4. 设置目标阈值
5. 集成到CI/CD

工具:
├── RAGAS: 端到端评估
├── ranx: 检索评估
└── LLM-as-judge: 生成评估
```

### 测试阶段

```
目标: 验证生产就绪

步骤:
1. 扩大测试集 (1000+条)
2. 添加性能评估
3. 添加成本评估
4. A/B测试验证
5. 压力测试

关注点:
├── 质量: 是否达标
├── 性能: P95延迟<2秒
├── 成本: 每次查询<$0.05
└── 稳定性: 错误率<1%
```

### 生产阶段

```
目标: 持续质量保障

步骤:
1. 实时监控核心指标
2. 采样评估质量 (10%)
3. 每日生成报告
4. 异常自动告警
5. 定期人工校准

监控频率:
├── 实时: 性能、成本
├── 每小时: 质量采样
├── 每日: 汇总报告
└── 每周: 深度分析
```

### 优化阶段

```
目标: 持续改进

步骤:
1. 分析评估数据
2. 识别问题模式
3. 设计优化方案
4. A/B测试验证
5. 灰度发布

优化循环:
评估 → 分析 → 优化 → 验证 → 发布 → 评估
```

---

## 卡片10: 2025-2026标准

### 质量标准

```
检索质量:
├── Recall@5: ≥0.85 (优秀)
├── Precision@5: ≥0.80 (优秀)
├── MRR: ≥0.80 (优秀)
└── NDCG@5: ≥0.85 (优秀)

生成质量:
├── Faithfulness: ≥0.95 (优秀)
├── Answer Relevancy: ≥0.90 (优秀)
└── Answer Correctness: ≥0.85 (优秀)

端到端:
├── Context Precision: ≥0.85 (优秀)
├── Context Recall: ≥0.85 (优秀)
├── Faithfulness: ≥0.95 (优秀)
└── Answer Relevancy: ≥0.90 (优秀)
```

### 性能标准

```
延迟:
├── P50: <1.5秒 (及格), <0.8秒 (优秀)
├── P95: <3.0秒 (及格), <2.0秒 (优秀)
└── P99: <5.0秒 (及格), <3.0秒 (优秀)

吞吐量:
├── QPS: >100 (及格), >500 (优秀)
└── 并发: >50 (及格), >200 (优秀)

可用性:
└── Uptime: >99.5% (及格), >99.9% (优秀)
```

### 成本标准

```
查询成本:
├── 每次查询: <$0.10 (及格), <$0.05 (优秀)
└── 日均成本: <$200 (及格), <$100 (优秀)

资源利用:
├── Token利用率: >60% (及格), >80% (优秀)
├── 缓存命中率: >40% (及格), >70% (优秀)
└── GPU利用率: >50% (及格), >80% (优秀)
```

### 行业采用率

```
评估方法:
├── RAGAS: 75%的项目使用
├── LLM-as-judge: 60%的项目使用
├── 人工评估: 30%的项目使用
└── 混合评估: 85%的项目使用

优化技术:
├── 语义缓存: 80%的项目使用
├── 动态模型路由: 65%的项目使用
├── Token压缩: 50%的项目使用
└── 批处理: 70%的项目使用

监控工具:
├── 开源工具: 60% (RAGAS, TruLens)
├── 商业平台: 40% (LangSmith, Arize)
└── 自建系统: 20%
```

---

## 使用建议

### 如何使用这些卡片

1. **快速查阅**: 遇到问题时快速找到相关卡片
2. **系统学习**: 按顺序阅读所有卡片，建立完整知识体系
3. **实践参考**: 在实际项目中参考卡片中的代码和配置
4. **面试准备**: 记忆关键数字和标准，准备面试

### 记忆技巧

1. **数字记忆**: 记住关键目标值（0.85, 0.90, 0.95）
2. **公式记忆**: 理解公式背后的含义，而非死记硬背
3. **场景记忆**: 将知识点与实际场景关联
4. **对比记忆**: 记住不同方法的优缺点对比

### 持续更新

这些卡片基于2025-2026年的标准，随着技术发展，标准会不断更新。建议：
- 关注行业动态
- 定期更新知识
- 实践验证标准
- 分享经验教训

---

**记住：知识速查卡是工具，实践才是关键。**
