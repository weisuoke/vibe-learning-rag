# 连接池配置 - 概览

> 数据库连接复用机制，提升并发性能的关键技术

---

## 文档导航

### 快速开始（5分钟）
1. [30字核心](./01_30字核心.md) - 一句话理解连接池
2. [一句话总结](./10_一句话总结.md) - 完整总结

### 深入理解（30分钟）
3. [第一性原理](./02_第一性原理.md) - 从根本理解连接池
4. [双重类比](./05_双重类比.md) - 前端开发 + 日常生活类比
5. [反直觉点](./06_反直觉点.md) - 5个常见误区

### 核心知识（60分钟）
6. [核心概念1：SQLAlchemy连接池机制](./03_核心概念_1_SQLAlchemy连接池机制.md)
7. [核心概念2：FastAPI中的连接池集成](./03_核心概念_2_FastAPI中的连接池集成.md)
8. [核心概念3：连接池监控与调优](./03_核心概念_3_连接池监控与调优.md)

### 实战应用（90分钟）
9. [最小可用](./04_最小可用.md) - 5个核心知识点快速上手
10. [实战代码 - 场景1：基础配置](./07_实战代码_场景1_基础配置.md)
11. [实战代码 - 场景2：监控告警](./07_实战代码_场景2_监控告警.md)
12. [实战代码 - 场景3：高级配置](./07_实战代码_场景3_高级配置.md)

### 面试准备（30分钟）
13. [面试必问](./08_面试必问.md) - 3个高频面试题 + 出彩回答

### 速查手册（随时查阅）
14. [化骨绵掌](./09_化骨绵掌.md) - 10个2分钟知识卡片

---

## 学习路径

### 路径1：快速上手（2小时）
适合：需要快速在项目中使用连接池

```
30字核心 (2分钟)
    ↓
最小可用 (20分钟)
    ↓
实战代码 - 场景1 (60分钟)
    ↓
化骨绵掌 (20分钟)
    ↓
开始在项目中使用 ✅
```

### 路径2：系统学习（1天）
适合：想要深入理解连接池原理和最佳实践

```
30字核心 (2分钟)
    ↓
第一性原理 (30分钟)
    ↓
核心概念1-3 (90分钟)
    ↓
双重类比 + 反直觉点 (40分钟)
    ↓
实战代码 - 场景1-3 (180分钟)
    ↓
面试必问 (30分钟)
    ↓
掌握连接池配置 ✅
```

### 路径3：面试准备（1小时）
适合：准备面试，需要快速复习

```
30字核心 (2分钟)
    ↓
化骨绵掌 (20分钟)
    ↓
面试必问 (30分钟)
    ↓
反直觉点 (10分钟)
    ↓
准备好面试 ✅
```

---

## 核心知识点

### 1. 连接池的本质

**连接池 = 预先创建并维护的一组可复用的数据库连接**

```python
# 没有连接池：每次都创建新连接
建立连接 (65-160ms) + 查询 (5-20ms) = 70-180ms

# 有连接池：复用已有连接
获取连接 (1-5ms) + 查询 (5-20ms) = 6-26ms

# 性能提升：10-30倍
```

### 2. 核心配置参数

| 参数 | 说明 | 推荐值 | 作用 |
|------|------|--------|------|
| **pool_size** | 常驻连接数 | 并发数 × 0.5 | 始终保持的连接数 |
| **max_overflow** | 临时连接数 | pool_size × 1.5 | 应对突发流量 |
| **pool_timeout** | 获取连接超时 | 30秒 | 防止无限等待 |
| **pool_recycle** | 连接回收时间 | 3600秒（1小时） | 防止连接失效 |
| **pool_pre_ping** | 连接前检测 | True | 避免使用失效连接 |

### 3. FastAPI 集成模式

```python
# 1. 创建引擎（带连接池）
engine = create_engine(
    DATABASE_URL,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
)

# 2. 创建 SessionLocal
SessionLocal = sessionmaker(bind=engine)

# 3. 依赖注入函数
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# 4. 在路由中使用
@app.get("/users/{user_id}")
async def get_user(user_id: int, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.id == user_id).first()
    return user
```

---

## 关键概念速查

### 连接池生命周期

```
应用启动
    ↓
预创建 pool_size 个连接
    ↓
请求到达 → 从连接池获取连接
    ↓
执行查询
    ↓
请求结束 → 归还连接到池中
    ↓
连接超过 pool_recycle 时间 → 回收并重建
    ↓
应用关闭 → 关闭所有连接
```

### 连接池状态监控

```python
# 获取连接池状态
pool = engine.pool
pool_size = pool.size()           # 连接池大小
checked_in = pool.checkedin()     # 空闲连接数
checked_out = pool.checkedout()   # 正在使用的连接数
overflow = pool.overflow()        # 溢出连接数

# 计算使用率
pool_usage = (checked_out + overflow) / (pool_size + max_overflow)

# 告警阈值
if pool_usage > 0.9:
    logger.error("连接池使用率过高")
```

---

## 常见问题

### Q1: 如何选择合适的 pool_size？

**公式（经验值）：**
```python
pool_size = 并发请求数 × 单个请求的数据库操作时间 / 请求总时间

# 示例：
# - 并发请求数：100
# - 单个请求的数据库操作时间：20ms
# - 请求总时间：100ms
pool_size = 100 × 0.02 / 0.1 = 20
```

**简化公式：**
```python
pool_size = 并发请求数 × 0.5
```

### Q2: 连接池耗尽怎么办？

**现象：**
```
TimeoutError: QueuePool limit of size 10 overflow 20 reached
```

**解决方案：**
1. 增大连接池：`pool_size=20, max_overflow=40`
2. 优化慢查询：添加索引、减少 JOIN
3. 使用缓存：减少数据库查询
4. 异步处理：将耗时操作移到后台任务

### Q3: 为什么要开启 pool_pre_ping？

**原因：**
- 数据库会主动断开空闲连接（MySQL 默认 8 小时）
- 网络设备可能断开连接
- 使用失效连接会导致错误

**性能开销：**
```python
# SELECT 1 的开销：<1ms
# 如果连接刚被使用过（<1秒），跳过 ping
# 实际开销几乎可以忽略

# 对比：
pool_pre_ping=True:  1ms + 5-20ms = 6-21ms
pool_pre_ping=False: 70-180ms（如果连接失效）
```

**结论：** 生产环境必须开启 `pool_pre_ping=True`

### Q4: Session 可以跨请求复用吗？

**答案：不可以！**

**原因：**
- Session 不是线程安全的
- Session 会缓存查询结果
- Session 会累积未提交的事务

**正确做法：**
```python
# ✅ 正确：每个请求独立 Session
@app.get("/users/{user_id}")
async def get_user(user_id: int, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.id == user_id).first()
    return user

# ❌ 错误：全局共享 Session
global_session = SessionLocal()

@app.get("/users/{user_id}")
async def get_user(user_id: int):
    user = global_session.query(User).filter(User.id == user_id).first()
    return user
```

### Q5: 如何监控连接池状态？

**方法1：健康检查端点**
```python
@app.get("/health/db")
async def check_db_health():
    pool = engine.pool
    pool_usage = (pool.checkedout() + pool.overflow()) / (pool.size() + max_overflow)
    return {
        "status": "healthy" if pool_usage < 0.8 else "warning",
        "pool_usage": f"{pool_usage * 100:.1f}%"
    }
```

**方法2：Prometheus 指标**
```python
from prometheus_client import Gauge

pool_usage_gauge = Gauge('db_pool_usage', '连接池使用率')

@event.listens_for(engine, "checkout")
def receive_checkout(dbapi_conn, connection_record, connection_proxy):
    pool_usage_gauge.set(pool_usage)
```

**方法3：定期监控任务**
```python
async def periodic_monitoring():
    while True:
        pool_usage = get_pool_usage()
        if pool_usage > 0.9:
            logger.error("连接池使用率过高")
        await asyncio.sleep(60)
```

---

## 最佳实践

### 1. 生产环境推荐配置

```python
# app/core/database.py
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

DATABASE_URL = os.getenv("DATABASE_URL")
POOL_SIZE = int(os.getenv("POOL_SIZE", "20"))
MAX_OVERFLOW = int(os.getenv("MAX_OVERFLOW", "30"))

engine = create_engine(
    DATABASE_URL,
    pool_size=POOL_SIZE,
    max_overflow=MAX_OVERFLOW,
    pool_timeout=30,
    pool_recycle=3600,
    pool_pre_ping=True,
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

### 2. 不同环境的配置

```bash
# 开发环境
POOL_SIZE=5
MAX_OVERFLOW=5

# 测试环境
POOL_SIZE=10
MAX_OVERFLOW=10

# 生产环境
POOL_SIZE=20
MAX_OVERFLOW=30
```

### 3. 监控和告警

```python
# 1. 添加健康检查端点
@app.get("/health/db")
async def check_db_health():
    pass

# 2. 集成 Prometheus
@app.get("/metrics")
async def metrics():
    pass

# 3. 定期监控任务
@app.on_event("startup")
async def startup_event():
    asyncio.create_task(periodic_monitoring())
```

---

## 进阶主题

### 1. 读写分离

适合读多写少的场景，将读操作路由到从库，写操作路由到主库。

**详见：** [实战代码 - 场景3：高级配置](./07_实战代码_场景3_高级配置.md)

### 2. 异步连接池

适合高并发场景，使用异步 I/O 提供更高的并发能力。

**详见：** [实战代码 - 场景3：高级配置](./07_实战代码_场景3_高级配置.md)

### 3. 连接池故障转移

主库失败时自动切换到从库，提高系统可用性。

**详见：** [实战代码 - 场景3：高级配置](./07_实战代码_场景3_高级配置.md)

---

## 学习检查清单

完成本知识点后，你应该能够：

- [ ] 理解连接池的本质和价值
- [ ] 配置 SQLAlchemy 连接池参数
- [ ] 在 FastAPI 中使用依赖注入管理 Session
- [ ] 根据并发量调整 pool_size 和 max_overflow
- [ ] 设置 pool_timeout、pool_recycle、pool_pre_ping
- [ ] 监控连接池状态并实现告警
- [ ] 处理连接池耗尽、连接泄漏等常见问题
- [ ] 实现读写分离和异步连接池（进阶）

---

## 下一步学习

### 相关知识点

- **Phase3_数据库层/01_SQLAlchemy_ORM基础** - 理解 ORM 基础
- **Phase3_数据库层/02_Session管理** - 深入理解 Session 生命周期
- **Phase3_数据库层/04_数据库迁移Alembic** - 管理数据库 schema 变更

### 实战项目

1. **用户管理系统** - 实现完整的 CRUD 操作
2. **高并发 API 服务** - 使用异步连接池
3. **读写分离架构** - 实现主从复制

---

## 参考资源

### 官方文档

- [SQLAlchemy 连接池文档](https://docs.sqlalchemy.org/en/20/core/pooling.html)
- [FastAPI 数据库集成](https://fastapi.tiangolo.com/tutorial/sql-databases/)
- [PostgreSQL 连接管理](https://www.postgresql.org/docs/current/runtime-config-connection.html)

### 推荐阅读

- 《高性能 MySQL》- 第11章：扩展性与高可用性
- 《数据库系统概念》- 第15章：查询处理
- 《Python Web 开发实战》- 第8章：数据库优化

---

## 总结

**连接池配置的核心要点：**

1. **本质**：通过复用连接避免频繁建立连接的开销（性能提升 10-30 倍）
2. **配置**：根据并发量调整 pool_size 和 max_overflow
3. **可靠性**：开启 pool_pre_ping，设置 pool_recycle
4. **监控**：定期检查连接池状态，及时发现问题
5. **调优**：根据实际负载动态调整配置

**记住：** 连接池配置是"性能"与"资源"的平衡，需要根据实际情况动态调整，不是一劳永逸的。

---

**版本：** v1.0
**最后更新：** 2026-02-11
**维护者：** Claude Code

**开始学习：** 建议从 [30字核心](./01_30字核心.md) 开始，然后根据你的学习目标选择合适的路径。
