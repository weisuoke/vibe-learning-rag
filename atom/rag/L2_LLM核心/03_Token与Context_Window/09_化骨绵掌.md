# Token与Context Window - 化骨绵掌

将 Token 与 Context Window 拆解为 10 个 2 分钟知识卡片，逐步掌握。

---

## 卡片1：Token 的直觉理解

**一句话：** Token 是 LLM 的"文字积木"，不是字符，不是单词，而是介于两者之间的单位。

**举例：**
```
"Hello" → 1 个 Token（常见词，整体处理）
"你好"  → 2 个 Token（每个汉字约 1-2 Token）
"🎉"    → 3 个 Token（emoji 很"贵"）
```

**类比：** 就像乐高积木，不是按原子（字符）卖，也不是按成品（句子）卖，而是按标准积木块（Token）卖。

**应用：** 理解 Token 是理解 LLM 计费和限制的基础。

---

## 卡片2：为什么用 Token 而不是字符

**一句话：** 字符太碎效率低，单词无法处理新词，Token 是最优平衡。

**对比：**

| 方式 | "Hello World" | 问题 |
|------|---------------|------|
| 字符 | 11 个单位 | 序列太长，计算量大 |
| 单词 | 2 个单位 | 无法处理新词、中文 |
| Token | 2-3 个单位 | ✅ 平衡效率和灵活性 |

**关键点：** Transformer 的计算复杂度是 O(n²)，n 是序列长度。Token 让序列更短，计算更快。

**应用：** 这就是为什么 API 按 Token 计费，而不是按字符或请求次数。

---

## 卡片3：如何计算 Token 数量

**一句话：** 使用 `tiktoken` 库精确计算，或用经验公式估算。

**代码：**
```python
import tiktoken

encoder = tiktoken.encoding_for_model("gpt-4o")
tokens = encoder.encode("你好，世界！")
print(len(tokens))  # 7
```

**经验估算：**
- 中文：字数 × 1.5 ≈ Token 数
- 英文：单词数 × 1.3 ≈ Token 数

**应用：** 在 RAG 开发中，必须计算检索内容的 Token 数，避免超限。

---

## 卡片4：Context Window 是什么

**一句话：** Context Window 是 LLM 一次能处理的最大 Token 数量，包括输入和输出。

**公式：**
```
输入 Token + 输出 Token ≤ Context Window
```

**主流模型：**
| 模型 | Context Window |
|------|----------------|
| GPT-4o | 128K |
| Claude 3.5 | 200K |
| GPT-3.5 | 16K |

**应用：** Context Window 决定了 RAG 能注入多少检索内容，是设计的核心约束。

---

## 卡片5：为什么有 Context Window 限制

**一句话：** 注意力机制的计算复杂度是 O(n²)，Token 越多，计算量指数增长。

**推导：**
```
Token 数翻倍 → 计算量翻 4 倍 → 显存需求翻 4 倍
```

| Context | 相对计算量 |
|---------|-----------|
| 4K | 1x |
| 16K | 16x |
| 128K | 1024x |

**关键点：** 这是硬件限制，不是软件 bug。更大的 Context 需要更强的硬件。

**应用：** 理解限制的来源，才能更好地设计 RAG 系统。

---

## 卡片6：Token 预算分配

**一句话：** 在有限的 Context Window 中，合理分配各部分 Token 是 RAG 设计的核心。

**典型分配（128K 模型）：**
```
系统提示：~500 Token（固定）
用户问题：~200 Token（可变）
检索内容：~117K Token（主要空间！）
输出预留：~4K Token（必须预留）
安全边际：~6K Token（防止意外）
```

**关键点：** 检索内容占大头，但必须给输出留空间。

**应用：** 先算预算，再决定检索多少文档。

---

## 卡片7：超限会怎样

**一句话：** 超出 Context Window 会报错或被截断，必须提前检查。

**错误示例：**
```
Error: This model's maximum context length is 128000 tokens.
However, your messages resulted in 150000 tokens.
```

**处理策略：**
1. 提前计算 Token 数
2. 截断检索内容
3. 预留安全边际（5-10%）

**应用：** RAG 系统必须有 Token 检查和截断逻辑。

---

## 卡片8：Lost in the Middle 问题

**一句话：** Context 太长时，中间位置的信息容易被模型"遗忘"。

**研究发现：**
```
关键信息在开头：准确率 ~80%
关键信息在中间：准确率 ~50%
关键信息在结尾：准确率 ~70%
```

**应对策略：**
- 关键信息放开头或结尾
- 使用 ReRank 重排序
- 控制 Context 长度，不要塞满

**应用：** Context 越大不一定越好，质量比数量重要。

---

## 卡片9：RAG 中的 Token 管理实践

**一句话：** RAG 系统需要完整的 Token 管理流程：计算 → 分配 → 截断 → 监控。

**流程：**
```
1. 计算各部分 Token 数
2. 分配检索内容预算
3. 按相关性截断文档
4. 构建最终 Prompt
5. 监控 finish_reason
```

**代码片段：**
```python
def build_rag_prompt(query, docs, budget):
    context = truncate_to_budget(docs, budget)
    return f"参考资料：\n{context}\n\n问题：{query}"
```

**应用：** 这是 RAG 系统的标准实践，必须掌握。

---

## 卡片10：Token 与成本优化

**一句话：** Token 直接决定 API 成本，优化 Token 使用就是优化成本。

**成本公式：**
```
成本 = 输入 Token × 输入价格 + 输出 Token × 输出价格
```

**优化策略：**
| 策略 | 效果 |
|------|------|
| 精简系统提示 | 减少固定成本 |
| 控制检索数量 | 减少输入 Token |
| 限制输出长度 | 减少输出 Token |
| 缓存常见问答 | 避免重复调用 |
| 用小模型初筛 | 降低单价 |

**应用：** 生产环境的 RAG 系统必须考虑成本优化。

---

## 学习路径

```
卡片1-3：理解 Token 基础
    ↓
卡片4-5：理解 Context Window
    ↓
卡片6-7：掌握预算分配
    ↓
卡片8-10：进阶优化技巧
```

---

**上一节：** [08_面试必问.md](./08_面试必问.md)
**下一节：** [10_一句话总结.md](./10_一句话总结.md)
