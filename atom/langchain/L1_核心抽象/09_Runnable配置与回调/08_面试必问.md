# 面试必问

> **RunnableConfig 和回调系统的高频面试题及标准答案**

---

## Q1：如何在生产环境中监控 LangChain 应用的成本和性能？

### 标准答案

**三层监控方案**：

1. **自定义回调**：实时追踪 token 消耗和延迟
2. **LangSmith 集成**：可视化追踪和成本分析
3. **OpenTelemetry**：分布式追踪和告警

**代码示例**：

```python
from langchain_core.callbacks import BaseCallbackHandler
import time

class ProductionMonitor(BaseCallbackHandler):
    def __init__(self):
        self.start_time = None
        self.total_cost = 0

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()
        metadata = kwargs.get("metadata", {})
        logger.info(f"LLM调用开始", extra={
            "user_id": metadata.get("user_id"),
            "prompts": prompts
        })

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time
        usage = response.llm_output.get("token_usage", {})

        # 计算成本（GPT-4价格）
        cost = (
            usage.get("prompt_tokens", 0) * 0.00003 +
            usage.get("completion_tokens", 0) * 0.00006
        )
        self.total_cost += cost

        # 记录指标
        metrics.record("llm.duration", duration)
        metrics.record("llm.tokens", usage.get("total_tokens", 0))
        metrics.record("llm.cost", cost)

        # 告警：延迟过高
        if duration > 5.0:
            alert("LLM调用延迟过高", duration=duration)

    def on_llm_error(self, error, **kwargs):
        logger.error(f"LLM调用失败: {error}")
        alert("LLM调用失败", error=str(error))

# 使用
monitor = ProductionMonitor()
config = RunnableConfig(
    callbacks=[monitor],
    tags=["production"],
    metadata={"user_id": "user_123", "env": "prod"}
)

chain.invoke(input, config=config)
```

**加分点**：
- 提到成本归因（按用户/会话统计）
- 提到实时告警机制
- 提到与现有监控系统集成（Datadog、Prometheus）

**引用**：
> "Production monitoring requires tracking costs, latency, and errors at the individual call level."
> — [LangChain Production Best Practices](https://python.langchain.com/docs/guides/productionization/)

---

## Q2：RunnableConfig 的 tags 和 metadata 有什么区别？

### 标准答案

**核心区别**：

| 维度 | Tags | Metadata |
|------|------|----------|
| **类型** | `List[str]` | `Dict[str, Any]` |
| **用途** | 过滤和分类 | 追踪和归因 |
| **场景** | 决定回调是否执行 | 记录上下文信息 |
| **示例** | `["production", "critical"]` | `{"user_id": "123", "session": "abc"}` |

**代码示例**：

```python
# Tags：用于过滤回调
class ProductionOnlyCallback(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        tags = kwargs.get("tags", [])
        if "production" not in tags:
            return  # 非生产环境不执行

        # 生产环境的监控逻辑
        send_to_monitoring(prompts)

# Metadata：用于追踪和归因
class CostAttributionCallback(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id")

        # 按用户归因成本
        cost = calculate_cost(response)
        record_user_cost(user_id, cost)

# 使用
config = RunnableConfig(
    callbacks=[ProductionOnlyCallback(), CostAttributionCallback()],
    tags=["production", "critical"],  # 过滤
    metadata={"user_id": "user_123", "session": "abc"}  # 追踪
)
```

**类比**：
- **Tags** = HTML 的 class 属性（用于 CSS 选择）
- **Metadata** = HTML 的 data 属性（用于 JS 读取）

**加分点**：
- 提到 metadata 必须 JSON 可序列化
- 提到 tags 可以用于 LangSmith 过滤
- 提到两者可以组合使用

---

## Q3：如何实现按用户统计 LLM 调用成本？

### 标准答案

**实现方案**：

```python
from collections import defaultdict
from langchain_core.callbacks import BaseCallbackHandler

class UserCostTracker(BaseCallbackHandler):
    def __init__(self):
        self.user_costs = defaultdict(lambda: {
            "total_tokens": 0,
            "total_cost": 0.0,
            "call_count": 0
        })

    def on_llm_end(self, response, **kwargs):
        # 从 metadata 获取 user_id
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        # 计算成本
        usage = response.llm_output.get("token_usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        total_tokens = usage.get("total_tokens", 0)

        # GPT-4 价格
        cost = prompt_tokens * 0.00003 + completion_tokens * 0.00006

        # 更新用户统计
        self.user_costs[user_id]["total_tokens"] += total_tokens
        self.user_costs[user_id]["total_cost"] += cost
        self.user_costs[user_id]["call_count"] += 1

    def get_report(self):
        """生成成本报告"""
        report = []
        for user_id, stats in sorted(self.user_costs.items()):
            report.append({
                "user_id": user_id,
                "total_cost": f"${stats['total_cost']:.4f}",
                "total_tokens": stats["total_tokens"],
                "call_count": stats["call_count"],
                "avg_cost_per_call": f"${stats['total_cost'] / stats['call_count']:.4f}"
            })
        return report

# 使用
tracker = UserCostTracker()

# 多个用户的调用
for user_id in ["user_001", "user_002", "user_001"]:
    chain.invoke(
        {"input": "你好"},
        config={
            "callbacks": [tracker],
            "metadata": {"user_id": user_id}
        }
    )

# 生成报告
print(tracker.get_report())
```

**输出示例**：
```python
[
    {
        "user_id": "user_001",
        "total_cost": "$0.0012",
        "total_tokens": 450,
        "call_count": 2,
        "avg_cost_per_call": "$0.0006"
    },
    {
        "user_id": "user_002",
        "total_cost": "$0.0006",
        "total_tokens": 225,
        "call_count": 1,
        "avg_cost_per_call": "$0.0006"
    }
]
```

**加分点**：
- 提到持久化存储（数据库）
- 提到预算告警机制
- 提到成本优化策略（缓存、模型降级）

---

## Q4：回调的执行顺序是怎样的？

### 标准答案

**执行顺序规则**：

1. **父链 → 子链**：从外层到内层
2. **按添加顺序**：callbacks 列表的顺序
3. **同步执行**：回调按顺序执行（除非使用 AsyncCallbackHandler）

**示例**：

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

class CallbackA(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        print("A: chain_start")

    def on_llm_start(self, serialized, prompts, **kwargs):
        print("A: llm_start")

    def on_llm_end(self, response, **kwargs):
        print("A: llm_end")

    def on_chain_end(self, outputs, **kwargs):
        print("A: chain_end")

class CallbackB(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        print("B: chain_start")

    def on_llm_start(self, serialized, prompts, **kwargs):
        print("B: llm_start")

    def on_llm_end(self, response, **kwargs):
        print("B: llm_end")

    def on_chain_end(self, outputs, **kwargs):
        print("B: chain_end")

# 创建链
chain = ChatOpenAI() | StrOutputParser()

# 执行
chain.invoke(
    "你好",
    config={"callbacks": [CallbackA(), CallbackB()]}
)
```

**输出顺序**：
```
A: chain_start
B: chain_start
A: llm_start
B: llm_start
A: llm_end
B: llm_end
A: chain_end
B: chain_end
```

**关键点**：
- 同一事件的回调按添加顺序执行
- 父链的回调先于子链
- 错误回调（on_*_error）会中断后续回调

**加分点**：
- 提到异步回调的并行执行
- 提到回调过滤机制（ignore_llm、ignore_chain）
- 提到回调异常不会中断主流程

---

## Q5：如何动态切换 LLM 模型而不重建链？

### 标准答案

**使用 configurable_fields()**：

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import ConfigurableField

# 1. 声明可配置字段
llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_fields(
    model=ConfigurableField(
        id="model_name",
        name="Model Name",
        description="要使用的模型名称"
    ),
    temperature=ConfigurableField(
        id="temp",
        name="Temperature",
        description="控制输出随机性"
    )
)

# 2. 运行时切换模型
# 使用 GPT-3.5（便宜）
response1 = llm.invoke(
    "你好",
    config={"configurable": {"model_name": "gpt-3.5-turbo", "temp": 0.7}}
)

# 使用 GPT-4（质量高）
response2 = llm.invoke(
    "你好",
    config={"configurable": {"model_name": "gpt-4", "temp": 0.3}}
)
```

**使用 configurable_alternatives()**：

```python
from langchain_core.runnables import ConfigurableField

# 定义多个模型选项
llm = ChatOpenAI(model="gpt-3.5-turbo").configurable_alternatives(
    ConfigurableField(id="model"),
    default_key="gpt35",
    gpt4=ChatOpenAI(model="gpt-4"),
    claude=ChatAnthropic(model="claude-3-opus-20240229")
)

# 运行时切换
llm.invoke("你好", config={"configurable": {"model": "gpt4"}})
llm.invoke("你好", config={"configurable": {"model": "claude"}})
```

**应用场景**：
- **A/B 测试**：对比不同模型的效果
- **成本优化**：根据查询复杂度选择模型
- **降级策略**：主模型失败时切换到备用模型

**加分点**：
- 提到配置优先级（调用时 > with_config > 构造时）
- 提到配置验证机制
- 提到与特征开关（Feature Flag）集成

**引用**：
> "Configurable fields enable runtime parameter adjustment without rebuilding chains."
> — [LangChain Configurable Runnables](https://python.langchain.com/docs/how_to/configure/)

---

## Q6：LangSmith 和自定义回调有什么区别？

### 标准答案

**对比表**：

| 维度 | LangSmith | 自定义回调 |
|------|-----------|-----------|
| **配置复杂度** | 零配置（环境变量） | 需要编写代码 |
| **可视化** | 内置仪表盘 | 需要自己实现 |
| **成本** | 付费服务 | 免费 |
| **数据控制** | 数据上传到 LangSmith | 数据完全自控 |
| **定制性** | 有限 | 完全定制 |
| **集成难度** | 简单 | 中等 |

**LangSmith 示例**：

```bash
# 只需设置环境变量
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=your_key
export LANGCHAIN_PROJECT=my_project
```

```python
# 代码无需改动
chain.invoke("你好")  # 自动追踪
```

**自定义回调示例**：

```python
class CustomMonitor(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        # 完全自定义的监控逻辑
        send_to_internal_monitoring(response)
        check_budget_limits()
        trigger_alerts_if_needed()

chain.invoke("你好", config={"callbacks": [CustomMonitor()]})
```

**选择建议**：
- **小团队/快速原型**：使用 LangSmith
- **大团队/定制需求**：使用自定义回调
- **混合方案**：LangSmith + 自定义回调（两者可以共存）

**加分点**：
- 提到 LangSmith 的实时告警功能（2025 新特性）
- 提到 OpenTelemetry 集成（2025 新特性）
- 提到数据隐私和合规性考虑

---

## Q7：如何处理回调中的异常？

### 标准答案

**回调异常不会中断主流程**：

```python
class BuggyCallback(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        raise Exception("回调出错了！")

# 回调异常不会影响主流程
result = llm.invoke("你好", config={"callbacks": [BuggyCallback()]})
print(result)  # 正常输出

# 异常会被记录到日志
# WARNING: Error in callback: 回调出错了！
```

**最佳实践：主动捕获异常**：

```python
import logging

logger = logging.getLogger(__name__)

class SafeCallback(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        try:
            # 可能出错的逻辑
            self.send_to_monitoring(response)
        except Exception as e:
            # 记录错误但不抛出
            logger.error(f"监控发送失败: {e}", exc_info=True)

            # 可选：发送告警
            alert("回调异常", error=str(e))

    def send_to_monitoring(self, response):
        # 可能失败的外部调用
        requests.post("https://monitoring.example.com", json=response)
```

**为什么这样设计？**
- **健壮性**：监控系统故障不应影响业务
- **调试友好**：可以随时添加实验性回调
- **生产安全**：避免因日志系统故障导致服务不可用

**加分点**：
- 提到回调的重试机制
- 提到回调的超时控制
- 提到回调的降级策略

---

## Q8：如何实现流式输出的监控？

### 标准答案

**使用 on_llm_new_token 回调**：

```python
from langchain_core.callbacks import BaseCallbackHandler

class StreamMonitor(BaseCallbackHandler):
    def __init__(self):
        self.tokens = []
        self.start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()
        self.tokens = []

    def on_llm_new_token(self, token: str, **kwargs):
        """每个 token 生成时触发"""
        self.tokens.append(token)

        # 实时监控
        elapsed = time.time() - self.start_time
        tokens_per_second = len(self.tokens) / elapsed

        print(f"Token: {token} | 速度: {tokens_per_second:.1f} tokens/s")

    def on_llm_end(self, response, **kwargs):
        total_time = time.time() - self.start_time
        total_tokens = len(self.tokens)

        print(f"总耗时: {total_time:.2f}s")
        print(f"总 tokens: {total_tokens}")
        print(f"平均速度: {total_tokens / total_time:.1f} tokens/s")

# 使用
monitor = StreamMonitor()
llm = ChatOpenAI(streaming=True)

for chunk in llm.stream("写一首诗", config={"callbacks": [monitor]}):
    print(chunk.content, end="", flush=True)
```

**输出示例**：
```
Token: 春 | 速度: 12.3 tokens/s
Token: 风 | 速度: 15.7 tokens/s
Token: 拂 | 速度: 18.2 tokens/s
...
总耗时: 3.45s
总 tokens: 56
平均速度: 16.2 tokens/s
```

**加分点**：
- 提到流式输出的用户体验优化
- 提到流式输出的成本计算
- 提到流式输出的错误处理

---

## Q9：如何实现多回调的组合使用？

### 标准答案

**组合多个回调**：

```python
# 1. 成本追踪
class CostTracker(BaseCallbackHandler):
    def on_llm_end(self, response, **kwargs):
        cost = calculate_cost(response)
        record_cost(cost)

# 2. 性能监控
class PerformanceMonitor(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time
        record_latency(duration)

# 3. 错误告警
class ErrorAlerter(BaseCallbackHandler):
    def on_llm_error(self, error, **kwargs):
        send_alert(error)

# 4. 日志记录
class Logger(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        logger.info(f"LLM调用开始: {prompts}")

    def on_llm_end(self, response, **kwargs):
        logger.info(f"LLM调用结束")

# 组合使用
config = RunnableConfig(
    callbacks=[
        CostTracker(),
        PerformanceMonitor(),
        ErrorAlerter(),
        Logger()
    ],
    tags=["production"],
    metadata={"user_id": "user_123"}
)

chain.invoke(input, config=config)
```

**最佳实践**：
- **职责单一**：每个回调只负责一件事
- **解耦设计**：回调之间不应相互依赖
- **性能考虑**：避免在回调中做耗时操作

**加分点**：
- 提到回调的优先级控制
- 提到回调的条件执行（基于 tags）
- 提到回调的工厂模式

---

## Q10：如何在 RAG 系统中使用回调监控检索质量？

### 标准答案

**监控检索器的回调**：

```python
from langchain_core.callbacks import BaseCallbackHandler

class RetrievalMonitor(BaseCallbackHandler):
    def on_retriever_start(self, serialized, query, **kwargs):
        """检索开始"""
        self.start_time = time.time()
        logger.info(f"检索查询: {query}")

    def on_retriever_end(self, documents, **kwargs):
        """检索结束"""
        duration = time.time() - self.start_time

        # 统计检索质量
        num_docs = len(documents)
        avg_score = sum(d.metadata.get("score", 0) for d in documents) / num_docs

        # 记录指标
        metrics.record("retrieval.duration", duration)
        metrics.record("retrieval.num_docs", num_docs)
        metrics.record("retrieval.avg_score", avg_score)

        # 告警：检索质量低
        if avg_score < 0.5:
            alert("检索质量低", avg_score=avg_score)

        logger.info(f"检索完成: {num_docs} 文档, 平均分数: {avg_score:.2f}")

    def on_retriever_error(self, error, **kwargs):
        """检索失败"""
        logger.error(f"检索失败: {error}")
        alert("检索失败", error=str(error))

# RAG 链
from langchain.chains import RetrievalQA

rag_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=vectorstore.as_retriever()
)

# 使用监控
monitor = RetrievalMonitor()
rag_chain.invoke(
    {"query": "什么是量子纠缠？"},
    config={"callbacks": [monitor]}
)
```

**监控指标**：
- **检索延迟**：检索耗时
- **文档数量**：返回的文档数
- **相似度分数**：平均相似度
- **检索失败率**：错误次数 / 总次数

**加分点**：
- 提到检索结果的多样性监控
- 提到检索缓存的命中率
- 提到检索策略的 A/B 测试

**引用**：
> "Monitoring retrieval quality is critical for RAG systems to ensure relevant context is provided to the LLM."
> — [LangChain RAG Best Practices](https://python.langchain.com/docs/use_cases/question_answering/)

---

## 总结：面试高频考点

| 考点 | 核心知识 | 加分项 |
|------|---------|--------|
| **成本监控** | 自定义回调追踪 token | 成本归因、预算告警 |
| **Tags vs Metadata** | 过滤 vs 追踪 | 组合使用、序列化 |
| **回调顺序** | 父链→子链、按顺序 | 异步回调、过滤机制 |
| **动态配置** | configurable_fields | A/B 测试、降级策略 |
| **LangSmith** | 零配置追踪 | 自定义回调对比 |
| **异常处理** | 不中断主流程 | 重试、降级 |
| **流式监控** | on_llm_new_token | 实时速度、用户体验 |
| **多回调组合** | 职责单一、解耦 | 条件执行、工厂模式 |
| **RAG 监控** | 检索质量指标 | 多样性、缓存命中率 |

---

## 参考资料

- [LangChain RunnableConfig API](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.config.RunnableConfig.html)
- [LangChain Custom Callbacks](https://python.langchain.com/docs/how_to/custom_callbacks/)
- [LangSmith Observability](https://www.langchain.com/langsmith/observability)
- [LangChain Production Best Practices](https://python.langchain.com/docs/guides/productionization/)
