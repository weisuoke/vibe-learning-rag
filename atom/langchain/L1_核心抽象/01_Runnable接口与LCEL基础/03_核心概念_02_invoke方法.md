# æ ¸å¿ƒæ¦‚å¿µ 02: invoke æ–¹æ³•

> ç†è§£ Runnable çš„åŒæ­¥å•æ¬¡æ‰§è¡Œæ–¹æ³•

---

## ä»€ä¹ˆæ˜¯ invoke æ–¹æ³•ï¼Ÿ

**invoke æ˜¯ Runnable åè®®çš„æ ¸å¿ƒæ–¹æ³•ï¼Œç”¨äºåŒæ­¥æ‰§è¡Œå•ä¸ªè¾“å…¥å¹¶è¿”å›å•ä¸ªè¾“å‡ºã€‚**

### ä¸€å¥è¯å®šä¹‰

invoke æ–¹æ³•æ¥æ”¶ä¸€ä¸ªè¾“å…¥å’Œå¯é€‰çš„é…ç½®å‚æ•°ï¼ŒåŒæ­¥æ‰§è¡Œå¤„ç†é€»è¾‘ï¼Œè¿”å›ä¸€ä¸ªè¾“å‡ºç»“æœã€‚

---

## æ–¹æ³•ç­¾å

### å®Œæ•´ç­¾å

```python
from typing import TypeVar, Optional
from langchain_core.runnables.config import RunnableConfig

Input = TypeVar("Input")
Output = TypeVar("Output")

def invoke(
    self,
    input: Input,
    config: Optional[RunnableConfig] = None
) -> Output:
    """
    åŒæ­¥å•æ¬¡æ‰§è¡Œ

    Args:
        input: è¾“å…¥æ•°æ®ï¼Œç±»å‹ç”± Runnable[Input, Output] çš„ Input æ³›å‹å®šä¹‰
        config: è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«æ ‡ç­¾ã€å…ƒæ•°æ®ã€å›è°ƒç­‰ï¼ˆå¯é€‰ï¼‰

    Returns:
        è¾“å‡ºæ•°æ®ï¼Œç±»å‹ç”± Runnable[Input, Output] çš„ Output æ³›å‹å®šä¹‰

    Raises:
        Exception: æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä»»ä½•å¼‚å¸¸éƒ½ä¼šå‘ä¸Šä¼ æ’­
    """
    ...
```

### ç±»å‹çº¦æŸ

```python
from langchain_core.runnables import Runnable

# æ˜ç¡®è¾“å…¥è¾“å‡ºç±»å‹
class TextProcessor(Runnable[str, dict]):
    def invoke(self, input: str, config=None) -> dict:
        return {
            "original": input,
            "length": len(input),
            "uppercase": input.upper()
        }

# ä½¿ç”¨æ—¶ç±»å‹å®‰å…¨
processor: Runnable[str, dict] = TextProcessor()
result: dict = processor.invoke("hello")  # âœ… ç±»å‹æ­£ç¡®
```

---

## Config å‚æ•°æ·±åº¦è§£æ

### RunnableConfig ç»“æ„

```python
from langchain_core.runnables.config import RunnableConfig
from langchain_core.callbacks import BaseCallbackHandler
from typing import Optional, List, Dict, Any

config = RunnableConfig(
    # ===== è¿½è¸ªå’Œåˆ†ç±» =====
    tags: Optional[List[str]] = None,
    # ç”¨é€”ï¼šæ ‡è®°æ‰§è¡Œï¼Œä¾¿äºåœ¨ LangSmith ä¸­è¿‡æ»¤å’Œåˆ†æ
    # ç¤ºä¾‹ï¼š["production", "translation", "v2"]

    # ===== å…ƒæ•°æ® =====
    metadata: Optional[Dict[str, Any]] = None,
    # ç”¨é€”ï¼šé™„åŠ ä»»æ„å…ƒæ•°æ®ï¼Œç”¨äºæ—¥å¿—å’Œåˆ†æ
    # ç¤ºä¾‹ï¼š{"user_id": "123", "session_id": "abc", "environment": "prod"}

    # ===== å›è°ƒç³»ç»Ÿ =====
    callbacks: Optional[List[BaseCallbackHandler]] = None,
    # ç”¨é€”ï¼šç›‘æ§æ‰§è¡Œè¿‡ç¨‹ï¼Œè®°å½•æ—¥å¿—ï¼Œå‘é€é€šçŸ¥
    # ç¤ºä¾‹ï¼š[StdOutCallbackHandler(), CustomMetricsHandler()]

    # ===== å¹¶å‘æ§åˆ¶ =====
    max_concurrency: Optional[int] = None,
    # ç”¨é€”ï¼šé™åˆ¶å¹¶å‘æ‰§è¡Œæ•°é‡ï¼ˆä¸»è¦ç”¨äº batchï¼‰
    # ç¤ºä¾‹ï¼š5ï¼ˆæœ€å¤šåŒæ—¶æ‰§è¡Œ 5 ä¸ªä»»åŠ¡ï¼‰

    # ===== é€’å½’é™åˆ¶ =====
    recursion_limit: int = 25,
    # ç”¨é€”ï¼šé˜²æ­¢æ— é™é€’å½’ï¼ˆä¸»è¦ç”¨äº Agentï¼‰
    # ç¤ºä¾‹ï¼š10ï¼ˆæœ€å¤šé€’å½’ 10 å±‚ï¼‰

    # ===== è¿è¡Œæ ‡è¯† =====
    run_name: Optional[str] = None,
    # ç”¨é€”ï¼šåœ¨ LangSmith ä¸­æ˜¾ç¤ºçš„è¿è¡Œåç§°
    # ç¤ºä¾‹ï¼š"translation_chain_v2"

    # ===== è¿è¡Œ ID =====
    run_id: Optional[str] = None,
    # ç”¨é€”ï¼šå”¯ä¸€æ ‡è¯†ä¸€æ¬¡æ‰§è¡Œï¼ˆé€šå¸¸è‡ªåŠ¨ç”Ÿæˆï¼‰
    # ç¤ºä¾‹ï¼šUUID("...")

    # ===== å¯é…ç½®å‚æ•° =====
    configurable: Optional[Dict[str, Any]] = None,
    # ç”¨é€”ï¼šä¼ é€’è‡ªå®šä¹‰é…ç½®å‚æ•°
    # ç¤ºä¾‹ï¼š{"temperature": 0.7, "model": "gpt-4"}
)
```

### Config ä¼ é€’æœºåˆ¶

**Config ä¼šè‡ªåŠ¨ä¼ é€’ç»™é“¾ä¸­çš„æ‰€æœ‰ç»„ä»¶**ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("ç¿»è¯‘: {text}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | llm | parser

# é…ç½®ä¼ é€’ç»™æ‰€æœ‰ç»„ä»¶
config = RunnableConfig(
    tags=["translation"],
    metadata={"user_id": "123"}
)

result = chain.invoke({"text": "ä½ å¥½"}, config=config)

# ç­‰ä»·äºï¼š
# step1 = prompt.invoke({"text": "ä½ å¥½"}, config=config)
# step2 = llm.invoke(step1, config=config)
# step3 = parser.invoke(step2, config=config)
```

### ä½¿ç”¨ configurable ä¼ é€’è‡ªå®šä¹‰å‚æ•°

```python
from langchain_core.runnables import RunnableConfig, Runnable

class ConfigurableProcessor(Runnable[str, str]):
    """æ”¯æŒé…ç½®çš„å¤„ç†å™¨"""

    def invoke(self, input: str, config: RunnableConfig = None) -> str:
        # ä» config ä¸­è¯»å–è‡ªå®šä¹‰å‚æ•°
        if config and config.get("configurable"):
            mode = config["configurable"].get("mode", "default")
            prefix = config["configurable"].get("prefix", "")
        else:
            mode = "default"
            prefix = ""

        # æ ¹æ®é…ç½®å¤„ç†
        if mode == "uppercase":
            result = input.upper()
        elif mode == "lowercase":
            result = input.lower()
        else:
            result = input

        return f"{prefix}{result}"

# ä½¿ç”¨ä¸åŒé…ç½®
processor = ConfigurableProcessor()

# é…ç½® 1: å¤§å†™æ¨¡å¼
config1 = RunnableConfig(configurable={"mode": "uppercase", "prefix": "[UP] "})
print(processor.invoke("Hello", config1))  # "[UP] HELLO"

# é…ç½® 2: å°å†™æ¨¡å¼
config2 = RunnableConfig(configurable={"mode": "lowercase", "prefix": "[LOW] "})
print(processor.invoke("Hello", config2))  # "[LOW] hello"
```

---

## é”™è¯¯å¤„ç†æ¨¡å¼

### åŸºç¡€é”™è¯¯å¤„ç†

```python
from langchain_core.runnables import Runnable
from typing import Optional

class SafeProcessor(Runnable[str, str]):
    """å¸¦é”™è¯¯å¤„ç†çš„å¤„ç†å™¨"""

    def invoke(self, input: str, config=None) -> str:
        try:
            # éªŒè¯è¾“å…¥
            if not input or not isinstance(input, str):
                raise ValueError("è¾“å…¥å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")

            # å¤„ç†é€»è¾‘
            result = input.upper()

            # éªŒè¯è¾“å‡º
            if not result:
                raise RuntimeError("å¤„ç†ç»“æœä¸ºç©º")

            return result

        except ValueError as e:
            # è¾“å…¥éªŒè¯é”™è¯¯
            print(f"è¾“å…¥é”™è¯¯: {e}")
            raise

        except RuntimeError as e:
            # å¤„ç†é€»è¾‘é”™è¯¯
            print(f"å¤„ç†é”™è¯¯: {e}")
            raise

        except Exception as e:
            # æœªé¢„æœŸçš„é”™è¯¯
            print(f"æœªçŸ¥é”™è¯¯: {e}")
            raise
```

### ä½¿ç”¨å›è°ƒå¤„ç†é”™è¯¯

```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.runnables import RunnableConfig

class ErrorLoggingHandler(BaseCallbackHandler):
    """é”™è¯¯æ—¥å¿—å›è°ƒ"""

    def on_chain_error(self, error: Exception, **kwargs):
        """é“¾æ‰§è¡Œé”™è¯¯æ—¶è°ƒç”¨"""
        print(f"âŒ é“¾æ‰§è¡Œå¤±è´¥: {error}")
        # å¯ä»¥å‘é€å‘Šè­¦ã€è®°å½•æ—¥å¿—ç­‰

    def on_llm_error(self, error: Exception, **kwargs):
        """LLM è°ƒç”¨é”™è¯¯æ—¶è°ƒç”¨"""
        print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {error}")

# ä½¿ç”¨é”™è¯¯å¤„ç†å›è°ƒ
config = RunnableConfig(callbacks=[ErrorLoggingHandler()])

try:
    result = chain.invoke({"text": "ä½ å¥½"}, config=config)
except Exception as e:
    print(f"æœ€ç»ˆæ•è·é”™è¯¯: {e}")
```

### é‡è¯•æ¨¡å¼

```python
import time
from langchain_core.runnables import Runnable
from typing import Optional

class RetryableRunnable(Runnable):
    """æ”¯æŒé‡è¯•çš„ Runnable åŒ…è£…å™¨"""

    def __init__(self, inner: Runnable, max_retries: int = 3, backoff: float = 1.0):
        self.inner = inner
        self.max_retries = max_retries
        self.backoff = backoff

    def invoke(self, input, config=None):
        last_error = None

        for attempt in range(self.max_retries):
            try:
                return self.inner.invoke(input, config)

            except Exception as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    wait_time = self.backoff * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    print(f"é‡è¯• {attempt + 1}/{self.max_retries}ï¼Œç­‰å¾… {wait_time}ç§’")
                    time.sleep(wait_time)
                else:
                    print(f"é‡è¯•å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§æ¬¡æ•°")

        raise last_error

# ä½¿ç”¨
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
reliable_llm = RetryableRunnable(llm, max_retries=3, backoff=1.0)

# è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è°ƒç”¨
result = reliable_llm.invoke("ä½ å¥½")
```

---

## æ€§èƒ½ç‰¹å¾

### åŒæ­¥æ‰§è¡Œç‰¹ç‚¹

**invoke æ˜¯åŒæ­¥æ–¹æ³•ï¼Œä¼šé˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°æ‰§è¡Œå®Œæˆã€‚**

```python
import time
from langchain_core.runnables import RunnableLambda

def slow_process(x: str) -> str:
    time.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return x.upper()

runnable = RunnableLambda(slow_process)

# åŒæ­¥æ‰§è¡Œï¼Œé˜»å¡ 2 ç§’
start = time.time()
result = runnable.invoke("hello")  # é˜»å¡ 2 ç§’
print(f"è€—æ—¶: {time.time() - start:.2f}ç§’")  # çº¦ 2 ç§’
```

### ä¸å¼‚æ­¥æ–¹æ³•å¯¹æ¯”

```python
import asyncio
from langchain_core.runnables import RunnableLambda

async def async_process(x: str) -> str:
    await asyncio.sleep(2)
    return x.upper()

runnable = RunnableLambda(async_process)

# åŒæ­¥ invokeï¼šé˜»å¡æ‰§è¡Œ
result = runnable.invoke("hello")  # é˜»å¡ 2 ç§’

# å¼‚æ­¥ ainvokeï¼šéé˜»å¡æ‰§è¡Œ
async def main():
    result = await runnable.ainvoke("hello")  # éé˜»å¡
    return result

# åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è¿è¡Œ
result = asyncio.run(main())
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

æ ¹æ® 2025-2026 å¹´çš„æœ€ä½³å®è·µ[^1][^2]ï¼š

| åœºæ™¯ | æ¨èæ–¹æ³• | åŸå›  |
|------|----------|------|
| å•æ¬¡æŸ¥è¯¢ | `invoke` | ç®€å•ç›´æ¥ |
| å¤šä¸ªç‹¬ç«‹æŸ¥è¯¢ | `batch` æˆ– `ainvoke` + `asyncio.gather` | å¹¶å‘æ‰§è¡Œ |
| å®æ—¶å“åº” | `stream` | é™ä½æ„ŸçŸ¥å»¶è¿Ÿ |
| IO å¯†é›†å‹ | `ainvoke` | éé˜»å¡ |
| CPU å¯†é›†å‹ | `invoke` + å¤šè¿›ç¨‹ | é¿å… GIL |

---

## å®æˆ˜ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹ 1: è‡ªå®šä¹‰ Runnable å®ç°

```python
"""
è‡ªå®šä¹‰ Runnable å®ç°
æ¼”ç¤º invoke æ–¹æ³•çš„å®Œæ•´å®ç°
"""

from langchain_core.runnables import Runnable
from langchain_core.runnables.config import RunnableConfig
from typing import Optional
import re

class TextAnalyzer(Runnable[str, dict]):
    """
    æ–‡æœ¬åˆ†æ Runnable

    è¾“å…¥: æ–‡æœ¬å­—ç¬¦ä¸²
    è¾“å‡º: åˆ†æç»“æœå­—å…¸
    """

    def invoke(
        self,
        input: str,
        config: Optional[RunnableConfig] = None
    ) -> dict:
        """
        åˆ†ææ–‡æœ¬å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯

        Args:
            input: å¾…åˆ†æçš„æ–‡æœ¬
            config: è¿è¡Œæ—¶é…ç½®

        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        # 1. è¾“å…¥éªŒè¯
        if not isinstance(input, str):
            raise TypeError(f"è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼Œå®é™…ç±»å‹: {type(input)}")

        if not input.strip():
            raise ValueError("è¾“å…¥ä¸èƒ½ä¸ºç©º")

        # 2. ä» config ä¸­è¯»å–é…ç½®
        include_details = False
        if config and config.get("configurable"):
            include_details = config["configurable"].get("include_details", False)

        # 3. æ‰§è¡Œåˆ†æ
        text = input.strip()

        # åŸºç¡€ç»Ÿè®¡
        result = {
            "char_count": len(text),
            "word_count": len(text.split()),
            "line_count": len(text.splitlines()),
            "has_chinese": bool(re.search(r'[\u4e00-\u9fff]', text)),
            "has_english": bool(re.search(r'[a-zA-Z]', text))
        }

        # è¯¦ç»†ç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
        if include_details:
            result["details"] = {
                "uppercase_count": sum(1 for c in text if c.isupper()),
                "lowercase_count": sum(1 for c in text if c.islower()),
                "digit_count": sum(1 for c in text if c.isdigit()),
                "space_count": sum(1 for c in text if c.isspace()),
                "punctuation_count": sum(1 for c in text if c in ",.!?;:")
            }

        # 4. è®°å½•æ—¥å¿—ï¼ˆå¦‚æœæœ‰å›è°ƒï¼‰
        if config and config.get("callbacks"):
            for callback in config["callbacks"]:
                if hasattr(callback, "on_text"):
                    callback.on_text(f"åˆ†æå®Œæˆ: {result['word_count']} ä¸ªè¯")

        return result


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    analyzer = TextAnalyzer()

    # ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨
    print("=== ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨ ===")
    text1 = "Hello World! ä½ å¥½ä¸–ç•Œï¼"
    result1 = analyzer.invoke(text1)
    print(f"è¾“å…¥: {text1}")
    print(f"ç»“æœ: {result1}")
    print()

    # ç¤ºä¾‹ 2: ä½¿ç”¨é…ç½®
    print("=== ç¤ºä¾‹ 2: ä½¿ç”¨é…ç½® ===")
    config = RunnableConfig(
        configurable={"include_details": True},
        tags=["analysis", "production"],
        metadata={"user_id": "user_123"}
    )
    result2 = analyzer.invoke(text1, config=config)
    print(f"ç»“æœï¼ˆå«è¯¦æƒ…ï¼‰: {result2}")
    print()

    # ç¤ºä¾‹ 3: é”™è¯¯å¤„ç†
    print("=== ç¤ºä¾‹ 3: é”™è¯¯å¤„ç† ===")
    try:
        analyzer.invoke("")  # ç©ºå­—ç¬¦ä¸²
    except ValueError as e:
        print(f"æ•è·é”™è¯¯: {e}")
    print()

    # ç¤ºä¾‹ 4: é›†æˆåˆ° LCEL é“¾
    print("=== ç¤ºä¾‹ 4: é›†æˆåˆ° LCEL é“¾ ===")
    from langchain_core.runnables import RunnableLambda

    # é¢„å¤„ç†
    preprocessor = RunnableLambda(lambda x: x.strip().lower())

    # åå¤„ç†
    def format_result(analysis: dict) -> str:
        return f"æ–‡æœ¬åŒ…å« {analysis['word_count']} ä¸ªè¯ï¼Œ{analysis['char_count']} ä¸ªå­—ç¬¦"

    postprocessor = RunnableLambda(format_result)

    # ç»„åˆæˆé“¾
    chain = preprocessor | analyzer | postprocessor

    result = chain.invoke("  HELLO WORLD  ")
    print(f"é“¾å¼å¤„ç†ç»“æœ: {result}")
```

**è¿è¡Œè¾“å‡º**ï¼š
```
=== ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨ ===
è¾“å…¥: Hello World! ä½ å¥½ä¸–ç•Œï¼
ç»“æœ: {'char_count': 18, 'word_count': 3, 'line_count': 1, 'has_chinese': True, 'has_english': True}

=== ç¤ºä¾‹ 2: ä½¿ç”¨é…ç½® ===
ç»“æœï¼ˆå«è¯¦æƒ…ï¼‰: {'char_count': 18, 'word_count': 3, 'line_count': 1, 'has_chinese': True, 'has_english': True, 'details': {'uppercase_count': 2, 'lowercase_count': 8, 'digit_count': 0, 'space_count': 2, 'punctuation_count': 2}}

=== ç¤ºä¾‹ 3: é”™è¯¯å¤„ç† ===
æ•è·é”™è¯¯: è¾“å…¥ä¸èƒ½ä¸ºç©º

=== ç¤ºä¾‹ 4: é›†æˆåˆ° LCEL é“¾ ===
é“¾å¼å¤„ç†ç»“æœ: æ–‡æœ¬åŒ…å« 2 ä¸ªè¯ï¼Œ11 ä¸ªå­—ç¬¦
```

### ç¤ºä¾‹ 2: å¸¦ç›‘æ§çš„ LLM è°ƒç”¨

```python
"""
å¸¦ç›‘æ§çš„ LLM è°ƒç”¨
æ¼”ç¤º Config å’Œå›è°ƒçš„å®é™…åº”ç”¨
"""

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import time

class PerformanceMonitor(BaseCallbackHandler):
    """æ€§èƒ½ç›‘æ§å›è°ƒ"""

    def __init__(self):
        self.start_time = None
        self.token_count = 0

    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLM å¼€å§‹æ—¶è®°å½•æ—¶é—´"""
        self.start_time = time.time()
        print(f"ğŸš€ LLM è°ƒç”¨å¼€å§‹")

    def on_llm_end(self, response, **kwargs):
        """LLM ç»“æŸæ—¶è®¡ç®—è€—æ—¶"""
        elapsed = time.time() - self.start_time
        print(f"âœ… LLM è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")

        # ç»Ÿè®¡ token ä½¿ç”¨
        if hasattr(response, 'llm_output') and response.llm_output:
            token_usage = response.llm_output.get('token_usage', {})
            print(f"ğŸ“Š Token ä½¿ç”¨: {token_usage}")

    def on_llm_error(self, error, **kwargs):
        """LLM é”™è¯¯æ—¶è®°å½•"""
        print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {error}")


# ===== ä½¿ç”¨ç¤ºä¾‹ =====
if __name__ == "__main__":
    # å®šä¹‰é“¾
    prompt = ChatPromptTemplate.from_template("å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡: {text}")
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    chain = prompt | llm

    # é…ç½®ç›‘æ§
    config = RunnableConfig(
        callbacks=[PerformanceMonitor()],
        tags=["translation", "production"],
        metadata={
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        },
        run_name="translation_with_monitoring"
    )

    # æ‰§è¡Œ
    print("=== å¸¦ç›‘æ§çš„ç¿»è¯‘ ===")
    result = chain.invoke({"text": "ä½ å¥½ï¼Œä¸–ç•Œï¼"}, config=config)
    print(f"ç¿»è¯‘ç»“æœ: {result.content}")
```

**è¿è¡Œè¾“å‡º**ï¼š
```
=== å¸¦ç›‘æ§çš„ç¿»è¯‘ ===
ğŸš€ LLM è°ƒç”¨å¼€å§‹
âœ… LLM è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: 1.23ç§’
ğŸ“Š Token ä½¿ç”¨: {'prompt_tokens': 15, 'completion_tokens': 5, 'total_tokens': 20}
ç¿»è¯‘ç»“æœ: Hello, World!
```

---

## ä½¿ç”¨åœºæ™¯

### é€‚åˆä½¿ç”¨ invoke çš„åœºæ™¯

| åœºæ™¯ | åŸå›  | ç¤ºä¾‹ |
|------|------|------|
| **å•æ¬¡æŸ¥è¯¢** | ç®€å•ç›´æ¥ï¼Œæ— éœ€å¹¶å‘ | ç¿»è¯‘ä¸€æ®µæ–‡æœ¬ |
| **åŒæ­¥å·¥ä½œæµ** | éœ€è¦ç­‰å¾…ç»“æœå†ç»§ç»­ | éªŒè¯ â†’ å¤„ç† â†’ ä¿å­˜ |
| **ç®€å•è„šæœ¬** | ä»£ç ç®€æ´ï¼Œæ˜“äºç†è§£ | å‘½ä»¤è¡Œå·¥å…· |
| **è°ƒè¯•æµ‹è¯•** | ä¾¿äºæ–­ç‚¹è°ƒè¯• | å•å…ƒæµ‹è¯• |

### ä¸é€‚åˆä½¿ç”¨ invoke çš„åœºæ™¯

| åœºæ™¯ | æ¨èæ–¹æ³• | åŸå›  |
|------|----------|------|
| **æ‰¹é‡å¤„ç†** | `batch` | å¹¶å‘æ‰§è¡Œæ›´å¿« |
| **å®æ—¶å“åº”** | `stream` | é™ä½æ„ŸçŸ¥å»¶è¿Ÿ |
| **é«˜å¹¶å‘** | `ainvoke` + `asyncio` | éé˜»å¡æ‰§è¡Œ |
| **é•¿æ—¶é—´ä»»åŠ¡** | `stream` æˆ–å¼‚æ­¥ | é¿å…é˜»å¡ |

---

## 2025-2026 æœ€ä½³å®è·µ

### 1. ä½¿ç”¨ç±»å‹æ³¨è§£

```python
from langchain_core.runnables import Runnable

# âœ… æ˜ç¡®ç±»å‹
class TypedProcessor(Runnable[str, dict]):
    def invoke(self, input: str, config=None) -> dict:
        return {"result": input}

# âŒ ç¼ºå°‘ç±»å‹
class UntypedProcessor(Runnable):
    def invoke(self, input, config=None):
        return {"result": input}
```

### 2. éªŒè¯è¾“å…¥

```python
def invoke(self, input: str, config=None) -> str:
    # âœ… éªŒè¯è¾“å…¥
    if not isinstance(input, str):
        raise TypeError(f"æœŸæœ› strï¼Œå®é™… {type(input)}")

    if not input.strip():
        raise ValueError("è¾“å…¥ä¸èƒ½ä¸ºç©º")

    return input.upper()
```

### 3. ä½¿ç”¨ Config è¿›è¡Œå¯è§‚æµ‹æ€§

```python
from langchain_core.runnables import RunnableConfig

# âœ… ç”Ÿäº§ç¯å¢ƒé…ç½®
config = RunnableConfig(
    tags=["production", "v2"],
    metadata={"user_id": "123"},
    callbacks=[monitoring_handler],
    run_name="my_chain"
)

result = chain.invoke(input, config=config)
```

### 4. é”™è¯¯å¤„ç†

```python
def invoke(self, input: str, config=None) -> str:
    try:
        return self._process(input)
    except ValueError as e:
        # âœ… è®°å½•å¹¶é‡æ–°æŠ›å‡º
        logger.error(f"è¾“å…¥éªŒè¯å¤±è´¥: {e}")
        raise
    except Exception as e:
        # âœ… åŒ…è£…æœªçŸ¥é”™è¯¯
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        raise RuntimeError(f"å¤„ç†å¤±è´¥: {e}") from e
```

---

## æ€»ç»“

### invoke æ–¹æ³•çš„æ ¸å¿ƒç‰¹ç‚¹

1. **åŒæ­¥æ‰§è¡Œ**: é˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°å®Œæˆ
2. **å•æ¬¡å¤„ç†**: ä¸€æ¬¡å¤„ç†ä¸€ä¸ªè¾“å…¥
3. **ç±»å‹å®‰å…¨**: é€šè¿‡æ³›å‹ç¡®ä¿ç±»å‹æ­£ç¡®
4. **é…ç½®çµæ´»**: é€šè¿‡ Config ä¼ é€’è¿è¡Œæ—¶å‚æ•°

### ä½•æ—¶ä½¿ç”¨ invoke

- âœ… å•æ¬¡æŸ¥è¯¢å’Œç®€å•è„šæœ¬
- âœ… åŒæ­¥å·¥ä½œæµå’Œè°ƒè¯•æµ‹è¯•
- âŒ æ‰¹é‡å¤„ç†ï¼ˆç”¨ batchï¼‰
- âŒ å®æ—¶å“åº”ï¼ˆç”¨ streamï¼‰

---

## å‚è€ƒèµ„æ–™

[^1]: [LangChain Runnable Methods Best Practices](https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557) - Medium, 2026
[^2]: [LangChain Best Practices](https://www.swarnendu.de/blog/langchain-best-practices) - Swarnendu De, 2025-2026

### å®˜æ–¹æ–‡æ¡£
- [Runnable invoke Reference](https://reference.langchain.com/python/langchain_core/runnables) - LangChain, 2025-2026
- [RunnableConfig Documentation](https://python.langchain.com/docs/concepts/runnables#config) - 2025-2026

---

**ä¸‹ä¸€æ­¥**: é˜…è¯» [03_æ ¸å¿ƒæ¦‚å¿µ_03_batchæ–¹æ³•.md](./03_æ ¸å¿ƒæ¦‚å¿µ_03_batchæ–¹æ³•.md) å­¦ä¹ æ‰¹é‡å¤„ç†å’Œæˆæœ¬ä¼˜åŒ–
