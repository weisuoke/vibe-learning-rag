# Token与Context Window - 核心概念

掌握以下 3 个核心概念，就能理解 LLM 的核心限制。

---

## 核心概念1：Token

**Token 是 LLM 处理文本的最小单位，介于字符和单词之间。**

### Token 是什么？

```python
# 使用 tiktoken 查看 Token 切分
import tiktoken

encoder = tiktoken.encoding_for_model("gpt-4o")

# 英文示例
text_en = "Hello, world!"
tokens_en = encoder.encode(text_en)
print(f"英文: '{text_en}'")
print(f"Token IDs: {tokens_en}")
print(f"Token 数量: {len(tokens_en)}")
# 输出: Token 数量: 4

# 中文示例
text_zh = "你好，世界！"
tokens_zh = encoder.encode(text_zh)
print(f"中文: '{text_zh}'")
print(f"Token IDs: {tokens_zh}")
print(f"Token 数量: {len(tokens_zh)}")
# 输出: Token 数量: 7
```

### Token 切分规律

| 文本类型 | 示例 | 大约 Token 数 |
|----------|------|---------------|
| 英文单词 | "hello" | 1 Token |
| 英文句子 | "Hello, world!" | 4 Token |
| 中文单字 | "你" | 1-2 Token |
| 中文句子 | "你好世界" | 4-8 Token |
| 代码 | `print("hi")` | 5-6 Token |
| 空格/标点 | " ", "," | 通常 1 Token |

**经验法则：**
- 英文：1 Token ≈ 4 个字符 ≈ 0.75 个单词
- 中文：1 Token ≈ 1-2 个汉字
- 代码：符号多，Token 密度高

### 在 RAG 开发中的应用

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-4o") -> int:
    """计算文本的 Token 数量"""
    encoder = tiktoken.encoding_for_model(model)
    return len(encoder.encode(text))

# RAG 场景：检查检索内容是否超限
retrieved_docs = "这是检索到的文档内容..." * 1000
token_count = count_tokens(retrieved_docs)

MAX_CONTEXT_TOKENS = 8000
if token_count > MAX_CONTEXT_TOKENS:
    print(f"警告：检索内容 {token_count} Token，超过限制 {MAX_CONTEXT_TOKENS}")
```

---

## 核心概念2：Context Window

**Context Window 是 LLM 一次能处理的最大 Token 数量（输入 + 输出）。**

### 主流模型的 Context Window

| 模型 | Context Window | 约等于 |
|------|----------------|--------|
| GPT-3.5 | 16K | ~12,000 汉字 |
| GPT-4o | 128K | ~96,000 汉字 |
| GPT-4o-mini | 128K | ~96,000 汉字 |
| Claude 3.5 Sonnet | 200K | ~150,000 汉字 |
| Claude 3 Opus | 200K | ~150,000 汉字 |

### Context Window 的组成

```
┌─────────────────────────────────────────┐
│           Context Window (128K)          │
├─────────────────────────────────────────┤
│  输入 Token                              │
│  ├── System Prompt (系统提示)            │
│  ├── 历史对话 (多轮对话场景)              │
│  ├── 检索内容 (RAG 注入)                 │
│  └── User Query (用户问题)               │
├─────────────────────────────────────────┤
│  输出 Token                              │
│  └── Assistant Response (模型回复)       │
└─────────────────────────────────────────┘

输入 Token + 输出 Token ≤ Context Window
```

### 超出限制会怎样？

```python
from openai import OpenAI

client = OpenAI()

# 构造超长输入
very_long_text = "这是一段很长的文本。" * 50000  # 故意超限

try:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": very_long_text}]
    )
except Exception as e:
    print(f"错误: {e}")
    # 错误: This model's maximum context length is 128000 tokens.
    # However, your messages resulted in 150000 tokens.
```

### 在 RAG 开发中的应用

```python
def check_context_limit(
    system_prompt: str,
    retrieved_docs: str,
    user_query: str,
    model: str = "gpt-4o",
    max_output_tokens: int = 4000
) -> bool:
    """检查是否超出 Context Window"""

    # 模型的 Context Window 限制
    MODEL_LIMITS = {
        "gpt-4o": 128000,
        "gpt-4o-mini": 128000,
        "gpt-3.5-turbo": 16000,
    }

    # 计算输入 Token
    total_input = count_tokens(system_prompt + retrieved_docs + user_query)

    # 预留输出空间
    available = MODEL_LIMITS.get(model, 8000) - max_output_tokens

    if total_input > available:
        print(f"超限！输入 {total_input} Token，可用 {available} Token")
        return False

    print(f"安全！输入 {total_input} Token，可用 {available} Token")
    return True
```

---

## 核心概念3：Token 预算分配

**Token 预算分配是在有限的 Context Window 中，合理分配各部分 Token 的策略。**

### RAG 的 Token 预算分配

```
Context Window: 128K Token
│
├── 系统提示词: ~500 Token (固定)
│   └── "你是一个专业的问答助手，基于以下资料回答问题..."
│
├── 用户问题: ~100-500 Token (可变)
│   └── 用户的实际问题
│
├── 检索内容: ~120K Token (主要空间！)
│   └── 从向量数据库检索的相关文档
│
└── 生成空间: ~4K-8K Token (预留给输出)
    └── 模型生成的回答
```

### 预算分配策略

```python
def allocate_token_budget(
    context_window: int = 128000,
    system_prompt_tokens: int = 500,
    user_query_tokens: int = 200,
    max_output_tokens: int = 4000,
    safety_margin: float = 0.95  # 留 5% 安全边际
) -> int:
    """计算可用于检索内容的 Token 预算"""

    available = int(context_window * safety_margin)
    reserved = system_prompt_tokens + user_query_tokens + max_output_tokens
    retrieval_budget = available - reserved

    print(f"Context Window: {context_window}")
    print(f"安全可用: {available}")
    print(f"已预留: {reserved}")
    print(f"检索预算: {retrieval_budget}")

    return retrieval_budget

# 示例
budget = allocate_token_budget()
# Context Window: 128000
# 安全可用: 121600
# 已预留: 4700
# 检索预算: 116900
```

### 检索内容的截断策略

```python
def truncate_to_budget(
    documents: list[str],
    budget: int,
    model: str = "gpt-4o"
) -> str:
    """将检索文档截断到预算内"""

    result = []
    current_tokens = 0

    for doc in documents:
        doc_tokens = count_tokens(doc, model)

        if current_tokens + doc_tokens <= budget:
            result.append(doc)
            current_tokens += doc_tokens
        else:
            # 预算不足，停止添加
            remaining = budget - current_tokens
            if remaining > 100:  # 至少能放 100 Token
                # 截断当前文档
                truncated = truncate_text_to_tokens(doc, remaining, model)
                result.append(truncated + "...[已截断]")
            break

    print(f"使用 {len(result)}/{len(documents)} 篇文档")
    print(f"使用 {current_tokens}/{budget} Token")

    return "\n\n---\n\n".join(result)
```

### 在 RAG 开发中的应用

```python
def build_rag_prompt(
    user_query: str,
    retrieved_docs: list[str],
    model: str = "gpt-4o"
) -> list[dict]:
    """构建 RAG Prompt，自动处理 Token 预算"""

    system_prompt = "基于以下参考资料回答问题。如果资料中没有相关信息，请说明。"

    # 计算预算
    budget = allocate_token_budget(
        context_window=128000,
        system_prompt_tokens=count_tokens(system_prompt),
        user_query_tokens=count_tokens(user_query),
        max_output_tokens=4000
    )

    # 截断检索内容
    context = truncate_to_budget(retrieved_docs, budget, model)

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"参考资料：\n{context}\n\n问题：{user_query}"}
    ]
```

---

## 概念关系图

```
                        Context Window
                             │
              ┌──────────────┼──────────────┐
              │              │              │
         输入 Token      输出 Token      硬限制
              │              │              │
    ┌─────────┼─────────┐    │              │
    │         │         │    │              │
 System    检索内容   User   │         超出报错
 Prompt      │       Query   │
             │               │
        Token 预算      max_tokens
             │               │
        截断/选择        生成长度
```

---

**上一节：** [02_第一性原理.md](./02_第一性原理.md)
**下一节：** [04_最小可用.md](./04_最小可用.md)
