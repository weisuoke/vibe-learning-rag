# 07_实战代码_02_RAG应用集成

> **本文档提供 RunnableLambda 在 RAG 应用中的完整实战代码。**

---

## 场景 1：文档分块自定义逻辑

### 需求

根据业务需求自定义文档分块策略，而不是使用默认的字符分割。

### 完整实现

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.documents import Document
import re

def smart_chunk_by_section(doc: Document) -> list[Document]:
    """按章节智能分块"""
    content = doc.page_content
    metadata = doc.metadata

    # 按标题分割（支持 Markdown 和纯文本）
    sections = re.split(r'\n#{1,3}\s+|\n\n[A-Z][^\n]+\n=+\n', content)

    chunks = []
    for i, section in enumerate(sections):
        if section.strip():
            chunks.append(Document(
                page_content=section.strip(),
                metadata={
                    **metadata,
                    "chunk_id": i,
                    "chunk_type": "section"
                }
            ))

    return chunks

def chunk_by_sentence(doc: Document, max_sentences: int = 5) -> list[Document]:
    """按句子分块"""
    content = doc.page_content
    sentences = re.split(r'[.!?]+\s+', content)

    chunks = []
    for i in range(0, len(sentences), max_sentences):
        chunk_sentences = sentences[i:i + max_sentences]
        chunks.append(Document(
            page_content='. '.join(chunk_sentences) + '.',
            metadata={
                **doc.metadata,
                "chunk_id": i // max_sentences,
                "sentence_count": len(chunk_sentences)
            }
        ))

    return chunks

def chunk_with_overlap(doc: Document, chunk_size: int = 500, overlap: int = 50) -> list[Document]:
    """带重叠的分块"""
    content = doc.page_content
    chunks = []

    start = 0
    chunk_id = 0
    while start < len(content):
        end = start + chunk_size
        chunk_text = content[start:end]

        chunks.append(Document(
            page_content=chunk_text,
            metadata={
                **doc.metadata,
                "chunk_id": chunk_id,
                "start_pos": start,
                "end_pos": end
            }
        ))

        start = end - overlap
        chunk_id += 1

    return chunks

# 使用示例
doc = Document(
    page_content="""
# Introduction
This is the introduction section.

# Methods
This section describes the methods used.

# Results
Here are the results of our study.
""",
    metadata={"source": "paper.pdf", "page": 1}
)

# 按章节分块
chunker = RunnableLambda(smart_chunk_by_section)
chunks = chunker.invoke(doc)
print(f"Generated {len(chunks)} chunks")

# 组合多种分块策略
def hybrid_chunking(doc: Document) -> list[Document]:
    """混合分块策略"""
    # 先按章节分
    sections = smart_chunk_by_section(doc)

    # 对长章节再按句子分
    final_chunks = []
    for section in sections:
        if len(section.page_content) > 1000:
            final_chunks.extend(chunk_by_sentence(section, max_sentences=3))
        else:
            final_chunks.append(section)

    return final_chunks

hybrid_chunker = RunnableLambda(hybrid_chunking)
```

---

## 场景 2：检索结果后处理

### 需求

对检索到的文档进行格式化、排序、去重等后处理。

### 完整实现

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.documents import Document

def format_retrieved_docs(docs: list[Document]) -> str:
    """格式化检索结果为文本"""
    if not docs:
        return "No relevant documents found."

    formatted = []
    for i, doc in enumerate(docs, 1):
        formatted.append(f"[Document {i}]")
        formatted.append(f"Content: {doc.page_content[:200]}...")
        formatted.append(f"Source: {doc.metadata.get('source', 'Unknown')}")
        if 'score' in doc.metadata:
            formatted.append(f"Relevance: {doc.metadata['score']:.2f}")
        formatted.append("")

    return "\n".join(formatted)

def deduplicate_docs(docs: list[Document]) -> list[Document]:
    """去重文档"""
    seen = set()
    unique_docs = []

    for doc in docs:
        # 使用内容的哈希作为唯一标识
        content_hash = hash(doc.page_content)
        if content_hash not in seen:
            seen.add(content_hash)
            unique_docs.append(doc)

    return unique_docs

def rerank_by_length(docs: list[Document]) -> list[Document]:
    """按长度重排序（优先短文档）"""
    return sorted(docs, key=lambda d: len(d.page_content))

def filter_by_score(docs: list[Document], min_score: float = 0.7) -> list[Document]:
    """按相似度分数过滤"""
    return [doc for doc in docs if doc.metadata.get('score', 0) >= min_score]

def add_context_window(docs: list[Document], window_size: int = 100) -> list[Document]:
    """添加上下文窗口"""
    # 假设文档有 start_pos 和 end_pos 元数据
    enhanced_docs = []
    for doc in docs:
        # 这里简化处理，实际需要从原文档中提取上下文
        enhanced_docs.append(Document(
            page_content=doc.page_content,
            metadata={
                **doc.metadata,
                "has_context": True
            }
        ))
    return enhanced_docs

# 组合后处理管道
postprocess_pipeline = (
    RunnableLambda(deduplicate_docs)
    | RunnableLambda(lambda docs: filter_by_score(docs, min_score=0.7))
    | RunnableLambda(rerank_by_length)
    | RunnableLambda(lambda docs: docs[:5])  # 只取前5个
    | RunnableLambda(format_retrieved_docs)
)

# 使用
retrieved_docs = [...]  # 从向量数据库检索的文档
formatted_context = postprocess_pipeline.invoke(retrieved_docs)
```

---

## 场景 3：完整的 RAG 链

### 需求

构建一个完整的 RAG 应用，包含检索、后处理、生成等步骤。

### 完整实现

```python
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import Chroma

# 1. 初始化组件
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)
model = ChatOpenAI(model="gpt-4")

# 2. 检索函数
def retrieve_docs(query: str) -> list[Document]:
    """检索相关文档"""
    docs = vectorstore.similarity_search_with_score(query, k=10)
    # 添加分数到元数据
    return [
        Document(
            page_content=doc.page_content,
            metadata={**doc.metadata, "score": score}
        )
        for doc, score in docs
    ]

# 3. 后处理函数
def postprocess_docs(docs: list[Document]) -> list[Document]:
    """后处理文档"""
    # 去重
    docs = deduplicate_docs(docs)
    # 过滤低分
    docs = filter_by_score(docs, min_score=0.7)
    # 只取前5个
    return docs[:5]

def docs_to_context(docs: list[Document]) -> str:
    """将文档转换为上下文字符串"""
    return "\n\n".join([
        f"Source: {doc.metadata.get('source', 'Unknown')}\n{doc.page_content}"
        for doc in docs
    ])

# 4. 答案后处理
def format_answer(answer: str) -> dict:
    """格式化答案"""
    return {
        "answer": answer,
        "length": len(answer),
        "word_count": len(answer.split())
    }

# 5. 构建完整 RAG 链
rag_chain = (
    # 步骤1: 准备输入
    RunnablePassthrough.assign(
        # 检索文档
        docs=RunnableLambda(lambda x: retrieve_docs(x["query"]))
    )
    # 步骤2: 后处理文档
    | RunnablePassthrough.assign(
        docs=RunnableLambda(lambda x: postprocess_docs(x["docs"]))
    )
    # 步骤3: 转换为上下文
    | RunnablePassthrough.assign(
        context=RunnableLambda(lambda x: docs_to_context(x["docs"]))
    )
    # 步骤4: 生成答案
    | ChatPromptTemplate.from_template(
        "Based on the following context:\n\n{context}\n\nAnswer the question: {query}"
    )
    | model
    | StrOutputParser()
    # 步骤5: 格式化答案
    | RunnableLambda(format_answer)
)

# 使用
result = rag_chain.invoke({"query": "What is LangChain?"})
print(result)
# {
#     "answer": "LangChain is a framework...",
#     "length": 150,
#     "word_count": 25
# }
```

---

## 场景 4：带缓存的 RAG

### 需求

为 RAG 应用添加缓存层，避免重复检索和生成。

### 完整实现

```python
from langchain_core.runnables import RunnableLambda
import hashlib
import json

class RAGCache:
    """RAG 缓存"""

    def __init__(self):
        self.cache = {}

    def get_key(self, query: str) -> str:
        """生成缓存键"""
        return hashlib.md5(query.encode()).hexdigest()

    def get(self, query: str):
        """获取缓存"""
        key = self.get_key(query)
        return self.cache.get(key)

    def set(self, query: str, result):
        """设置缓存"""
        key = self.get_key(query)
        self.cache[key] = result

# 创建缓存实例
cache = RAGCache()

def cached_retrieve(query: str) -> list[Document]:
    """带缓存的检索"""
    # 检查缓存
    cached = cache.get(f"retrieve:{query}")
    if cached:
        print("Cache hit for retrieval")
        return cached

    # 执行检索
    docs = retrieve_docs(query)

    # 存入缓存
    cache.set(f"retrieve:{query}", docs)
    return docs

def cached_generate(input: dict) -> str:
    """带缓存的生成"""
    query = input["query"]
    context = input["context"]

    # 检查缓存
    cache_key = f"generate:{query}:{hash(context)}"
    cached = cache.get(cache_key)
    if cached:
        print("Cache hit for generation")
        return cached

    # 执行生成
    prompt = ChatPromptTemplate.from_template(
        "Based on:\n{context}\n\nAnswer: {query}"
    )
    result = (prompt | model | StrOutputParser()).invoke(input)

    # 存入缓存
    cache.set(cache_key, result)
    return result

# 带缓存的 RAG 链
cached_rag_chain = (
    RunnablePassthrough.assign(
        docs=RunnableLambda(lambda x: cached_retrieve(x["query"]))
    )
    | RunnablePassthrough.assign(
        docs=RunnableLambda(lambda x: postprocess_docs(x["docs"]))
    )
    | RunnablePassthrough.assign(
        context=RunnableLambda(lambda x: docs_to_context(x["docs"]))
    )
    | RunnableLambda(cached_generate)
)

# 测试缓存
result1 = cached_rag_chain.invoke({"query": "What is LangChain?"})
result2 = cached_rag_chain.invoke({"query": "What is LangChain?"})  # 使用缓存
```

---

## 场景 5：多路召回 RAG

### 需求

使用多种检索策略，然后合并结果。

### 完整实现

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

def vector_search(query: str) -> list[Document]:
    """向量检索"""
    return vectorstore.similarity_search(query, k=5)

def keyword_search(query: str) -> list[Document]:
    """关键词检索"""
    # 使用 BM25 或其他关键词检索
    return bm25_retriever.get_relevant_documents(query)[:5]

def hybrid_search(query: str) -> list[Document]:
    """混合检索"""
    # 结合向量和关键词
    vector_docs = vector_search(query)
    keyword_docs = keyword_search(query)

    # 合并去重
    all_docs = vector_docs + keyword_docs
    return deduplicate_docs(all_docs)

# 并行多路召回
multi_retrieval = RunnableParallel(
    vector=RunnableLambda(vector_search),
    keyword=RunnableLambda(keyword_search),
    hybrid=RunnableLambda(hybrid_search)
)

def merge_results(results: dict) -> list[Document]:
    """合并多路召回结果"""
    all_docs = []

    # 添加来源标记
    for source, docs in results.items():
        for doc in docs:
            doc.metadata["retrieval_source"] = source
            all_docs.append(doc)

    # 去重并排序
    unique_docs = deduplicate_docs(all_docs)
    return unique_docs[:10]

# 完整的多路召回 RAG
multi_rag_chain = (
    RunnableLambda(lambda x: x["query"])
    | multi_retrieval
    | RunnableLambda(merge_results)
    | RunnableLambda(postprocess_docs)
    | RunnableLambda(docs_to_context)
    | RunnableLambda(lambda context: {
        "context": context,
        "query": query  # 需要从外部传入
    })
    | ChatPromptTemplate.from_template(
        "Context:\n{context}\n\nQuestion: {query}"
    )
    | model
    | StrOutputParser()
)
```

---

## 实战技巧

### 技巧 1：动态调整检索数量

```python
def adaptive_retrieve(query: str) -> list[Document]:
    """根据查询复杂度调整检索数量"""
    # 简单查询：少检索
    if len(query.split()) < 5:
        k = 3
    # 复杂查询：多检索
    else:
        k = 10

    return vectorstore.similarity_search(query, k=k)
```

### 技巧 2：文档质量过滤

```python
def filter_quality(docs: list[Document]) -> list[Document]:
    """过滤低质量文档"""
    return [
        doc for doc in docs
        if len(doc.page_content) > 50  # 最小长度
        and len(doc.page_content) < 2000  # 最大长度
        and doc.metadata.get('score', 0) > 0.6  # 最小分数
    ]
```

### 技巧 3：上下文压缩

```python
def compress_context(docs: list[Document], max_length: int = 2000) -> str:
    """压缩上下文到指定长度"""
    context = docs_to_context(docs)

    if len(context) <= max_length:
        return context

    # 截断并添加省略号
    return context[:max_length] + "..."
```

---

## 完整示例：生产级 RAG

```python
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

# 构建生产级 RAG 链
production_rag = (
    # 输入验证
    RunnableLambda(lambda x: {
        "query": x["query"].strip(),
        "max_docs": x.get("max_docs", 5)
    })
    # 检索（带重试）
    | RunnablePassthrough.assign(
        docs=RunnableLambda(adaptive_retrieve).with_retry(
            stop_after_attempt=3
        )
    )
    # 后处理
    | RunnablePassthrough.assign(
        docs=RunnableLambda(lambda x: postprocess_docs(x["docs"]))
    )
    # 质量过滤
    | RunnablePassthrough.assign(
        docs=RunnableLambda(lambda x: filter_quality(x["docs"]))
    )
    # 转换上下文
    | RunnablePassthrough.assign(
        context=RunnableLambda(lambda x: compress_context(x["docs"], max_length=2000))
    )
    # 生成答案（带重试）
    | ChatPromptTemplate.from_template(
        "Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"
    )
    | model.with_retry(stop_after_attempt=2)
    | StrOutputParser()
    # 格式化输出
    | RunnableLambda(format_answer)
)

# 使用
result = production_rag.invoke({
    "query": "What is LangChain?",
    "max_docs": 5
})
```

---

## 总结

### 核心模式

1. **文档分块**: 自定义分块策略
2. **检索后处理**: 去重、过滤、排序
3. **多路召回**: 并行检索 + 合并
4. **缓存优化**: 避免重复计算

### 最佳实践

- 添加错误重试
- 实现缓存机制
- 质量过滤
- 上下文压缩

---

## 引用来源

- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [MUFG Bank Case Study](https://blog.langchain.dev/customers-mufgbank)

---

**版本**: v1.0
**最后更新**: 2026-02-20
