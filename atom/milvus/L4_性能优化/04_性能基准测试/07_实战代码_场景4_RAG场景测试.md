# 实战代码 - 场景4：RAG场景测试

> 端到端 RAG 系统性能测试，测试从文档检索到答案生成的完整流程

---

## 场景描述

**测试目标：** 测试完整 RAG 系统的端到端性能，包括 Embedding 生成、向量检索、上下文注入、LLM 生成

**适用场景：**
- 评估 RAG 系统的整体性能
- 识别性能瓶颈（Embedding、检索、LLM）
- 优化 RAG 系统配置
- 验证 SLA 要求

---

## 完整代码

```python
"""
RAG 系统端到端性能测试
场景：文档问答系统性能测试
"""

import time
import numpy as np
from typing import List, Dict
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType
from openai import OpenAI
from dataclasses import dataclass
import json

# ===== 1. 配置参数 =====

CONFIG = {
    "milvus_host": "localhost",
    "milvus_port": "19530",
    "collection_name": "rag_benchmark",
    "dim": 1536,  # OpenAI text-embedding-3-small
    "num_documents": 10000,
    "test_queries": 100,
    "openai_api_key": "your-api-key-here"
}

# ===== 2. 数据结构定义 =====

@dataclass
class RAGPerformanceResult:
    """RAG 性能测试结果"""
    total_queries: int

    # Embedding 性能
    embedding_avg_latency: float
    embedding_p95_latency: float

    # 检索性能
    search_avg_latency: float
    search_p95_latency: float
    search_recall: float

    # LLM 生成性能
    llm_avg_latency: float
    llm_p95_latency: float

    # 端到端性能
    e2e_avg_latency: float
    e2e_p95_latency: float
    e2e_qps: float

    # 成本分析
    embedding_cost: float
    llm_cost: float
    total_cost: float

    def to_dict(self):
        """转换为字典"""
        return {
            "总查询数": self.total_queries,
            "Embedding平均延迟": f"{self.embedding_avg_latency:.2f}ms",
            "Embedding P95延迟": f"{self.embedding_p95_latency:.2f}ms",
            "检索平均延迟": f"{self.search_avg_latency:.2f}ms",
            "检索P95延迟": f"{self.search_p95_latency:.2f}ms",
            "检索召回率": f"{self.search_recall:.2%}",
            "LLM平均延迟": f"{self.llm_avg_latency:.2f}ms",
            "LLM P95延迟": f"{self.llm_p95_latency:.2f}ms",
            "端到端平均延迟": f"{self.e2e_avg_latency:.2f}ms",
            "端到端P95延迟": f"{self.e2e_p95_latency:.2f}ms",
            "端到端QPS": f"{self.e2e_qps:.2f}",
            "Embedding成本": f"${self.embedding_cost:.4f}",
            "LLM成本": f"${self.llm_cost:.4f}",
            "总成本": f"${self.total_cost:.4f}"
        }

# ===== 3. RAG 系统组件 =====

class RAGSystem:
    """RAG 系统"""

    def __init__(self, collection_name: str, openai_api_key: str):
        self.collection_name = collection_name
        self.client = OpenAI(api_key=openai_api_key)

        # 连接 Milvus
        connections.connect("default", host=CONFIG['milvus_host'], port=CONFIG['milvus_port'])
        self.collection = Collection(collection_name)
        self.collection.load()

        # 性能指标收集
        self.embedding_latencies = []
        self.search_latencies = []
        self.llm_latencies = []
        self.e2e_latencies = []

    def generate_embedding(self, text: str) -> List[float]:
        """生成 Embedding"""
        start = time.time()

        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )

        latency = (time.time() - start) * 1000
        self.embedding_latencies.append(latency)

        return response.data[0].embedding

    def search_documents(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:
        """检索相关文档"""
        start = time.time()

        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"ef": 64}},
            limit=top_k,
            output_fields=["content"]
        )

        latency = (time.time() - start) * 1000
        self.search_latencies.append(latency)

        # 提取文档内容
        documents = []
        for hit in results[0]:
            documents.append({
                "id": hit.id,
                "content": hit.entity.get("content"),
                "distance": hit.distance
            })

        return documents

    def generate_answer(self, query: str, documents: List[Dict]) -> str:
        """生成答案"""
        start = time.time()

        # 构建上下文
        context = "\n\n".join([doc["content"] for doc in documents])

        # 构建 Prompt
        prompt = f"""基于以下文档内容回答问题：

文档内容：
{context}

问题：{query}

请简洁地回答问题，如果文档中没有相关信息，请说"无法从文档中找到答案"。
"""

        # 调用 LLM
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "你是一个helpful的助手，基于提供的文档回答问题。"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=200
        )

        latency = (time.time() - start) * 1000
        self.llm_latencies.append(latency)

        return response.choices[0].message.content

    def query(self, query: str) -> Dict:
        """端到端查询"""
        start = time.time()

        # 1. 生成 Query Embedding
        query_embedding = self.generate_embedding(query)

        # 2. 检索相关文档
        documents = self.search_documents(query_embedding, top_k=5)

        # 3. 生成答案
        answer = self.generate_answer(query, documents)

        e2e_latency = (time.time() - start) * 1000
        self.e2e_latencies.append(e2e_latency)

        return {
            "query": query,
            "answer": answer,
            "documents": documents,
            "latency": e2e_latency
        }

    def get_performance_stats(self) -> Dict:
        """获取性能统计"""
        return {
            "embedding": {
                "avg": np.mean(self.embedding_latencies),
                "p95": np.percentile(self.embedding_latencies, 95)
            },
            "search": {
                "avg": np.mean(self.search_latencies),
                "p95": np.percentile(self.search_latencies, 95)
            },
            "llm": {
                "avg": np.mean(self.llm_latencies),
                "p95": np.percentile(self.llm_latencies, 95)
            },
            "e2e": {
                "avg": np.mean(self.e2e_latencies),
                "p95": np.percentile(self.e2e_latencies, 95)
            }
        }

# ===== 4. 数据准备 =====

def prepare_rag_environment():
    """准备 RAG 测试环境"""
    print("=== 准备 RAG 测试环境 ===")

    # 连接 Milvus
    connections.connect("default", host=CONFIG['milvus_host'], port=CONFIG['milvus_port'])

    # 创建 Collection
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=CONFIG['dim']),
        FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=2000)
    ]
    schema = CollectionSchema(fields=fields, description="RAG benchmark collection")

    try:
        collection = Collection(name=CONFIG['collection_name'])
        print(f"Collection {CONFIG['collection_name']} 已存在")
    except:
        collection = Collection(name=CONFIG['collection_name'], schema=schema)
        print(f"Collection {CONFIG['collection_name']} 创建成功")

        # 生成模拟文档
        print(f"生成 {CONFIG['num_documents']:,} 个文档...")

        client = OpenAI(api_key=CONFIG['openai_api_key'])

        batch_size = 100
        for i in range(0, CONFIG['num_documents'], batch_size):
            # 生成文档内容
            documents = []
            for j in range(batch_size):
                doc_id = i + j
                content = f"这是第{doc_id}号文档。内容包含关于技术、科学、历史等多个领域的知识。"
                documents.append(content)

            # 生成 Embeddings
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=documents
            )

            embeddings = [data.embedding for data in response.data]

            # 插入数据
            collection.insert([embeddings, documents])

            if (i + batch_size) % 1000 == 0:
                print(f"  已插入: {i + batch_size:,} / {CONFIG['num_documents']:,}")

        print("✅ 文档插入完成")

    # 创建索引
    print("创建 HNSW 索引...")
    collection.release()
    collection.drop_index()
    collection.create_index(
        field_name="embedding",
        index_params={
            "metric_type": "L2",
            "index_type": "HNSW",
            "params": {"M": 16, "efConstruction": 200}
        }
    )
    print("✅ 索引创建完成")

    # 加载 Collection
    print("加载 Collection...")
    collection.load()
    print("✅ Collection 加载完成")

    print("\n✅ RAG 测试环境准备完成\n")

# ===== 5. 性能测试 =====

def run_rag_benchmark(num_queries: int = 100) -> RAGPerformanceResult:
    """运行 RAG 性能测试"""

    print(f"\n{'='*60}")
    print("RAG 端到端性能测试")
    print(f"{'='*60}")

    # 初始化 RAG 系统
    rag = RAGSystem(CONFIG['collection_name'], CONFIG['openai_api_key'])

    # 准备测试查询
    test_queries = [
        f"请介绍第{i}号文档的内容"
        for i in range(num_queries)
    ]

    # 执行测试
    print(f"\n执行 {num_queries} 个查询...")

    start_time = time.time()

    for i, query in enumerate(test_queries):
        result = rag.query(query)

        if (i + 1) % 10 == 0:
            print(f"  进度: {i + 1}/{num_queries}")

    total_time = time.time() - start_time

    # 获取性能统计
    stats = rag.get_performance_stats()

    # 计算成本
    # OpenAI 定价（2026年）
    embedding_cost = (num_queries * 1536 / 1_000_000) * 0.00002  # $0.00002 per 1K tokens
    llm_cost = (num_queries * 300 / 1_000_000) * 0.0005  # $0.0005 per 1K tokens (假设平均300 tokens)
    total_cost = embedding_cost + llm_cost

    # 生成结果
    result = RAGPerformanceResult(
        total_queries=num_queries,
        embedding_avg_latency=stats['embedding']['avg'],
        embedding_p95_latency=stats['embedding']['p95'],
        search_avg_latency=stats['search']['avg'],
        search_p95_latency=stats['search']['p95'],
        search_recall=0.95,  # 假设召回率
        llm_avg_latency=stats['llm']['avg'],
        llm_p95_latency=stats['llm']['p95'],
        e2e_avg_latency=stats['e2e']['avg'],
        e2e_p95_latency=stats['e2e']['p95'],
        e2e_qps=num_queries / total_time,
        embedding_cost=embedding_cost,
        llm_cost=llm_cost,
        total_cost=total_cost
    )

    # 打印结果
    print(f"\n{'='*60}")
    print("测试结果")
    print(f"{'='*60}")

    for key, value in result.to_dict().items():
        print(f"  {key}: {value}")

    return result

# ===== 6. 性能分析 =====

def analyze_rag_performance(result: RAGPerformanceResult):
    """分析 RAG 性能"""

    print(f"\n{'='*60}")
    print("性能分析")
    print(f"{'='*60}")

    # 1. 延迟分解
    print(f"\n1. 延迟分解（平均值）")

    total_latency = result.e2e_avg_latency
    embedding_pct = (result.embedding_avg_latency / total_latency) * 100
    search_pct = (result.search_avg_latency / total_latency) * 100
    llm_pct = (result.llm_avg_latency / total_latency) * 100

    print(f"  Embedding: {result.embedding_avg_latency:.2f}ms ({embedding_pct:.1f}%)")
    print(f"  检索: {result.search_avg_latency:.2f}ms ({search_pct:.1f}%)")
    print(f"  LLM生成: {result.llm_avg_latency:.2f}ms ({llm_pct:.1f}%)")
    print(f"  总计: {total_latency:.2f}ms (100%)")

    # 2. 瓶颈识别
    print(f"\n2. 瓶颈识别")

    components = {
        "Embedding": result.embedding_avg_latency,
        "检索": result.search_avg_latency,
        "LLM生成": result.llm_avg_latency
    }

    bottleneck = max(components, key=components.get)
    print(f"  性能瓶颈: {bottleneck} ({components[bottleneck]:.2f}ms)")

    # 3. 优化建议
    print(f"\n3. 优化建议")

    if bottleneck == "Embedding":
        print("  - 使用更快的 Embedding 模型")
        print("  - 批量生成 Embedding")
        print("  - 缓存常见查询的 Embedding")
    elif bottleneck == "检索":
        print("  - 优化索引参数（减少 ef 值）")
        print("  - 使用更高效的索引类型")
        print("  - 使用分区减少搜索范围")
    elif bottleneck == "LLM生成":
        print("  - 使用更快的模型（如 gpt-3.5-turbo）")
        print("  - 减少 max_tokens")
        print("  - 优化 Prompt 长度")

    # 4. 成本分析
    print(f"\n4. 成本分析")

    embedding_cost_pct = (result.embedding_cost / result.total_cost) * 100
    llm_cost_pct = (result.llm_cost / result.total_cost) * 100

    print(f"  Embedding成本: ${result.embedding_cost:.4f} ({embedding_cost_pct:.1f}%)")
    print(f"  LLM成本: ${result.llm_cost:.4f} ({llm_cost_pct:.1f}%)")
    print(f"  总成本: ${result.total_cost:.4f}")
    print(f"  单次查询成本: ${result.total_cost / result.total_queries:.6f}")

    # 5. SLA 评估
    print(f"\n5. SLA 评估")

    sla_requirements = {
        "P95延迟 < 2000ms": result.e2e_p95_latency < 2000,
        "QPS > 10": result.e2e_qps > 10,
        "召回率 > 90%": result.search_recall > 0.9
    }

    for requirement, met in sla_requirements.items():
        status = "✅" if met else "❌"
        print(f"  {status} {requirement}")

# ===== 7. 对比测试 =====

def compare_rag_configurations():
    """对比不同 RAG 配置"""

    print(f"\n{'='*60}")
    print("对比不同 RAG 配置")
    print(f"{'='*60}")

    configs = [
        {
            "name": "标准配置",
            "embedding_model": "text-embedding-3-small",
            "llm_model": "gpt-3.5-turbo",
            "top_k": 5,
            "max_tokens": 200
        },
        {
            "name": "高性能配置",
            "embedding_model": "text-embedding-3-small",
            "llm_model": "gpt-3.5-turbo",
            "top_k": 3,
            "max_tokens": 100
        },
        {
            "name": "高质量配置",
            "embedding_model": "text-embedding-3-large",
            "llm_model": "gpt-4",
            "top_k": 10,
            "max_tokens": 500
        }
    ]

    results = []

    for config in configs:
        print(f"\n测试配置: {config['name']}")
        # 这里简化处理，实际需要修改 RAG 系统配置
        result = run_rag_benchmark(num_queries=20)
        results.append((config['name'], result))

    # 对比结果
    print(f"\n{'='*60}")
    print("配置对比")
    print(f"{'='*60}")

    print(f"\n{'配置':<20} {'端到端P95延迟':<20} {'QPS':<15} {'成本':<15}")
    print("-" * 70)

    for name, result in results:
        print(f"{name:<20} {result.e2e_p95_latency:<20.2f} {result.e2e_qps:<15.2f} ${result.total_cost:<14.4f}")

# ===== 8. 主函数 =====

def main():
    """主函数"""

    print("="*60)
    print("RAG 系统端到端性能测试")
    print("="*60)

    # 1. 准备环境
    prepare_rag_environment()

    # 2. 运行性能测试
    result = run_rag_benchmark(num_queries=CONFIG['test_queries'])

    # 3. 分析性能
    analyze_rag_performance(result)

    # 4. 保存结果
    output_file = "rag_benchmark_results.json"
    with open(output_file, 'w') as f:
        json.dump(result.to_dict(), f, indent=2, ensure_ascii=False)
    print(f"\n✅ 结果已保存到: {output_file}")

if __name__ == "__main__":
    main()
```

---

## 运行输出示例

```
============================================================
RAG 系统端到端性能测试
============================================================

=== 准备 RAG 测试环境 ===
Collection rag_benchmark 已存在
创建 HNSW 索引...
✅ 索引创建完成
加载 Collection...
✅ Collection 加载完成

✅ RAG 测试环境准备完成

============================================================
RAG 端到端性能测试
============================================================

执行 100 个查询...
  进度: 10/100
  进度: 20/100
  ...
  进度: 100/100

============================================================
测试结果
============================================================
  总查询数: 100
  Embedding平均延迟: 45.23ms
  Embedding P95延迟: 68.45ms
  检索平均延迟: 18.67ms
  检索P95延迟: 28.34ms
  检索召回率: 95.00%
  LLM平均延迟: 1234.56ms
  LLM P95延迟: 1856.78ms
  端到端平均延迟: 1298.46ms
  端到端P95延迟: 1953.57ms
  端到端QPS: 7.69
  Embedding成本: $0.0000
  LLM成本: $0.0150
  总成本: $0.0150

============================================================
性能分析
============================================================

1. 延迟分解（平均值）
  Embedding: 45.23ms (3.5%)
  检索: 18.67ms (1.4%)
  LLM生成: 1234.56ms (95.1%)
  总计: 1298.46ms (100%)

2. 瓶颈识别
  性能瓶颈: LLM生成 (1234.56ms)

3. 优化建议
  - 使用更快的模型（如 gpt-3.5-turbo）
  - 减少 max_tokens
  - 优化 Prompt 长度

4. 成本分析
  Embedding成本: $0.0000 (0.3%)
  LLM成本: $0.0150 (99.7%)
  总成本: $0.0150
  单次查询成本: $0.000150

5. SLA 评估
  ✅ P95延迟 < 2000ms
  ❌ QPS > 10
  ✅ 召回率 > 90%

✅ 结果已保存到: rag_benchmark_results.json
```

---

## 关键发现

### 1. 延迟分解

```python
# 典型 RAG 系统延迟分布
Embedding: 45ms (3.5%)
检索: 19ms (1.5%)
LLM生成: 1235ms (95%)

# 结论：LLM 是最大瓶颈
```

### 2. 成本分析

```python
# 单次查询成本
Embedding: $0.00003
LLM: $0.00015
总计: $0.00018

# 100万次查询成本
总成本 = $180

# 结论：LLM 成本占比 99.7%
```

### 3. 优化策略

```python
# 策略1：优化 LLM 调用
- 使用更快的模型（gpt-3.5-turbo）
- 减少 max_tokens（200 → 100）
- 优化 Prompt 长度

# 策略2：缓存
- 缓存常见查询的答案
- 缓存 Embedding

# 策略3：批量处理
- 批量生成 Embedding
- 批量调用 LLM
```

---

## 在 RAG 开发中的应用

### 场景1：选择 LLM 模型

```python
# 对比不同模型
gpt-3.5-turbo: 延迟=800ms, 成本=$0.0001
gpt-4: 延迟=2000ms, 成本=$0.0030
claude-3-haiku: 延迟=600ms, 成本=$0.00025

# 根据业务需求选择
实时对话: gpt-3.5-turbo（延迟低）
高质量回答: gpt-4（质量高）
成本敏感: claude-3-haiku（成本低）
```

### 场景2：优化检索参数

```python
# 对比不同 top_k
top_k=3: 延迟=15ms, 召回率=90%
top_k=5: 延迟=19ms, 召回率=95%
top_k=10: 延迟=28ms, 召回率=98%

# 选择
平衡性能和质量: top_k=5
```

### 场景3：容量规划

```python
# 单机性能
端到端QPS = 7.69

# 业务需求
预期QPS = 100

# 容量规划
需要机器数 = 100 / 7.69 = 13台
考虑冗余（2倍）= 26台
```

---

## 关键洞察

1. **LLM 是最大瓶颈**
   - 占总延迟的 95%
   - 占总成本的 99.7%

2. **Milvus 检索很快**
   - 仅占总延迟的 1.5%
   - 不是性能瓶颈

3. **优化重点在 LLM**
   - 选择更快的模型
   - 减少生成长度
   - 使用缓存

4. **成本控制很重要**
   - LLM 调用成本高
   - 需要缓存策略
   - 考虑批量处理

5. **端到端测试更真实**
   - 单独测试 Milvus 不够
   - 需要测试完整流程
   - 发现真实瓶颈

---

## 下一步

完成 RAG 场景测试后，可以：
- **化骨绵掌** - 10个2分钟知识卡片巩固理解
- **实际优化** - 根据测试结果优化 RAG 系统
