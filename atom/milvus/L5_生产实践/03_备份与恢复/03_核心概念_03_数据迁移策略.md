# æ ¸å¿ƒæ¦‚å¿µ3ï¼šæ•°æ®è¿ç§»ç­–ç•¥

> ç”Ÿäº§ç¯å¢ƒæ•°æ®è¿ç§»çš„å®Œæ•´æ–¹æ¡ˆ

---

## ä»€ä¹ˆæ˜¯æ•°æ®è¿ç§»ï¼Ÿ

**æ•°æ®è¿ç§»** æ˜¯å°†æ•°æ®ä»ä¸€ä¸ª Milvus å®ä¾‹è¿ç§»åˆ°å¦ä¸€ä¸ªå®ä¾‹çš„è¿‡ç¨‹ï¼ŒåŒ…æ‹¬è·¨é›†ç¾¤ã€è·¨ç‰ˆæœ¬ã€è·¨ç¯å¢ƒçš„è¿ç§»ã€‚

**å¸¸è§è¿ç§»åœºæ™¯ï¼š**
- ğŸ”„ å¼€å‘ç¯å¢ƒ â†’ æµ‹è¯•ç¯å¢ƒ â†’ ç”Ÿäº§ç¯å¢ƒ
- ğŸš€ æœ¬åœ°éƒ¨ç½² â†’ äº‘ç«¯éƒ¨ç½²
- ğŸ“¦ å•æœºç‰ˆ â†’ åˆ†å¸ƒå¼é›†ç¾¤
- ğŸ”§ ç‰ˆæœ¬å‡çº§ï¼ˆMilvus 2.x â†’ 2.yï¼‰
- ğŸŒ è·¨åœ°åŸŸè¿ç§»ï¼ˆUS â†’ EUï¼‰

---

## è¿ç§»ç­–ç•¥å¯¹æ¯”

### 1. å››ç§è¿ç§»ç­–ç•¥

| ç­–ç•¥ | åœæœºæ—¶é—´ | å¤æ‚åº¦ | æ•°æ®ä¸€è‡´æ€§ | é€‚ç”¨åœºæ™¯ |
|------|---------|--------|-----------|----------|
| **å†·è¿ç§»** | é•¿ï¼ˆå°æ—¶çº§ï¼‰ | ä½ | å®Œå…¨ä¸€è‡´ | å°è§„æ¨¡ã€å¯åœæœº |
| **çƒ­è¿ç§»** | çŸ­ï¼ˆåˆ†é’Ÿçº§ï¼‰ | ä¸­ | æœ€ç»ˆä¸€è‡´ | ä¸­è§„æ¨¡ã€ä½åœæœº |
| **åŒå†™è¿ç§»** | æ—  | é«˜ | å¼ºä¸€è‡´ | å¤§è§„æ¨¡ã€é›¶åœæœº |
| **å¢é‡è¿ç§»** | æçŸ­ï¼ˆç§’çº§ï¼‰ | ä¸­ | æœ€ç»ˆä¸€è‡´ | æŒç»­æ›´æ–°çš„æ•°æ® |

### 2. ç­–ç•¥é€‰æ‹©å†³ç­–æ ‘

```
æ•°æ®é‡å¤§å°ï¼Ÿ
â”œâ”€ < 1GBï¼šå†·è¿ç§»ï¼ˆæœ€ç®€å•ï¼‰
â”œâ”€ 1GB - 100GBï¼šçƒ­è¿ç§»ï¼ˆå¹³è¡¡ï¼‰
â””â”€ > 100GBï¼š
    â””â”€ èƒ½å¦åœæœºï¼Ÿ
        â”œâ”€ èƒ½ï¼šå†·è¿ç§» + å¹¶è¡Œ
        â””â”€ ä¸èƒ½ï¼šåŒå†™è¿ç§»
```

---

## ç­–ç•¥1ï¼šå†·è¿ç§»ï¼ˆåœæœºè¿ç§»ï¼‰

### åŸç†

```
1. åœæ­¢åº”ç”¨å†™å…¥
2. å¤‡ä»½æºæ•°æ®
3. ä¼ è¾“åˆ°ç›®æ ‡
4. æ¢å¤æ•°æ®
5. éªŒè¯å®Œæ•´æ€§
6. åˆ‡æ¢æµé‡
```

### å®ç°

```python
class ColdMigration:
    """å†·è¿ç§»ï¼ˆåœæœºè¿ç§»ï¼‰"""

    def migrate(
        self,
        source_host: str,
        target_host: str,
        collection_name: str
    ):
        """æ‰§è¡Œå†·è¿ç§»"""
        print("=== å†·è¿ç§»å¼€å§‹ ===")

        # 1. åœæ­¢åº”ç”¨å†™å…¥
        print("[1/6] åœæ­¢åº”ç”¨å†™å…¥...")
        self.stop_application()

        # 2. å¤‡ä»½æºæ•°æ®
        print("[2/6] å¤‡ä»½æºæ•°æ®...")
        backup_dir = self.backup_source(source_host, collection_name)

        # 3. ä¼ è¾“åˆ°ç›®æ ‡
        print("[3/6] ä¼ è¾“æ•°æ®...")
        self.transfer_data(backup_dir, target_host)

        # 4. æ¢å¤æ•°æ®
        print("[4/6] æ¢å¤æ•°æ®...")
        self.restore_target(target_host, collection_name, backup_dir)

        # 5. éªŒè¯å®Œæ•´æ€§
        print("[5/6] éªŒè¯æ•°æ®...")
        if not self.verify_migration(source_host, target_host, collection_name):
            raise Exception("æ•°æ®éªŒè¯å¤±è´¥")

        # 6. åˆ‡æ¢æµé‡
        print("[6/6] åˆ‡æ¢æµé‡...")
        self.switch_traffic(target_host)

        print("âœ… å†·è¿ç§»å®Œæˆ")

    def backup_source(self, host: str, collection_name: str) -> str:
        """å¤‡ä»½æºæ•°æ®"""
        connections.connect(alias="source", host=host, port=19530)
        exporter = CollectionExporter()
        backup_dir = f"./migration/{collection_name}"
        exporter.export_collection(collection_name, backup_dir)
        return backup_dir

    def restore_target(self, host: str, collection_name: str, backup_dir: str):
        """æ¢å¤åˆ°ç›®æ ‡"""
        connections.connect(alias="target", host=host, port=19530)
        importer = CollectionImporter()
        importer.import_collection(collection_name, backup_dir)

    def verify_migration(
        self,
        source_host: str,
        target_host: str,
        collection_name: str
    ) -> bool:
        """éªŒè¯è¿ç§»"""
        # è¿æ¥æºå’Œç›®æ ‡
        connections.connect(alias="source", host=source_host)
        connections.connect(alias="target", host=target_host)

        source = Collection(collection_name, using="source")
        target = Collection(collection_name, using="target")

        # éªŒè¯æ•°æ®é‡
        if source.num_entities != target.num_entities:
            print(f"âŒ æ•°æ®é‡ä¸ä¸€è‡´: {source.num_entities} vs {target.num_entities}")
            return False

        # æŠ½æ ·éªŒè¯
        sample_size = min(1000, source.num_entities)
        sample_ids = random.sample(range(source.num_entities), sample_size)

        source_data = source.query(f"id in {sample_ids}", output_fields=["*"])
        target_data = target.query(f"id in {sample_ids}", output_fields=["*"])

        if source_data != target_data:
            print("âŒ æ•°æ®å†…å®¹ä¸ä¸€è‡´")
            return False

        print("âœ… æ•°æ®éªŒè¯é€šè¿‡")
        return True
```

### ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹ï¼š**
- âœ… å®ç°ç®€å•
- âœ… æ•°æ®å®Œå…¨ä¸€è‡´
- âœ… å®¹æ˜“å›æ»š

**ç¼ºç‚¹ï¼š**
- âŒ éœ€è¦åœæœº
- âŒ åœæœºæ—¶é—´é•¿
- âŒ å½±å“ä¸šåŠ¡

---

## ç­–ç•¥2ï¼šçƒ­è¿ç§»ï¼ˆåœ¨çº¿è¿ç§»ï¼‰

### åŸç†

```
1. å¤‡ä»½æºæ•°æ®ï¼ˆä¸åœæœºï¼‰
2. ä¼ è¾“åˆ°ç›®æ ‡
3. æ¢å¤æ•°æ®
4. åŒæ­¥å¢é‡æ•°æ®
5. å¿«é€Ÿåˆ‡æ¢
```

### å®ç°

```python
class HotMigration:
    """çƒ­è¿ç§»ï¼ˆåœ¨çº¿è¿ç§»ï¼‰"""

    def migrate(
        self,
        source_host: str,
        target_host: str,
        collection_name: str
    ):
        """æ‰§è¡Œçƒ­è¿ç§»"""
        print("=== çƒ­è¿ç§»å¼€å§‹ ===")

        # 1. è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # 2. å…¨é‡å¤‡ä»½ï¼ˆä¸åœæœºï¼‰
        print("[1/5] å…¨é‡å¤‡ä»½...")
        backup_dir = self.backup_source(source_host, collection_name)

        # 3. ä¼ è¾“å¹¶æ¢å¤
        print("[2/5] ä¼ è¾“å¹¶æ¢å¤...")
        self.restore_target(target_host, collection_name, backup_dir)

        # 4. åŒæ­¥å¢é‡æ•°æ®
        print("[3/5] åŒæ­¥å¢é‡æ•°æ®...")
        self.sync_incremental(
            source_host,
            target_host,
            collection_name,
            start_time
        )

        # 5. éªŒè¯æ•°æ®
        print("[4/5] éªŒè¯æ•°æ®...")
        if not self.verify_migration(source_host, target_host, collection_name):
            raise Exception("æ•°æ®éªŒè¯å¤±è´¥")

        # 6. å¿«é€Ÿåˆ‡æ¢ï¼ˆçŸ­æš‚åœæœºï¼‰
        print("[5/5] åˆ‡æ¢æµé‡...")
        self.quick_switch(source_host, target_host, collection_name)

        print("âœ… çƒ­è¿ç§»å®Œæˆ")

    def sync_incremental(
        self,
        source_host: str,
        target_host: str,
        collection_name: str,
        since_time: float
    ):
        """åŒæ­¥å¢é‡æ•°æ®"""
        connections.connect(alias="source", host=source_host)
        connections.connect(alias="target", host=target_host)

        source = Collection(collection_name, using="source")
        target = Collection(collection_name, using="target")

        # æŸ¥è¯¢å¢é‡æ•°æ®
        incremental_data = source.query(
            expr=f"timestamp > {since_time}",
            output_fields=["*"]
        )

        if incremental_data:
            print(f"åŒæ­¥ {len(incremental_data)} æ¡å¢é‡æ•°æ®")
            target.insert(incremental_data)
            target.flush()

    def quick_switch(
        self,
        source_host: str,
        target_host: str,
        collection_name: str
    ):
        """å¿«é€Ÿåˆ‡æ¢ï¼ˆçŸ­æš‚åœæœºï¼‰"""
        # 1. åœæ­¢å†™å…¥
        self.stop_writes()

        # 2. æœ€åä¸€æ¬¡å¢é‡åŒæ­¥
        self.sync_incremental(
            source_host,
            target_host,
            collection_name,
            time.time() - 60  # æœ€è¿‘1åˆ†é’Ÿ
        )

        # 3. åˆ‡æ¢æµé‡
        self.switch_traffic(target_host)

        # 4. æ¢å¤å†™å…¥
        self.resume_writes()
```

### ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹ï¼š**
- âœ… åœæœºæ—¶é—´çŸ­ï¼ˆåˆ†é’Ÿçº§ï¼‰
- âœ… å¯¹ä¸šåŠ¡å½±å“å°
- âœ… å¯ä»¥å›æ»š

**ç¼ºç‚¹ï¼š**
- âŒ éœ€è¦å¢é‡åŒæ­¥
- âŒ å®ç°è¾ƒå¤æ‚
- âŒ ä»æœ‰çŸ­æš‚åœæœº

---

## ç­–ç•¥3ï¼šåŒå†™è¿ç§»ï¼ˆé›¶åœæœºï¼‰

### åŸç†

```
1. å…¨é‡è¿ç§»å†å²æ•°æ®
2. å¯åŠ¨åŒå†™ï¼ˆåŒæ—¶å†™å…¥æºå’Œç›®æ ‡ï¼‰
3. éªŒè¯æ•°æ®ä¸€è‡´æ€§
4. ç°åº¦åˆ‡æ¢è¯»æµé‡
5. åœæ­¢åŒå†™
```

### å®ç°

```python
class DualWriteMigration:
    """åŒå†™è¿ç§»ï¼ˆé›¶åœæœºï¼‰"""

    def __init__(self):
        self.dual_write_enabled = False
        self.source_host = None
        self.target_host = None

    def migrate(
        self,
        source_host: str,
        target_host: str,
        collection_name: str
    ):
        """æ‰§è¡ŒåŒå†™è¿ç§»"""
        print("=== åŒå†™è¿ç§»å¼€å§‹ ===")

        self.source_host = source_host
        self.target_host = target_host

        # 1. å…¨é‡è¿ç§»å†å²æ•°æ®
        print("[1/6] å…¨é‡è¿ç§»å†å²æ•°æ®...")
        self.full_migration(source_host, target_host, collection_name)

        # 2. å¯åŠ¨åŒå†™
        print("[2/6] å¯åŠ¨åŒå†™...")
        self.enable_dual_write(source_host, target_host)

        # 3. éªŒè¯æ•°æ®ä¸€è‡´æ€§
        print("[3/6] éªŒè¯æ•°æ®ä¸€è‡´æ€§...")
        self.verify_consistency(source_host, target_host, collection_name)

        # 4. ç°åº¦åˆ‡æ¢è¯»æµé‡
        print("[4/6] ç°åº¦åˆ‡æ¢è¯»æµé‡...")
        self.gradual_switch_reads(target_host)

        # 5. ç›‘æ§ä¸€æ®µæ—¶é—´
        print("[5/6] ç›‘æ§è¿è¡Œ...")
        self.monitor_and_wait(duration=3600)  # ç›‘æ§1å°æ—¶

        # 6. åœæ­¢åŒå†™
        print("[6/6] åœæ­¢åŒå†™...")
        self.disable_dual_write()

        print("âœ… åŒå†™è¿ç§»å®Œæˆ")

    def enable_dual_write(self, source_host: str, target_host: str):
        """å¯åŠ¨åŒå†™"""
        self.dual_write_enabled = True
        print("âœ… åŒå†™å·²å¯åŠ¨")

    def insert_with_dual_write(self, collection_name: str, data: List[Dict]):
        """åŒå†™æ’å…¥"""
        if not self.dual_write_enabled:
            # åªå†™å…¥æº
            connections.connect(alias="source", host=self.source_host)
            collection = Collection(collection_name, using="source")
            collection.insert(data)
        else:
            # åŒæ—¶å†™å…¥æºå’Œç›®æ ‡
            try:
                # å†™å…¥æº
                connections.connect(alias="source", host=self.source_host)
                source = Collection(collection_name, using="source")
                source.insert(data)

                # å†™å…¥ç›®æ ‡
                connections.connect(alias="target", host=self.target_host)
                target = Collection(collection_name, using="target")
                target.insert(data)

            except Exception as e:
                print(f"åŒå†™å¤±è´¥: {e}")
                # è®°å½•å¤±è´¥ï¼Œç¨åé‡è¯•
                self.log_failed_write(data)

    def gradual_switch_reads(self, target_host: str):
        """ç°åº¦åˆ‡æ¢è¯»æµé‡"""
        # 10% â†’ 50% â†’ 100%
        for ratio in [0.1, 0.5, 1.0]:
            print(f"åˆ‡æ¢ {ratio*100}% è¯»æµé‡åˆ°ç›®æ ‡...")
            self.set_read_ratio(ratio)
            time.sleep(300)  # ç­‰å¾…5åˆ†é’Ÿè§‚å¯Ÿ

            # æ£€æŸ¥é”™è¯¯ç‡
            if self.get_error_rate() > 0.01:  # é”™è¯¯ç‡ > 1%
                print("âŒ é”™è¯¯ç‡è¿‡é«˜ï¼Œå›æ»š")
                self.set_read_ratio(0)
                raise Exception("ç°åº¦åˆ‡æ¢å¤±è´¥")

        print("âœ… è¯»æµé‡å·²å®Œå…¨åˆ‡æ¢")

    def verify_consistency(
        self,
        source_host: str,
        target_host: str,
        collection_name: str
    ):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        connections.connect(alias="source", host=source_host)
        connections.connect(alias="target", host=target_host)

        source = Collection(collection_name, using="source")
        target = Collection(collection_name, using="target")

        # æŒç»­éªŒè¯
        for i in range(10):
            if source.num_entities != target.num_entities:
                print(f"æ•°æ®é‡ä¸ä¸€è‡´: {source.num_entities} vs {target.num_entities}")
                time.sleep(10)
            else:
                print("âœ… æ•°æ®é‡ä¸€è‡´")
                return

        raise Exception("æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥")
```

### ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹ï¼š**
- âœ… é›¶åœæœº
- âœ… å¯ä»¥ç°åº¦åˆ‡æ¢
- âœ… å®¹æ˜“å›æ»š

**ç¼ºç‚¹ï¼š**
- âŒ å®ç°å¤æ‚
- âŒ éœ€è¦ä¿®æ”¹åº”ç”¨ä»£ç 
- âŒ åŒå†™æœŸé—´èµ„æºæ¶ˆè€—å¤§

---

## ç­–ç•¥4ï¼šå¢é‡è¿ç§»

### åŸç†

```
1. å…¨é‡è¿ç§»åŸºå‡†æ•°æ®
2. æŒç»­åŒæ­¥å¢é‡
3. ç¼©å°å¢é‡çª—å£
4. å¿«é€Ÿåˆ‡æ¢
```

### å®ç°

```python
class IncrementalMigration:
    """å¢é‡è¿ç§»"""

    def migrate(
        self,
        source_host: str,
        target_host: str,
        collection_name: str
    ):
        """æ‰§è¡Œå¢é‡è¿ç§»"""
        print("=== å¢é‡è¿ç§»å¼€å§‹ ===")

        # 1. å…¨é‡è¿ç§»åŸºå‡†æ•°æ®
        print("[1/4] å…¨é‡è¿ç§»åŸºå‡†æ•°æ®...")
        baseline_time = time.time()
        self.full_migration(source_host, target_host, collection_name)

        # 2. æŒç»­åŒæ­¥å¢é‡ï¼ˆç¼©å°çª—å£ï¼‰
        print("[2/4] æŒç»­åŒæ­¥å¢é‡...")
        last_sync_time = baseline_time

        while True:
            # åŒæ­¥å¢é‡
            current_time = time.time()
            incremental_count = self.sync_incremental(
                source_host,
                target_host,
                collection_name,
                last_sync_time
            )

            last_sync_time = current_time

            # å¦‚æœå¢é‡å¾ˆå°ï¼Œå‡†å¤‡åˆ‡æ¢
            if incremental_count < 100:
                print(f"å¢é‡æ•°æ®å¾ˆå°({incremental_count}æ¡)ï¼Œå‡†å¤‡åˆ‡æ¢")
                break

            time.sleep(60)  # æ¯åˆ†é’ŸåŒæ­¥ä¸€æ¬¡

        # 3. æœ€åä¸€æ¬¡åŒæ­¥
        print("[3/4] æœ€åä¸€æ¬¡åŒæ­¥...")
        self.stop_writes()
        self.sync_incremental(
            source_host,
            target_host,
            collection_name,
            last_sync_time
        )

        # 4. åˆ‡æ¢æµé‡
        print("[4/4] åˆ‡æ¢æµé‡...")
        self.switch_traffic(target_host)
        self.resume_writes()

        print("âœ… å¢é‡è¿ç§»å®Œæˆ")

    def sync_incremental(
        self,
        source_host: str,
        target_host: str,
        collection_name: str,
        since_time: float
    ) -> int:
        """åŒæ­¥å¢é‡æ•°æ®"""
        connections.connect(alias="source", host=source_host)
        connections.connect(alias="target", host=target_host)

        source = Collection(collection_name, using="source")
        target = Collection(collection_name, using="target")

        # æŸ¥è¯¢å¢é‡æ•°æ®
        incremental_data = source.query(
            expr=f"timestamp > {since_time}",
            output_fields=["*"]
        )

        if incremental_data:
            print(f"åŒæ­¥ {len(incremental_data)} æ¡å¢é‡æ•°æ®")
            target.insert(incremental_data)
            target.flush()

        return len(incremental_data)
```

---

## åœ¨ RAG ç³»ç»Ÿä¸­çš„åº”ç”¨

### åœºæ™¯1ï¼šçŸ¥è¯†åº“ç¯å¢ƒè¿ç§»

```python
class RAGKnowledgeBaseMigration:
    """RAG çŸ¥è¯†åº“è¿ç§»"""

    def migrate_dev_to_prod(self):
        """ä»å¼€å‘ç¯å¢ƒè¿ç§»åˆ°ç”Ÿäº§ç¯å¢ƒ"""
        # ä½¿ç”¨çƒ­è¿ç§»ç­–ç•¥
        migration = HotMigration()

        # è¿ç§»æ–‡æ¡£ Collection
        migration.migrate(
            source_host="dev-milvus.internal",
            target_host="prod-milvus.internal",
            collection_name="rag_documents"
        )

        # è¿ç§» Embedding Collection
        migration.migrate(
            source_host="dev-milvus.internal",
            target_host="prod-milvus.internal",
            collection_name="rag_embeddings"
        )

        print("âœ… RAG çŸ¥è¯†åº“è¿ç§»å®Œæˆ")
```

### åœºæ™¯2ï¼šçŸ¥è¯†åº“æ‰©å®¹è¿ç§»

```python
class RAGScaleMigration:
    """RAG çŸ¥è¯†åº“æ‰©å®¹è¿ç§»"""

    def migrate_to_cluster(self):
        """ä»å•æœºè¿ç§»åˆ°é›†ç¾¤"""
        # ä½¿ç”¨åŒå†™è¿ç§»ï¼ˆé›¶åœæœºï¼‰
        migration = DualWriteMigration()

        migration.migrate(
            source_host="standalone-milvus.internal",
            target_host="cluster-milvus.internal",
            collection_name="rag_knowledge_base"
        )

        print("âœ… çŸ¥è¯†åº“æ‰©å®¹è¿ç§»å®Œæˆ")
```

---

## æœ€ä½³å®è·µ

### 1. è¿ç§»å‰æ£€æŸ¥æ¸…å•

```python
def pre_migration_checklist():
    """è¿ç§»å‰æ£€æŸ¥"""
    checks = {
        "æºç¯å¢ƒå¥åº·": check_source_health(),
        "ç›®æ ‡ç¯å¢ƒå°±ç»ª": check_target_ready(),
        "ç½‘ç»œè¿é€šæ€§": check_network(),
        "å­˜å‚¨ç©ºé—´": check_storage(),
        "å¤‡ä»½å·²åˆ›å»º": check_backup_exists(),
        "å›æ»šæ–¹æ¡ˆ": check_rollback_plan()
    }

    for check, result in checks.items():
        if not result:
            print(f"âŒ {check} æ£€æŸ¥å¤±è´¥")
            return False
        print(f"âœ… {check}")

    return True
```

### 2. è¿ç§»ç›‘æ§

```python
class MigrationMonitor:
    """è¿ç§»ç›‘æ§"""

    def monitor(self, source_host: str, target_host: str):
        """ç›‘æ§è¿ç§»è¿‡ç¨‹"""
        while True:
            metrics = {
                "æºæ•°æ®é‡": self.get_entity_count(source_host),
                "ç›®æ ‡æ•°æ®é‡": self.get_entity_count(target_host),
                "æ•°æ®å·®å¼‚": self.get_diff(source_host, target_host),
                "é”™è¯¯ç‡": self.get_error_rate(),
                "å»¶è¿Ÿ": self.get_latency()
            }

            print(f"ç›‘æ§æŒ‡æ ‡: {metrics}")

            # å‘Šè­¦
            if metrics["æ•°æ®å·®å¼‚"] > 1000:
                self.alert("æ•°æ®å·®å¼‚è¿‡å¤§")

            if metrics["é”™è¯¯ç‡"] > 0.01:
                self.alert("é”™è¯¯ç‡è¿‡é«˜")

            time.sleep(60)
```

### 3. å›æ»šæ–¹æ¡ˆ

```python
def rollback_migration(backup_dir: str, target_host: str):
    """å›æ»šè¿ç§»"""
    print("å¼€å§‹å›æ»š...")

    # 1. åˆ‡æ¢æµé‡å›æº
    switch_traffic_back()

    # 2. æ¸…ç†ç›®æ ‡æ•°æ®
    cleanup_target(target_host)

    # 3. æ¢å¤å¤‡ä»½ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if backup_dir:
        restore_backup(backup_dir, target_host)

    print("âœ… å›æ»šå®Œæˆ")
```

---

## æ€»ç»“

### ç­–ç•¥é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èç­–ç•¥ | ç†ç”± |
|------|---------|------|
| å°æ•°æ®é‡ï¼ˆ< 1GBï¼‰ | å†·è¿ç§» | ç®€å•å¿«é€Ÿ |
| ä¸­æ•°æ®é‡ï¼ˆ1-100GBï¼‰ | çƒ­è¿ç§» | å¹³è¡¡åœæœºå’Œå¤æ‚åº¦ |
| å¤§æ•°æ®é‡ï¼ˆ> 100GBï¼‰ | åŒå†™è¿ç§» | é›¶åœæœº |
| æŒç»­æ›´æ–°çš„æ•°æ® | å¢é‡è¿ç§» | å‡å°‘åœæœºæ—¶é—´ |
| å¼€å‘ â†’ ç”Ÿäº§ | çƒ­è¿ç§» | å¯æ¥å—çŸ­æš‚åœæœº |
| å•æœº â†’ é›†ç¾¤ | åŒå†™è¿ç§» | éœ€è¦é›¶åœæœº |

### æ ¸å¿ƒè¦ç‚¹

1. **é€‰æ‹©åˆé€‚çš„ç­–ç•¥**ï¼šæ ¹æ®æ•°æ®é‡ã€åœæœºè¦æ±‚é€‰æ‹©
2. **å……åˆ†æµ‹è¯•**ï¼šåœ¨æµ‹è¯•ç¯å¢ƒå…ˆéªŒè¯
3. **ç›‘æ§å’Œå‘Šè­¦**ï¼šå®æ—¶ç›‘æ§è¿ç§»è¿‡ç¨‹
4. **å‡†å¤‡å›æ»šæ–¹æ¡ˆ**ï¼šå‡ºé—®é¢˜æ—¶å¿«é€Ÿå›æ»š
5. **éªŒè¯æ•°æ®ä¸€è‡´æ€§**ï¼šè¿ç§»åå¿…é¡»éªŒè¯

### ä¸‹ä¸€æ­¥

- å®è·µ [Backup å·¥å…·å¤‡ä»½æ¢å¤](./07_å®æˆ˜ä»£ç _01_Backupå·¥å…·å¤‡ä»½æ¢å¤.md)
- å®è·µ [è·¨é›†ç¾¤æ•°æ®è¿ç§»](./07_å®æˆ˜ä»£ç _03_è·¨é›†ç¾¤æ•°æ®è¿ç§».md)
