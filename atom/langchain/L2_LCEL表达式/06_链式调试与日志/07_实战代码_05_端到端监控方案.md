# 实战代码5：端到端监控方案

## 概述

本文档提供完整的端到端监控方案，集成多种监控工具，实现生产级可观测性。

**涵盖场景**：
1. 完整监控方案集成
2. 告警和仪表板配置
3. 生产最佳实践
4. 常见问题排查

**前置要求**：
- Python 3.13+
- LangChain v0.3+
- OpenTelemetry、LangSmith

---

## 场景1：完整监控方案集成

### 目标

集成OpenTelemetry、LangSmith和自定义回调，实现完整的监控方案。

### 完整代码

```python
"""
场景1：完整监控方案
集成多种监控工具
"""

from opentelemetry import trace, metrics
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader, ConsoleMetricExporter
from opentelemetry.sdk.resources import Resource
from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from datetime import datetime
import time
import json
import logging
import os

load_dotenv()

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ComprehensiveMonitor:
    """综合监控管理器"""

    def __init__(self, service_name="langchain-app"):
        self.service_name = service_name

        # 配置OpenTelemetry
        self._setup_opentelemetry()

        # 配置自定义回调
        self.callbacks = [
            FileLogHandler("comprehensive_logs.jsonl"),
            PerformanceHandler(),
            ErrorHandler()
        ]

    def _setup_opentelemetry(self):
        """配置OpenTelemetry"""
        # 配置资源
        resource = Resource.create({
            "service.name": self.service_name,
            "service.version": "1.0.0",
            "deployment.environment": os.getenv("ENV", "production")
        })

        # 配置Trace
        trace_provider = TracerProvider(resource=resource)
        trace.set_tracer_provider(trace_provider)
        trace_provider.add_span_processor(
            BatchSpanProcessor(ConsoleSpanExporter())
        )
        self.tracer = trace.get_tracer(__name__)

        # 配置Metrics
        metric_reader = PeriodicExportingMetricReader(
            ConsoleMetricExporter(),
            export_interval_millis=10000
        )
        meter_provider = MeterProvider(
            resource=resource,
            metric_readers=[metric_reader]
        )
        metrics.set_meter_provider(meter_provider)
        self.meter = metrics.get_meter(__name__)

        # 创建指标
        self.request_counter = self.meter.create_counter(
            "requests_total",
            description="Total requests"
        )
        self.latency_histogram = self.meter.create_histogram(
            "latency_ms",
            description="Request latency"
        )
        self.error_counter = self.meter.create_counter(
            "errors_total",
            description="Total errors"
        )

    def execute_chain(self, chain, inputs, **kwargs):
        """执行链并监控"""
        # 配置LangSmith追踪
        config = {
            "callbacks": self.callbacks,
            "tags": kwargs.get("tags", []),
            "metadata": kwargs.get("metadata", {})
        }

        # 使用OpenTelemetry span
        with self.tracer.start_as_current_span("chain_execution") as span:
            span.set_attribute("inputs", str(inputs))

            start = time.time()

            try:
                # 执行链
                result = chain.invoke(inputs, config=config)
                duration_ms = (time.time() - start) * 1000

                # 记录成功
                span.set_attribute("status", "success")
                span.set_attribute("latency_ms", duration_ms)

                self.request_counter.add(1, {"status": "success"})
                self.latency_histogram.record(duration_ms)

                return result

            except Exception as e:
                # 记录错误
                span.set_status(trace.Status(trace.StatusCode.ERROR))
                span.record_exception(e)

                self.request_counter.add(1, {"status": "error"})
                self.error_counter.add(1, {"error_type": e.__class__.__name__})

                raise

    def get_metrics(self):
        """获取性能指标"""
        perf_handler = next(
            (cb for cb in self.callbacks if isinstance(cb, PerformanceHandler)),
            None
        )
        if perf_handler:
            return perf_handler.get_metrics()
        return {}

    def get_errors(self):
        """获取错误列表"""
        error_handler = next(
            (cb for cb in self.callbacks if isinstance(cb, ErrorHandler)),
            None
        )
        if error_handler:
            return error_handler.get_errors()
        return []


class FileLogHandler(BaseCallbackHandler):
    """文件日志处理器"""

    def __init__(self, log_file):
        self.log_file = log_file

    def _write_log(self, event_type, data):
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "event": event_type,
            "data": data
        }
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"Failed to write log: {e}")

    def on_chain_start(self, serialized, inputs, **kwargs):
        self._write_log("chain_start", {
            "inputs": inputs,
            "run_id": kwargs.get("run_id")
        })

    def on_chain_end(self, outputs, **kwargs):
        self._write_log("chain_end", {
            "outputs": str(outputs)[:200],
            "run_id": kwargs.get("run_id")
        })

    def on_chain_error(self, error, **kwargs):
        self._write_log("chain_error", {
            "error": str(error),
            "run_id": kwargs.get("run_id")
        })


class PerformanceHandler(BaseCallbackHandler):
    """性能追踪处理器"""

    def __init__(self):
        self.start_times = {}
        self.metrics = {
            "chain_duration": 0,
            "llm_duration": 0,
            "total_tokens": 0
        }

    def on_chain_start(self, serialized, inputs, **kwargs):
        run_id = kwargs.get("run_id")
        self.start_times[f"chain_{run_id}"] = time.time()

    def on_chain_end(self, outputs, **kwargs):
        run_id = kwargs.get("run_id")
        key = f"chain_{run_id}"
        if key in self.start_times:
            duration = time.time() - self.start_times[key]
            self.metrics["chain_duration"] = duration

    def on_llm_start(self, serialized, prompts, **kwargs):
        run_id = kwargs.get("run_id")
        self.start_times[f"llm_{run_id}"] = time.time()

    def on_llm_end(self, response, **kwargs):
        run_id = kwargs.get("run_id")
        key = f"llm_{run_id}"
        if key in self.start_times:
            duration = time.time() - self.start_times[key]
            self.metrics["llm_duration"] = duration

        tokens = response.llm_output.get("token_usage", {})
        self.metrics["total_tokens"] = tokens.get("total_tokens", 0)

    def get_metrics(self):
        return self.metrics


class ErrorHandler(BaseCallbackHandler):
    """错误处理器"""

    def __init__(self):
        self.errors = []

    def on_chain_error(self, error, **kwargs):
        self._handle_error("chain", error, kwargs)

    def on_llm_error(self, error, **kwargs):
        self._handle_error("llm", error, kwargs)

    def _handle_error(self, error_type, error, kwargs):
        error_info = {
            "timestamp": datetime.now().isoformat(),
            "type": error_type,
            "error": str(error),
            "error_class": error.__class__.__name__,
            "run_id": kwargs.get("run_id")
        }
        self.errors.append(error_info)
        logger.error(f"[{error_type}] {error}")

    def get_errors(self):
        return self.errors


def main():
    """完整监控方案示例"""
    # 初始化监控
    monitor = ComprehensiveMonitor("production-app")

    # 构建链
    prompt = ChatPromptTemplate.from_template("回答问题: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model | StrOutputParser()

    # 执行请求
    try:
        result = monitor.execute_chain(
            chain,
            {"question": "什么是LCEL?"},
            tags=["production", "user_123"],
            metadata={"user_id": "123", "session_id": "abc"}
        )

        print(result)

        # 打印指标
        metrics = monitor.get_metrics()
        print(f"\n性能指标: {metrics}")

        # 打印错误
        errors = monitor.get_errors()
        if errors:
            print(f"错误: {errors}")

    except Exception as e:
        print(f"执行失败: {e}")


if __name__ == "__main__":
    main()
```

---

## 场景2：告警和仪表板配置

### 目标

配置告警规则和监控仪表板。

### 告警规则配置

```python
"""
场景2：告警规则配置
设置性能和错误告警
"""

from dataclasses import dataclass
from typing import List, Callable
import logging

logger = logging.getLogger(__name__)


@dataclass
class AlertRule:
    """告警规则"""
    name: str
    condition: Callable
    message: str
    severity: str  # "info", "warning", "error", "critical"


class AlertManager:
    """告警管理器"""

    def __init__(self):
        self.rules: List[AlertRule] = []
        self.alerts = []

    def add_rule(self, rule: AlertRule):
        """添加告警规则"""
        self.rules.append(rule)

    def check_metrics(self, metrics: dict):
        """检查指标并触发告警"""
        for rule in self.rules:
            if rule.condition(metrics):
                self._trigger_alert(rule, metrics)

    def _trigger_alert(self, rule: AlertRule, metrics: dict):
        """触发告警"""
        alert = {
            "rule": rule.name,
            "message": rule.message,
            "severity": rule.severity,
            "metrics": metrics
        }
        self.alerts.append(alert)

        # 记录日志
        log_func = getattr(logger, rule.severity, logger.info)
        log_func(f"🚨 ALERT: {rule.message}")

        # 发送通知（示例）
        self._send_notification(alert)

    def _send_notification(self, alert):
        """发送通知（示例）"""
        # 实际实现可以发送到Slack、Email、PagerDuty等
        print(f"📧 Notification sent: {alert['message']}")


def setup_alerts():
    """配置告警规则"""
    manager = AlertManager()

    # 规则1：高延迟告警
    manager.add_rule(AlertRule(
        name="high_latency",
        condition=lambda m: m.get("chain_duration", 0) > 5.0,
        message="Chain latency > 5s",
        severity="warning"
    ))

    # 规则2：错误率告警
    manager.add_rule(AlertRule(
        name="high_error_rate",
        condition=lambda m: m.get("error_count", 0) > 10,
        message="Error count > 10",
        severity="error"
    ))

    # 规则3：高成本告警
    manager.add_rule(AlertRule(
        name="high_cost",
        condition=lambda m: m.get("total_cost", 0) > 1.0,
        message="Total cost > $1.00",
        severity="warning"
    ))

    # 规则4：Token使用告警
    manager.add_rule(AlertRule(
        name="high_token_usage",
        condition=lambda m: m.get("total_tokens", 0) > 10000,
        message="Token usage > 10k",
        severity="info"
    ))

    return manager


def main():
    """告警示例"""
    manager = setup_alerts()

    # 模拟指标
    metrics = {
        "chain_duration": 6.5,  # 触发高延迟告警
        "error_count": 2,
        "total_cost": 0.5,
        "total_tokens": 5000
    }

    # 检查告警
    manager.check_metrics(metrics)

    # 打印所有告警
    print(f"\n触发的告警: {len(manager.alerts)}")
    for alert in manager.alerts:
        print(f"  - [{alert['severity']}] {alert['message']}")


if __name__ == "__main__":
    main()
```

### 输出示例

```
2026-02-20 14:30:00 - __main__ - WARNING - 🚨 ALERT: Chain latency > 5s
📧 Notification sent: Chain latency > 5s

触发的告警: 1
  - [warning] Chain latency > 5s
```

---

## 场景3：生产最佳实践

### 目标

实现生产环境的最佳实践。

### 完整代码

```python
"""
场景3：生产最佳实践
实现健壮的生产监控
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import os
import random
import logging
from functools import wraps
import time

load_dotenv()

logger = logging.getLogger(__name__)


class ProductionMonitor:
    """生产监控最佳实践"""

    def __init__(self):
        self.sample_rate = float(os.getenv("TRACE_SAMPLE_RATE", "0.1"))
        self.enable_detailed_logging = os.getenv("DETAILED_LOGGING", "false") == "true"

    def should_trace(self, error=None, duration=None):
        """采样策略"""
        # 总是追踪错误
        if error:
            return True

        # 总是追踪慢请求
        if duration and duration > 5.0:
            return True

        # 随机采样
        return random.random() < self.sample_rate

    def with_monitoring(self, func):
        """监控装饰器"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            start = time.time()
            error = None

            try:
                result = func(*args, **kwargs)
                duration = time.time() - start
                return result

            except Exception as e:
                duration = time.time() - start
                error = e
                raise

            finally:
                # 决定是否追踪
                if self.should_trace(error, duration):
                    self._log_execution(func.__name__, duration, error)

        return wrapper

    def _log_execution(self, func_name, duration, error):
        """记录执行信息"""
        if error:
            logger.error(f"{func_name} failed after {duration:.2f}s: {error}")
        elif duration > 5.0:
            logger.warning(f"{func_name} slow: {duration:.2f}s")
        else:
            logger.info(f"{func_name} completed in {duration:.2f}s")


# 使用示例
monitor = ProductionMonitor()


@monitor.with_monitoring
def execute_chain(question):
    """执行链"""
    prompt = ChatPromptTemplate.from_template("回答问题: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model

    result = chain.invoke({"question": question})
    return result.content


def main():
    """生产最佳实践示例"""
    # 配置环境变量
    os.environ["TRACE_SAMPLE_RATE"] = "0.1"  # 10%采样
    os.environ["DETAILED_LOGGING"] = "false"

    # 执行多次请求
    for i in range(10):
        try:
            result = execute_chain(f"问题{i+1}")
            print(f"请求{i+1}完成")
        except Exception as e:
            print(f"请求{i+1}失败: {e}")


if __name__ == "__main__":
    main()
```

---

## 场景4：常见问题排查

### 目标

提供常见问题的排查方法。

### 问题排查指南

```python
"""
场景4：常见问题排查
诊断和解决常见问题
"""

import logging
from typing import Dict, List

logger = logging.getLogger(__name__)


class DiagnosticTool:
    """诊断工具"""

    @staticmethod
    def check_latency(metrics: Dict) -> List[str]:
        """检查延迟问题"""
        issues = []

        chain_duration = metrics.get("chain_duration", 0)
        llm_duration = metrics.get("llm_duration", 0)

        if chain_duration > 5.0:
            issues.append(f"⚠️ 链延迟过高: {chain_duration:.2f}s")

            if llm_duration > chain_duration * 0.9:
                issues.append("  → 瓶颈在LLM调用")
                issues.append("  → 建议: 使用更快的模型或减少prompt长度")
            else:
                issues.append("  → 瓶颈在其他组件")
                issues.append("  → 建议: 检查检索器或数据处理逻辑")

        return issues

    @staticmethod
    def check_cost(metrics: Dict) -> List[str]:
        """检查成本问题"""
        issues = []

        total_tokens = metrics.get("total_tokens", 0)
        total_cost = metrics.get("total_cost", 0)

        if total_tokens > 10000:
            issues.append(f"⚠️ Token使用过高: {total_tokens}")
            issues.append("  → 建议: 优化prompt或使用更便宜的模型")

        if total_cost > 1.0:
            issues.append(f"⚠️ 成本过高: ${total_cost:.2f}")
            issues.append("  → 建议: 实施缓存或减少调用频率")

        return issues

    @staticmethod
    def check_errors(errors: List[Dict]) -> List[str]:
        """检查错误问题"""
        issues = []

        if not errors:
            return issues

        # 统计错误类型
        error_types = {}
        for error in errors:
            error_class = error.get("error_class", "Unknown")
            error_types[error_class] = error_types.get(error_class, 0) + 1

        issues.append(f"⚠️ 发现 {len(errors)} 个错误:")
        for error_type, count in error_types.items():
            issues.append(f"  - {error_type}: {count}次")

            # 提供建议
            if "RateLimitError" in error_type:
                issues.append("    → 建议: 实施指数退避或增加配额")
            elif "TimeoutError" in error_type:
                issues.append("    → 建议: 增加超时时间或优化请求")
            elif "AuthenticationError" in error_type:
                issues.append("    → 建议: 检查API密钥配置")

        return issues

    @staticmethod
    def run_diagnostics(metrics: Dict, errors: List[Dict]):
        """运行完整诊断"""
        print("\n=== 诊断报告 ===\n")

        # 检查延迟
        latency_issues = DiagnosticTool.check_latency(metrics)
        if latency_issues:
            print("延迟问题:")
            for issue in latency_issues:
                print(issue)
            print()

        # 检查成本
        cost_issues = DiagnosticTool.check_cost(metrics)
        if cost_issues:
            print("成本问题:")
            for issue in cost_issues:
                print(issue)
            print()

        # 检查错误
        error_issues = DiagnosticTool.check_errors(errors)
        if error_issues:
            print("错误问题:")
            for issue in error_issues:
                print(issue)
            print()

        if not (latency_issues or cost_issues or error_issues):
            print("✅ 未发现问题")


def main():
    """诊断示例"""
    # 模拟指标
    metrics = {
        "chain_duration": 6.5,
        "llm_duration": 6.0,
        "total_tokens": 15000,
        "total_cost": 1.5
    }

    # 模拟错误
    errors = [
        {"error_class": "RateLimitError"},
        {"error_class": "RateLimitError"},
        {"error_class": "TimeoutError"}
    ]

    # 运行诊断
    DiagnosticTool.run_diagnostics(metrics, errors)


if __name__ == "__main__":
    main()
```

### 输出示例

```
=== 诊断报告 ===

延迟问题:
⚠️ 链延迟过高: 6.50s
  → 瓶颈在LLM调用
  → 建议: 使用更快的模型或减少prompt长度

成本问题:
⚠️ Token使用过高: 15000
  → 建议: 优化prompt或使用更便宜的模型
⚠️ 成本过高: $1.50
  → 建议: 实施缓存或减少调用频率

错误问题:
⚠️ 发现 3 个错误:
  - RateLimitError: 2次
    → 建议: 实施指数退避或增加配额
  - TimeoutError: 1次
    → 建议: 增加超时时间或优化请求
```

---

## 监控清单

### 开发环境

- [ ] 使用 `astream_events()` 详细调试
- [ ] 实现基础的文件日志
- [ ] 记录所有错误

### 预发布环境

- [ ] 配置OpenTelemetry追踪
- [ ] 实现性能追踪回调
- [ ] 配置基础告警规则
- [ ] 测试监控数据收集

### 生产环境

- [ ] 启用LangSmith追踪（采样）
- [ ] 配置OpenTelemetry导出到监控系统
- [ ] 实现完整的告警规则
- [ ] 配置监控仪表板
- [ ] 设置成本预算告警
- [ ] 定期审查监控数据

---

## 最佳实践总结

### 1. 分层监控

- **开发**: astream_events + 文件日志
- **集成**: 自定义回调 + 结构化日志
- **生产**: OpenTelemetry + LangSmith + 告警

### 2. 采样策略

- 正常请求: 1-10%采样
- 错误请求: 100%追踪
- 慢请求: 100%追踪

### 3. 告警规则

- 延迟 > 5s
- 错误率 > 1%
- 成本超预算
- Token使用异常

### 4. 定期审查

- 每周审查性能趋势
- 每月审查成本
- 及时优化瓶颈

---

## 总结

**核心技巧**：

1. **多层监控**：开发、集成、生产分层
2. **采样策略**：减少成本，保持可观测性
3. **告警规则**：及时发现问题
4. **诊断工具**：快速排查问题

**最佳实践**：

- 使用分层监控策略
- 实施采样减少成本
- 配置合理的告警规则
- 定期审查监控数据
- 快速响应告警

---

**版本信息**
- LangChain: v0.3+ (2025-2026)
- Python: 3.13+
- 最后更新: 2026-02-20
