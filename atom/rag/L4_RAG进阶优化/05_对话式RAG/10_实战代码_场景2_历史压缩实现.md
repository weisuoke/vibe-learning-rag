# 实战代码：场景2 - 历史压缩实现

> 实现三种历史压缩策略的完整代码

---

## 场景说明

本场景实现`HistoryCompressor`类，提供三种历史压缩策略：
1. **滑动窗口截断**：最简单，零成本
2. **LLM摘要压缩**：保留关键信息，适合长对话
3. **关键信息提取**：精准保留重要内容

**适用场景：**
- 长对话场景（>15轮）
- 需要控制Context Window的应用
- 成本敏感的生产环境

---

## 完整代码实现

```python
"""
对话历史压缩策略实现
包含三种策略：滑动窗口、LLM摘要、关键信息提取
"""

from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime
from openai import OpenAI
import json


@dataclass
class Message:
    """对话消息数据结构"""
    role: str
    content: str
    timestamp: datetime

    def to_dict(self) -> Dict:
        return {
            "role": self.role,
            "content": self.content,
            "timestamp": self.timestamp.isoformat()
        }


class HistoryCompressor:
    """历史压缩器"""

    def __init__(self, strategy: str = "sliding_window"):
        """
        初始化压缩器

        Args:
            strategy: 压缩策略 ("sliding_window", "llm_summary", "key_info")
        """
        self.strategy = strategy
        self.client = OpenAI() if strategy in ["llm_summary", "key_info"] else None

    def compress(self, messages: List[Message], **kwargs) -> List[Message]:
        """
        压缩历史消息

        Args:
            messages: 消息列表
            **kwargs: 策略特定参数

        Returns:
            压缩后的消息列表
        """
        if self.strategy == "sliding_window":
            return self.compress_sliding_window(messages, **kwargs)
        elif self.strategy == "llm_summary":
            return self.compress_with_summary(messages, **kwargs)
        elif self.strategy == "key_info":
            return self.compress_key_info(messages, **kwargs)
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")

    # ===== 策略1：滑动窗口截断 =====

    def compress_sliding_window(self, messages: List[Message],
                               keep_last: int = 5) -> List[Message]:
        """
        策略1：滑动窗口截断
        只保留最近N轮对话

        Args:
            messages: 消息列表
            keep_last: 保留最近N轮（每轮=2条消息）

        Returns:
            压缩后的消息列表
        """
        if len(messages) <= keep_last * 2:
            return messages

        # 只保留最近的消息
        compressed = messages[-(keep_last * 2):]

        print(f"[滑动窗口] {len(messages)}条 → {len(compressed)}条 (保留{keep_last}轮)")
        return compressed

    # ===== 策略2：LLM摘要压缩 =====

    def compress_with_summary(self, messages: List[Message],
                             keep_last: int = 3,
                             summary_threshold: int = 10) -> List[Message]:
        """
        策略2：LLM摘要压缩
        使用LLM总结历史对话，保留摘要+最近N轮

        Args:
            messages: 消息列表
            keep_last: 保留最近N轮原文
            summary_threshold: 超过N轮才触发摘要

        Returns:
            压缩后的消息列表
        """
        # 不需要压缩
        if len(messages) <= summary_threshold * 2:
            return messages

        # 分割：需要摘要的部分 + 保留的最近部分
        keep_count = keep_last * 2
        to_summarize = messages[:-keep_count] if keep_count > 0 else messages
        to_keep = messages[-keep_count:] if keep_count > 0 else []

        # 生成摘要
        summary_text = self._generate_summary(to_summarize)

        # 构建摘要消息
        summary_message = Message(
            role="system",
            content=f"[历史摘要] {summary_text}",
            timestamp=datetime.now()
        )

        # 返回：摘要 + 最近N轮
        compressed = [summary_message] + to_keep

        print(f"[LLM摘要] {len(messages)}条 → {len(compressed)}条 (摘要+{keep_last}轮)")
        return compressed

    def _generate_summary(self, messages: List[Message]) -> str:
        """
        使用LLM生成对话摘要
        """
        # 构建历史文本
        history_text = "\n".join([
            f"{msg.role}: {msg.content}"
            for msg in messages
        ])

        # 调用LLM生成摘要
        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[{
                    "role": "system",
                    "content": """你是一个对话摘要助手。
请总结以下对话的关键信息，包括：
1. 讨论的主要主题
2. 用户提出的关键问题
3. 重要的事实和结论
4. 用户的偏好和需求

要求：
- 简洁明了（200字以内）
- 保留关键信息
- 使用第三人称描述
"""
                }, {
                    "role": "user",
                    "content": f"对话历史:\n{history_text}\n\n请生成摘要:"
                }],
                temperature=0.3,
                max_tokens=300
            )

            summary = response.choices[0].message.content.strip()
            return summary

        except Exception as e:
            print(f"[警告] LLM摘要失败: {e}")
            return "对话历史摘要生成失败"

    # ===== 策略3：关键信息提取 =====

    def compress_key_info(self, messages: List[Message],
                         keep_last: int = 2) -> List[Message]:
        """
        策略3：关键信息提取
        提取实体、意图、关键事实

        Args:
            messages: 消息列表
            keep_last: 保留最近N轮原文

        Returns:
            压缩后的消息列表
        """
        if len(messages) <= keep_last * 2:
            return messages

        # 分割
        keep_count = keep_last * 2
        to_extract = messages[:-keep_count] if keep_count > 0 else messages
        to_keep = messages[-keep_count:] if keep_count > 0 else []

        # 提取关键信息
        key_info = self._extract_key_info(to_extract)

        # 格式化为文本
        key_info_text = self._format_key_info(key_info)

        # 构建关键信息消息
        key_info_message = Message(
            role="system",
            content=f"[关键信息]\n{key_info_text}",
            timestamp=datetime.now()
        )

        # 返回：关键信息 + 最近N轮
        compressed = [key_info_message] + to_keep

        print(f"[关键信息] {len(messages)}条 → {len(compressed)}条 (关键信息+{keep_last}轮)")
        return compressed

    def _extract_key_info(self, messages: List[Message]) -> Dict:
        """
        使用LLM提取关键信息
        """
        history_text = "\n".join([
            f"{msg.role}: {msg.content}"
            for msg in messages
        ])

        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[{
                    "role": "system",
                    "content": """你是一个信息提取助手。
从对话中提取以下信息，以JSON格式返回：
{
  "entities": {"实体类型": "实体值"},
  "facts": ["关键事实1", "关键事实2"],
  "intent": "用户意图",
  "topic": "讨论主题"
}

示例：
对话: "我的订单12345什么时候发货？"
输出: {
  "entities": {"order_id": "12345"},
  "facts": [],
  "intent": "查询订单状态",
  "topic": "订单配送"
}
"""
                }, {
                    "role": "user",
                    "content": f"对话:\n{history_text}\n\n请提取关键信息:"
                }],
                temperature=0.1,
                max_tokens=500
            )

            # 解析JSON
            content = response.choices[0].message.content.strip()
            # 移除可能的markdown代码块标记
            if content.startswith("```"):
                content = content.split("\n", 1)[1]
                content = content.rsplit("\n```", 1)[0]

            key_info = json.loads(content)
            return key_info

        except Exception as e:
            print(f"[警告] 关键信息提取失败: {e}")
            return {"entities": {}, "facts": [], "intent": "", "topic": ""}

    def _format_key_info(self, key_info: Dict) -> str:
        """
        格式化关键信息为文本
        """
        lines = []

        if key_info.get("topic"):
            lines.append(f"主题: {key_info['topic']}")

        if key_info.get("intent"):
            lines.append(f"意图: {key_info['intent']}")

        if key_info.get("entities"):
            entities_str = ", ".join([f"{k}={v}" for k, v in key_info['entities'].items()])
            lines.append(f"实体: {entities_str}")

        if key_info.get("facts"):
            lines.append("关键事实:")
            for fact in key_info['facts']:
                lines.append(f"  - {fact}")

        return "\n".join(lines) if lines else "无关键信息"


# ===== 对比测试 =====

def compare_strategies():
    """对比三种压缩策略"""

    # 模拟10轮对话
    messages = []
    topics = [
        ("RAG定义", "RAG是检索增强生成，结合了检索和生成的优势"),
        ("RAG优势", "RAG的优势包括知识更新及时、减少幻觉、成本较低"),
        ("RAG应用", "RAG可用于文档问答、智能客服、代码助手等场景"),
        ("RAG实现", "实现RAG需要向量数据库、Embedding模型和LLM"),
        ("RAG优化", "RAG优化包括混合检索、ReRank、Query改写等"),
        ("RAG评估", "RAG评估指标包括准确率、召回率、响应时间"),
        ("RAG部署", "RAG部署需要考虑向量库选型、API限流、成本控制"),
        ("RAG成本", "RAG成本主要来自Embedding和LLM调用"),
        ("RAG未来", "RAG未来发展方向包括多模态、实时更新、个性化"),
        ("RAG总结", "RAG是当前最实用的LLM应用范式之一"),
    ]

    for topic, content in topics:
        messages.append(Message(
            role="user",
            content=f"请介绍{topic}",
            timestamp=datetime.now()
        ))
        messages.append(Message(
            role="assistant",
            content=content,
            timestamp=datetime.now()
        ))

    print(f"原始消息数: {len(messages)}条 ({len(messages)//2}轮对话)\n")

    # 测试策略1：滑动窗口
    print("=== 策略1：滑动窗口截断 ===")
    compressor1 = HistoryCompressor(strategy="sliding_window")
    result1 = compressor1.compress(messages, keep_last=3)
    print(f"压缩后: {len(result1)}条消息")
    print(f"保留的对话:")
    for msg in result1:
        print(f"  {msg.role}: {msg.content[:40]}...")
    print()

    # 测试策略2：LLM摘要
    print("=== 策略2：LLM摘要压缩 ===")
    compressor2 = HistoryCompressor(strategy="llm_summary")
    result2 = compressor2.compress(messages, keep_last=2, summary_threshold=5)
    print(f"压缩后: {len(result2)}条消息")
    if result2[0].role == "system":
        print(f"摘要内容:\n{result2[0].content}\n")
    print(f"保留的最近对话:")
    for msg in result2[1:]:
        print(f"  {msg.role}: {msg.content[:40]}...")
    print()

    # 测试策略3：关键信息提取
    print("=== 策略3：关键信息提取 ===")
    compressor3 = HistoryCompressor(strategy="key_info")
    result3 = compressor3.compress(messages, keep_last=2)
    print(f"压缩后: {len(result3)}条消息")
    if result3[0].role == "system":
        print(f"关键信息:\n{result3[0].content}\n")
    print(f"保留的最近对话:")
    for msg in result3[1:]:
        print(f"  {msg.role}: {msg.content[:40]}...")
    print()

    # 性能对比
    print("=== 性能对比 ===")
    print(f"{'策略':<15} {'压缩后条数':<10} {'压缩比':<10} {'成本':<10}")
    print("-" * 50)

    for name, result in [("滑动窗口", result1), ("LLM摘要", result2), ("关键信息提取", result3)]:
        ratio = f"{len(result)/len(messages)*100:.1f}%"
        cost = "零成本" if name == "滑动窗口" else "~$0.01-0.05"
        print(f"{name:<15} {len(result):<10} {ratio:<10} {cost:<10}")


# ===== 自适应压缩器 =====

class AdaptiveCompressor:
    """自适应压缩器：根据对话长度自动选择策略"""

    def __init__(self):
        self.sliding_window = HistoryCompressor(strategy="sliding_window")
        self.llm_summary = HistoryCompressor(strategy="llm_summary")

    def should_compress(self, messages: List[Message]) -> bool:
        """判断是否需要压缩"""
        # 策略1：基于消息数量
        if len(messages) > 20:  # 超过10轮
            return True

        # 策略2：基于Token数量（粗略估计）
        total_tokens = sum(len(m.content.split()) * 1.3 for m in messages)
        if total_tokens > 3000:
            return True

        return False

    def compress(self, messages: List[Message]) -> List[Message]:
        """自适应压缩"""
        if not self.should_compress(messages):
            return messages

        # 根据消息数量选择策略
        if len(messages) < 30:
            # 短对话：使用滑动窗口
            return self.sliding_window.compress(messages, keep_last=5)
        else:
            # 长对话：使用LLM摘要
            return self.llm_summary.compress(messages, keep_last=3)


def test_adaptive_compressor():
    """测试自适应压缩器"""
    print("=== 自适应压缩器测试 ===\n")

    compressor = AdaptiveCompressor()

    # 测试1：短对话（不压缩）
    print("测试1：短对话（5轮）")
    short_messages = [
        Message(role="user", content=f"问题{i}", timestamp=datetime.now())
        for i in range(5)
    ]
    result = compressor.compress(short_messages)
    print(f"原始: {len(short_messages)}条, 压缩后: {len(result)}条")
    print(f"是否压缩: {'否' if len(result) == len(short_messages) else '是'}\n")

    # 测试2：中等对话（滑动窗口）
    print("测试2：中等对话（15轮）")
    medium_messages = []
    for i in range(15):
        medium_messages.append(Message(role="user", content=f"问题{i}", timestamp=datetime.now()))
        medium_messages.append(Message(role="assistant", content=f"回答{i}", timestamp=datetime.now()))
    result = compressor.compress(medium_messages)
    print(f"原始: {len(medium_messages)}条, 压缩后: {len(result)}条")
    print(f"使用策略: 滑动窗口\n")

    # 测试3：长对话（LLM摘要）
    print("测试3：长对话（20轮）")
    long_messages = []
    for i in range(20):
        long_messages.append(Message(role="user", content=f"问题{i}", timestamp=datetime.now()))
        long_messages.append(Message(role="assistant", content=f"回答{i}", timestamp=datetime.now()))
    result = compressor.compress(long_messages)
    print(f"原始: {len(long_messages)}条, 压缩后: {len(result)}条")
    print(f"使用策略: LLM摘要")


# ===== 主函数 =====

def main():
    """主函数"""
    print("=" * 60)
    print("对话历史压缩策略对比测试")
    print("=" * 60)
    print()

    # 对比三种策略
    compare_strategies()

    print("\n" + "=" * 60)
    print()

    # 测试自适应压缩器
    test_adaptive_compressor()


if __name__ == "__main__":
    main()
```

---

## 运行输出示例

```
============================================================
对话历史压缩策略对比测试
============================================================

原始消息数: 20条 (10轮对话)

=== 策略1：滑动窗口截断 ===
[滑动窗口] 20条 → 6条 (保留3轮)
压缩后: 6条消息
保留的对话:
  user: 请介绍RAG部署...
  assistant: RAG部署需要考虑向量库选型、API限流、成本控制...
  user: 请介绍RAG成本...
  assistant: RAG成本主要来自Embedding和LLM调用...
  user: 请介绍RAG未来...
  assistant: RAG未来发展方向包括多模态、实时更新、个性化...

=== 策略2：LLM摘要压缩 ===
[LLM摘要] 20条 → 5条 (摘要+2轮)
压缩后: 5条消息
摘要内容:
[历史摘要] 用户系统地询问了RAG的多个方面，从基本定义、优势、应用场景，
到技术实现、优化策略、评估方法、部署考虑和成本分析。讨论涵盖了RAG的完整
生命周期，用户展现出对RAG技术的全面学习需求。

保留的最近对话:
  user: 请介绍RAG未来...
  assistant: RAG未来发展方向包括多模态、实时更新、个性化...
  user: 请介绍RAG总结...
  assistant: RAG是当前最实用的LLM应用范式之一...

=== 策略3：关键信息提取 ===
[关键信息] 20条 → 5条 (关键信息+2轮)
压缩后: 5条消息
关键信息:
[关键信息]
主题: RAG技术学习
意图: 系统学习RAG相关知识
实体: RAG=检索增强生成
关键事实:
  - RAG结合检索和生成优势
  - 需要向量数据库、Embedding模型和LLM
  - 优化包括混合检索、ReRank、Query改写
  - 成本主要来自Embedding和LLM调用

保留的最近对话:
  user: 请介绍RAG未来...
  assistant: RAG未来发展方向包括多模态、实时更新、个性化...

=== 性能对比 ===
策略            压缩后条数    压缩比      成本
--------------------------------------------------
滑动窗口          6          30.0%      零成本
LLM摘要          5          25.0%      ~$0.01-0.05
关键信息提取       5          25.0%      ~$0.01-0.05

============================================================

=== 自适应压缩器测试 ===

测试1：短对话（5轮）
原始: 5条, 压缩后: 5条
是否压缩: 否

测试2：中等对话（15轮）
[滑动窗口] 30条 → 10条 (保留5轮)
原始: 30条, 压缩后: 10条
使用策略: 滑动窗口

测试3：长对话（20轮）
[LLM摘要] 40条 → 7条 (摘要+3轮)
原始: 40条, 压缩后: 7条
使用策略: LLM摘要
```

---

## 在RAG系统中的集成

### 集成到对话管理器

```python
class ConversationManagerWithCompression:
    """带压缩功能的对话管理器"""

    def __init__(self, compression_strategy: str = "adaptive"):
        self.sessions: Dict[str, List[Message]] = {}

        if compression_strategy == "adaptive":
            self.compressor = AdaptiveCompressor()
        else:
            self.compressor = HistoryCompressor(strategy=compression_strategy)

    def add_message(self, session_id: str, role: str, content: str):
        """添加消息（自动压缩）"""
        if session_id not in self.sessions:
            self.sessions[session_id] = []

        message = Message(role=role, content=content, timestamp=datetime.now())
        self.sessions[session_id].append(message)

        # 自动压缩
        if isinstance(self.compressor, AdaptiveCompressor):
            if self.compressor.should_compress(self.sessions[session_id]):
                print(f"[触发压缩] Session {session_id}")
                self.sessions[session_id] = self.compressor.compress(
                    self.sessions[session_id]
                )

    def get_context(self, session_id: str) -> List[Message]:
        """获取上下文（已压缩）"""
        return self.sessions.get(session_id, [])
```

---

## 成本分析

### Token消耗对比

```python
def estimate_tokens(messages: List[Message]) -> int:
    """估算Token数量（粗略）"""
    return sum(len(msg.content.split()) * 1.3 for msg in messages)

# 示例：20轮对话
messages = [...]  # 20轮对话
original_tokens = estimate_tokens(messages)  # 约5000 tokens

# 策略1：滑动窗口（保留3轮）
compressed1 = compressor1.compress(messages, keep_last=3)
tokens1 = estimate_tokens(compressed1)  # 约1500 tokens
savings1 = (1 - tokens1/original_tokens) * 100  # 节省70%

# 策略2：LLM摘要（摘要+2轮）
compressed2 = compressor2.compress(messages, keep_last=2)
tokens2 = estimate_tokens(compressed2)  # 约1200 tokens
savings2 = (1 - tokens2/original_tokens) * 100  # 节省76%
```

### 成本计算

```
假设GPT-4价格:
- 输入: $0.03/1K tokens
- 输出: $0.06/1K tokens

无压缩（20轮对话）:
- 输入: 5000 tokens × $0.03 = $0.15

滑动窗口（保留3轮）:
- 输入: 1500 tokens × $0.03 = $0.045
- 节省: $0.105 (70%)

LLM摘要（摘要+2轮）:
- 摘要成本: 5000 tokens × $0.03 + 300 tokens × $0.06 = $0.168
- 后续输入: 1200 tokens × $0.03 = $0.036
- 总成本: $0.204（首次）→ $0.036（后续）
- 长期节省: 76%
```

---

## 总结

**本场景实现了：**
- ✅ 三种压缩策略的完整实现
- ✅ 自适应压缩器（自动选择策略）
- ✅ 性能对比和成本分析

**选择建议：**
- 原型开发：滑动窗口
- 短对话（<15轮）：滑动窗口
- 长对话（>15轮）：LLM摘要
- 特殊场景（客服）：关键信息提取

**下一步：**
- 学习 `11_实战代码_场景3_指代消解实现.md` - Query重写实现
- 学习 `12_实战代码_场景4_完整对话式RAG.md` - 整合所有模块
