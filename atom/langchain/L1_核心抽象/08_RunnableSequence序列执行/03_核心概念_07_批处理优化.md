# 核心概念7：批处理优化

> 理解 batch/abatch 的性能优化原理

---

## 批处理的本质

**一句话**：批处理通过并行执行多个输入，显著提升吞吐量。

```python
# 串行执行（慢）
results = []
for input in inputs:
    results.append(chain.invoke(input))
# 100个输入需要 200秒

# 批处理（快）
results = chain.batch(inputs)
# 100个输入只需要 7秒（加速 28x）
```

---

## 为什么需要批处理

### 问题：循环调用效率低

```python
# 评估测试集（100个问题）
test_questions = [
    "什么是 AI？",
    "什么是 ML？",
    # ... 98 more questions
]

# 串行执行
import time
start = time.time()
results = []
for question in test_questions:
    answer = chain.invoke({"question": question})
    results.append(answer)
duration = time.time() - start

print(f"耗时: {duration:.2f}s")  # 200秒
```

**痛点**：
- 串行执行：一个接一个
- 等待时间长：总时间 = 单次时间 × 数量
- 资源利用率低：CPU/网络空闲

### 解决方案：批处理

```python
# 批处理
start = time.time()
results = chain.batch([
    {"question": q} for q in test_questions
])
duration = time.time() - start

print(f"耗时: {duration:.2f}s")  # 7秒
print(f"加速比: {200 / 7:.1f}x")  # 28x
```

**优势**：
- 并行执行：同时处理多个输入
- 时间短：总时间 ≈ 单次时间
- 资源利用率高：充分利用 CPU/网络

---

## 批处理的实现原理

### 基础实现

```python
from typing import List, Any
import asyncio

class BatchableRunnable:
    """支持批处理的 Runnable"""

    def invoke(self, input: Any) -> Any:
        """单次执行"""
        raise NotImplementedError

    def batch(self, inputs: List[Any]) -> List[Any]:
        """
        批处理（同步版本）

        默认实现：串行执行
        """
        return [self.invoke(input) for input in inputs]

    async def abatch(self, inputs: List[Any]) -> List[Any]:
        """
        批处理（异步版本）

        默认实现：并行执行
        """
        tasks = [asyncio.to_thread(self.invoke, input) for input in inputs]
        return await asyncio.gather(*tasks)


# ===== 测试 =====

class SlowProcessor(BatchableRunnable):
    """慢速处理器（模拟 API 调用）"""

    def invoke(self, input: int) -> int:
        import time
        time.sleep(0.5)  # 模拟 API 延迟
        return input * 2


if __name__ == "__main__":
    import time

    processor = SlowProcessor()
    inputs = list(range(10))

    # 串行执行
    start = time.time()
    results_serial = [processor.invoke(i) for i in inputs]
    serial_time = time.time() - start
    print(f"串行: {serial_time:.2f}s")

    # 批处理（同步）
    start = time.time()
    results_batch = processor.batch(inputs)
    batch_time = time.time() - start
    print(f"批处理（同步）: {batch_time:.2f}s")

    # 批处理（异步）
    start = time.time()
    results_abatch = asyncio.run(processor.abatch(inputs))
    abatch_time = time.time() - start
    print(f"批处理（异步）: {abatch_time:.2f}s")
    print(f"加速比: {serial_time / abatch_time:.1f}x")
```

**输出**：
```
串行: 5.00s
批处理（同步）: 5.00s
批处理（异步）: 0.50s
加速比: 10.0x
```

### 并行执行机制

**来源**：LangChain 源码

```python
import asyncio
from typing import List, Any

async def parallel_invoke(runnable, inputs: List[Any]) -> List[Any]:
    """并行执行多个输入"""

    async def invoke_one(input: Any) -> Any:
        """执行单个输入"""
        # 如果 runnable 支持异步，使用 ainvoke
        if hasattr(runnable, 'ainvoke'):
            return await runnable.ainvoke(input)
        # 否则，在线程池中执行
        else:
            return await asyncio.to_thread(runnable.invoke, input)

    # 并行执行所有输入
    tasks = [invoke_one(input) for input in inputs]
    return await asyncio.gather(*tasks)


# 使用
results = asyncio.run(parallel_invoke(chain, inputs))
```

**关键技术**：
- `asyncio.gather()`: 并行执行多个协程
- `asyncio.to_thread()`: 在线程池中执行同步函数
- 自动管理并发数

---

## LangChain 中的批处理

### 基础用法

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("解释：{topic}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | llm | parser

# 批处理
inputs = [
    {"topic": "AI"},
    {"topic": "ML"},
    {"topic": "DL"}
]

results = chain.batch(inputs)
for i, result in enumerate(results):
    print(f"{i+1}. {result[:50]}...")
```

### 控制并发数

```python
# 控制最大并发数（避免 API 限流）
results = chain.batch(
    inputs,
    config={"max_concurrency": 5}  # 最多5个并发
)
```

**为什么需要控制并发数？**
- API 限流：大多数 API 有并发限制
- 资源限制：避免耗尽内存/连接
- 稳定性：避免雪崩效应

### 异步批处理

```python
import asyncio

async def async_batch():
    """异步批处理"""
    results = await chain.abatch(inputs)
    return results

results = asyncio.run(async_batch())
```

---

## 性能对比

### 实测数据

**来源**：Analytics Vidhya - LCEL Guide

| 输入数量 | 串行时间 | 批处理时间 | 加速比 |
|---------|---------|-----------|--------|
| 10 | 20s | 2.5s | 8x |
| 50 | 100s | 5s | 20x |
| 100 | 200s | 7s | 28x |
| 500 | 1000s | 35s | 28x |

**关键发现**：
- 加速比随输入数量增加而增加
- 受限于 API 限流和并发数
- 最大加速比约 20-30x

### 性能瓶颈

```python
import time

def benchmark_batch(chain, num_inputs, max_concurrency=None):
    """性能基准测试"""
    inputs = [{"topic": f"Topic {i}"} for i in range(num_inputs)]

    # 串行
    start = time.time()
    for input in inputs:
        chain.invoke(input)
    serial_time = time.time() - start

    # 批处理
    start = time.time()
    config = {"max_concurrency": max_concurrency} if max_concurrency else {}
    chain.batch(inputs, config=config)
    batch_time = time.time() - start

    print(f"输入数量: {num_inputs}")
    print(f"串行时间: {serial_time:.2f}s")
    print(f"批处理时间: {batch_time:.2f}s")
    print(f"加速比: {serial_time / batch_time:.1f}x")
    print()

# 测试不同并发数
benchmark_batch(chain, 100, max_concurrency=5)
benchmark_batch(chain, 100, max_concurrency=10)
benchmark_batch(chain, 100, max_concurrency=20)
```

---

## 手写批处理优化

### 版本1：简单线程池

```python
from concurrent.futures import ThreadPoolExecutor
from typing import List, Any, Callable

def simple_batch(
    func: Callable,
    inputs: List[Any],
    max_workers: int = 10
) -> List[Any]:
    """简单的批处理实现"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(func, inputs))
    return results


# 测试
def slow_function(x):
    import time
    time.sleep(0.5)
    return x * 2

inputs = list(range(10))

import time
start = time.time()
results = simple_batch(slow_function, inputs, max_workers=10)
duration = time.time() - start

print(f"批处理时间: {duration:.2f}s")
print(f"结果: {results}")
```

### 版本2：异步批处理

```python
import asyncio
from typing import List, Any, Callable

async def async_batch(
    func: Callable,
    inputs: List[Any],
    max_concurrency: int = 10
) -> List[Any]:
    """异步批处理实现"""

    # 创建信号量限制并发数
    semaphore = asyncio.Semaphore(max_concurrency)

    async def invoke_with_semaphore(input: Any) -> Any:
        """带信号量的执行"""
        async with semaphore:
            return await asyncio.to_thread(func, input)

    # 并行执行
    tasks = [invoke_with_semaphore(input) for input in inputs]
    return await asyncio.gather(*tasks)


# 测试
results = asyncio.run(async_batch(slow_function, inputs, max_concurrency=5))
```

### 版本3：带重试的批处理

```python
import asyncio
from typing import List, Any, Callable

async def batch_with_retry(
    func: Callable,
    inputs: List[Any],
    max_concurrency: int = 10,
    max_retries: int = 3
) -> List[Any]:
    """带重试的批处理"""

    semaphore = asyncio.Semaphore(max_concurrency)

    async def invoke_with_retry(input: Any) -> Any:
        """带重试的执行"""
        async with semaphore:
            for attempt in range(max_retries + 1):
                try:
                    return await asyncio.to_thread(func, input)
                except Exception as e:
                    if attempt == max_retries:
                        raise
                    await asyncio.sleep(2 ** attempt)

    tasks = [invoke_with_retry(input) for input in inputs]
    return await asyncio.gather(*tasks)
```

---

## 2025-2026 成本优化：langasync

### langasync 简介

**来源**：社区工具 - langasync

**langasync** 是 2026 年社区推出的工具，通过批处理 API 降低 LLM 成本 50%。

**核心原理**：
- 使用 OpenAI/Anthropic 批处理 API
- 批量请求享受 50% 折扣
- 适用于非实时任务

### 使用示例

```python
from langasync import wrap_chain
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 构建链
prompt = ChatPromptTemplate.from_template("解释：{topic}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | llm | parser

# 包装为批处理模式
async_chain = wrap_chain(chain, batch_size=10)

# 批量执行（成本降低 50%）
import asyncio

async def run_batch():
    inputs = [{"topic": f"Topic {i}"} for i in range(100)]
    results = await async_chain.abatch(inputs)
    return results

results = asyncio.run(run_batch())
```

**成本对比**：

| 方式 | 100个请求成本 | 说明 |
|------|--------------|------|
| 循环 invoke | $1.00 | 标准价格 |
| batch | $1.00 | 标准价格（并行但不打折） |
| langasync | $0.50 | 批处理 API 折扣 50% |

### 适用场景

✅ **适合 langasync**：
- 批量评估和测试
- 数据标注任务
- 离线分析和报告
- 非实时的批量处理

❌ **不适合 langasync**：
- 实时对话应用
- 需要即时响应的场景
- 单次查询

---

## 批处理的优化技巧

### 1. 选择合适的并发数

```python
# ❌ 并发数过高（触发限流）
results = chain.batch(inputs, config={"max_concurrency": 100})

# ✅ 合理的并发数
results = chain.batch(inputs, config={"max_concurrency": 10})
```

**如何选择并发数？**
- OpenAI：10-20
- Anthropic：5-10
- 本地模型：根据 CPU/GPU 资源

### 2. 分批处理大量输入

```python
def batch_in_chunks(chain, inputs, chunk_size=100):
    """分批处理大量输入"""
    results = []
    for i in range(0, len(inputs), chunk_size):
        chunk = inputs[i:i + chunk_size]
        chunk_results = chain.batch(chunk)
        results.extend(chunk_results)
        print(f"已处理: {len(results)}/{len(inputs)}")
    return results

# 处理 10000 个输入
large_inputs = [{"topic": f"Topic {i}"} for i in range(10000)]
results = batch_in_chunks(chain, large_inputs, chunk_size=100)
```

### 3. 错误处理

```python
def safe_batch(chain, inputs):
    """安全的批处理，单个失败不影响其他"""
    results = []
    errors = []

    for i, input in enumerate(inputs):
        try:
            result = chain.invoke(input)
            results.append({"index": i, "success": True, "result": result})
        except Exception as e:
            results.append({"index": i, "success": False, "error": str(e)})
            errors.append({"index": i, "input": input, "error": e})

    return results, errors

# 使用
results, errors = safe_batch(chain, inputs)
print(f"成功: {sum(1 for r in results if r['success'])}/{len(inputs)}")
print(f"失败: {len(errors)}")
```

### 4. 进度显示

```python
from tqdm import tqdm

def batch_with_progress(chain, inputs):
    """带进度条的批处理"""
    results = []
    for input in tqdm(inputs, desc="处理中"):
        result = chain.invoke(input)
        results.append(result)
    return results

# 使用
results = batch_with_progress(chain, inputs)
```

---

## 实战场景

### 场景1：批量评估测试集

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 构建评估链
prompt = ChatPromptTemplate.from_template("""
问题：{question}
参考答案：{reference}

请评估以下答案的质量（1-5分）：
{answer}

评分：
""")

llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

eval_chain = prompt | llm | parser

# 准备测试集
test_cases = [
    {
        "question": "什么是 AI？",
        "reference": "人工智能是...",
        "answer": "AI 是人工智能的缩写..."
    },
    # ... 更多测试用例
]

# 批量评估
import time
start = time.time()
scores = eval_chain.batch(test_cases, config={"max_concurrency": 10})
duration = time.time() - start

print(f"评估 {len(test_cases)} 个用例")
print(f"耗时: {duration:.2f}s")
print(f"平均: {duration / len(test_cases):.2f}s/个")
```

### 场景2：批量数据标注

```python
# 构建标注链
prompt = ChatPromptTemplate.from_template("""
请为以下文本标注情感（正面/负面/中性）：

文本：{text}

情感：
""")

llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

label_chain = prompt | llm | parser

# 批量标注
texts = [
    {"text": "这个产品太棒了！"},
    {"text": "质量很差，不推荐。"},
    {"text": "还可以，一般般。"},
    # ... 更多文本
]

labels = label_chain.batch(texts, config={"max_concurrency": 10})

# 保存结果
import json
with open("labels.json", "w", encoding="utf-8") as f:
    json.dump([
        {"text": t["text"], "label": l}
        for t, l in zip(texts, labels)
    ], f, ensure_ascii=False, indent=2)
```

### 场景3：批量翻译

```python
# 构建翻译链
prompt = ChatPromptTemplate.from_template("""
将以下文本翻译成{target_lang}：

{text}

翻译：
""")

llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

translate_chain = prompt | llm | parser

# 批量翻译
texts = [
    {"text": "Hello, world!", "target_lang": "中文"},
    {"text": "Good morning!", "target_lang": "中文"},
    {"text": "Thank you!", "target_lang": "中文"},
    # ... 更多文本
]

translations = translate_chain.batch(texts, config={"max_concurrency": 10})

for text, translation in zip(texts, translations):
    print(f"{text['text']} → {translation}")
```

---

## 批处理 vs 循环的选择

### 何时使用批处理

✅ **推荐使用批处理**：
- 批量评估和测试
- 数据标注任务
- 批量翻译
- 离线分析
- 非实时任务

### 何时使用循环

✅ **推荐使用循环**：
- 需要实时反馈
- 需要根据前一个结果决定下一个输入
- 单个任务很重要（不能失败）
- 需要精细的错误处理

### 对比表

| 场景 | 批处理 | 循环 | 推荐 |
|------|--------|------|------|
| 批量评估 | ✅ 快 | ❌ 慢 | 批处理 |
| 实时反馈 | ❌ 无反馈 | ✅ 有反馈 | 循环 |
| 数据标注 | ✅ 高效 | ❌ 低效 | 批处理 |
| 依赖前一个结果 | ❌ 不支持 | ✅ 支持 | 循环 |
| 成本敏感 | ✅ 可用 langasync | ❌ 标准价格 | 批处理 |

---

## 常见问题

### Q1: batch 和多次 invoke 有什么区别？

**A**: batch 自动并行执行，性能更好。

```python
# ❌ 多次 invoke（串行）
results = [chain.invoke(i) for i in inputs]  # 慢

# ✅ batch（并行）
results = chain.batch(inputs)  # 快
```

### Q2: 如何处理批处理中的错误？

**A**: 使用 try-catch 包装每个输入：

```python
def safe_batch(chain, inputs):
    results = []
    for input in inputs:
        try:
            result = chain.invoke(input)
            results.append({"success": True, "result": result})
        except Exception as e:
            results.append({"success": False, "error": str(e)})
    return results
```

### Q3: 批处理会增加内存占用吗？

**A**: 会。批处理会同时保存多个输入和输出，需要根据内存大小调整批次大小。

---

## 学习检查清单

- [ ] 理解批处理的原理
- [ ] 理解并行执行机制
- [ ] 掌握 batch/abatch 的使用
- [ ] 能手写批处理实现
- [ ] 理解性能瓶颈
- [ ] 了解 langasync 成本优化
- [ ] 能应用批处理到实际场景

---

**版本**: v1.0
**最后更新**: 2026-02-18
**参考来源**:
- LangChain 官方文档
- Analytics Vidhya - LCEL Guide
- langasync 社区工具
