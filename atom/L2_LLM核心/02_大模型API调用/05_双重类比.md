# 大模型API调用 - 双重类比

用你熟悉的概念理解大模型 API。

---

## 类比1：API 调用 ≈ fetch 请求 ≈ 打电话

### 前端类比：fetch 请求

```javascript
// 前端：调用后端 API
const response = await fetch('/api/search', {
    method: 'POST',
    body: JSON.stringify({ query: '搜索词' })
});
const data = await response.json();
```

```python
# LLM：调用大模型 API
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "搜索词"}]
)
answer = response.choices[0].message.content
```

**相似点：**
- 都是发送请求 → 等待响应
- 都需要指定端点（model vs URL）
- 都需要传递参数（messages vs body）
- 都需要解析响应

### 日常生活类比：打电话咨询

```
打电话给客服：
1. 拨号（创建连接）      → client = OpenAI()
2. 说明问题（发送请求）   → messages=[{"role": "user", "content": "..."}]
3. 等待回答（等待响应）   → response = client.chat.completions.create(...)
4. 记录答案（解析响应）   → answer = response.choices[0].message.content
```

---

## 类比2：流式输出 ≈ SSE ≈ 看直播

### 前端类比：Server-Sent Events (SSE)

```javascript
// 前端：SSE 实时接收数据
const eventSource = new EventSource('/api/stream');
eventSource.onmessage = (event) => {
    console.log(event.data);  // 逐条接收
};
```

```python
# LLM：流式接收生成内容
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    stream=True
)
for chunk in stream:
    print(chunk.choices[0].delta.content)  # 逐块接收
```

**相似点：**
- 都是服务器主动推送数据
- 都是逐块/逐条接收
- 都能实现实时更新 UI

### 日常生活类比：看直播 vs 看录播

| 方式 | 直播（流式） | 录播（非流式） |
|------|-------------|---------------|
| 体验 | 实时看到内容 | 等待下载完成 |
| 延迟 | 几乎无延迟 | 需要等待 |
| 适用 | 聊天、实时交互 | 批量处理 |

```
看直播：主播说一句 → 你立刻听到 → 继续说 → 继续听
       （流式：生成一点 → 返回一点 → 继续生成 → 继续返回）

看录播：等待上传完成 → 一次性观看全部
       （非流式：全部生成完 → 一次性返回）
```

---

## 类比3：messages 数组 ≈ 聊天记录 ≈ 对话历史

### 前端类比：聊天应用的消息列表

```javascript
// 前端：聊天消息列表
const chatHistory = [
    { sender: 'system', text: '欢迎使用客服系统' },
    { sender: 'user', text: '我想退货' },
    { sender: 'bot', text: '好的，请提供订单号' },
    { sender: 'user', text: '订单号是12345' }
];
```

```python
# LLM：消息列表
messages = [
    {"role": "system", "content": "你是客服助手"},
    {"role": "user", "content": "我想退货"},
    {"role": "assistant", "content": "好的，请提供订单号"},
    {"role": "user", "content": "订单号是12345"}
]
```

**相似点：**
- 都是有序的消息数组
- 都区分发送者（role vs sender）
- 都保留完整对话历史

### 日常生活类比：微信聊天记录

```
微信聊天：
- 系统消息："已添加好友"        → role: "system"
- 你发的消息："在吗？"          → role: "user"
- 对方回复："在的"              → role: "assistant"
- 你继续问："帮我查个东西"       → role: "user"

LLM 需要看到完整聊天记录，才能理解上下文！
```

---

## 类比4：temperature ≈ Math.random() ≈ 厨师的创意程度

### 前端类比：随机数控制

```javascript
// 前端：控制随机性
const randomness = 0;  // 完全确定
const result = randomness === 0 ? 'A' : ['A', 'B', 'C'][Math.floor(Math.random() * 3)];
```

```python
# LLM：控制输出随机性
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    temperature=0  # 0=确定性，1=平衡，2=高随机
)
```

### 日常生活类比：厨师做菜

| temperature | 厨师行为 | 适用场景 |
|-------------|----------|----------|
| 0 | 严格按菜谱做，每次一样 | RAG、事实问答 |
| 0.7 | 基本按菜谱，偶尔小创新 | 通用对话 |
| 1.5 | 大胆创新，可能惊喜或翻车 | 创意写作 |

```
temperature = 0：
  问："1+1=?"
  答："2"（每次都是2）

temperature = 1：
  问："写一首关于春天的诗"
  答：每次都不一样，有创意
```

---

## 类比5：max_tokens ≈ maxLength ≈ 字数限制

### 前端类比：输入框字数限制

```javascript
// 前端：限制输入长度
<input maxLength={100} />  // 最多100字符
```

```python
# LLM：限制输出长度
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    max_tokens=100  # 最多100个token
)
```

### 日常生活类比：微博字数限制

```
微博：最多140字，超过就发不出去
LLM：max_tokens=100，超过就被截断

区别：
- 微博：超过不让发
- LLM：超过直接截断（可能话说一半）
```

---

## 类比总结表

| RAG/LLM 概念 | 前端类比 | 日常生活类比 |
|--------------|----------|--------------|
| API 调用 | fetch 请求 | 打电话咨询 |
| 流式输出 | SSE / WebSocket | 看直播 |
| messages 数组 | 聊天记录列表 | 微信聊天历史 |
| temperature | Math.random() 种子 | 厨师创意程度 |
| max_tokens | maxLength 属性 | 微博字数限制 |
| API Key | JWT Token | 会员卡/门禁卡 |
| rate limit | 请求限流 | 排队叫号 |
| model 参数 | API 版本号 | 选择哪个客服 |

---

## 代码对比：前端 vs LLM

```javascript
// 前端：调用后端 API
async function askBackend(question) {
    const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
            'Authorization': 'Bearer ' + token,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            message: question,
            history: chatHistory
        })
    });
    const data = await response.json();
    return data.answer;
}
```

```python
# LLM：调用大模型 API
def ask_llm(question: str, history: list) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",  # 相当于 API 版本
        messages=history + [{"role": "user", "content": question}],
        temperature=0.1,
        max_tokens=500
    )
    return response.choices[0].message.content
```

**核心相似点：**
1. 都是 HTTP 请求
2. 都需要认证（API Key vs Token）
3. 都传递结构化数据
4. 都解析 JSON 响应

---

**上一节：** [04_最小可用.md](./04_最小可用.md)
**下一节：** [06_反直觉点.md](./06_反直觉点.md) - 避开常见误区
