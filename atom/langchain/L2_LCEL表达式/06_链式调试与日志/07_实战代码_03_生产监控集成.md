# 实战代码3：生产监控集成

## 概述

本文档提供OpenTelemetry生产监控的完整实战代码，展示如何集成分布式追踪和指标收集。

**涵盖场景**：
1. OpenTelemetry基础配置
2. 自定义Span和属性
3. Metrics指标收集
4. 完整监控方案

**前置要求**：
- Python 3.13+
- LangChain v0.3+
- OpenTelemetry库

---

## 场景1：OpenTelemetry基础配置

### 目标

配置OpenTelemetry，实现基础的分布式追踪。

### 完整代码

```python
"""
场景1：OpenTelemetry基础配置
配置TracerProvider和导出器
"""

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()


def setup_opentelemetry(service_name="langchain-app", endpoint=None):
    """配置OpenTelemetry"""
    # 配置资源（服务信息）
    resource = Resource.create({
        "service.name": service_name,
        "service.version": "1.0.0",
        "deployment.environment": "production"
    })

    # 配置TracerProvider
    provider = TracerProvider(resource=resource)
    trace.set_tracer_provider(provider)

    # 配置导出器
    if endpoint:
        # 导出到OTLP端点（如Uptrace、Jaeger等）
        exporter = OTLPSpanExporter(endpoint=endpoint)
    else:
        # 导出到控制台（用于测试）
        exporter = ConsoleSpanExporter()

    # 添加处理器
    provider.add_span_processor(BatchSpanProcessor(exporter))

    print(f"✅ OpenTelemetry configured for {service_name}")
    return trace.get_tracer(__name__)


def main():
    """基础使用示例"""
    # 配置OpenTelemetry
    tracer = setup_opentelemetry("my-langchain-app")

    # 构建链
    prompt = ChatPromptTemplate.from_template("回答问题: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model

    # 使用span追踪
    with tracer.start_as_current_span("langchain_request") as span:
        # 设置span属性
        span.set_attribute("question", "什么是LCEL?")
        span.set_attribute("model", "gpt-4o-mini")

        # 执行链
        result = chain.invoke({"question": "什么是LCEL?"})

        # 记录结果
        span.set_attribute("response_length", len(result.content))
        span.set_attribute("status", "success")

        print(result.content)


if __name__ == "__main__":
    main()
```

### 输出示例

```
✅ OpenTelemetry configured for my-langchain-app
LCEL是LangChain Expression Language的缩写...

{
  "name": "langchain_request",
  "context": {...},
  "kind": "SpanKind.INTERNAL",
  "parent_id": null,
  "start_time": "2026-02-20T14:30:00.000Z",
  "end_time": "2026-02-20T14:30:02.000Z",
  "attributes": {
    "question": "什么是LCEL?",
    "model": "gpt-4o-mini",
    "response_length": 150,
    "status": "success"
  }
}
```

---

## 场景2：自定义Span和属性

### 目标

创建多层级的span，追踪复杂链的执行过程。

### 完整代码

```python
"""
场景2：自定义Span和属性
创建多层级span追踪
"""

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
from opentelemetry.sdk.resources import Resource
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from dotenv import load_dotenv
import time

load_dotenv()

# 配置OpenTelemetry
resource = Resource.create({"service.name": "rag-app"})
provider = TracerProvider(resource=resource)
trace.set_tracer_provider(provider)
provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))
tracer = trace.get_tracer(__name__)


def build_rag_chain():
    """构建RAG链"""
    texts = [
        "LCEL是LangChain Expression Language的缩写",
        "LCEL使用管道符|连接组件",
        "LCEL支持流式处理和异步执行"
    ]
    vectorstore = Chroma.from_texts(texts, embedding=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()

    template = """基于以下上下文回答问题:
    
上下文: {context}
问题: {question}
回答:"""
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n".join(doc.page_content for doc in docs)

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | ChatOpenAI(model="gpt-4o-mini")
        | StrOutputParser()
    )

    return chain, retriever


def main():
    """多层级span示例"""
    chain, retriever = build_rag_chain()

    # 父span：整个RAG请求
    with tracer.start_as_current_span("rag_request") as parent_span:
        parent_span.set_attribute("operation", "rag_query")
        parent_span.set_attribute("question", "什么是LCEL?")

        # 子span1：检索
        with tracer.start_as_current_span("retrieval") as retrieval_span:
            retrieval_span.set_attribute("query", "什么是LCEL?")
            start = time.time()

            docs = retriever.invoke("什么是LCEL?")

            retrieval_span.set_attribute("num_docs", len(docs))
            retrieval_span.set_attribute("duration_ms", (time.time() - start) * 1000)

            # 记录事件
            retrieval_span.add_event("documents_retrieved", {
                "count": len(docs)
            })

        # 子span2：生成
        with tracer.start_as_current_span("generation") as gen_span:
            gen_span.set_attribute("model", "gpt-4o-mini")
            start = time.time()

            result = chain.invoke("什么是LCEL?")

            gen_span.set_attribute("response_length", len(result))
            gen_span.set_attribute("duration_ms", (time.time() - start) * 1000)

        # 记录总体结果
        parent_span.set_attribute("status", "success")
        parent_span.set_attribute("total_docs", len(docs))

        print(result)


if __name__ == "__main__":
    main()
```

---

## 场景3：Metrics指标收集

### 目标

收集性能指标（延迟、吞吐量、错误率）。

### 完整代码

```python
"""
场景3：Metrics指标收集
收集延迟、吞吐量、错误率
"""

from opentelemetry import trace, metrics
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader, ConsoleMetricExporter
from opentelemetry.sdk.resources import Resource
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import time

load_dotenv()

# 配置资源
resource = Resource.create({"service.name": "metrics-app"})

# 配置Trace
trace_provider = TracerProvider(resource=resource)
trace.set_tracer_provider(trace_provider)
trace_provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))
tracer = trace.get_tracer(__name__)

# 配置Metrics
metric_reader = PeriodicExportingMetricReader(
    ConsoleMetricExporter(),
    export_interval_millis=5000  # 每5秒导出一次
)
meter_provider = MeterProvider(resource=resource, metric_readers=[metric_reader])
metrics.set_meter_provider(meter_provider)
meter = metrics.get_meter(__name__)

# 创建指标
request_counter = meter.create_counter(
    name="chain_requests_total",
    description="Total number of chain requests",
    unit="1"
)

latency_histogram = meter.create_histogram(
    name="chain_latency_ms",
    description="Chain execution latency in milliseconds",
    unit="ms"
)

error_counter = meter.create_counter(
    name="chain_errors_total",
    description="Total number of chain errors",
    unit="1"
)

token_counter = meter.create_counter(
    name="tokens_total",
    description="Total tokens used",
    unit="1"
)


def execute_chain_with_metrics(chain, question):
    """执行链并收集指标"""
    start = time.time()

    with tracer.start_as_current_span("chain_execution") as span:
        try:
            # 执行链
            result = chain.invoke({"question": question})

            # 计算延迟
            duration_ms = (time.time() - start) * 1000

            # 记录指标
            request_counter.add(1, {"status": "success", "model": "gpt-4o-mini"})
            latency_histogram.record(duration_ms, {"model": "gpt-4o-mini"})

            # 记录Token（如果可用）
            if hasattr(result, "response_metadata"):
                tokens = result.response_metadata.get("token_usage", {})
                token_counter.add(
                    tokens.get("total_tokens", 0),
                    {"type": "total", "model": "gpt-4o-mini"}
                )

            span.set_attribute("status", "success")
            span.set_attribute("latency_ms", duration_ms)

            return result

        except Exception as e:
            # 记录错误
            error_counter.add(1, {"error_type": e.__class__.__name__})
            request_counter.add(1, {"status": "error"})

            span.set_status(trace.Status(trace.StatusCode.ERROR))
            span.record_exception(e)

            raise


def main():
    """指标收集示例"""
    prompt = ChatPromptTemplate.from_template("回答问题: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model

    # 执行多次请求
    questions = [
        "什么是LCEL?",
        "LCEL有什么优势?",
        "如何使用LCEL?"
    ]

    for question in questions:
        try:
            result = execute_chain_with_metrics(chain, question)
            print(f"Q: {question}")
            print(f"A: {result.content[:100]}...\n")
        except Exception as e:
            print(f"Error: {e}\n")

    print("等待指标导出...")
    time.sleep(6)  # 等待指标导出


if __name__ == "__main__":
    main()
```

### 输出示例

```
Q: 什么是LCEL?
A: LCEL是LangChain Expression Language的缩写...

Q: LCEL有什么优势?
A: LCEL的主要优势包括...

Q: 如何使用LCEL?
A: 使用LCEL非常简单...

等待指标导出...

Metrics:
- chain_requests_total{status="success", model="gpt-4o-mini"}: 3
- chain_latency_ms{model="gpt-4o-mini"}: [1234, 1456, 1123]
- tokens_total{type="total", model="gpt-4o-mini"}: 405
```

---

## 场景4：完整监控方案

### 目标

集成Trace和Metrics，实现完整的生产监控方案。

### 完整代码

```python
"""
场景4：完整监控方案
集成Trace和Metrics
"""

from opentelemetry import trace, metrics
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter
from opentelemetry.sdk.resources import Resource
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.callbacks import BaseCallbackHandler
from dotenv import load_dotenv
import time
import os

load_dotenv()


class OpenTelemetryMonitor:
    """OpenTelemetry监控管理器"""

    def __init__(self, service_name, otlp_endpoint=None):
        self.service_name = service_name
        self.otlp_endpoint = otlp_endpoint or "http://localhost:4318"

        # 配置资源
        self.resource = Resource.create({
            "service.name": service_name,
            "service.version": "1.0.0",
            "deployment.environment": os.getenv("ENV", "production")
        })

        # 配置Trace
        self._setup_trace()

        # 配置Metrics
        self._setup_metrics()

    def _setup_trace(self):
        """配置追踪"""
        provider = TracerProvider(resource=self.resource)
        trace.set_tracer_provider(provider)

        exporter = OTLPSpanExporter(
            endpoint=f"{self.otlp_endpoint}/v1/traces"
        )
        provider.add_span_processor(BatchSpanProcessor(exporter))

        self.tracer = trace.get_tracer(__name__)

    def _setup_metrics(self):
        """配置指标"""
        exporter = OTLPMetricExporter(
            endpoint=f"{self.otlp_endpoint}/v1/metrics"
        )
        reader = PeriodicExportingMetricReader(exporter, export_interval_millis=10000)
        provider = MeterProvider(resource=self.resource, metric_readers=[reader])
        metrics.set_meter_provider(provider)

        self.meter = metrics.get_meter(__name__)

        # 创建指标
        self.request_counter = self.meter.create_counter(
            "requests_total",
            description="Total requests"
        )
        self.latency_histogram = self.meter.create_histogram(
            "latency_ms",
            description="Request latency"
        )
        self.error_counter = self.meter.create_counter(
            "errors_total",
            description="Total errors"
        )
        self.token_counter = self.meter.create_counter(
            "tokens_total",
            description="Total tokens"
        )

    def trace_chain_execution(self, chain, inputs, **kwargs):
        """追踪链执行"""
        with self.tracer.start_as_current_span("chain_execution") as span:
            # 设置属性
            span.set_attribute("inputs", str(inputs))
            span.set_attribute("model", kwargs.get("model", "unknown"))

            start = time.time()

            try:
                # 执行链
                result = chain.invoke(inputs)
                duration_ms = (time.time() - start) * 1000

                # 记录成功
                span.set_attribute("status", "success")
                span.set_attribute("latency_ms", duration_ms)

                # 记录指标
                self.request_counter.add(1, {"status": "success"})
                self.latency_histogram.record(duration_ms)

                # 记录Token
                if hasattr(result, "response_metadata"):
                    tokens = result.response_metadata.get("token_usage", {})
                    self.token_counter.add(tokens.get("total_tokens", 0))

                return result

            except Exception as e:
                # 记录错误
                span.set_status(trace.Status(trace.StatusCode.ERROR))
                span.record_exception(e)

                self.request_counter.add(1, {"status": "error"})
                self.error_counter.add(1, {"error_type": e.__class__.__name__})

                raise


def main():
    """完整监控示例"""
    # 初始化监控
    monitor = OpenTelemetryMonitor(
        service_name="production-langchain-app",
        otlp_endpoint="http://localhost:4318"  # 替换为实际端点
    )

    # 构建链
    prompt = ChatPromptTemplate.from_template("回答问题: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model

    # 执行请求
    try:
        result = monitor.trace_chain_execution(
            chain,
            {"question": "什么是LCEL?"},
            model="gpt-4o-mini"
        )
        print(result.content)
    except Exception as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    main()
```

---

## 集成Prometheus示例

### 完整代码

```python
"""
集成Prometheus
导出指标到Prometheus
"""

from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from prometheus_client import start_http_server
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import time

load_dotenv()

# 配置Prometheus导出器
reader = PrometheusMetricReader()
provider = MeterProvider(metric_readers=[reader])
metrics.set_meter_provider(provider)
meter = metrics.get_meter(__name__)

# 创建指标
request_counter = meter.create_counter("chain_requests")
latency_histogram = meter.create_histogram("chain_latency_seconds")


def main():
    """Prometheus集成示例"""
    # 启动Prometheus HTTP服务器
    start_http_server(port=8000)
    print("Prometheus metrics available at http://localhost:8000/metrics")

    # 构建链
    prompt = ChatPromptTemplate.from_template("回答问题: {question}")
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model

    # 执行请求
    while True:
        start = time.time()
        result = chain.invoke({"question": "什么是LCEL?"})
        duration = time.time() - start

        # 记录指标
        request_counter.add(1)
        latency_histogram.record(duration)

        print(f"Request completed in {duration:.2f}s")
        time.sleep(10)  # 每10秒执行一次


if __name__ == "__main__":
    main()
```

访问 http://localhost:8000/metrics 查看指标：

```
# HELP chain_requests_total Total chain requests
# TYPE chain_requests_total counter
chain_requests_total 10

# HELP chain_latency_seconds Chain latency in seconds
# TYPE chain_latency_seconds histogram
chain_latency_seconds_bucket{le="0.1"} 0
chain_latency_seconds_bucket{le="0.5"} 2
chain_latency_seconds_bucket{le="1.0"} 5
chain_latency_seconds_bucket{le="2.0"} 10
chain_latency_seconds_sum 12.5
chain_latency_seconds_count 10
```

---

## 最佳实践

### 1. 使用语义化的span名称

```python
# ✅ 好的span名称
with tracer.start_as_current_span("rag_retrieval"):
    pass

# ❌ 不好的span名称
with tracer.start_as_current_span("step1"):
    pass
```

### 2. 添加有意义的属性

```python
span.set_attribute("query", question)
span.set_attribute("num_results", len(docs))
span.set_attribute("model", "gpt-4o-mini")
```

### 3. 记录关键事件

```python
span.add_event("cache_hit", {"key": cache_key})
span.add_event("rate_limit_reached")
```

### 4. 正确处理错误

```python
try:
    result = chain.invoke(input)
except Exception as e:
    span.set_status(trace.Status(trace.StatusCode.ERROR))
    span.record_exception(e)
    raise
```

---

## 总结

**核心技巧**：

1. **配置OpenTelemetry**：TracerProvider + Exporter
2. **创建Span**：多层级追踪
3. **收集Metrics**：延迟、吞吐量、错误率
4. **导出数据**：OTLP、Prometheus、Console

**最佳实践**：

- 使用语义化的span名称
- 添加有意义的属性
- 记录关键事件
- 正确处理错误

---

**版本信息**
- LangChain: v0.3+ (2025-2026)
- Python: 3.13+
- OpenTelemetry: 1.20+
- 最后更新: 2026-02-20
