# 第一性原理

> 从最基础的真理出发，理解 RunnableSequence 的本质

---

## 什么是第一性原理？

**第一性原理**：回到事物最基本的真理，从源头思考问题，而不是基于类比或经验。

这是物理学家和工程师常用的思维方式：
- 不问"别人怎么做"
- 而问"这个问题的本质是什么"
- 从基本事实出发，逐步推导

**为什么用第一性原理思考 RunnableSequence？**
- 避免被表面的语法糖迷惑
- 理解设计背后的深层原因
- 知道何时使用、何时不使用

---

## RunnableSequence 的第一性原理

### 1. 最基础的定义

**RunnableSequence = 一个函数组合器（Function Compositor）**

仅此而已！没有更基础的了。

让我们从最简单的函数组合开始：

```python
# 最基础的函数组合
def f(x):
    return x + 1

def g(x):
    return x * 2

def h(x):
    return x ** 2

# 手动组合
result = h(g(f(10)))  # ((10 + 1) * 2) ** 2 = 484
```

这就是函数组合的本质：**将多个函数按顺序连接，前一个函数的输出是后一个函数的输入。**

RunnableSequence 做的就是这件事，只是：
1. 用 `|` 操作符代替嵌套调用
2. 提供统一的执行接口（invoke/batch/stream）
3. 添加了错误处理、可观测性等生产特性

```python
# RunnableSequence 的本质
chain = f | g | h
result = chain.invoke(10)  # 484
```

**核心洞察**：RunnableSequence 不是什么神奇的东西，它只是一个更好用的函数组合器。

---

### 2. 为什么需要 RunnableSequence？

**核心问题：AI 应用需要组合多个步骤，但手动组合很痛苦。**

#### 问题1：手动组合代码冗长

```python
# 手动组合（痛苦）
def process_question(question: str) -> str:
    # 步骤1：构建 prompt
    prompt_result = prompt_template.invoke({"question": question})

    # 步骤2：调用 LLM
    llm_result = llm.invoke(prompt_result)

    # 步骤3：解析输出
    final_result = parser.invoke(llm_result)

    return final_result

# 使用 RunnableSequence（简洁）
chain = prompt_template | llm | parser
result = chain.invoke({"question": question})
```

**痛点**：
- 每次都要写重复的中间变量
- 代码冗长，难以阅读
- 难以复用和组合

#### 问题2：批量处理需要手动实现并行

```python
# 手动实现批量处理（复杂）
import asyncio

async def process_batch(questions: List[str]) -> List[str]:
    tasks = []
    for question in questions:
        # 需要手动创建异步任务
        task = asyncio.create_task(process_question_async(question))
        tasks.append(task)

    # 需要手动管理并发
    results = await asyncio.gather(*tasks)
    return results

# 使用 RunnableSequence（自动）
chain = prompt_template | llm | parser
results = chain.batch(questions)  # 自动并行！
```

**痛点**：
- 需要手动管理异步任务
- 需要手动控制并发数
- 容易出错（忘记 await、并发数过大导致限流）

#### 问题3：流式输出需要手动实现

```python
# 手动实现流式输出（复杂）
async def process_stream(question: str):
    prompt_result = prompt_template.invoke({"question": question})

    # 需要手动处理流式输出
    async for chunk in llm.astream(prompt_result):
        parsed_chunk = parser.invoke(chunk)
        yield parsed_chunk

# 使用 RunnableSequence（自动）
chain = prompt_template | llm | parser
async for chunk in chain.astream({"question": question}):
    print(chunk, end="", flush=True)
```

**痛点**：
- 需要手动处理每个步骤的流式输出
- 需要手动传递流式数据
- 代码复杂，难以维护

#### 问题4：错误处理需要每个步骤单独实现

```python
# 手动实现错误处理（繁琐）
def process_with_retry(question: str, max_retries: int = 3) -> str:
    for attempt in range(max_retries):
        try:
            prompt_result = prompt_template.invoke({"question": question})
            llm_result = llm.invoke(prompt_result)
            final_result = parser.invoke(llm_result)
            return final_result
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)  # 指数退避

# 使用 RunnableSequence（简单）
chain = (prompt_template | llm | parser).with_retry(stop_after_attempt=3)
result = chain.invoke({"question": question})
```

**痛点**：
- 需要手动实现重试逻辑
- 需要手动实现指数退避
- 需要为每个步骤单独处理错误

#### 问题5：可观测性需要手动添加日志

```python
# 手动添加日志（繁琐）
def process_with_logging(question: str) -> str:
    print(f"[1/3] 构建 prompt...")
    prompt_result = prompt_template.invoke({"question": question})
    print(f"Prompt: {prompt_result}")

    print(f"[2/3] 调用 LLM...")
    llm_result = llm.invoke(prompt_result)
    print(f"LLM output: {llm_result}")

    print(f"[3/3] 解析输出...")
    final_result = parser.invoke(llm_result)
    print(f"Final result: {final_result}")

    return final_result

# 使用 RunnableSequence（自动）
chain = prompt_template | llm | parser
# 自动集成 LangSmith 追踪
result = chain.invoke({"question": question})
```

**痛点**：
- 需要手动添加日志
- 日志格式不统一
- 难以集成专业的追踪工具

---

### 3. RunnableSequence 的三层价值

#### 价值1：声明式编程（Declarative Programming）

**命令式 vs 声明式**：

```python
# 命令式：告诉计算机"怎么做"
def process(question):
    step1 = prompt_template.invoke({"question": question})
    step2 = llm.invoke(step1)
    step3 = parser.invoke(step2)
    return step3

# 声明式：告诉计算机"做什么"
chain = prompt_template | llm | parser
result = chain.invoke({"question": question})
```

**声明式的优势**：
- **更简洁**：用 `|` 声明数据流，而非命令式调用
- **更易读**：一眼看出数据流向
- **更易维护**：修改流程只需修改 `|` 连接
- **更易测试**：每个组件可以独立测试

**类比**：
- 命令式 = 告诉厨师每一步怎么做（切菜、炒菜、装盘）
- 声明式 = 告诉厨师做什么菜（宫保鸡丁），厨师自己知道怎么做

**在 AI Agent 开发中**：
- RAG 应用：`retriever | prompt | llm | parser`（声明数据流）
- 对话系统：`history | prompt | llm | parser`（声明对话流程）
- 数据处理：`loader | transformer | analyzer | saver`（声明处理流程）

#### 价值2：组合性（Composability）

**组合性**：小的组件可以组合成大的组件，大的组件可以继续组合。

```python
# 小组件
prompt = ChatPromptTemplate.from_template("解释：{topic}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# 组合成中等组件
explain_chain = prompt | llm | parser

# 继续组合成大组件
summarize_prompt = ChatPromptTemplate.from_template("总结：{text}")
summarize_chain = summarize_prompt | llm | parser

# 组合成完整流程
full_chain = explain_chain | summarize_chain
```

**组合性的优势**：
- **可复用**：小组件可以在多个地方使用
- **可测试**：每个组件可以独立测试
- **可扩展**：添加新功能只需添加新组件
- **可维护**：修改一个组件不影响其他组件

**类比**：
- 像乐高积木：小积木可以组合成大积木
- 像函数：小函数可以组合成大函数
- 像管道：小管道可以连接成大管道

**在 AI Agent 开发中**：
- 构建可复用的组件库（prompt 模板、解析器、工具）
- 快速组合出新的应用
- 团队协作：不同人负责不同组件

#### 价值3：统一接口（Uniform Interface）

**统一接口**：所有 Runnable 都有相同的接口（invoke/batch/stream），无论内部实现如何。

```python
# 所有 Runnable 都有相同的接口
prompt.invoke(input)
llm.invoke(input)
parser.invoke(input)
chain.invoke(input)  # 组合后的链也有相同接口

# 批量执行
prompt.batch(inputs)
llm.batch(inputs)
parser.batch(inputs)
chain.batch(inputs)  # 组合后的链也支持批量

# 流式输出
prompt.stream(input)
llm.stream(input)
parser.stream(input)
chain.stream(input)  # 组合后的链也支持流式
```

**统一接口的优势**：
- **里氏替换原则**：任何 Runnable 都可以替换为 RunnableSequence
- **易于组合**：不需要关心内部实现，只需关心接口
- **易于测试**：可以用 Mock 对象替换真实组件
- **易于扩展**：添加新组件只需实现 Runnable 接口

**类比**：
- 像 USB 接口：所有 USB 设备都有相同的接口
- 像插座：所有电器都可以插入插座
- 像 API：所有服务都有相同的 HTTP 接口

**在 AI Agent 开发中**：
- 可以轻松替换 LLM（GPT-4 → Claude → Gemini）
- 可以轻松替换检索器（Chroma → Pinecone → Weaviate）
- 可以轻松添加新组件（缓存、日志、监控）

---

### 4. 从第一性原理推导 RAG 应用

让我们从第一性原理推导一个 RAG 问答系统。

#### 推理链

```
1. 前提：用户提出问题，需要基于知识库回答
   ↓
2. 推导：需要先检索相关文档
   → 需要一个检索器（Retriever）
   ↓
3. 推导：检索到的文档需要注入到 prompt 中
   → 需要一个 prompt 模板（PromptTemplate）
   ↓
4. 推导：prompt 需要发送给 LLM 生成答案
   → 需要一个 LLM（ChatModel）
   ↓
5. 推导：LLM 的输出需要解析成文本
   → 需要一个解析器（OutputParser）
   ↓
6. 推导：这些步骤需要按顺序执行
   → 需要一个序列组合器（RunnableSequence）
   ↓
7. 推导：生产环境需要错误处理
   → 需要重试和降级机制（with_retry, with_fallbacks）
   ↓
8. 推导：需要实时显示回复
   → 需要流式输出（stream）
   ↓
9. 推导：需要批量评估测试集
   → 需要批量处理（batch）
   ↓
10. 最终应用：生产级 RAG 问答系统
```

#### 代码实现

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ===== 步骤1：检索器 =====
# 从第一性原理：需要根据问题检索相关文档
vectorstore = Chroma(
    embedding_function=OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ===== 步骤2：Prompt 模板 =====
# 从第一性原理：需要将检索到的文档注入到 prompt 中
prompt = ChatPromptTemplate.from_template("""
根据以下上下文回答问题：

上下文：
{context}

问题：{question}

答案：
""")

# ===== 步骤3：LLM =====
# 从第一性原理：需要 LLM 生成答案
llm = ChatOpenAI(model="gpt-4o-mini")

# ===== 步骤4：解析器 =====
# 从第一性原理：需要解析 LLM 的输出
parser = StrOutputParser()

# ===== 步骤5：序列组合 =====
# 从第一性原理：需要按顺序执行这些步骤
base_chain = (
    {
        "context": retriever,  # 检索相关文档
        "question": RunnablePassthrough()  # 透传问题
    }
    | prompt  # 注入上下文
    | llm     # 生成答案
    | parser  # 解析输出
)

# ===== 步骤6：错误处理 =====
# 从第一性原理：生产环境需要错误处理
chain_with_retry = base_chain.with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
)

# 添加降级方案
fallback_llm = ChatOpenAI(model="gpt-3.5-turbo")
fallback_chain = (
    {
        "context": retriever,
        "question": RunnablePassthrough()
    }
    | prompt
    | fallback_llm
    | parser
)

production_chain = chain_with_retry.with_fallbacks([fallback_chain])

# ===== 步骤7：使用 =====

# 单次查询
answer = production_chain.invoke("什么是向量数据库？")
print(answer)

# 流式输出（实时显示）
for chunk in production_chain.stream("RAG 的优势是什么？"):
    print(chunk, end="", flush=True)

# 批量评估（并行处理）
questions = [
    "什么是 Embedding？",
    "什么是向量检索？",
    "什么是 Chunking？"
]
answers = production_chain.batch(questions)
```

**关键洞察**：
- 每一步都是从需求出发，而非从技术出发
- 每一步都是必要的，没有多余的抽象
- 最终的代码简洁、清晰、易维护

---

### 5. 一句话总结第一性原理

**RunnableSequence 是函数组合器，通过声明式语法和统一接口，解决了 AI 应用中多步骤组合的痛点，让代码更简洁、更易维护、更易扩展。**

---

## 从第一性原理理解核心设计决策

### 决策1：为什么用 `|` 而不是 `+` 或 `>>`？

**从第一性原理思考**：
- **目标**：表达数据流（data flow）
- **候选符号**：`+`、`>>`、`|`、`->`
- **选择 `|` 的原因**：
  1. Unix 管道使用 `|`，开发者熟悉
  2. `|` 在 Python 中是位或运算符，可以重载
  3. `|` 视觉上像管道，符合直觉
  4. `+` 暗示加法，`>>` 暗示位移，都不如 `|` 直观

**对比**：
```python
# 如果用 +（不直观）
chain = prompt + llm + parser

# 如果用 >>（还可以，但不如 |）
chain = prompt >> llm >> parser

# 使用 |（最直观）
chain = prompt | llm | parser
```

### 决策2：为什么返回新对象而不是修改原对象？

**从第一性原理思考**：
- **目标**：保持不可变性（immutability）
- **候选方案**：
  1. 修改原对象（可变）
  2. 返回新对象（不可变）
- **选择不可变的原因**：
  1. 避免副作用（side effects）
  2. 易于测试和调试
  3. 支持并发（多线程安全）
  4. 符合函数式编程理念

**对比**：
```python
# 如果是可变的（危险）
chain = prompt
chain.append(llm)  # 修改原对象
chain.append(parser)

# 不可变（安全）
chain = prompt | llm | parser  # 返回新对象
# prompt、llm、parser 都没有被修改
```

### 决策3：为什么扁平化嵌套的 RunnableSequence？

**从第一性原理思考**：
- **目标**：提升性能和调试体验
- **问题**：`a | b | c` 可能创建嵌套的 `RunnableSequence(RunnableSequence(a, b), c)`
- **解决方案**：扁平化为 `RunnableSequence(a, b, c)`
- **原因**：
  1. 减少函数调用层级
  2. 更容易调试（查看所有步骤）
  3. 更容易优化（批处理、并行）

**对比**：
```python
# 不扁平化（嵌套）
chain = a | b | c
# 内部结构：RunnableSequence(RunnableSequence(a, b), c)
# 调用栈深度：3 层

# 扁平化
chain = a | b | c
# 内部结构：RunnableSequence(a, b, c)
# 调用栈深度：1 层
```

### 决策4：为什么提供 invoke/batch/stream 三种接口？

**从第一性原理思考**：
- **目标**：满足不同场景的需求
- **场景分析**：
  1. **单次查询**：API 服务、命令行工具 → invoke
  2. **批量处理**：评估、数据标注 → batch
  3. **实时反馈**：对话应用、进度显示 → stream
- **设计决策**：提供三种接口，而非只提供一种
- **原因**：
  1. 不同场景有不同需求
  2. 统一接口让组件可以互换
  3. 框架自动优化每种模式

**对比**：
```python
# 如果只有 invoke（不够用）
result = chain.invoke(input)  # 单次查询 ✅
results = [chain.invoke(i) for i in inputs]  # 批量处理 ❌ 慢
# 流式输出 ❌ 不支持

# 提供三种接口（完整）
result = chain.invoke(input)  # 单次查询 ✅
results = chain.batch(inputs)  # 批量处理 ✅ 快
for chunk in chain.stream(input):  # 流式输出 ✅
    print(chunk)
```

---

## 从第一性原理理解局限性

### 局限性1：不支持条件分支

**从第一性原理分析**：
- **RunnableSequence 的本质**：线性函数组合
- **条件分支的本质**：根据中间结果决定下一步
- **冲突**：线性组合无法表达条件分支

**解决方案**：
- 使用 `RunnableBranch`（条件路由）
- 使用 `LangGraph`（状态机）

```python
# RunnableSequence 不支持条件分支
chain = step1 | step2 | step3  # 总是执行 step2 和 step3

# 使用 RunnableBranch
from langchain_core.runnables import RunnableBranch

chain = step1 | RunnableBranch(
    (condition1, step2a),
    (condition2, step2b),
    step2_default
) | step3
```

### 局限性2：不支持循环

**从第一性原理分析**：
- **RunnableSequence 的本质**：单向数据流
- **循环的本质**：数据流回到之前的步骤
- **冲突**：单向流无法表达循环

**解决方案**：
- 使用 `LangGraph`（支持循环边）

```python
# RunnableSequence 不支持循环
chain = step1 | step2 | step3  # 只执行一次

# 使用 LangGraph
from langgraph.graph import StateGraph

workflow = StateGraph(State)
workflow.add_node("step1", step1)
workflow.add_node("step2", step2)
workflow.add_edge("step1", "step2")
workflow.add_edge("step2", "step1")  # 循环边
```

### 局限性3：状态管理困难

**从第一性原理分析**：
- **RunnableSequence 的本质**：无状态函数组合
- **状态管理的本质**：在多次调用间保持状态
- **冲突**：无状态设计无法保持状态

**解决方案**：
- 使用外部状态存储（数据库、Redis）
- 使用 `LangGraph`（内置状态管理）

```python
# RunnableSequence 需要手动管理状态
state = {}  # 外部状态
chain = step1 | step2 | step3
result = chain.invoke({"input": input, "state": state})

# LangGraph 内置状态管理
workflow = StateGraph(State)
# 状态自动在节点间传递
```

---

## 从第一性原理理解何时使用

### 使用 RunnableSequence 的场景

**从第一性原理判断**：
- ✅ 步骤有明确的顺序依赖
- ✅ 不需要条件分支
- ✅ 不需要循环
- ✅ 不需要复杂的状态管理

**典型场景**：
1. **RAG 问答**：检索 → 注入 → 生成 → 解析
2. **文本处理**：加载 → 清洗 → 分析 → 输出
3. **简单对话**：历史 → prompt → LLM → 解析
4. **数据转换**：输入 → 转换1 → 转换2 → 输出

### 不使用 RunnableSequence 的场景

**从第一性原理判断**：
- ❌ 需要根据中间结果决定下一步
- ❌ 需要多次迭代直到满足条件
- ❌ 需要复杂的状态管理
- ❌ 需要多代理协作

**典型场景**：
1. **多步推理**：需要根据推理结果决定是否继续
2. **自主代理**：需要循环执行直到完成任务
3. **复杂对话**：需要管理多轮对话状态
4. **多代理系统**：需要代理间通信和协作

**解决方案**：使用 `LangGraph`

---

## 学习检查清单

完成本节学习后，你应该能够：

### 理解层面
- [ ] 理解 RunnableSequence 的本质是函数组合器
- [ ] 理解为什么需要 RunnableSequence（5个痛点）
- [ ] 理解 RunnableSequence 的三层价值
- [ ] 理解核心设计决策的原因
- [ ] 理解 RunnableSequence 的局限性

### 应用层面
- [ ] 能从第一性原理推导 RAG 应用
- [ ] 能判断何时使用 RunnableSequence
- [ ] 能判断何时不使用 RunnableSequence
- [ ] 能解释设计决策给他人听

### 思维层面
- [ ] 能用第一性原理思考其他技术
- [ ] 能从需求出发设计系统
- [ ] 能识别不必要的抽象

---

## 下一步学习

### 如果你想深入理解实现

阅读：
- `03_核心概念_02_管道操作符实现.md` - `|` 的实现原理
- `03_核心概念_03_数据流转机制.md` - 数据如何流转

### 如果你想看更多实战案例

阅读：
- `07_实战代码_01_基础序列链构建.md` - RAG 问答链
- `07_实战代码_02_复杂数据流转.md` - 多步骤处理

### 如果你想理解局限性

阅读：
- `RunnableBranch条件路由`（L1-07）- 条件分支
- `LangGraph与状态管理`（L7）- 复杂工作流

---

**版本**: v1.0
**最后更新**: 2026-02-18
**参考来源**:
- LangChain 官方文档
- 函数式编程原理
- Unix 哲学
