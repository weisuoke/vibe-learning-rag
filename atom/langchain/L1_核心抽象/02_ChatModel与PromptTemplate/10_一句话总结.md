# 一句话总结

**ChatModel 是基于消息列表的模型抽象，提供统一的 Runnable 接口（invoke/batch/stream）和角色感知能力；PromptTemplate 是可复用的模板系统，支持变量替换、类型验证和 LCEL 组合；两者共同构成了 LangChain 中构建 AI Agent 对话能力的核心基础设施，在 2025-2026 年已成为生产级应用的标准选择。**

---

## 核心要点回顾

### ChatModel 的本质
- **输入**: 消息列表（SystemMessage/HumanMessage/AIMessage）
- **输出**: AIMessage 对象
- **特性**: 角色感知、统一接口、可组合
- **价值**: 屏蔽不同 LLM 的差异，提供一致的开发体验

### PromptTemplate 的本质
- **输入**: 变量字典
- **输出**: 消息列表
- **特性**: 可复用、类型安全、Runnable 协议
- **价值**: 提高提示词的可维护性和工程化水平

### 两者的关系
```
用户输入 → PromptTemplate → 消息列表 → ChatModel → AI响应
         (模板格式化)    (结构化)    (推理生成)   (结果)
```

---

## 关键记忆点

### 三个核心区别

1. **ChatModel vs LLM**
   - ChatModel: 消息列表 → 消息
   - LLM: 字符串 → 字符串
   - 推荐: 现代应用优先用 ChatModel

2. **PromptTemplate vs 字符串格式化**
   - PromptTemplate: Runnable，支持 LCEL
   - 字符串格式化: 简单替换
   - 推荐: 生产环境必须用 PromptTemplate

3. **invoke vs stream vs batch**
   - invoke: 等待完整响应
   - stream: 逐字输出，降低感知延迟
   - batch: 批量处理，降低成本
   - 推荐: 根据场景选择

### 三个必须记住

1. **SystemMessage 不可省略**（生产环境）
2. **模板注入是安全风险**（CVE-2025-65106）
3. **MessagesPlaceholder 是通用机制**（不只是历史）

### 三个 2025-2026 特性

1. **LangChain 1.0 稳定版**（2025年10月）
2. **MCP 协议集成**（统一模型调用）
3. **结构化输出原生支持**（with_structured_output）

---

## 学习成果检验

完成本知识点学习后，你应该能够：

**理解层面**
- ✅ 解释 ChatModel 和 LLM 的本质区别
- ✅ 说明为什么需要不同的消息类型
- ✅ 理解 PromptTemplate 的 Runnable 特性
- ✅ 认识模板注入的安全风险

**应用层面**
- ✅ 构建基础的对话应用
- ✅ 使用模板管理提示词
- ✅ 实现多轮对话历史管理
- ✅ 选择合适的调用方式（invoke/stream/batch）

**进阶层面**
- ✅ 回答面试中的相关问题
- ✅ 识别并避免常见误区
- ✅ 应用 2025-2026 的最佳实践
- ✅ 为 Agent 开发打下基础

---

## 下一步学习路径

### 横向扩展（同层级）
- **L1-03**: OutputParser与结构化输出
- **L1-04**: 链式组合（管道操作符）
- **L1-05**: RunnablePassthrough与RunnableLambda

### 纵向深入（下一层级）
- **L2**: LCEL表达式
- **L3**: 组件生态
- **L4**: Agent系统

### 实战应用
- 构建聊天机器人
- 实现 RAG 应用
- 开发 AI Agent

---

## 快速参考卡

### 最小可用代码

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

# 1. 创建模型
model = ChatOpenAI(model="gpt-4o-mini")

# 2. 创建模板
template = ChatPromptTemplate.from_messages([
    ("system", "你是{role}"),
    ("human", "{question}")
])

# 3. 组合链
chain = template | model

# 4. 调用
response = chain.invoke({
    "role": "Python专家",
    "question": "什么是装饰器？"
})

print(response.content)
```

### 常用模式速查

| 场景 | 代码模式 |
|------|----------|
| 简单问答 | `model.invoke([HumanMessage(...)])` |
| 使用模板 | `(template \| model).invoke({...})` |
| 流式输出 | `for chunk in chain.stream({...})` |
| 批量处理 | `chain.batch([{...}, {...}])` |
| 对话历史 | `MessagesPlaceholder(variable_name="history")` |
| 结构化输出 | `model.with_structured_output(Schema)` |

---

## 核心价值总结

### 为什么 ChatModel 重要？

1. **统一抽象**: 一套代码适配所有 LLM
2. **角色感知**: 模型理解消息的语义角色
3. **可组合性**: 与 LCEL 无缝集成
4. **生产就绪**: 2025-2026 已是行业标准

### 为什么 PromptTemplate 重要？

1. **可维护性**: 集中管理提示词
2. **可复用性**: 一次定义，多处使用
3. **类型安全**: 编译时发现错误
4. **工程化**: 支持版本控制和测试

### 为什么两者是基础设施？

1. **必经之路**: 所有 LangChain 应用都要用
2. **架构基石**: Agent、Chain、Graph 都依赖它们
3. **最佳实践**: 2025-2026 的生产标准
4. **生态支持**: 完整的工具链和社区

---

## 最后的建议

### 立即行动

1. **复制代码**: 把最小可用代码跑起来
2. **修改变量**: 尝试不同的 role 和 question
3. **对比方式**: 体验 invoke 和 stream 的差异
4. **阅读文档**: 查看 LangChain 官方文档

### 持续学习

1. **深入概念**: 阅读核心概念文件
2. **动手实践**: 完成实战代码练习
3. **准备面试**: 学习面试必问题目
4. **体系梳理**: 复习化骨绵掌卡片

### 避免陷阱

1. **不要混淆** ChatModel 和 LLM
2. **不要省略** SystemMessage（生产环境）
3. **不要忽视** 模板注入风险
4. **不要误解** Streaming 的价值

---

**恭喜你完成了 ChatModel 与 PromptTemplate 的学习！**

这是 LangChain 学习路径中最重要的基础知识点之一。掌握它们，你已经为构建 AI Agent 打下了坚实的基础。

**继续前进**: 下一个知识点是 `L1-03: OutputParser与结构化输出`

---

**版本信息**: 基于 LangChain 0.3.x (2025-2026标准)
**最后更新**: 2026-02-18
**维护者**: Claude Code
