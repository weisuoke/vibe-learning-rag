# 核心概念1：IVF系列索引参数（nlist, nprobe）

## 概念定义

**IVF（Inverted File Index）系列索引是基于聚类的向量索引算法，通过将向量空间划分成多个聚类（clusters），在搜索时只检索最相关的聚类，从而大幅减少搜索范围。**

核心参数：
- **nlist**：构建参数，决定将向量空间分成多少个聚类
- **nprobe**：搜索参数，决定搜索多少个聚类

---

## 算法原理

### 1. 构建阶段：K-Means 聚类

```python
"""
IVF 索引构建过程的简化实现
"""
import numpy as np
from sklearn.cluster import KMeans

def build_ivf_index(vectors, nlist):
    """
    构建 IVF 索引

    Args:
        vectors: 向量数据，shape=(N, dim)
        nlist: 聚类数量

    Returns:
        centroids: 聚类中心，shape=(nlist, dim)
        inverted_lists: 倒排列表，dict[cluster_id] = [vector_ids]
    """
    print(f"开始构建 IVF 索引，nlist={nlist}")

    # 步骤1：使用 K-Means 聚类
    kmeans = KMeans(n_clusters=nlist, random_state=42)
    cluster_labels = kmeans.fit_predict(vectors)
    centroids = kmeans.cluster_centers_

    # 步骤2：构建倒排列表
    inverted_lists = {}
    for cluster_id in range(nlist):
        # 找到属于该聚类的所有向量 ID
        vector_ids = np.where(cluster_labels == cluster_id)[0]
        inverted_lists[cluster_id] = vector_ids.tolist()

    # 统计信息
    avg_cluster_size = len(vectors) / nlist
    print(f"索引构建完成：")
    print(f"  - 聚类数量: {nlist}")
    print(f"  - 平均每个聚类: {avg_cluster_size:.1f} 个向量")

    return centroids, inverted_lists

# 示例：构建 IVF 索引
num_vectors = 10000
dim = 128
vectors = np.random.rand(num_vectors, dim).astype(np.float32)

centroids, inverted_lists = build_ivf_index(vectors, nlist=100)

# 输出:
# 开始构建 IVF 索引，nlist=100
# 索引构建完成：
#   - 聚类数量: 100
#   - 平均每个聚类: 100.0 个向量
```

### 2. 搜索阶段：nprobe 控制搜索范围

```python
def search_ivf_index(query_vector, centroids, inverted_lists, vectors, nprobe, top_k=10):
    """
    在 IVF 索引中搜索

    Args:
        query_vector: 查询向量，shape=(dim,)
        centroids: 聚类中心
        inverted_lists: 倒排列表
        vectors: 原始向量数据
        nprobe: 搜索的聚类数量
        top_k: 返回的结果数量

    Returns:
        top_k_ids: Top-K 向量的 ID
        top_k_distances: Top-K 向量的距离
    """
    # 步骤1：计算查询向量到所有聚类中心的距离
    distances_to_centroids = np.linalg.norm(
        centroids - query_vector, axis=1
    )

    # 步骤2：选择最近的 nprobe 个聚类
    nearest_cluster_ids = np.argsort(distances_to_centroids)[:nprobe]

    print(f"搜索 {nprobe} 个最近的聚类: {nearest_cluster_ids[:5]}...")

    # 步骤3：在选中的聚类中搜索
    candidate_ids = []
    for cluster_id in nearest_cluster_ids:
        candidate_ids.extend(inverted_lists[cluster_id])

    print(f"候选向量数量: {len(candidate_ids)} (总共 {len(vectors)} 个向量)")

    # 步骤4：计算查询向量到候选向量的距离
    candidate_vectors = vectors[candidate_ids]
    distances = np.linalg.norm(
        candidate_vectors - query_vector, axis=1
    )

    # 步骤5：返回 Top-K 结果
    top_k_indices = np.argsort(distances)[:top_k]
    top_k_ids = [candidate_ids[i] for i in top_k_indices]
    top_k_distances = distances[top_k_indices]

    return top_k_ids, top_k_distances

# 示例：搜索
query_vector = np.random.rand(dim).astype(np.float32)

# 测试不同的 nprobe 值
for nprobe in [1, 5, 10, 20]:
    print(f"\n--- nprobe={nprobe} ---")
    top_k_ids, top_k_distances = search_ivf_index(
        query_vector, centroids, inverted_lists, vectors, nprobe, top_k=10
    )
    print(f"Top-1 距离: {top_k_distances[0]:.4f}")

# 输出:
# --- nprobe=1 ---
# 搜索 1 个最近的聚类: [42]...
# 候选向量数量: 98 (总共 10000 个向量)
# Top-1 距离: 3.2145
#
# --- nprobe=5 ---
# 搜索 5 个最近的聚类: [42 17 89 33 56]...
# 候选向量数量: 512 (总共 10000 个向量)
# Top-1 距离: 2.8934
#
# --- nprobe=10 ---
# 搜索 10 个最近的聚类: [42 17 89 33 56]...
# 候选向量数量: 1024 (总共 10000 个向量)
# Top-1 距离: 2.7821
```

---

## 参数详解

### nlist：聚类数量（构建参数）

**定义：** 将向量空间分成多少个聚类

**影响：**
- **nlist 越大**：
  - ✅ 每个聚类越小，聚类内向量越相似
  - ✅ 召回率越高（理论上）
  - ❌ 构建时间越长（K-Means 复杂度 O(N * nlist * iterations)）
  - ❌ 聚类中心越多，搜索聚类中心的时间越长

- **nlist 越小**：
  - ✅ 构建时间短
  - ✅ 搜索聚类中心快
  - ❌ 每个聚类太大，召回率低
  - ❌ 失去聚类意义

**经验公式：**

```python
import math

def calculate_optimal_nlist(num_vectors):
    """
    根据向量数量计算最优 nlist

    经验公式：nlist = sqrt(N) ~ 4*sqrt(N)
    推荐值：nlist = 2*sqrt(N)
    """
    nlist_min = int(math.sqrt(num_vectors))
    nlist_max = int(4 * math.sqrt(num_vectors))
    nlist_recommended = int(2 * math.sqrt(num_vectors))

    return {
        "min": nlist_min,
        "recommended": nlist_recommended,
        "max": nlist_max
    }

# 示例：不同数据规模的 nlist 推荐值
data_scales = [10_000, 100_000, 1_000_000, 10_000_000]

print("数据规模与 nlist 推荐值：")
print("-" * 60)
print(f"{'数据规模':<15} {'nlist_min':<12} {'推荐值':<12} {'nlist_max':<12}")
print("-" * 60)

for num_vectors in data_scales:
    result = calculate_optimal_nlist(num_vectors)
    print(f"{num_vectors:<15,} {result['min']:<12} {result['recommended']:<12} {result['max']:<12}")

# 输出:
# 数据规模与 nlist 推荐值：
# ------------------------------------------------------------
# 数据规模          nlist_min    推荐值        nlist_max
# ------------------------------------------------------------
# 10,000          100          200          400
# 100,000         316          632          1264
# 1,000,000       1000         2000         4000
# 10,000,000      3162         6324         12648
```

### nprobe：搜索聚类数量（搜索参数）

**定义：** 搜索时检索多少个最近的聚类

**影响：**
- **nprobe 越大**：
  - ✅ 召回率越高（搜索更多聚类，更可能找到相关向量）
  - ❌ 搜索时间越长（线性关系：nprobe 翻倍，时间翻倍）

- **nprobe 越小**：
  - ✅ 搜索速度快
  - ❌ 召回率低（可能漏掉相关向量）

**经验公式：**

```python
def calculate_optimal_nprobe(nlist, target_speed="balanced"):
    """
    根据 nlist 和目标速度计算最优 nprobe

    Args:
        nlist: 聚类数量
        target_speed: "fast" | "balanced" | "accurate"

    Returns:
        推荐的 nprobe 值
    """
    if target_speed == "fast":
        # 快速模式：搜索 1% 的聚类
        nprobe = max(1, nlist // 100)
    elif target_speed == "balanced":
        # 平衡模式：搜索 5% 的聚类
        nprobe = max(8, nlist // 20)
    else:  # accurate
        # 精确模式：搜索 10% 的聚类
        nprobe = max(16, nlist // 10)

    return nprobe

# 示例：不同场景的 nprobe 推荐值
nlist = 2000

print("不同场景的 nprobe 推荐值（nlist=2000）：")
print("-" * 60)

for speed in ["fast", "balanced", "accurate"]:
    nprobe = calculate_optimal_nprobe(nlist, speed)
    percentage = (nprobe / nlist) * 100
    print(f"{speed:<12}: nprobe={nprobe:<4} ({percentage:.1f}% 的聚类)")

# 输出:
# 不同场景的 nprobe 推荐值（nlist=2000）：
# ------------------------------------------------------------
# fast        : nprobe=20   (1.0% 的聚类)
# balanced    : nprobe=100  (5.0% 的聚类)
# accurate    : nprobe=200  (10.0% 的聚类)
```

---

## 在 Milvus 中的实际应用

### 1. 创建 IVF_FLAT 索引

```python
from pymilvus import Collection, connections, FieldSchema, CollectionSchema, DataType

# 连接到 Milvus
connections.connect("default", host="localhost", port="19530")

# 创建 collection
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
]
schema = CollectionSchema(fields, description="IVF index demo")
collection = Collection("ivf_demo", schema)

# 插入数据
import numpy as np
num_vectors = 100_000
vectors = np.random.rand(num_vectors, 768).astype(np.float32).tolist()
collection.insert([vectors])

# 计算 nlist
import math
nlist = int(2 * math.sqrt(num_vectors))  # 632

# 创建 IVF_FLAT 索引
index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "L2",
    "params": {
        "nlist": nlist  # 构建参数
    }
}

print(f"创建 IVF_FLAT 索引，nlist={nlist}")
collection.create_index("embedding", index_params)
collection.load()

print("索引创建完成")
```

### 2. 调整 nprobe 进行搜索

```python
import time

# 准备查询向量
query_vector = np.random.rand(768).astype(np.float32).tolist()

# 测试不同的 nprobe 值
nprobe_values = [8, 16, 32, 64, 128]

print("\n测试不同 nprobe 值的性能：")
print("-" * 70)
print(f"{'nprobe':<10} {'延迟(ms)':<15} {'搜索范围':<20}")
print("-" * 70)

for nprobe in nprobe_values:
    search_params = {
        "metric_type": "L2",
        "params": {"nprobe": nprobe}
    }

    # 测量延迟
    start = time.time()
    results = collection.search(
        data=[query_vector],
        anns_field="embedding",
        param=search_params,
        limit=10
    )
    latency_ms = (time.time() - start) * 1000

    # 计算搜索范围
    search_percentage = (nprobe / nlist) * 100
    avg_vectors_searched = (num_vectors / nlist) * nprobe

    print(f"{nprobe:<10} {latency_ms:<15.2f} {avg_vectors_searched:.0f} 个向量 ({search_percentage:.1f}%)")

# 输出:
# 测试不同 nprobe 值的性能：
# ----------------------------------------------------------------------
# nprobe     延迟(ms)         搜索范围
# ----------------------------------------------------------------------
# 8          12.50           1266 个向量 (1.3%)
# 16         22.30           2532 个向量 (2.5%)
# 32         42.80           5063 个向量 (5.1%)
# 64         82.50           10127 个向量 (10.1%)
# 128        158.20          20253 个向量 (20.3%)
```

### 3. 根据业务场景选择参数

```python
def recommend_ivf_params(num_vectors, scenario):
    """
    根据业务场景推荐 IVF 参数

    Args:
        num_vectors: 向量数量
        scenario: "realtime" | "offline" | "recommendation"

    Returns:
        推荐的 nlist 和 nprobe
    """
    import math

    # 计算 nlist
    nlist = int(2 * math.sqrt(num_vectors))

    # 根据场景选择 nprobe
    if scenario == "realtime":
        # 实时对话：优先速度
        nprobe = max(8, nlist // 50)  # 2% 的聚类
        target_latency = "< 50ms"
        target_recall = "> 85%"
    elif scenario == "offline":
        # 离线分析：优先召回率
        nprobe = max(32, nlist // 10)  # 10% 的聚类
        target_latency = "< 500ms"
        target_recall = "> 95%"
    else:  # recommendation
        # 推荐系统：平衡
        nprobe = max(16, nlist // 20)  # 5% 的聚类
        target_latency = "< 100ms"
        target_recall = "> 90%"

    return {
        "nlist": nlist,
        "nprobe": nprobe,
        "target_latency": target_latency,
        "target_recall": target_recall
    }

# 示例：不同场景的参数推荐
scenarios = ["realtime", "offline", "recommendation"]
num_vectors = 1_000_000

print("不同场景的 IVF 参数推荐（100万向量）：")
print("-" * 80)

for scenario in scenarios:
    params = recommend_ivf_params(num_vectors, scenario)
    print(f"\n{scenario.upper()} 场景：")
    print(f"  nlist:  {params['nlist']}")
    print(f"  nprobe: {params['nprobe']}")
    print(f"  目标延迟: {params['target_latency']}")
    print(f"  目标召回率: {params['target_recall']}")

# 输出:
# 不同场景的 IVF 参数推荐（100万向量）：
# --------------------------------------------------------------------------------
#
# REALTIME 场景：
#   nlist:  2000
#   nprobe: 40
#   目标延迟: < 50ms
#   目标召回率: > 85%
#
# OFFLINE 场景：
#   nlist:  2000
#   nprobe: 200
#   目标延迟: < 500ms
#   目标召回率: > 95%
#
# RECOMMENDATION 场景：
#   nlist:  2000
#   nprobe: 100
#   目标延迟: < 100ms
#   目标召回率: > 90%
```

---

## 在 RAG 系统中的应用

### 场景：企业知识库问答系统

```python
"""
企业知识库 RAG 系统的 IVF 参数配置
"""

from pymilvus import Collection, connections
import numpy as np

# 系统配置
SYSTEM_CONFIG = {
    "num_documents": 50_000,      # 5万文档
    "chunks_per_doc": 5,          # 每个文档5个chunk
    "total_vectors": 250_000,     # 总共25万向量
    "embedding_dim": 768,
    "target_latency_ms": 200,     # 目标延迟 < 200ms
    "target_recall": 0.90         # 目标召回率 > 90%
}

# 步骤1：计算 IVF 参数
import math

num_vectors = SYSTEM_CONFIG["total_vectors"]
nlist = int(2 * math.sqrt(num_vectors))  # 1000

print(f"企业知识库 RAG 系统配置：")
print(f"  文档数量: {SYSTEM_CONFIG['num_documents']:,}")
print(f"  向量数量: {SYSTEM_CONFIG['total_vectors']:,}")
print(f"  推荐 nlist: {nlist}")

# 步骤2：创建 collection 和索引
connections.connect("default", host="localhost", port="19530")

# ... 创建 collection ...

index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "L2",
    "params": {"nlist": nlist}
}

# collection.create_index("embedding", index_params)
# collection.load()

# 步骤3：根据查询类型动态调整 nprobe
def get_search_params(query_type):
    """
    根据查询类型返回搜索参数

    Args:
        query_type: "quick" | "standard" | "thorough"
    """
    if query_type == "quick":
        # 快速查询：用户输入时的实时建议
        nprobe = 16
        expected_latency = "50-80ms"
    elif query_type == "standard":
        # 标准查询：正常的问答
        nprobe = 50
        expected_latency = "100-150ms"
    else:  # thorough
        # 深度查询：需要高召回率的场景
        nprobe = 100
        expected_latency = "180-220ms"

    return {
        "metric_type": "L2",
        "params": {"nprobe": nprobe}
    }, expected_latency

# 步骤4：实际查询示例
def rag_search(query_text, query_type="standard"):
    """
    RAG 系统的检索函数
    """
    # 1. 将查询文本转换为向量（使用 embedding 模型）
    # query_vector = embedding_model.encode(query_text)
    query_vector = np.random.rand(768).astype(np.float32).tolist()  # 示例

    # 2. 获取搜索参数
    search_params, expected_latency = get_search_params(query_type)

    print(f"\n查询类型: {query_type}")
    print(f"  nprobe: {search_params['params']['nprobe']}")
    print(f"  预期延迟: {expected_latency}")

    # 3. 执行向量检索
    # results = collection.search(
    #     data=[query_vector],
    #     anns_field="embedding",
    #     param=search_params,
    #     limit=5  # 检索 Top-5 相关文档
    # )

    # 4. 返回相关文档
    # return results

    print(f"  检索完成，返回 Top-5 相关文档")

# 示例：不同类型的查询
rag_search("什么是机器学习？", query_type="quick")
rag_search("详细解释深度学习的原理", query_type="standard")
rag_search("对比分析所有神经网络架构", query_type="thorough")

# 输出:
# 查询类型: quick
#   nprobe: 16
#   预期延迟: 50-80ms
#   检索完成，返回 Top-5 相关文档
#
# 查询类型: standard
#   nprobe: 50
#   预期延迟: 100-150ms
#   检索完成，返回 Top-5 相关文档
#
# 查询类型: thorough
#   nprobe: 100
#   预期延迟: 180-220ms
#   检索完成，返回 Top-5 相关文档
```

---

## 性能优化技巧

### 1. 动态调整 nprobe

```python
class AdaptiveNprobeManager:
    """
    根据系统负载动态调整 nprobe
    """

    def __init__(self, nlist, base_nprobe=50):
        self.nlist = nlist
        self.base_nprobe = base_nprobe
        self.current_load = 0.0  # 0.0 - 1.0

    def update_load(self, current_qps, max_qps):
        """更新系统负载"""
        self.current_load = current_qps / max_qps

    def get_adaptive_nprobe(self):
        """
        根据负载动态调整 nprobe

        负载高时降低 nprobe，负载低时提高 nprobe
        """
        if self.current_load > 0.8:
            # 高负载：降低 nprobe
            nprobe = max(8, self.base_nprobe // 2)
        elif self.current_load < 0.3:
            # 低负载：提高 nprobe
            nprobe = min(self.nlist // 10, self.base_nprobe * 2)
        else:
            # 正常负载：使用基准值
            nprobe = self.base_nprobe

        return nprobe

# 使用示例
manager = AdaptiveNprobeManager(nlist=2000, base_nprobe=50)

# 模拟不同负载
loads = [
    (100, 1000, "低负载"),
    (500, 1000, "正常负载"),
    (900, 1000, "高负载")
]

print("动态 nprobe 调整：")
for current_qps, max_qps, desc in loads:
    manager.update_load(current_qps, max_qps)
    nprobe = manager.get_adaptive_nprobe()
    print(f"{desc} (QPS: {current_qps}/{max_qps}): nprobe={nprobe}")

# 输出:
# 动态 nprobe 调整：
# 低负载 (QPS: 100/1000): nprobe=100
# 正常负载 (QPS: 500/1000): nprobe=50
# 高负载 (QPS: 900/1000): nprobe=25
```

### 2. 批量查询优化

```python
def batch_search_with_optimal_nprobe(collection, query_vectors, batch_size=100):
    """
    批量查询时使用更小的 nprobe

    原理：批量查询可以分摊固定开销，使用更小的 nprobe 仍能保持高吞吐量
    """
    num_queries = len(query_vectors)

    # 批量查询使用更小的 nprobe
    if num_queries >= batch_size:
        nprobe = 16  # 批量查询
    else:
        nprobe = 50  # 单次查询

    search_params = {
        "metric_type": "L2",
        "params": {"nprobe": nprobe}
    }

    print(f"批量查询 {num_queries} 个向量，nprobe={nprobe}")

    results = collection.search(
        data=query_vectors,
        anns_field="embedding",
        param=search_params,
        limit=10
    )

    return results
```

---

## 总结

**IVF 系列索引的核心思想：**
1. **构建阶段**：用 K-Means 将向量空间分成 nlist 个聚类
2. **搜索阶段**：只搜索最近的 nprobe 个聚类，大幅减少搜索范围
3. **参数权衡**：nlist 和 nprobe 共同决定速度和召回率的平衡

**关键要点：**
- nlist = 2*sqrt(N) 是最常用的配置
- nprobe = nlist * (1%-10%) 根据场景调整
- nprobe 可以动态调整，无需重建索引
- 不同业务场景使用不同的 nprobe 配置

**在 RAG 中的应用：**
- 实时对话：nprobe 小（16-32），优先速度
- 标准查询：nprobe 中（50-64），平衡配置
- 深度查询：nprobe 大（100-128），优先召回率
