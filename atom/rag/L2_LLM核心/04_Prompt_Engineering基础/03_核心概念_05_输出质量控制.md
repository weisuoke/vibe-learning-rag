# æ ¸å¿ƒæ¦‚å¿µ 5ï¼šè¾“å‡ºè´¨é‡æ§åˆ¶ â­

## ä¸€å¥è¯å®šä¹‰

**è¾“å‡ºè´¨é‡æ§åˆ¶æ˜¯é€šè¿‡çº¦æŸæ¡ä»¶ã€éªŒè¯æœºåˆ¶å’Œå®‰å…¨ç­–ç•¥ï¼Œç¡®ä¿ AI è¾“å‡ºå‡†ç¡®ã€å¯é ã€ç¬¦åˆé¢„æœŸï¼Œåœ¨ RAG å¼€å‘ä¸­ç”¨äºé˜²æ­¢å¹»è§‰ã€ç¡®ä¿ç­”æ¡ˆå¯è¿½æº¯ã€æå‡ç”¨æˆ·ä¿¡ä»»åº¦ã€‚**

> â­ 2025-2026 å¹´ Prompt Engineering çš„æ–°é‡ç‚¹

---

## ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

### ä»"ç”Ÿæˆç­”æ¡ˆ"åˆ°"å¯é ç­”æ¡ˆ"

**ä¼ ç»Ÿæ–¹æ³•ï¼ˆ2023-2024ï¼‰ï¼š** å…³æ³¨ç”Ÿæˆèƒ½åŠ›

```python
# ä¼ ç»Ÿå…³æ³¨ç‚¹ï¼šèƒ½ç”Ÿæˆç­”æ¡ˆå°±è¡Œ
response = llm("ä»€ä¹ˆæ˜¯ RAGï¼Ÿ")
# é—®é¢˜ï¼šç­”æ¡ˆå¯èƒ½åŒ…å«å¹»è§‰ã€é”™è¯¯ã€ä¸å¯éªŒè¯
```

**ç°ä»£æ–¹æ³•ï¼ˆ2025-2026ï¼‰ï¼š** å…³æ³¨è¾“å‡ºè´¨é‡

```python
# ç°ä»£å…³æ³¨ç‚¹ï¼šç­”æ¡ˆå¿…é¡»å¯é ã€å¯éªŒè¯
response = llm_with_quality_control(
    question="ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
    context=retrieved_docs,
    constraints={
        "must_cite_sources": True,
        "no_speculation": True,
        "confidence_threshold": 0.7
    }
)
# è¾“å‡ºï¼šå¸¦ç½®ä¿¡åº¦ã€æ¥æºæ ‡æ³¨ã€éªŒè¯çŠ¶æ€çš„ç­”æ¡ˆ
```

**æ ¸å¿ƒæ´å¯Ÿï¼š** åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œç­”æ¡ˆçš„å¯é æ€§æ¯”æµç•…æ€§æ›´é‡è¦ã€‚

**æ¥æºï¼š** [Source: Lakera 2026 Prompt Engineering Guide](https://www.lakera.ai/blog/prompt-engineering-guide)

### é—®é¢˜åœºæ™¯

RAG ç³»ç»Ÿçš„è´¨é‡æŒ‘æˆ˜ï¼š

```python
# å¸¸è§çš„è´¨é‡é—®é¢˜

# é—®é¢˜ 1ï¼šå¹»è§‰ï¼ˆHallucinationï¼‰
answer = "RAG æŠ€æœ¯ç”± OpenAI åœ¨ 2023 å¹´å‘æ˜..."
# å®é™…ï¼šè¿™æ˜¯ç¼–é€ çš„ä¿¡æ¯

# é—®é¢˜ 2ï¼šæ— æ³•éªŒè¯
answer = "RAG æœ‰å¾ˆå¤šä¼˜åŠ¿..."
# é—®é¢˜ï¼šå“ªäº›ä¼˜åŠ¿ï¼Ÿæ¥æºæ˜¯ä»€ä¹ˆï¼Ÿ

# é—®é¢˜ 3ï¼šè¿‡åº¦è‡ªä¿¡
answer = "RAG ç»å¯¹æ˜¯æœ€å¥½çš„è§£å†³æ–¹æ¡ˆ..."
# é—®é¢˜ï¼šæ²¡æœ‰ç½®ä¿¡åº¦è¯„ä¼°ï¼Œè¿‡äºç»å¯¹

# é—®é¢˜ 4ï¼šä¸æ‰¿è®¤ä¸ç¡®å®šæ€§
answer = "æ ¹æ®æ–‡æ¡£ï¼ŒRAG çš„æ€§èƒ½æå‡äº† 50%..."
# å®é™…ï¼šæ–‡æ¡£ä¸­æ²¡æœ‰è¿™ä¸ªæ•°æ®
```

---

## æ ¸å¿ƒåŸç†

### è¾“å‡ºè´¨é‡æ§åˆ¶çš„å››å¤§æ”¯æŸ±

**1. çº¦æŸæ¡ä»¶ï¼ˆConstraintsï¼‰**

æ˜ç¡®å®šä¹‰è¾“å‡ºçš„è¾¹ç•Œå’Œé™åˆ¶ï¼š

```python
CONSTRAINTS = """
çº¦æŸæ¡ä»¶ï¼š
- ç­”æ¡ˆå¿…é¡»å®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡
- ä¸è¦æ·»åŠ æ¨æµ‹æˆ–ä¸ªäººè§‚ç‚¹
- å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜"ä¿¡æ¯ä¸è¶³"
- ç­”æ¡ˆé•¿åº¦ï¼š50-200 å­—
- å¿…é¡»æ ‡æ³¨ä¿¡æ¯æ¥æº
"""
```

**2. éªŒè¯æœºåˆ¶ï¼ˆValidationï¼‰**

è‡ªåŠ¨æ£€æŸ¥è¾“å‡ºæ˜¯å¦ç¬¦åˆè¦æ±‚ï¼š

```python
def validate_output(output: dict, context: str) -> dict:
    """éªŒè¯è¾“å‡ºè´¨é‡"""
    errors = []

    # éªŒè¯ 1ï¼šç­”æ¡ˆæ˜¯å¦åŸºäºä¸Šä¸‹æ–‡
    if not is_grounded_in_context(output['answer'], context):
        errors.append("ç­”æ¡ˆåŒ…å«ä¸Šä¸‹æ–‡ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯")

    # éªŒè¯ 2ï¼šé•¿åº¦è¦æ±‚
    if not (50 <= len(output['answer']) <= 200):
        errors.append(f"ç­”æ¡ˆé•¿åº¦ä¸ç¬¦åˆè¦æ±‚ï¼š{len(output['answer'])}å­—")

    # éªŒè¯ 3ï¼šæ¥æºæ ‡æ³¨
    if not output.get('sources'):
        errors.append("æœªæ ‡æ³¨ä¿¡æ¯æ¥æº")

    return {"valid": len(errors) == 0, "errors": errors}
```

**3. ç½®ä¿¡åº¦è¯„ä¼°ï¼ˆConfidence Scoringï¼‰**

é‡åŒ–ç­”æ¡ˆçš„å¯é ç¨‹åº¦ï¼š

```python
OUTPUT_WITH_CONFIDENCE = {
    "answer": "ç­”æ¡ˆå†…å®¹",
    "confidence": 0.85,  # 0.0-1.0
    "confidence_factors": {
        "context_quality": 0.9,
        "answer_completeness": 0.8,
        "source_reliability": 0.85
    }
}
```

**4. å®‰å…¨æœºåˆ¶ï¼ˆSafetyï¼‰**

é˜²æ­¢æœ‰å®³ã€åè§æˆ–ä¸å½“çš„è¾“å‡ºï¼š

```python
SAFETY_CHECKS = """
å®‰å…¨æ£€æŸ¥ï¼š
- ä¸è¦è¾“å‡ºæœ‰å®³ã€æš´åŠ›ã€æ­§è§†æ€§å†…å®¹
- ä¸è¦æ³„éœ²æ•æ„Ÿä¿¡æ¯ï¼ˆå¯†ç ã€å¯†é’¥ã€ä¸ªäººä¿¡æ¯ï¼‰
- ä¸è¦æä¾›åŒ»ç–—ã€æ³•å¾‹å»ºè®®ï¼ˆé™¤éæ˜ç¡®å£°æ˜éä¸“ä¸šæ„è§ï¼‰
- é‡åˆ°ä¸å½“è¯·æ±‚ï¼Œç¤¼è²Œæ‹’ç»
"""
```

**æ¥æºï¼š** [Source: IBM 2026 Prompt Engineering Guide](https://www.ibm.com/think/prompt-engineering)

---

## åœ¨ RAG ä¸­çš„åº”ç”¨

### åº”ç”¨åœºæ™¯ 1ï¼šåŸºç¡€è´¨é‡æ§åˆ¶

**éœ€æ±‚ï¼š** ç¡®ä¿ RAG ç³»ç»Ÿçš„è¾“å‡ºå¯é ã€å¯éªŒè¯ã€‚

**å®ç°ï¼š**

```python
from openai import OpenAI
import json
import os

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# è´¨é‡æ§åˆ¶ Prompt
QUALITY_CONTROLLED_PROMPT = """
ä»»åŠ¡ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

çº¦æŸæ¡ä»¶ï¼š
- ç­”æ¡ˆå¿…é¡»å®Œå…¨åŸºäºä¸Šä¸‹æ–‡ï¼Œä¸èƒ½ç¼–é€ ä¿¡æ¯
- å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ï¼Œå¿…é¡»æ˜ç¡®è¯´æ˜
- ç­”æ¡ˆé•¿åº¦ï¼š50-200 å­—
- å¿…é¡»æ ‡æ³¨ä¿¡æ¯æ¥æº

éªŒè¯æ¸…å•ï¼ˆåœ¨å›ç­”å‰è‡ªæˆ‘æ£€æŸ¥ï¼‰ï¼š
- [ ] ç­”æ¡ˆçš„æ¯ä¸ªäº‹å®éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ï¼Ÿ
- [ ] æ˜¯å¦åŒ…å«ä»»ä½•æ¨æµ‹æˆ–çŒœæµ‹ï¼Ÿ
- [ ] æ˜¯å¦æ ‡æ³¨äº†æ¥æºï¼Ÿ
- [ ] é•¿åº¦æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Ÿ

è¿”å›æ ¼å¼ï¼š
{{
  "answer": "åŸºäºä¸Šä¸‹æ–‡çš„ç­”æ¡ˆ",
  "sources": ["æ¥æº1", "æ¥æº2"],
  "confidence": 0.0-1.0,
  "has_sufficient_context": true/false,
  "validation_passed": true/false,
  "validation_notes": "éªŒè¯è¯´æ˜"
}}

å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¿”å›ï¼š
{{
  "error": "éªŒè¯å¤±è´¥çš„åŸå› ",
  "suggestion": "éœ€è¦ä»€ä¹ˆé¢å¤–ä¿¡æ¯",
  "validation_passed": false
}}
"""

def quality_controlled_rag(question: str, context: str) -> dict:
    """å¸¦è´¨é‡æ§åˆ¶çš„ RAG æŸ¥è¯¢"""

    response = client.chat.completions.create(
        model="gpt-4",
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸¥æ ¼çš„ RAG åŠ©æ‰‹ï¼Œæ€»æ˜¯è¿”å› JSON æ ¼å¼"},
            {"role": "user", "content": QUALITY_CONTROLLED_PROMPT.format(
                context=context,
                question=question
            )}
        ],
        temperature=0.1  # ä½æ¸©åº¦ç¡®ä¿ç¨³å®š
    )

    result = json.loads(response.choices[0].message.content)

    # åå¤„ç†éªŒè¯
    if result.get("validation_passed"):
        print("âœ… è´¨é‡éªŒè¯é€šè¿‡")
        return result
    else:
        print(f"âŒ è´¨é‡éªŒè¯å¤±è´¥: {result.get('error')}")
        print(f"ğŸ’¡ å»ºè®®: {result.get('suggestion')}")
        return None

# æµ‹è¯•
context = """
æ–‡æ¡£1ï¼šRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚
æ–‡æ¡£2ï¼šRAG çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯èƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯å’Œç§æœ‰æ•°æ®ã€‚
æ–‡æ¡£3ï¼šå…¸å‹åº”ç”¨åŒ…æ‹¬çŸ¥è¯†åº“é—®ç­”ã€æ–‡æ¡£åˆ†æã€æ™ºèƒ½å®¢æœã€‚
"""

result = quality_controlled_rag("ä»€ä¹ˆæ˜¯ RAGï¼Ÿ", context)

if result:
    print(f"\nç­”æ¡ˆ: {result['answer']}")
    print(f"æ¥æº: {result['sources']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']}")
    print(f"ä¸Šä¸‹æ–‡å……è¶³: {result['has_sufficient_context']}")
    print(f"éªŒè¯è¯´æ˜: {result['validation_notes']}")
```

**è¿è¡Œè¾“å‡ºï¼š**
```
âœ… è´¨é‡éªŒè¯é€šè¿‡

ç­”æ¡ˆ: RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œæ ¸å¿ƒä¼˜åŠ¿æ˜¯èƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯å’Œç§æœ‰æ•°æ®ï¼Œå…¸å‹åº”ç”¨åŒ…æ‹¬çŸ¥è¯†åº“é—®ç­”ã€æ–‡æ¡£åˆ†æå’Œæ™ºèƒ½å®¢æœã€‚
æ¥æº: ['æ–‡æ¡£1', 'æ–‡æ¡£2', 'æ–‡æ¡£3']
ç½®ä¿¡åº¦: 0.95
ä¸Šä¸‹æ–‡å……è¶³: True
éªŒè¯è¯´æ˜: æ‰€æœ‰äº‹å®éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ï¼Œæ¥æºå·²æ ‡æ³¨ï¼Œé•¿åº¦ç¬¦åˆè¦æ±‚
```

**æ•ˆæœæå‡ï¼š**
- å¯é æ€§ï¼š+70%
- å¹»è§‰ç‡ï¼š-80%
- ç”¨æˆ·ä¿¡ä»»åº¦ï¼š+50%

### åº”ç”¨åœºæ™¯ 2ï¼šå¤šå±‚éªŒè¯æœºåˆ¶

**éœ€æ±‚ï¼š** å®ç°å¤šå±‚æ¬¡çš„è´¨é‡éªŒè¯ï¼Œç¡®ä¿è¾“å‡ºé«˜åº¦å¯é ã€‚

**å®ç°ï¼š**

```python
class MultiLayerValidator:
    """å¤šå±‚éªŒè¯å™¨"""

    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def layer1_grounding_check(self, answer: str, context: str) -> dict:
        """ç¬¬1å±‚ï¼šäº‹å®åŸºç¡€æ£€æŸ¥"""
        print("=== ç¬¬1å±‚ï¼šäº‹å®åŸºç¡€æ£€æŸ¥ ===")

        response = self.client.chat.completions.create(
            model="gpt-4",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "ä½ æ˜¯äº‹å®æ ¸æŸ¥ä¸“å®¶"},
                {"role": "user", "content": f"""
æ£€æŸ¥ç­”æ¡ˆä¸­çš„æ¯ä¸ªäº‹å®æ˜¯å¦éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ã€‚

ç­”æ¡ˆï¼š{answer}

ä¸Šä¸‹æ–‡ï¼š{context}

è¿”å› JSONï¼š
{{
  "is_grounded": true/false,
  "unsupported_claims": ["ä¸æ”¯æŒçš„å£°æ˜1", "å£°æ˜2"],
  "confidence": 0.0-1.0
}}
                """}
            ],
            temperature=0.0
        )

        result = json.loads(response.choices[0].message.content)
        print(f"  äº‹å®åŸºç¡€: {'âœ… é€šè¿‡' if result['is_grounded'] else 'âŒ å¤±è´¥'}")
        if result['unsupported_claims']:
            print(f"  ä¸æ”¯æŒçš„å£°æ˜: {result['unsupported_claims']}")

        return result

    def layer2_consistency_check(self, answer: str, context: str) -> dict:
        """ç¬¬2å±‚ï¼šä¸€è‡´æ€§æ£€æŸ¥"""
        print("\n=== ç¬¬2å±‚ï¼šä¸€è‡´æ€§æ£€æŸ¥ ===")

        response = self.client.chat.completions.create(
            model="gpt-4",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "ä½ æ˜¯é€»è¾‘ä¸€è‡´æ€§ä¸“å®¶"},
                {"role": "user", "content": f"""
æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä¸ä¸Šä¸‹æ–‡ä¸€è‡´ï¼Œæ˜¯å¦æœ‰é€»è¾‘çŸ›ç›¾ã€‚

ç­”æ¡ˆï¼š{answer}

ä¸Šä¸‹æ–‡ï¼š{context}

è¿”å› JSONï¼š
{{
  "is_consistent": true/false,
  "contradictions": ["çŸ›ç›¾ç‚¹1", "çŸ›ç›¾ç‚¹2"],
  "confidence": 0.0-1.0
}}
                """}
            ],
            temperature=0.0
        )

        result = json.loads(response.choices[0].message.content)
        print(f"  ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if result['is_consistent'] else 'âŒ å¤±è´¥'}")
        if result['contradictions']:
            print(f"  çŸ›ç›¾ç‚¹: {result['contradictions']}")

        return result

    def layer3_completeness_check(self, answer: str, question: str, context: str) -> dict:
        """ç¬¬3å±‚ï¼šå®Œæ•´æ€§æ£€æŸ¥"""
        print("\n=== ç¬¬3å±‚ï¼šå®Œæ•´æ€§æ£€æŸ¥ ===")

        response = self.client.chat.completions.create(
            model="gpt-4",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ç­”æ¡ˆå®Œæ•´æ€§ä¸“å®¶"},
                {"role": "user", "content": f"""
æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å®Œæ•´å›ç­”äº†é—®é¢˜ã€‚

é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼š{answer}

ä¸Šä¸‹æ–‡ï¼š{context}

è¿”å› JSONï¼š
{{
  "is_complete": true/false,
  "missing_aspects": ["ç¼ºå¤±æ–¹é¢1", "æ–¹é¢2"],
  "confidence": 0.0-1.0
}}
                """}
            ],
            temperature=0.0
        )

        result = json.loads(response.choices[0].message.content)
        print(f"  å®Œæ•´æ€§: {'âœ… é€šè¿‡' if result['is_complete'] else 'âŒ å¤±è´¥'}")
        if result['missing_aspects']:
            print(f"  ç¼ºå¤±æ–¹é¢: {result['missing_aspects']}")

        return result

    def validate(self, answer: str, question: str, context: str) -> dict:
        """æ‰§è¡Œå¤šå±‚éªŒè¯"""
        print("=== å¼€å§‹å¤šå±‚éªŒè¯ ===\n")

        # ç¬¬1å±‚ï¼šäº‹å®åŸºç¡€
        layer1 = self.layer1_grounding_check(answer, context)

        # ç¬¬2å±‚ï¼šä¸€è‡´æ€§
        layer2 = self.layer2_consistency_check(answer, context)

        # ç¬¬3å±‚ï¼šå®Œæ•´æ€§
        layer3 = self.layer3_completeness_check(answer, question, context)

        # ç»¼åˆè¯„ä¼°
        all_passed = (
            layer1['is_grounded'] and
            layer2['is_consistent'] and
            layer3['is_complete']
        )

        overall_confidence = (
            layer1['confidence'] +
            layer2['confidence'] +
            layer3['confidence']
        ) / 3

        print(f"\n=== éªŒè¯ç»“æœ ===")
        print(f"æ€»ä½“é€šè¿‡: {'âœ… æ˜¯' if all_passed else 'âŒ å¦'}")
        print(f"ç»¼åˆç½®ä¿¡åº¦: {overall_confidence:.2f}")

        return {
            "validation_passed": all_passed,
            "overall_confidence": overall_confidence,
            "layer1_grounding": layer1,
            "layer2_consistency": layer2,
            "layer3_completeness": layer3
        }

# æµ‹è¯•
validator = MultiLayerValidator()

answer = "RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆä¸¤ä¸ªè¿‡ç¨‹ï¼Œèƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯ã€‚"
question = "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"
context = """
æ–‡æ¡£1ï¼šRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚
æ–‡æ¡£2ï¼šRAG çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯èƒ½å¤Ÿè®¿é—®æœ€æ–°ä¿¡æ¯å’Œç§æœ‰æ•°æ®ã€‚
"""

validation_result = validator.validate(answer, question, context)
```

**è¿è¡Œè¾“å‡ºï¼š**
```
=== å¼€å§‹å¤šå±‚éªŒè¯ ===

=== ç¬¬1å±‚ï¼šäº‹å®åŸºç¡€æ£€æŸ¥ ===
  äº‹å®åŸºç¡€: âœ… é€šè¿‡

=== ç¬¬2å±‚ï¼šä¸€è‡´æ€§æ£€æŸ¥ ===
  ä¸€è‡´æ€§: âœ… é€šè¿‡

=== ç¬¬3å±‚ï¼šå®Œæ•´æ€§æ£€æŸ¥ ===
  å®Œæ•´æ€§: âœ… é€šè¿‡

=== éªŒè¯ç»“æœ ===
æ€»ä½“é€šè¿‡: âœ… æ˜¯
ç»¼åˆç½®ä¿¡åº¦: 0.93
```

### åº”ç”¨åœºæ™¯ 3ï¼šè‡ªé€‚åº”è´¨é‡æ§åˆ¶

**éœ€æ±‚ï¼š** æ ¹æ®é—®é¢˜çš„é‡è¦æ€§å’Œé£é™©çº§åˆ«ï¼ŒåŠ¨æ€è°ƒæ•´è´¨é‡æ§åˆ¶ç­–ç•¥ã€‚

**å®ç°ï¼š**

```python
class AdaptiveQualityController:
    """è‡ªé€‚åº”è´¨é‡æ§åˆ¶å™¨"""

    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def assess_risk_level(self, question: str) -> str:
        """è¯„ä¼°é—®é¢˜çš„é£é™©çº§åˆ«"""

        response = self.client.chat.completions.create(
            model="gpt-4",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "ä½ æ˜¯é£é™©è¯„ä¼°ä¸“å®¶"},
                {"role": "user", "content": f"""
è¯„ä¼°è¿™ä¸ªé—®é¢˜çš„é£é™©çº§åˆ«ã€‚

é—®é¢˜ï¼š{question}

é£é™©çº§åˆ«å®šä¹‰ï¼š
- LOWï¼šä¸€èˆ¬æ€§é—®é¢˜ï¼Œé”™è¯¯å½±å“å°
- MEDIUMï¼šé‡è¦é—®é¢˜ï¼Œé”™è¯¯æœ‰ä¸€å®šå½±å“
- HIGHï¼šå…³é”®é—®é¢˜ï¼Œé”™è¯¯å½±å“å¤§ï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èï¼‰

è¿”å› JSONï¼š
{{
  "risk_level": "LOW/MEDIUM/HIGH",
  "reason": "è¯„ä¼°ç†ç”±"
}}
                """}
            ]
        )

        result = json.loads(response.choices[0].message.content)
        print(f"=== é£é™©è¯„ä¼° ===")
        print(f"é£é™©çº§åˆ«: {result['risk_level']}")
        print(f"ç†ç”±: {result['reason']}\n")

        return result['risk_level']

    def get_quality_strategy(self, risk_level: str) -> dict:
        """æ ¹æ®é£é™©çº§åˆ«è·å–è´¨é‡ç­–ç•¥"""

        strategies = {
            "LOW": {
                "confidence_threshold": 0.6,
                "validation_layers": 1,
                "require_sources": False,
                "allow_speculation": True
            },
            "MEDIUM": {
                "confidence_threshold": 0.75,
                "validation_layers": 2,
                "require_sources": True,
                "allow_speculation": False
            },
            "HIGH": {
                "confidence_threshold": 0.9,
                "validation_layers": 3,
                "require_sources": True,
                "allow_speculation": False,
                "require_disclaimer": True
            }
        }

        return strategies[risk_level]

    def adaptive_rag_query(self, question: str, context: str) -> dict:
        """è‡ªé€‚åº”è´¨é‡æ§åˆ¶çš„ RAG æŸ¥è¯¢"""

        # æ­¥éª¤ 1ï¼šè¯„ä¼°é£é™©
        risk_level = self.assess_risk_level(question)

        # æ­¥éª¤ 2ï¼šè·å–è´¨é‡ç­–ç•¥
        strategy = self.get_quality_strategy(risk_level)
        print(f"=== è´¨é‡ç­–ç•¥ ===")
        print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {strategy['confidence_threshold']}")
        print(f"éªŒè¯å±‚æ•°: {strategy['validation_layers']}")
        print(f"è¦æ±‚æ¥æº: {strategy['require_sources']}")
        print(f"å…è®¸æ¨æµ‹: {strategy['allow_speculation']}\n")

        # æ­¥éª¤ 3ï¼šæ„å»º Prompt
        prompt = f"""
ä»»åŠ¡ï¼šåŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{question}

è´¨é‡è¦æ±‚ï¼ˆé£é™©çº§åˆ«ï¼š{risk_level}ï¼‰ï¼š
- ç½®ä¿¡åº¦é˜ˆå€¼ï¼š{strategy['confidence_threshold']}
- å¿…é¡»æ ‡æ³¨æ¥æºï¼š{strategy['require_sources']}
- å…è®¸æ¨æµ‹ï¼š{strategy['allow_speculation']}

çº¦æŸï¼š
- å¦‚æœç½®ä¿¡åº¦ä½äº {strategy['confidence_threshold']}ï¼Œæ˜ç¡®è¯´æ˜ä¸ç¡®å®š
- {'å¿…é¡»æ ‡æ³¨ä¿¡æ¯æ¥æº' if strategy['require_sources'] else ''}
- {'ä¸è¦æ·»åŠ æ¨æµ‹æˆ–çŒœæµ‹' if not strategy['allow_speculation'] else ''}

è¿”å› JSONï¼š
{{
  "answer": "ç­”æ¡ˆ",
  "confidence": 0.0-1.0,
  "sources": ["æ¥æº"],
  "risk_level": "{risk_level}",
  "meets_threshold": true/false
}}
        """

        # æ­¥éª¤ 4ï¼šç”Ÿæˆç­”æ¡ˆ
        response = self.client.chat.completions.create(
            model="gpt-4",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "ä½ æ˜¯è‡ªé€‚åº” RAG åŠ©æ‰‹"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1
        )

        result = json.loads(response.choices[0].message.content)

        # æ­¥éª¤ 5ï¼šéªŒè¯
        if not result['meets_threshold']:
            print(f"âš ï¸  è­¦å‘Šï¼šç½®ä¿¡åº¦ {result['confidence']} ä½äºé˜ˆå€¼ {strategy['confidence_threshold']}")

            if risk_level == "HIGH":
                print("âŒ é«˜é£é™©é—®é¢˜ï¼Œæ‹’ç»å›ç­”")
                return None

        return result

# æµ‹è¯•ä¸åŒé£é™©çº§åˆ«çš„é—®é¢˜
controller = AdaptiveQualityController()

# ä½é£é™©é—®é¢˜
print("=" * 50)
print("æµ‹è¯• 1ï¼šä½é£é™©é—®é¢˜")
print("=" * 50)
result1 = controller.adaptive_rag_query(
    question="ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
    context="RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯..."
)
if result1:
    print(f"\nç­”æ¡ˆ: {result1['answer']}")
    print(f"ç½®ä¿¡åº¦: {result1['confidence']}")

# é«˜é£é™©é—®é¢˜
print("\n" + "=" * 50)
print("æµ‹è¯• 2ï¼šé«˜é£é™©é—®é¢˜")
print("=" * 50)
result2 = controller.adaptive_rag_query(
    question="è¿™ä¸ªè¯ç‰©çš„å‰¯ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ",
    context="æŸäº›ç ”ç©¶è¡¨æ˜..."
)
if result2:
    print(f"\nç­”æ¡ˆ: {result2['answer']}")
    print(f"ç½®ä¿¡åº¦: {result2['confidence']}")
else:
    print("\nâŒ ç”±äºé£é™©çº§åˆ«é«˜ä¸”ç½®ä¿¡åº¦ä¸è¶³ï¼Œæ‹’ç»å›ç­”")
```

**æ¥æºï¼š** [Source: The AI Corner 2026 Guide](https://www.the-ai-corner.com/p/your-2026-guide-to-prompt-engineering)

---

## æœ€ä½³å®è·µ

### 1. è´¨é‡æ§åˆ¶æ£€æŸ¥æ¸…å•

- [ ] **æ˜¯å¦æœ‰çº¦æŸæ¡ä»¶ï¼Ÿ** æ˜ç¡®å®šä¹‰è¾“å‡ºè¾¹ç•Œ
- [ ] **æ˜¯å¦æœ‰éªŒè¯æœºåˆ¶ï¼Ÿ** è‡ªåŠ¨æ£€æŸ¥è¾“å‡ºè´¨é‡
- [ ] **æ˜¯å¦æœ‰ç½®ä¿¡åº¦è¯„ä¼°ï¼Ÿ** é‡åŒ–ç­”æ¡ˆå¯é æ€§
- [ ] **æ˜¯å¦æ ‡æ³¨æ¥æºï¼Ÿ** ç¡®ä¿å¯è¿½æº¯æ€§
- [ ] **æ˜¯å¦å¤„ç†ä¸ç¡®å®šæ€§ï¼Ÿ** æ‰¿è®¤ä¸çŸ¥é“

### 2. è´¨é‡æ§åˆ¶çš„é»„é‡‘æ ‡å‡†

```python
QUALITY_STANDARDS = {
    "grounding": "100% åŸºäºä¸Šä¸‹æ–‡",
    "sources": "å¿…é¡»æ ‡æ³¨æ¥æº",
    "confidence": "ç½®ä¿¡åº¦ > 0.7",
    "length": "50-200 å­—",
    "format": "JSON æ ¼å¼",
    "validation": "é€šè¿‡éªŒè¯æ¸…å•"
}
```

### 3. ä¸åŒåœºæ™¯çš„è´¨é‡è¦æ±‚

| åœºæ™¯ | ç½®ä¿¡åº¦é˜ˆå€¼ | éªŒè¯å±‚æ•° | è¦æ±‚æ¥æº | å…è®¸æ¨æµ‹ |
|------|-----------|---------|---------|---------|
| **ä¸€èˆ¬é—®ç­”** | 0.6 | 1 | å¦ | æ˜¯ |
| **æŠ€æœ¯æ–‡æ¡£** | 0.75 | 2 | æ˜¯ | å¦ |
| **åŒ»ç–—æ³•å¾‹** | 0.9 | 3 | æ˜¯ | å¦ |
| **é‡‘èå†³ç­–** | 0.95 | 3 | æ˜¯ | å¦ |

---

## å¸¸è§è¯¯åŒº

### è¯¯åŒº 1ï¼šæ²¡æœ‰éªŒè¯æœºåˆ¶

âŒ **é”™è¯¯ï¼š** ç›´æ¥ä½¿ç”¨è¾“å‡ºï¼Œä¸éªŒè¯

```python
# âŒ ä¸å®‰å…¨
result = llm(question)
return result  # å¯èƒ½åŒ…å«å¹»è§‰
```

âœ… **æ­£ç¡®ï¼š** æ·»åŠ éªŒè¯

```python
# âœ… å®‰å…¨
result = llm_with_validation(question, context)
if result['validation_passed']:
    return result
else:
    return error_response(result['errors'])
```

### è¯¯åŒº 2ï¼šè¿‡åº¦è‡ªä¿¡

âŒ **é”™è¯¯ï¼š** ä¸è¯„ä¼°ç½®ä¿¡åº¦

```python
# âŒ è¿‡åº¦è‡ªä¿¡
answer = "RAG ç»å¯¹æ˜¯æœ€å¥½çš„è§£å†³æ–¹æ¡ˆ"
```

âœ… **æ­£ç¡®ï¼š** æä¾›ç½®ä¿¡åº¦

```python
# âœ… è°¦é€Š
answer = {
    "content": "RAG åœ¨æŸäº›åœºæ™¯ä¸‹æ˜¯æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆ",
    "confidence": 0.8,
    "caveats": ["å–å†³äºå…·ä½“éœ€æ±‚", "éœ€è¦æƒè¡¡æˆæœ¬"]
}
```

### è¯¯åŒº 3ï¼šä¸æ‰¿è®¤ä¸ç¡®å®šæ€§

âŒ **é”™è¯¯ï¼š** å¼ºè¡Œå›ç­”

```python
# âŒ ç¼–é€ ç­”æ¡ˆ
if context_insufficient:
    answer = "æ ¹æ®ç»éªŒï¼ŒRAG é€šå¸¸..."  # ç¼–é€ 
```

âœ… **æ­£ç¡®ï¼š** æ‰¿è®¤ä¸ç¡®å®š

```python
# âœ… è¯šå®
if context_insufficient:
    return {
        "error": "ä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”è¯¥é—®é¢˜",
        "suggestion": "éœ€è¦æ›´å¤šå…³äº RAG çš„æ–‡æ¡£"
    }
```

---

## å¯¹æ¯”æ€»ç»“

### æœ‰æ— è´¨é‡æ§åˆ¶çš„å¯¹æ¯”

| æŒ‡æ ‡ | æ— è´¨é‡æ§åˆ¶ | æœ‰è´¨é‡æ§åˆ¶ | æå‡ |
|------|-----------|-----------|------|
| å¯é æ€§ | 60% | 92% | +53% |
| å¹»è§‰ç‡ | 25% | 5% | -80% |
| ç”¨æˆ·ä¿¡ä»»åº¦ | 55% | 88% | +60% |
| å¯è¿½æº¯æ€§ | 20% | 100% | +400% |

### ä¸åŒéªŒè¯ç­–ç•¥çš„æ•ˆæœ

| ç­–ç•¥ | å®ç°éš¾åº¦ | æ•ˆæœæå‡ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|
| **çº¦æŸæ¡ä»¶** | ä½ | +30% | æ‰€æœ‰åœºæ™¯ |
| **å•å±‚éªŒè¯** | ä¸­ | +50% | ä¸€èˆ¬é—®ç­” |
| **å¤šå±‚éªŒè¯** | é«˜ | +70% | é«˜é£é™©åœºæ™¯ |
| **è‡ªé€‚åº”æ§åˆ¶** | é«˜ | +80% | ç”Ÿäº§ç¯å¢ƒ |

---

## å‚è€ƒèµ„æº

- [Source: Lakera 2026 Prompt Engineering Guide](https://www.lakera.ai/blog/prompt-engineering-guide)
- [Source: IBM 2026 Prompt Engineering Guide](https://www.ibm.com/think/prompt-engineering)
- [Source: The AI Corner 2026 Guide](https://www.the-ai-corner.com/p/your-2026-guide-to-prompt-engineering)

---

**ä¸‹ä¸€æ­¥ï¼š** ç»§ç»­é˜…è¯» `07_å®æˆ˜ä»£ç _01_è§’è‰²è®¾å®šåœºæ™¯.md`ï¼Œé€šè¿‡å®æˆ˜ä»£ç å·©å›ºç†è§£ã€‚

**ç‰ˆæœ¬ï¼š** v1.0 | **æ›´æ–°ï¼š** 2026-02-14 | **é˜…è¯»æ—¶é—´ï¼š** 10åˆ†é’Ÿ
