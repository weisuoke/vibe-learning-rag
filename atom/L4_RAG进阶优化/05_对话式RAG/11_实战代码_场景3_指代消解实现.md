# 实战代码：场景3 - 指代消解实现

> 使用LLM重写Query，解决对话中的指代问题

---

## 场景说明

本场景实现`CoreferenceResolver`类，使用LLM将含有指代词的问题重写为独立完整的问题。

**核心功能：**
- 识别指代词（它、这个、那个等）
- 结合历史对话理解指代关系
- 使用LLM重写为独立完整的Query
- 支持复杂嵌套指代

**适用场景：**
- 所有对话式RAG系统（推荐）
- 需要支持自然对话的应用
- 向量检索前的Query预处理

---

## 完整代码实现

```python
"""
指代消解实现
使用LLM将含有指代的问题重写为独立完整的问题
"""

from typing import List, Optional, Dict
from dataclasses import dataclass
from datetime import datetime
from openai import OpenAI
import re


@dataclass
class Message:
    """对话消息数据结构"""
    role: str
    content: str
    timestamp: datetime


class CoreferenceResolver:
    """指代消解器"""

    def __init__(self, model: str = "gpt-4"):
        """
        初始化指代消解器

        Args:
            model: 使用的LLM模型
        """
        self.client = OpenAI()
        self.model = model

        # 常见指代词列表
        self.pronouns = [
            "它", "这个", "那个", "这", "那",
            "它们", "这些", "那些",
            "前面", "上面", "刚才",
            "第一个", "第二个", "最后一个"
        ]

    def resolve(self, current_query: str, history: List[Message],
                force: bool = False) -> str:
        """
        解析Query中的指代

        Args:
            current_query: 当前问题
            history: 对话历史
            force: 是否强制重写（即使没有检测到指代词）

        Returns:
            重写后的Query
        """
        # 检查是否需要解析
        if not force and not self._needs_resolution(current_query, history):
            return current_query

        # 使用LLM重写
        resolved = self._rewrite_with_llm(current_query, history)

        # 记录日志
        if resolved != current_query:
            print(f"[指代消解] {current_query} → {resolved}")

        return resolved

    def _needs_resolution(self, query: str, history: List[Message]) -> bool:
        """
        判断是否需要指代消解
        """
        # 没有历史对话，不需要消解
        if not history:
            return False

        # 检查是否包含指代词
        has_pronoun = any(p in query for p in self.pronouns)

        # 检查问题是否过短（可能省略了主语）
        is_short = len(query) < 10

        return has_pronoun or is_short

    def _rewrite_with_llm(self, query: str, history: List[Message]) -> str:
        """
        使用LLM重写Query
        """
        # 构建上下文（最近3-5轮）
        context = self._build_context(history[-6:])

        # 调用LLM
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{
                    "role": "system",
                    "content": """你是一个Query重写助手。
根据对话历史，将用户的当前问题重写为一个独立完整的问题。

要求：
1. 解决所有指代（它、这个、那个、前面提到的等）
2. 补充必要的上下文信息
3. 保持原问题的意图不变
4. 只输出重写后的问题，不要解释

示例1：
对话历史:
user: 什么是RAG？
assistant: RAG是检索增强生成...

当前问题: 它有什么优势？
重写后: RAG有什么优势？

示例2：
对话历史:
user: RAG有哪些组件？
assistant: RAG包含检索器、向量数据库和生成器

当前问题: 第一个怎么工作？
重写后: RAG的检索器组件怎么工作？

示例3：
对话历史:
user: RAG和微调有什么区别？
assistant: RAG是检索增强...微调是训练模型...

当前问题: 它们各自的优势是什么？
重写后: RAG和微调各自的优势是什么？
"""
                }, {
                    "role": "user",
                    "content": f"对话历史:\n{context}\n\n当前问题: {query}\n\n重写后的问题:"
                }],
                temperature=0.3,  # 降低随机性
                max_tokens=100
            )

            resolved = response.choices[0].message.content.strip()

            # 移除可能的引号
            resolved = resolved.strip('"\'')

            return resolved

        except Exception as e:
            print(f"[警告] 指代消解失败: {e}")
            return query

    def _build_context(self, messages: List[Message]) -> str:
        """
        构建上下文文本
        """
        lines = []
        for msg in messages:
            lines.append(f"{msg.role}: {msg.content}")
        return "\n".join(lines)

    def resolve_batch(self, queries: List[str], history: List[Message]) -> List[str]:
        """
        批量解析（优化成本）
        """
        if not queries:
            return []

        # 构建上下文
        context = self._build_context(history[-6:])

        # 构建批量请求
        queries_text = "\n".join([f"{i+1}. {q}" for i, q in enumerate(queries)])

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{
                    "role": "system",
                    "content": """你是一个Query重写助手。
根据对话历史，将用户的多个问题重写为独立完整的问题。

要求：
1. 解决所有指代
2. 保持原问题意图
3. 按照编号顺序输出，每行一个问题
"""
                }, {
                    "role": "user",
                    "content": f"对话历史:\n{context}\n\n当前问题:\n{queries_text}\n\n重写后的问题:"
                }],
                temperature=0.3,
                max_tokens=300
            )

            # 解析结果
            result_text = response.choices[0].message.content.strip()
            resolved_queries = []

            for line in result_text.split("\n"):
                # 移除编号
                line = re.sub(r'^\d+\.\s*', '', line).strip()
                if line:
                    resolved_queries.append(line)

            # 确保数量匹配
            if len(resolved_queries) != len(queries):
                print(f"[警告] 批量解析数量不匹配，使用原始queries")
                return queries

            return resolved_queries

        except Exception as e:
            print(f"[警告] 批量指代消解失败: {e}")
            return queries


# ===== 测试用例 =====

def test_basic_resolution():
    """测试基础指代消解"""
    print("=== 测试1：基础指代消解 ===\n")

    resolver = CoreferenceResolver()

    # 构建历史
    history = [
        Message(role="user", content="什么是RAG？", timestamp=datetime.now()),
        Message(role="assistant", content="RAG是检索增强生成，结合了检索和生成的优势", timestamp=datetime.now()),
    ]

    # 测试用例
    test_cases = [
        "它有什么优势？",
        "这个技术怎么实现？",
        "能详细说说吗？",
    ]

    for query in test_cases:
        resolved = resolver.resolve(query, history)
        print(f"原问题: {query}")
        print(f"解析后: {resolved}\n")


def test_complex_resolution():
    """测试复杂指代消解"""
    print("=== 测试2：复杂指代消解 ===\n")

    resolver = CoreferenceResolver()

    # 构建更复杂的历史
    history = [
        Message(role="user", content="RAG和微调有什么区别？", timestamp=datetime.now()),
        Message(role="assistant", content="RAG是检索增强生成，通过检索外部知识库来增强回答。微调是训练模型参数来适应特定任务。", timestamp=datetime.now()),
        Message(role="user", content="RAG有哪些组件？", timestamp=datetime.now()),
        Message(role="assistant", content="RAG主要包含三个组件：检索器、向量数据库和生成器。", timestamp=datetime.now()),
    ]

    # 测试复杂指代
    test_cases = [
        "它们各自的优势是什么？",  # "它们" = RAG和微调
        "第一个组件怎么工作？",     # "第一个" = 检索器
        "这些组件如何协作？",       # "这些" = 三个组件
    ]

    for query in test_cases:
        resolved = resolver.resolve(query, history)
        print(f"原问题: {query}")
        print(f"解析后: {resolved}\n")


def test_nested_resolution():
    """测试嵌套指代"""
    print("=== 测试3：嵌套指代消解 ===\n")

    resolver = CoreferenceResolver()

    history = [
        Message(role="user", content="介绍一下Python的装饰器", timestamp=datetime.now()),
        Message(role="assistant", content="装饰器是Python的一个特性，可以在不修改函数代码的情况下增加功能。", timestamp=datetime.now()),
        Message(role="user", content="它有哪些应用场景？", timestamp=datetime.now()),
        Message(role="assistant", content="装饰器常用于日志记录、性能测试、权限验证等场景。", timestamp=datetime.now()),
    ]

    # 嵌套指代
    query = "前面提到的第一个场景怎么实现？"
    resolved = resolver.resolve(query, history)
    print(f"原问题: {query}")
    print(f"解析后: {resolved}\n")


def test_batch_resolution():
    """测试批量解析"""
    print("=== 测试4：批量指代消解 ===\n")

    resolver = CoreferenceResolver()

    history = [
        Message(role="user", content="什么是RAG？", timestamp=datetime.now()),
        Message(role="assistant", content="RAG是检索增强生成...", timestamp=datetime.now()),
    ]

    # 批量查询
    queries = [
        "它有什么优势？",
        "如何实现它？",
        "这个技术的成本如何？",
    ]

    print(f"批量解析{len(queries)}个问题:")
    resolved_queries = resolver.resolve_batch(queries, history)

    for original, resolved in zip(queries, resolved_queries):
        print(f"  {original} → {resolved}")
    print()


def test_no_resolution_needed():
    """测试不需要消解的情况"""
    print("=== 测试5：不需要消解的情况 ===\n")

    resolver = CoreferenceResolver()

    history = [
        Message(role="user", content="什么是RAG？", timestamp=datetime.now()),
        Message(role="assistant", content="RAG是检索增强生成...", timestamp=datetime.now()),
    ]

    # 完整独立的问题
    query = "什么是Transformer？"
    resolved = resolver.resolve(query, history)

    print(f"原问题: {query}")
    print(f"解析后: {resolved}")
    print(f"是否重写: {'否' if query == resolved else '是'}\n")


def test_force_resolution():
    """测试强制重写"""
    print("=== 测试6：强制重写 ===\n")

    resolver = CoreferenceResolver()

    history = [
        Message(role="user", content="介绍RAG的检索模块", timestamp=datetime.now()),
        Message(role="assistant", content="检索模块使用向量相似度...", timestamp=datetime.now()),
    ]

    # 看似完整但可能需要补充上下文的问题
    query = "支持哪些向量数据库？"

    # 不强制
    resolved1 = resolver.resolve(query, history, force=False)
    print(f"不强制重写: {query} → {resolved1}")

    # 强制
    resolved2 = resolver.resolve(query, history, force=True)
    print(f"强制重写: {query} → {resolved2}\n")


# ===== 性能测试 =====

def test_performance():
    """测试性能和成本"""
    print("=== 测试7：性能和成本 ===\n")

    resolver = CoreferenceResolver()

    history = [
        Message(role="user", content="什么是RAG？", timestamp=datetime.now()),
        Message(role="assistant", content="RAG是检索增强生成...", timestamp=datetime.now()),
    ]

    query = "它有什么优势？"

    import time

    # 测试延迟
    start = time.time()
    resolved = resolver.resolve(query, history)
    elapsed = time.time() - start

    print(f"原问题: {query}")
    print(f"解析后: {resolved}")
    print(f"延迟: {elapsed:.2f}秒")

    # 估算成本
    # GPT-4: 输入$0.03/1K tokens, 输出$0.06/1K tokens
    # 估算：输入约200 tokens，输出约20 tokens
    input_tokens = 200
    output_tokens = 20
    cost = (input_tokens / 1000 * 0.03) + (output_tokens / 1000 * 0.06)

    print(f"估算成本: ${cost:.4f}/次\n")


# ===== 在RAG中的应用 =====

def demo_rag_integration():
    """演示在RAG中的集成"""
    print("=== 演示：在RAG中的集成 ===\n")

    resolver = CoreferenceResolver()

    # 模拟对话场景
    history = []

    # 第1轮
    print("第1轮对话:")
    q1 = "什么是RAG？"
    print(f"用户: {q1}")

    # 不需要消解
    resolved_q1 = resolver.resolve(q1, history)
    print(f"解析后: {resolved_q1}")

    # 模拟RAG回答
    a1 = "RAG是检索增强生成，结合了检索和生成的优势..."
    print(f"助手: {a1}\n")

    # 更新历史
    history.append(Message(role="user", content=q1, timestamp=datetime.now()))
    history.append(Message(role="assistant", content=a1, timestamp=datetime.now()))

    # 第2轮（含指代）
    print("第2轮对话:")
    q2 = "它有什么优势？"
    print(f"用户: {q2}")

    # 需要消解
    resolved_q2 = resolver.resolve(q2, history)
    print(f"解析后: {resolved_q2}")

    # 使用resolved_q2进行向量检索
    print(f"[向量检索] 使用Query: {resolved_q2}")

    # 模拟RAG回答
    a2 = "RAG的优势包括：1. 知识更新及时 2. 减少幻觉 3. 成本较低..."
    print(f"助手: {a2}\n")

    # 更新历史
    history.append(Message(role="user", content=q2, timestamp=datetime.now()))
    history.append(Message(role="assistant", content=a2, timestamp=datetime.now()))

    # 第3轮（复杂指代）
    print("第3轮对话:")
    q3 = "第一个优势详细说说"
    print(f"用户: {q3}")

    # 需要消解
    resolved_q3 = resolver.resolve(q3, history)
    print(f"解析后: {resolved_q3}")

    print(f"[向量检索] 使用Query: {resolved_q3}")


# ===== 主函数 =====

def main():
    """主函数"""
    print("=" * 60)
    print("指代消解实现测试")
    print("=" * 60)
    print()

    # 运行所有测试
    test_basic_resolution()
    test_complex_resolution()
    test_nested_resolution()
    test_batch_resolution()
    test_no_resolution_needed()
    test_force_resolution()
    test_performance()

    print("=" * 60)
    print()

    # 演示RAG集成
    demo_rag_integration()


if __name__ == "__main__":
    main()
```

---

## 运行输出示例

```
============================================================
指代消解实现测试
============================================================

=== 测试1：基础指代消解 ===

[指代消解] 它有什么优势？ → RAG有什么优势？
原问题: 它有什么优势？
解析后: RAG有什么优势？

[指代消解] 这个技术怎么实现？ → RAG技术怎么实现？
原问题: 这个技术怎么实现？
解析后: RAG技术怎么实现？

[指代消解] 能详细说说吗？ → 能详细说说RAG吗？
原问题: 能详细说说吗？
解析后: 能详细说说RAG吗？

=== 测试2：复杂指代消解 ===

[指代消解] 它们各自的优势是什么？ → RAG和微调各自的优势是什么？
原问题: 它们各自的优势是什么？
解析后: RAG和微调各自的优势是什么？

[指代消解] 第一个组件怎么工作？ → RAG的检索器组件怎么工作？
原问题: 第一个组件怎么工作？
解析后: RAG的检索器组件怎么工作？

[指代消解] 这些组件如何协作？ → RAG的检索器、向量数据库和生成器这些组件如何协作？
原问题: 这些组件如何协作？
解析后: RAG的检索器、向量数据库和生成器这些组件如何协作？

=== 测试3：嵌套指代消解 ===

[指代消解] 前面提到的第一个场景怎么实现？ → Python装饰器在日志记录场景中怎么实现？
原问题: 前面提到的第一个场景怎么实现？
解析后: Python装饰器在日志记录场景中怎么实现？

=== 测试4：批量指代消解 ===

批量解析3个问题:
  它有什么优势？ → RAG有什么优势？
  如何实现它？ → 如何实现RAG？
  这个技术的成本如何？ → RAG技术的成本如何？

=== 测试5：不需要消解的情况 ===

原问题: 什么是Transformer？
解析后: 什么是Transformer？
是否重写: 否

=== 测试6：强制重写 ===

不强制重写: 支持哪些向量数据库？ → 支持哪些向量数据库？
[指代消解] 支持哪些向量数据库？ → RAG的检索模块支持哪些向量数据库？
强制重写: 支持哪些向量数据库？ → RAG的检索模块支持哪些向量数据库？

=== 测试7：性能和成本 ===

[指代消解] 它有什么优势？ → RAG有什么优势？
原问题: 它有什么优势？
解析后: RAG有什么优势？
延迟: 1.23秒
估算成本: $0.0072/次

============================================================

=== 演示：在RAG中的集成 ===

第1轮对话:
用户: 什么是RAG？
解析后: 什么是RAG？
助手: RAG是检索增强生成，结合了检索和生成的优势...

第2轮对话:
用户: 它有什么优势？
[指代消解] 它有什么优势？ → RAG有什么优势？
解析后: RAG有什么优势？
[向量检索] 使用Query: RAG有什么优势？
助手: RAG的优势包括：1. 知识更新及时 2. 减少幻觉 3. 成本较低...

第3轮对话:
用户: 第一个优势详细说说
[指代消解] 第一个优势详细说说 → RAG知识更新及时这个优势详细说说
解析后: RAG知识更新及时这个优势详细说说
[向量检索] 使用Query: RAG知识更新及时这个优势详细说说
```

---

## 优化策略

### 1. 缓存优化

```python
from functools import lru_cache
import hashlib

class CachedCoreferenceResolver(CoreferenceResolver):
    """带缓存的指代消解器"""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.cache: Dict[str, str] = {}

    def resolve(self, current_query: str, history: List[Message],
                force: bool = False) -> str:
        """带缓存的解析"""
        # 生成缓存键
        cache_key = self._generate_cache_key(current_query, history)

        # 检查缓存
        if cache_key in self.cache:
            print(f"[缓存命中] {current_query}")
            return self.cache[cache_key]

        # 调用父类方法
        resolved = super().resolve(current_query, history, force)

        # 存入缓存
        self.cache[cache_key] = resolved

        return resolved

    def _generate_cache_key(self, query: str, history: List[Message]) -> str:
        """生成缓存键"""
        # 使用最近3轮历史+当前query生成hash
        recent = history[-6:] if len(history) > 6 else history
        context = "".join([m.content for m in recent]) + query
        return hashlib.md5(context.encode()).hexdigest()
```

### 2. 使用更便宜的模型

```python
# 使用GPT-3.5-turbo（比GPT-4便宜10倍）
resolver = CoreferenceResolver(model="gpt-3.5-turbo")

# 成本对比：
# GPT-4: $0.0072/次
# GPT-3.5-turbo: $0.0007/次（节省90%）
```

### 3. 条件触发

```python
def smart_resolve(query: str, history: List[Message]) -> str:
    """智能判断是否需要消解"""
    resolver = CoreferenceResolver()

    # 策略1：检查指代词
    has_pronoun = resolver._needs_resolution(query, history)

    # 策略2：检查query长度
    is_short = len(query) < 15

    # 策略3：检查是否是追问
    is_followup = not query.endswith("？") and not query.endswith("?")

    # 只有满足条件才消解
    if has_pronoun or (is_short and is_followup):
        return resolver.resolve(query, history)

    return query
```

---

## 在完整RAG系统中的集成

```python
def conversational_rag_with_resolution(session_id: str, user_query: str) -> str:
    """
    带指代消解的对话式RAG
    """
    from openai import OpenAI

    # 初始化
    manager = ConversationManager()
    resolver = CoreferenceResolver()
    client = OpenAI()

    # 1. 获取历史
    history = manager.get_context(session_id, last_n=5)

    # 2. 指代消解
    resolved_query = resolver.resolve(user_query, history)

    print(f"[Query] 原始: {user_query}")
    print(f"[Query] 解析: {resolved_query}")

    # 3. 向量检索（使用解析后的query）
    # docs = vector_store.search(resolved_query, top_k=3)

    # 4. 构建Prompt
    history_text = "\n".join([f"{m.role}: {m.content}" for m in history[-3:]])

    prompt = f"""
对话历史:
{history_text}

用户问题: {user_query}
（解析后: {resolved_query}）

请基于对话历史回答问题。
"""

    # 5. LLM生成
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    answer = response.choices[0].message.content

    # 6. 保存历史
    manager.add_message(session_id, "user", user_query)
    manager.add_message(session_id, "assistant", answer)

    return answer
```

---

## 总结

**本场景实现了：**
- ✅ 基于LLM的指代消解
- ✅ 支持简单和复杂指代
- ✅ 批量解析优化
- ✅ 性能和成本分析

**关键要点：**
- 指代消解必须在向量检索前完成
- 使用LLM重写是最实用的方案
- 可以通过缓存和条件触发优化成本

**下一步：**
- 学习 `12_实战代码_场景4_完整对话式RAG.md` - 整合所有模块的端到端系统
