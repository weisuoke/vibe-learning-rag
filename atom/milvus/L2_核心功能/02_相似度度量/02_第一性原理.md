# 第一性原理

## 什么是第一性原理？

**第一性原理**：回到事物最基本的真理，从源头思考问题，而不是通过类比或经验。

在物理学中，第一性原理是指从最基本的物理定律推导出复杂现象。在学习中，第一性原理帮助我们理解"为什么"而不仅仅是"是什么"。

---

## 相似度度量的第一性原理

### 1. 最基础的定义

**相似度度量 = 一个函数 f(x, y) → 数字**

输入：两个向量 x 和 y
输出：一个数字，表示它们的"距离"或"相似程度"

仅此而已！没有更基础的了。

**数学形式：**
```
f: ℝⁿ × ℝⁿ → ℝ
```

这个函数必须满足一些基本性质（取决于度量类型），但核心就是：**把两个向量映射到一个数字**。

---

### 2. 为什么需要相似度度量？

#### 核心问题：如何判断两个向量"像不像"？

**场景：**
```python
# 你有两个向量
vector_a = [0.1, 0.2, 0.3, 0.4]
vector_b = [0.15, 0.25, 0.35, 0.45]

# 问题：它们相似吗？
# 如何回答这个问题？
```

**困境：**
- 向量只是一串数字
- 人类无法直观判断高维向量的相似性
- 需要一个**数学标准**来量化"相似"

**解决方案：**
定义一个度量函数，把"相似"这个模糊概念转化为精确的数字。

---

### 3. 相似度度量的三层价值

#### 价值1：量化模糊概念

**问题：** "相似"是一个主观、模糊的概念

**度量的作用：** 把主观感觉转化为客观数字

**示例：**
```python
# 没有度量：
"这两个文档看起来挺像的" ❌ 模糊

# 有度量：
"这两个文档的余弦相似度是 0.87" ✅ 精确
```

**类比：**
- 就像温度计把"热"量化为摄氏度
- 就像体重秤把"胖"量化为公斤数

---

#### 价值2：实现排序

**问题：** 给定一个查询向量，如何从100万个向量中找出最相似的10个？

**度量的作用：** 提供排序依据

**示例：**
```python
# 查询向量
query = [0.1, 0.2, 0.3]

# 候选向量
candidates = [
    [0.15, 0.25, 0.35],  # 候选1
    [0.9, 0.1, 0.2],     # 候选2
    [0.12, 0.22, 0.32],  # 候选3
]

# 计算度量值
scores = [
    cosine(query, candidates[0]),  # 0.9998
    cosine(query, candidates[1]),  # 0.6234
    cosine(query, candidates[2]),  # 0.9999
]

# 排序：候选3 > 候选1 > 候选2
```

**没有度量 = 无法排序 = 无法检索**

---

#### 价值3：定义"相似"的含义

**关键洞察：** 不同的度量方式 = 不同的"相似"定义

**三种定义：**

1. **L2距离：相似 = 空间距离近**
   ```
   两个点在空间中靠得近 → 相似
   ```

2. **内积IP：相似 = 方向一致且幅度大**
   ```
   两个向量指向同一方向，且都很长 → 相似
   ```

3. **余弦相似度：相似 = 方向一致（忽略长度）**
   ```
   两个向量指向同一方向，不管长度 → 相似
   ```

**示例：**
```python
# 两个向量
a = [1, 0]
b = [2, 0]  # b是a的2倍

# L2距离
L2(a, b) = 1.0  # 距离为1，不太相似

# 内积
IP(a, b) = 2.0  # 投影长度为2，比较相似

# 余弦相似度
COSINE(a, b) = 1.0  # 方向完全一致，非常相似
```

**结论：** 选择度量方式 = 选择"相似"的定义

---

### 4. 从第一性原理推导RAG系统的度量选择

**推理链：**

```
1. RAG系统的目标：找到与查询语义最相关的文档
   ↓
2. "语义相关" = 表达的意思相近，而非字面相同
   ↓
3. Embedding模型把语义转化为向量
   ↓
4. 语义相近 → 向量方向相近（核心假设）
   ↓
5. 需要一个度量方式来衡量"方向相近"
   ↓
6. 余弦相似度专门测量方向，忽略幅度
   ↓
7. 因此，RAG系统通常使用COSINE度量
```

**关键假设：**
- Embedding模型的训练目标：语义相近的文本 → 向量方向相近
- 向量的幅度（长度）不携带语义信息
- 因此，只需要比较方向，不需要比较幅度

**验证：**
```python
# 两个语义相近的句子
text1 = "今天天气很好"
text2 = "今天天气不错"

# Embedding
emb1 = model.encode(text1)  # [0.1, 0.2, 0.3, ...]
emb2 = model.encode(text2)  # [0.12, 0.22, 0.32, ...]

# 余弦相似度
cosine(emb1, emb2) = 0.98  # 非常高，符合预期

# 如果用L2距离
L2(emb1, emb2) = 0.05  # 很小，也表示相似

# 但如果向量幅度不同
emb3 = emb2 * 2  # 语义相同，但幅度翻倍

# 余弦相似度
cosine(emb1, emb3) = 0.98  # 仍然很高 ✅

# L2距离
L2(emb1, emb3) = 很大  # 变大了 ❌
```

**结论：** COSINE对向量幅度不敏感，更适合语义搜索。

---

### 5. 从第一性原理推导推荐系统的度量选择

**推理链：**

```
1. 推荐系统的目标：找到用户可能喜欢的物品
   ↓
2. 用户和物品都用向量表示（User Embedding, Item Embedding）
   ↓
3. "用户喜欢物品" = 用户向量和物品向量的匹配度
   ↓
4. 匹配度 = 方向一致性 × 强度
   ↓
5. 方向一致性：用户兴趣方向与物品特征方向对齐
   强度：用户兴趣强度 × 物品热度
   ↓
6. 内积IP = 方向 × 幅度，正好符合需求
   ↓
7. 因此，推荐系统通常使用IP度量
```

**关键区别：**
- RAG系统：只关注语义方向，忽略幅度
- 推荐系统：既关注方向，也关注幅度（热度、强度）

**示例：**
```python
# 用户向量（兴趣强度）
user = [0.8, 0.1, 0.2]  # 对第1类兴趣很强

# 物品向量（特征强度）
item1 = [0.9, 0.1, 0.1]  # 第1类特征强，热门物品
item2 = [0.3, 0.05, 0.05]  # 第1类特征弱，冷门物品

# 内积IP
IP(user, item1) = 0.8*0.9 + 0.1*0.1 + 0.2*0.1 = 0.75  # 高分
IP(user, item2) = 0.8*0.3 + 0.1*0.05 + 0.2*0.05 = 0.255  # 低分

# 余弦相似度
COSINE(user, item1) = 0.98  # 方向很接近
COSINE(user, item2) = 0.98  # 方向也很接近（几乎一样）

# 结论：IP能区分热门和冷门，COSINE不能
```

**结论：** IP保留了幅度信息，适合需要考虑"强度"的场景。

---

### 6. 从第一性原理理解归一化的作用

**推理链：**

```
1. 归一化 = 把向量缩放到单位长度（‖x‖ = 1）
   ↓
2. 归一化后，所有向量的幅度都是1
   ↓
3. 内积IP = x·y = ‖x‖‖y‖cos(θ)
   ↓
4. 当‖x‖ = ‖y‖ = 1时，IP = cos(θ) = COSINE
   ↓
5. 因此，归一化向量的IP = COSINE
   ↓
6. 但IP计算更快（不需要计算范数）
   ↓
7. 结论：归一化 + IP = 最快的COSINE
```

**数学证明：**
```
余弦相似度定义：
COSINE(x, y) = (x·y) / (‖x‖‖y‖)

如果x和y都归一化（‖x‖ = ‖y‖ = 1）：
COSINE(x, y) = (x·y) / (1×1) = x·y = IP(x, y)

证毕。
```

**实践意义：**
```python
# 方案1：直接用COSINE
cosine_score = cosine_similarity(x, y)
# 需要计算：x·y, ‖x‖, ‖y‖, 除法

# 方案2：归一化 + IP
x_norm = x / np.linalg.norm(x)
y_norm = y / np.linalg.norm(y)
ip_score = np.dot(x_norm, y_norm)
# 只需要计算：x·y（归一化可以预先做）

# 结果：ip_score == cosine_score
# 性能：方案2快10-20%
```

**结论：** 归一化是一个性能优化技巧，不改变语义。

---

### 7. 一句话总结第一性原理

**相似度度量是把两个向量映射到一个数字的函数，不同的度量方式定义了不同的"相似"含义，选择度量方式就是选择如何定义"相似"。**

---

## 第一性原理的应用

### 应用1：调试度量选择错误

**问题：** 检索结果不准确

**第一性原理分析：**
```
1. 检索不准 = 排序不对
   ↓
2. 排序不对 = 度量值不对
   ↓
3. 度量值不对 = 度量方式不匹配数据特性
   ↓
4. 检查：向量是否归一化？度量方式是否合适？
```

**示例：**
```python
# 问题：用L2度量，但向量幅度差异很大
vector1 = [0.1, 0.2, 0.3]  # 幅度小
vector2 = [1.0, 2.0, 3.0]  # 幅度大（10倍）

# L2距离
L2(vector1, vector2) = 很大  # 被判定为不相似

# 但实际上方向完全一致
COSINE(vector1, vector2) = 1.0  # 完全相似

# 解决方案：改用COSINE或归一化后用L2
```

---

### 应用2：理解为什么OpenAI推荐COSINE

**OpenAI的Embedding模型特性：**
1. 输出向量是归一化的（‖x‖ = 1）
2. 训练目标是语义相似性
3. 向量幅度不携带语义信息

**第一性原理推导：**
```
1. 向量归一化 → 幅度信息已丢失
   ↓
2. 只剩下方向信息
   ↓
3. COSINE专门测量方向
   ↓
4. 因此推荐COSINE
```

**验证：**
```python
# OpenAI Embedding
emb = openai.Embedding.create(input="hello", model="text-embedding-3-small")
vector = emb['data'][0]['embedding']

# 检查是否归一化
norm = np.linalg.norm(vector)
print(norm)  # 输出：1.0（或非常接近1.0）

# 结论：已归一化，用COSINE或IP都可以
```

---

### 应用3：设计自定义度量

**场景：** 你想设计一个新的度量方式

**第一性原理指导：**
1. 明确"相似"的定义（业务含义）
2. 设计数学函数来量化这个定义
3. 验证函数的性质（对称性、三角不等式等）

**示例：加权余弦相似度**
```python
# 业务需求：某些维度更重要
# 例如：在文档检索中，标题的相似度比正文更重要

def weighted_cosine(x, y, weights):
    """
    加权余弦相似度
    weights: 每个维度的权重
    """
    # 加权点积
    weighted_dot = np.sum(x * y * weights)

    # 加权范数
    norm_x = np.sqrt(np.sum((x ** 2) * weights))
    norm_y = np.sqrt(np.sum((y ** 2) * weights))

    return weighted_dot / (norm_x * norm_y)

# 使用
x = [0.8, 0.1, 0.1]  # [标题, 正文1, 正文2]
y = [0.7, 0.2, 0.15]
weights = [3.0, 1.0, 1.0]  # 标题权重3倍

score = weighted_cosine(x, y, weights)
```

---

## 总结

### 核心洞察

1. **度量是函数**：把两个向量映射到一个数字
2. **度量定义相似**：不同度量 = 不同的"相似"定义
3. **选择度量 = 选择定义**：根据业务需求选择合适的定义
4. **归一化改变度量**：归一化后IP = COSINE
5. **度量决定排序**：度量错误 → 排序错误 → 检索失败

### 决策框架

```
选择度量方式的第一性原理：
1. 明确业务目标（找什么样的"相似"）
2. 分析数据特性（向量是否归一化）
3. 匹配度量定义（L2/IP/COSINE的含义）
4. 验证效果（检索结果是否符合预期）
```

---

**下一步：** [03_核心概念_L2距离.md](./03_核心概念_L2距离.md) - 深入理解L2距离
