# 实战代码：分布式任务队列

## 概述

**本文实现分布式任务队列系统,包括Redis队列、Celery集成、Hatchet工作流(2025-2026)等生产级方案。**

---

## 环境准备

```bash
# 安装依赖
pip install redis celery hatchet-sdk

# 启动Redis
docker run -d -p 6379:6379 redis:latest

# 启动RabbitMQ
docker run -d -p 5672:5672 -p 15672:15672 rabbitmq:management
```

---

## 方案1:Redis任务队列

### 基础Redis队列

```python
import redis
import json
import time
from typing import Any, Optional

class RedisQueue:
    """Redis任务队列"""

    def __init__(self, name: str, host: str = 'localhost', port: int = 6379):
        self.redis = redis.Redis(host=host, port=port, decode_responses=True)
        self.name = name

    def enqueue(self, task: dict) -> None:
        """入队"""
        self.redis.rpush(self.name, json.dumps(task))

    def dequeue(self, timeout: int = 0) -> Optional[dict]:
        """出队(阻塞)"""
        result = self.redis.blpop(self.name, timeout=timeout)
        if result:
            _, task_json = result
            return json.loads(task_json)
        return None

    def size(self) -> int:
        """队列长度"""
        return self.redis.llen(self.name)

    def clear(self) -> None:
        """清空队列"""
        self.redis.delete(self.name)

# 使用
queue = RedisQueue('tasks')

# 生产者
queue.enqueue({'task_id': '1', 'action': 'process', 'data': 'test'})

# 消费者
task = queue.dequeue(timeout=5)
if task:
    print(f"处理任务: {task}")
```

### Redis优先级队列

```python
class RedisPriorityQueue:
    """Redis优先级队列(使用ZSET)"""

    def __init__(self, name: str, host: str = 'localhost'):
        self.redis = redis.Redis(host=host, decode_responses=True)
        self.name = name

    def enqueue(self, task: dict, priority: int) -> None:
        """入队(优先级越小越高)"""
        task_json = json.dumps(task)
        self.redis.zadd(self.name, {task_json: priority})

    def dequeue(self) -> Optional[dict]:
        """出队(优先级最高)"""
        # 获取优先级最小的任务
        result = self.redis.zpopmin(self.name, count=1)
        if result:
            task_json, priority = result[0]
            return json.loads(task_json)
        return None

    def size(self) -> int:
        """队列长度"""
        return self.redis.zcard(self.name)

# 使用
pq = RedisPriorityQueue('priority_tasks')

pq.enqueue({'task_id': '1', 'action': 'low'}, priority=5)
pq.enqueue({'task_id': '2', 'action': 'high'}, priority=1)

task = pq.dequeue()
print(f"处理任务: {task}")  # 高优先级任务先出队
```

---

## 方案2:Celery分布式任务队列

### Celery基础配置

```python
from celery import Celery

# 创建Celery应用
app = Celery(
    'distributed_tasks',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

# 配置
app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=300,
    task_soft_time_limit=240,
)

@app.task(bind=True, max_retries=3)
def process_task(self, task_id: str, data: dict):
    """处理任务"""
    try:
        print(f"处理任务 {task_id}: {data}")
        time.sleep(2)  # 模拟处理
        return {'task_id': task_id, 'status': 'completed'}

    except Exception as e:
        # 重试
        raise self.retry(exc=e, countdown=5)

@app.task
def long_running_task(task_id: str):
    """长时间任务"""
    for i in range(10):
        print(f"任务 {task_id} 进度: {i+1}/10")
        time.sleep(1)
    return {'task_id': task_id, 'status': 'completed'}

# 提交任务
if __name__ == "__main__":
    # 异步提交
    result = process_task.delay('task_1', {'data': 'test'})

    # 获取结果
    print(result.get(timeout=10))

    # 提交长时间任务
    result = long_running_task.delay('long_task_1')
    print(f"任务ID: {result.id}")
```

### Celery任务路由

```python
# 配置任务路由
app.conf.task_routes = {
    'tasks.high_priority': {'queue': 'high'},
    'tasks.normal_priority': {'queue': 'normal'},
    'tasks.low_priority': {'queue': 'low'},
}

@app.task(queue='high')
def high_priority_task(data):
    """高优先级任务"""
    pass

@app.task(queue='normal')
def normal_priority_task(data):
    """普通优先级任务"""
    pass

@app.task(queue='low')
def low_priority_task(data):
    """低优先级任务"""
    pass

# 启动不同队列的Worker
# celery -A tasks worker -Q high --loglevel=info
# celery -A tasks worker -Q normal --loglevel=info
# celery -A tasks worker -Q low --loglevel=info
```

---

## 方案3:Hatchet分布式工作流(2025-2026)

### Hatchet基础使用

根据2025-2026最新文档,Hatchet是专为AI工作流设计的分布式任务队列:

```python
from hatchet_sdk import Hatchet
from typing import Dict

# 初始化Hatchet
hatchet = Hatchet()

@hatchet.workflow(name="process-data")
class ProcessDataWorkflow:
    """数据处理工作流"""

    @hatchet.step()
    def load_data(self, context) -> Dict:
        """加载数据"""
        print("加载数据...")
        return {"data": "loaded"}

    @hatchet.step(parents=["load_data"])
    def process_data(self, context) -> Dict:
        """处理数据"""
        data = context.step_output("load_data")
        print(f"处理数据: {data}")
        return {"result": "processed"}

    @hatchet.step(parents=["process_data"])
    def save_result(self, context) -> Dict:
        """保存结果"""
        result = context.step_output("process_data")
        print(f"保存结果: {result}")
        return {"status": "completed"}

# 触发工作流
workflow = hatchet.client.admin.run_workflow(
    "process-data",
    {"input": "test"}
)

# 等待完成
result = workflow.result()
print(f"工作流结果: {result}")
```

### Hatchet AI Agent工作流

```python
from hatchet_sdk import Hatchet
from langchain_openai import ChatOpenAI

hatchet = Hatchet()

@hatchet.workflow(name="ai-agent-pipeline")
class AIAgentWorkflow:
    """AI Agent工作流"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4")

    @hatchet.step()
    async def search(self, context) -> Dict:
        """搜索步骤"""
        query = context.workflow_input()["query"]
        print(f"搜索: {query}")

        # 调用LLM
        response = await self.llm.ainvoke(f"搜索: {query}")

        return {"search_result": response.content}

    @hatchet.step(parents=["search"])
    async def analyze(self, context) -> Dict:
        """分析步骤"""
        search_result = context.step_output("search")["search_result"]
        print(f"分析: {search_result[:50]}...")

        # 调用LLM
        response = await self.llm.ainvoke(f"分析: {search_result}")

        return {"analysis": response.content}

    @hatchet.step(parents=["analyze"])
    async def summarize(self, context) -> Dict:
        """总结步骤"""
        analysis = context.step_output("analyze")["analysis"]
        print(f"总结: {analysis[:50]}...")

        # 调用LLM
        response = await self.llm.ainvoke(f"总结: {analysis}")

        return {"summary": response.content}

# 使用
workflow = hatchet.client.admin.run_workflow(
    "ai-agent-pipeline",
    {"query": "什么是量子计算?"}
)

result = workflow.result()
print(f"最终总结: {result['summary']}")
```

### Hatchet定时任务

```python
from hatchet_sdk import Hatchet, CronTrigger

hatchet = Hatchet()

@hatchet.workflow(
    name="daily-report",
    on_crons=[CronTrigger(cron="0 9 * * *")]  # 每天9点
)
class DailyReportWorkflow:
    """每日报告工作流"""

    @hatchet.step()
    def generate_report(self, context) -> Dict:
        """生成报告"""
        print("生成每日报告...")
        return {"report": "daily_report.pdf"}

    @hatchet.step(parents=["generate_report"])
    def send_email(self, context) -> Dict:
        """发送邮件"""
        report = context.step_output("generate_report")["report"]
        print(f"发送报告: {report}")
        return {"status": "sent"}

# Hatchet会自动按Cron调度执行
```

---

## 方案4:RabbitMQ消息队列

### RabbitMQ基础使用

```python
import pika
import json

class RabbitMQQueue:
    """RabbitMQ队列"""

    def __init__(self, queue_name: str, host: str = 'localhost'):
        self.queue_name = queue_name
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(host)
        )
        self.channel = self.connection.channel()
        self.channel.queue_declare(queue=queue_name, durable=True)

    def enqueue(self, task: dict) -> None:
        """入队"""
        self.channel.basic_publish(
            exchange='',
            routing_key=self.queue_name,
            body=json.dumps(task),
            properties=pika.BasicProperties(
                delivery_mode=2,  # 持久化
            )
        )

    def consume(self, callback):
        """消费"""
        def wrapper(ch, method, properties, body):
            task = json.loads(body)
            callback(task)
            ch.basic_ack(delivery_tag=method.delivery_tag)

        self.channel.basic_qos(prefetch_count=1)
        self.channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=wrapper
        )

        print('等待任务...')
        self.channel.start_consuming()

    def close(self):
        """关闭连接"""
        self.connection.close()

# 生产者
queue = RabbitMQQueue('tasks')
queue.enqueue({'task_id': '1', 'action': 'process'})
queue.close()

# 消费者
def process_task(task):
    print(f"处理任务: {task}")

queue = RabbitMQQueue('tasks')
queue.consume(process_task)
```

### RabbitMQ优先级队列

```python
class RabbitMQPriorityQueue:
    """RabbitMQ优先级队列"""

    def __init__(self, queue_name: str, host: str = 'localhost'):
        self.queue_name = queue_name
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(host)
        )
        self.channel = self.connection.channel()

        # 声明优先级队列
        self.channel.queue_declare(
            queue=queue_name,
            durable=True,
            arguments={'x-max-priority': 10}
        )

    def enqueue(self, task: dict, priority: int = 5) -> None:
        """入队(优先级0-10)"""
        self.channel.basic_publish(
            exchange='',
            routing_key=self.queue_name,
            body=json.dumps(task),
            properties=pika.BasicProperties(
                delivery_mode=2,
                priority=priority
            )
        )

# 使用
pq = RabbitMQPriorityQueue('priority_tasks')
pq.enqueue({'task_id': '1'}, priority=1)  # 高优先级
pq.enqueue({'task_id': '2'}, priority=10)  # 低优先级
```

---

## 方案5:完整分布式系统

### 生产级分布式调度器

```python
import asyncio
from typing import Dict, List
from dataclasses import dataclass
from enum import Enum

class TaskStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class DistributedTask:
    task_id: str
    worker_id: str
    status: TaskStatus
    retry_count: int
    max_retries: int

class DistributedScheduler:
    """分布式调度器"""

    def __init__(self, redis_host: str = 'localhost'):
        self.redis = redis.Redis(host=redis_host, decode_responses=True)
        self.task_queue = 'distributed:tasks'
        self.worker_id = f"worker_{id(self)}"

    def submit_task(self, task: dict) -> str:
        """提交任务"""
        task_id = f"task_{time.time()}"
        task_data = {
            'task_id': task_id,
            'data': task,
            'status': TaskStatus.PENDING.value,
            'retry_count': 0,
            'max_retries': 3
        }

        # 添加到Redis队列
        self.redis.rpush(self.task_queue, json.dumps(task_data))

        return task_id

    async def worker(self):
        """工作协程"""
        while True:
            # 从Redis获取任务
            result = self.redis.blpop(self.task_queue, timeout=1)

            if not result:
                await asyncio.sleep(0.1)
                continue

            _, task_json = result
            task_data = json.loads(task_json)

            # 处理任务
            await self._process_task(task_data)

    async def _process_task(self, task_data: dict):
        """处理任务"""
        task_id = task_data['task_id']
        print(f"Worker {self.worker_id} 处理任务 {task_id}")

        try:
            # 更新状态为运行中
            task_data['status'] = TaskStatus.RUNNING.value

            # 执行任务
            await asyncio.sleep(1)  # 模拟处理

            # 更新状态为完成
            task_data['status'] = TaskStatus.COMPLETED.value
            print(f"任务 {task_id} 完成")

        except Exception as e:
            print(f"任务 {task_id} 失败: {e}")

            # 重试
            if task_data['retry_count'] < task_data['max_retries']:
                task_data['retry_count'] += 1
                task_data['status'] = TaskStatus.PENDING.value

                # 重新入队
                self.redis.rpush(self.task_queue, json.dumps(task_data))
            else:
                task_data['status'] = TaskStatus.FAILED.value

    async def start(self, num_workers: int = 3):
        """启动多个Worker"""
        workers = [
            asyncio.create_task(self.worker())
            for _ in range(num_workers)
        ]
        await asyncio.gather(*workers)

# 使用
async def main():
    scheduler = DistributedScheduler()

    # 提交任务
    for i in range(10):
        scheduler.submit_task({'data': f'task_{i}'})

    # 启动Worker
    await asyncio.wait_for(scheduler.start(num_workers=3), timeout=30)

asyncio.run(main())
```

---

## 性能对比

### 基准测试

```python
import time

def benchmark_redis_queue(n: int = 1000):
    """Redis队列基准测试"""
    queue = RedisQueue('benchmark')
    queue.clear()

    # 入队
    start = time.time()
    for i in range(n):
        queue.enqueue({'task_id': i})
    enqueue_time = time.time() - start

    # 出队
    start = time.time()
    for i in range(n):
        queue.dequeue()
    dequeue_time = time.time() - start

    print(f"Redis队列 ({n}个任务):")
    print(f"  入队: {enqueue_time:.4f}秒")
    print(f"  出队: {dequeue_time:.4f}秒")
    print(f"  总时间: {enqueue_time + dequeue_time:.4f}秒")

def benchmark_celery(n: int = 1000):
    """Celery基准测试"""
    start = time.time()

    # 提交任务
    results = []
    for i in range(n):
        result = process_task.delay(f'task_{i}', {})
        results.append(result)

    # 等待完成
    for result in results:
        result.get(timeout=10)

    total_time = time.time() - start

    print(f"Celery ({n}个任务):")
    print(f"  总时间: {total_time:.4f}秒")

# 运行测试
benchmark_redis_queue(1000)
benchmark_celery(100)  # Celery较慢,测试较少任务
```

---

## 总结

### 方案对比

| 方案 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| Redis队列 | 简单,快速 | 功能有限 | 简单任务队列 |
| Celery | 成熟,功能丰富 | 配置复杂 | 通用分布式任务 |
| Hatchet | AI工作流优化 | 较新 | AI Agent编排 |
| RabbitMQ | 可靠,灵活 | 运维复杂 | 企业级应用 |

### 选择指南

1. **简单场景** → Redis队列
2. **通用分布式** → Celery
3. **AI工作流** → Hatchet(2025-2026推荐)
4. **企业级** → RabbitMQ

### 关键洞察

- **2025-2026趋势**:Hatchet成为AI工作流首选
- **持久化很重要**:生产环境必须持久化
- **监控必不可少**:任务状态、队列长度、Worker健康
- **容错机制**:重试、死信队列、超时处理

---

**下一步**:学习[07_实战代码_05_生产级实践](./07_实战代码_05_生产级实践.md),了解监控、容错、性能优化等生产级实践。
