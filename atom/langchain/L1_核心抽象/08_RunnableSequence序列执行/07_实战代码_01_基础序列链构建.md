# 实战代码1：基础序列链构建

> 构建一个简单的 RAG 问答链

---

## 场景描述

**目标**：构建一个基于向量数据库的 RAG 问答系统，能够根据用户问题检索相关文档并生成答案。

**技术栈**：
- LangChain 0.3.x
- OpenAI GPT-4o-mini
- ChromaDB 向量数据库
- Python 3.13+

**功能需求**：
1. 加载文档到向量数据库
2. 根据问题检索相关文档
3. 将文档注入到 Prompt
4. 调用 LLM 生成答案
5. 解析输出为文本

---

## 完整代码

```python
"""
基础 RAG 问答链
演示：使用 RunnableSequence 构建简单的 RAG 系统
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document

# 加载环境变量
load_dotenv()

# ===== 1. 准备示例文档 =====
print("=== 步骤1: 准备示例文档 ===\n")

# 创建示例文档（实际应用中从文件加载）
documents = [
    Document(
        page_content="向量数据库是专门用于存储和检索向量数据的数据库。它支持高效的相似度搜索，常用于 RAG 系统中。",
        metadata={"source": "doc1", "topic": "向量数据库"}
    ),
    Document(
        page_content="RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。它先检索相关文档，再基于文档生成答案。",
        metadata={"source": "doc2", "topic": "RAG"}
    ),
    Document(
        page_content="Embedding 是将文本转换为向量的技术。向量可以表示文本的语义，用于计算文本之间的相似度。",
        metadata={"source": "doc3", "topic": "Embedding"}
    ),
    Document(
        page_content="LangChain 是一个用于构建 LLM 应用的框架。它提供了 LCEL 表达式语言，可以用管道操作符组合组件。",
        metadata={"source": "doc4", "topic": "LangChain"}
    ),
    Document(
        page_content="Chunking 是将长文档分割成小块的技术。合适的分块大小可以提升检索质量和生成效果。",
        metadata={"source": "doc5", "topic": "Chunking"}
    )
]

print(f"准备了 {len(documents)} 个示例文档")
for i, doc in enumerate(documents, 1):
    print(f"{i}. {doc.page_content[:50]}...")
print()

# ===== 2. 创建向量数据库 =====
print("=== 步骤2: 创建向量数据库 ===\n")

# 初始化 Embedding 模型
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# 创建向量数据库
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    collection_name="rag_demo"
)

print("向量数据库创建成功")
print(f"存储了 {len(documents)} 个文档的向量")
print()

# ===== 3. 创建检索器 =====
print("=== 步骤3: 创建检索器 ===\n")

# 配置检索器
retriever = vectorstore.as_retriever(
    search_type="similarity",  # 相似度搜索
    search_kwargs={"k": 3}     # 返回前3个最相关的文档
)

print("检索器配置:")
print(f"  搜索类型: similarity")
print(f"  返回数量: 3")
print()

# ===== 4. 创建 Prompt 模板 =====
print("=== 步骤4: 创建 Prompt 模板 ===\n")

prompt = ChatPromptTemplate.from_template("""
你是一个专业的 AI 助手，请根据以下上下文回答问题。

上下文：
{context}

问题：{question}

要求：
1. 只根据上下文回答，不要编造信息
2. 如果上下文中没有相关信息，请说"根据提供的信息无法回答"
3. 回答要简洁明了

答案：
""")

print("Prompt 模板创建成功")
print()

# ===== 5. 创建 LLM =====
print("=== 步骤5: 创建 LLM ===\n")

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0  # 降低随机性，提高一致性
)

print("LLM 配置:")
print(f"  模型: gpt-4o-mini")
print(f"  温度: 0")
print()

# ===== 6. 创建输出解析器 =====
print("=== 步骤6: 创建输出解析器 ===\n")

parser = StrOutputParser()

print("输出解析器创建成功")
print()

# ===== 7. 构建 RAG 链 =====
print("=== 步骤7: 构建 RAG 链 ===\n")

# 使用 RunnableSequence 组合所有组件
rag_chain = (
    {
        "context": retriever,  # 检索相关文档
        "question": RunnablePassthrough()  # 透传问题
    }
    | prompt  # 注入上下文到 Prompt
    | llm     # 调用 LLM 生成答案
    | parser  # 解析输出为文本
)

print("RAG 链构建成功")
print("链结构: retriever | prompt | llm | parser")
print()

# ===== 8. 测试 RAG 链 =====
print("=== 步骤8: 测试 RAG 链 ===\n")

# 测试问题列表
test_questions = [
    "什么是向量数据库？",
    "RAG 是什么？",
    "什么是 Embedding？",
    "LangChain 有什么特点？"
]

# 逐个测试
for i, question in enumerate(test_questions, 1):
    print(f"问题 {i}: {question}")
    print("-" * 60)

    # 调用 RAG 链
    answer = rag_chain.invoke(question)

    print(f"答案: {answer}")
    print()

# ===== 9. 查看检索到的文档 =====
print("=== 步骤9: 查看检索到的文档 ===\n")

# 单独测试检索器
question = "什么是向量数据库？"
retrieved_docs = retriever.invoke(question)

print(f"问题: {question}")
print(f"检索到 {len(retrieved_docs)} 个相关文档:\n")

for i, doc in enumerate(retrieved_docs, 1):
    print(f"文档 {i}:")
    print(f"  内容: {doc.page_content}")
    print(f"  来源: {doc.metadata.get('source', 'unknown')}")
    print(f"  主题: {doc.metadata.get('topic', 'unknown')}")
    print()

# ===== 10. 流式输出测试 =====
print("=== 步骤10: 流式输出测试 ===\n")

question = "请详细解释 RAG 的工作原理"
print(f"问题: {question}")
print("答案（流式）: ", end="", flush=True)

# 流式输出
for chunk in rag_chain.stream(question):
    print(chunk, end="", flush=True)
print("\n")

# ===== 11. 批量处理测试 =====
print("=== 步骤11: 批量处理测试 ===\n")

batch_questions = [
    "什么是 Chunking？",
    "向量数据库的用途是什么？",
    "LangChain 的核心特性是什么？"
]

print(f"批量处理 {len(batch_questions)} 个问题...\n")

# 批量执行
import time
start = time.time()
batch_answers = rag_chain.batch(batch_questions)
duration = time.time() - start

# 显示结果
for question, answer in zip(batch_questions, batch_answers):
    print(f"Q: {question}")
    print(f"A: {answer}")
    print()

print(f"批量处理耗时: {duration:.2f}秒")
print(f"平均每个问题: {duration / len(batch_questions):.2f}秒")
print()

# ===== 12. 性能对比：循环 vs 批处理 =====
print("=== 步骤12: 性能对比 ===\n")

# 循环执行
print("循环执行...")
start = time.time()
for question in batch_questions:
    rag_chain.invoke(question)
loop_duration = time.time() - start

# 批处理
print("批处理执行...")
start = time.time()
rag_chain.batch(batch_questions)
batch_duration = time.time() - start

print(f"\n性能对比:")
print(f"  循环执行: {loop_duration:.2f}秒")
print(f"  批处理: {batch_duration:.2f}秒")
print(f"  加速比: {loop_duration / batch_duration:.2f}x")
print()

# ===== 13. 清理资源 =====
print("=== 步骤13: 清理资源 ===\n")

# 删除向量数据库（可选）
# vectorstore.delete_collection()
print("演示完成！")
```

---

## 运行输出示例

```
=== 步骤1: 准备示例文档 ===

准备了 5 个示例文档
1. 向量数据库是专门用于存储和检索向量数据的数据库。它支持高效的相似度搜索...
2. RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术...
3. Embedding 是将文本转换为向量的技术。向量可以表示文本的语义...
4. LangChain 是一个用于构建 LLM 应用的框架。它提供了 LCEL 表达式语言...
5. Chunking 是将长文档分割成小块的技术。合适的分块大小可以提升检索质量...

=== 步骤2: 创建向量数据库 ===

向量数据库创建成功
存储了 5 个文档的向量

=== 步骤3: 创建检索器 ===

检索器配置:
  搜索类型: similarity
  返回数量: 3

=== 步骤4: 创建 Prompt 模板 ===

Prompt 模板创建成功

=== 步骤5: 创建 LLM ===

LLM 配置:
  模型: gpt-4o-mini
  温度: 0

=== 步骤6: 创建输出解析器 ===

输出解析器创建成功

=== 步骤7: 构建 RAG 链 ===

RAG 链构建成功
链结构: retriever | prompt | llm | parser

=== 步骤8: 测试 RAG 链 ===

问题 1: 什么是向量数据库？
------------------------------------------------------------
答案: 向量数据库是专门用于存储和检索向量数据的数据库，支持高效的相似度搜索，常用于 RAG 系统中。

问题 2: RAG 是什么？
------------------------------------------------------------
答案: RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术，它先检索相关文档，再基于文档生成答案。

问题 3: 什么是 Embedding？
------------------------------------------------------------
答案: Embedding 是将文本转换为向量的技术，向量可以表示文本的语义，用于计算文本之间的相似度。

问题 4: LangChain 有什么特点？
------------------------------------------------------------
答案: LangChain 是一个用于构建 LLM 应用的框架，它提供了 LCEL 表达式语言，可以用管道操作符组合组件。

=== 步骤9: 查看检索到的文档 ===

问题: 什么是向量数据库？
检索到 3 个相关文档:

文档 1:
  内容: 向量数据库是专门用于存储和检索向量数据的数据库。它支持高效的相似度搜索，常用于 RAG 系统中。
  来源: doc1
  主题: 向量数据库

文档 2:
  内容: Embedding 是将文本转换为向量的技术。向量可以表示文本的语义，用于计算文本之间的相似度。
  来源: doc3
  主题: Embedding

文档 3:
  内容: RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。它先检索相关文档，再基于文档生成答案。
  来源: doc2
  主题: RAG

=== 步骤10: 流式输出测试 ===

问题: 请详细解释 RAG 的工作原理
答案（流式）: RAG（Retrieval-Augmented Generation）的工作原理是先通过检索系统从知识库中找到与问题相关的文档，然后将这些文档作为上下文提供给生成模型，最后由生成模型基于检索到的信息生成答案。这种方式结合了检索的准确性和生成的灵活性。

=== 步骤11: 批量处理测试 ===

批量处理 3 个问题...

Q: 什么是 Chunking？
A: Chunking 是将长文档分割成小块的技术，合适的分块大小可以提升检索质量和生成效果。

Q: 向量数据库的用途是什么？
A: 向量数据库用于存储和检索向量数据，支持高效的相似度搜索，常用于 RAG 系统中。

Q: LangChain 的核心特性是什么？
A: LangChain 的核心特性是提供了 LCEL 表达式语言，可以用管道操作符组合组件，方便构建 LLM 应用。

批量处理耗时: 2.35秒
平均每个问题: 0.78秒

=== 步骤12: 性能对比 ===

循环执行...
批处理执行...

性能对比:
  循环执行: 6.82秒
  批处理: 2.41秒
  加速比: 2.83x

=== 步骤13: 清理资源 ===

演示完成！
```

---

## 代码详解

### 1. 文档准备

```python
documents = [
    Document(
        page_content="...",  # 文档内容
        metadata={"source": "doc1", "topic": "..."}  # 元数据
    ),
    # ...
]
```

**关键点**：
- 使用 `Document` 对象包装文档
- `page_content`: 文档内容
- `metadata`: 元数据（来源、主题等）

### 2. 向量数据库创建

```python
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    collection_name="rag_demo"
)
```

**关键点**：
- `from_documents`: 从文档列表创建
- `embedding`: Embedding 模型
- `collection_name`: 集合名称

### 3. RAG 链构建

```python
rag_chain = (
    {
        "context": retriever,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | parser
)
```

**关键点**：
- 字典语法创建 `RunnableParallel`
- `retriever`: 检索相关文档
- `RunnablePassthrough()`: 透传问题
- `|` 操作符连接组件

### 4. 数据流转

```
输入: "什么是向量数据库？"
  ↓
RunnableParallel:
  - retriever.invoke("什么是向量数据库？") → [Document(...), ...]
  - RunnablePassthrough().invoke("什么是向量数据库？") → "什么是向量数据库？"
  ↓
{"context": [Document(...), ...], "question": "什么是向量数据库？"}
  ↓
prompt.invoke({...}) → PromptValue
  ↓
llm.invoke(PromptValue) → AIMessage
  ↓
parser.invoke(AIMessage) → str
  ↓
输出: "向量数据库是..."
```

---

## 2025-2026 最新实践

### 1. 使用最新的 Embedding 模型

**来源**：OpenAI 2025 更新

```python
# ✅ 使用最新的 text-embedding-3-small
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"  # 更快、更便宜
)

# 或使用更强大的模型
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large"  # 更高质量
)
```

### 2. 优化检索策略

```python
# ✅ 使用混合检索（2025 最佳实践）
retriever = vectorstore.as_retriever(
    search_type="mmr",  # Maximum Marginal Relevance
    search_kwargs={
        "k": 3,
        "fetch_k": 10,  # 先取10个，再用 MMR 选3个
        "lambda_mult": 0.5  # 多样性参数
    }
)
```

### 3. 添加错误处理

```python
# ✅ 生产级 RAG 链
production_rag_chain = (
    rag_chain
    .with_retry(stop_after_attempt=3)
    .with_fallbacks([fallback_chain])
)
```

### 4. 集成 LangSmith

```python
import os

# ✅ 启用追踪
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "your_key"

# 自动追踪所有执行
result = rag_chain.invoke(question)
```

---

## 常见问题

### Q1: 如何加载真实文档？

**A**: 使用 LangChain 的文档加载器：

```python
from langchain_community.document_loaders import TextLoader, PyPDFLoader

# 加载文本文件
loader = TextLoader("document.txt")
documents = loader.load()

# 加载 PDF
loader = PyPDFLoader("document.pdf")
documents = loader.load()
```

### Q2: 如何分块长文档？

**A**: 使用文本分割器：

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = splitter.split_documents(documents)
```

### Q3: 如何持久化向量数据库？

**A**: 指定持久化目录：

```python
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory="./chroma_db"  # 持久化目录
)
```

### Q4: 如何提升检索质量？

**A**:
- 使用更好的 Embedding 模型
- 优化分块大小
- 使用混合检索（MMR）
- 添加元数据过滤

---

## 扩展练习

1. **添加文档加载**：从文件加载真实文档
2. **添加分块**：使用文本分割器分块长文档
3. **添加元数据过滤**：根据元数据过滤检索结果
4. **添加重排序**：使用 ReRank 提升检索质量
5. **添加对话历史**：支持多轮对话

---

## 学习检查清单

- [ ] 理解 RAG 链的构建流程
- [ ] 理解 RunnableParallel 的使用
- [ ] 理解数据流转过程
- [ ] 能运行完整代码
- [ ] 能修改代码适应自己的需求
- [ ] 了解 2025-2026 最新实践

---

**版本**: v1.0
**最后更新**: 2026-02-18
**参考来源**:
- LangChain 官方文档
- Towards AI - Master LangChain in 2025
- OpenAI Embeddings 文档
