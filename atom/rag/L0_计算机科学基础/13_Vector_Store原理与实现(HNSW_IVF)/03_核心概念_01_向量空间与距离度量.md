# 核心概念01：向量空间与距离度量

> 理解向量空间的数学基础，掌握三种主流距离度量方法

---

## 概述

向量空间与距离度量是向量存储的数学基础。理解它们是掌握HNSW、IVF等算法的前提。

**核心问题**：如何在高维空间中衡量两个向量的"相似度"？

---

## 1. 向量空间基础

### 1.1 什么是向量空间？

**定义**：向量空间是由向量组成的数学结构，支持向量加法和标量乘法。

**在RAG中的体现**：
```python
# 文本通过Embedding转换为向量
text1 = "Python编程"
text2 = "机器学习"

# 每个文本变成一个高维向量
vector1 = [0.1, 0.9, 0.2, 0.3, ...]  # 768维
vector2 = [0.2, 0.8, 0.3, 0.4, ...]  # 768维

# 这些向量存在于768维空间中
# ℝ⁷⁶⁸ = 768维实数空间
```

**维度的含义**：
- 2维空间：平面（x, y）
- 3维空间：立体（x, y, z）
- 768维空间：无法可视化，但数学上完全定义

---

### 1.2 向量的基本运算

**向量加法**：
```python
import numpy as np

v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

# 向量加法
v_sum = v1 + v2  # [5, 7, 9]
```

**标量乘法**：
```python
v = np.array([1, 2, 3])
scalar = 2

# 标量乘法
v_scaled = scalar * v  # [2, 4, 6]
```

**点积（内积）**：
```python
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

# 点积
dot_product = np.dot(v1, v2)  # 1*4 + 2*5 + 3*6 = 32
```

**向量范数（长度）**：
```python
v = np.array([3, 4])

# L2范数（欧氏长度）
norm = np.linalg.norm(v)  # √(3² + 4²) = 5
```

---

### 1.3 高维空间的特性

**维度诅咒（Curse of Dimensionality）**：

随着维度增加，空间变得"稀疏"，距离失去区分度。

**实验验证**：
```python
import numpy as np
import matplotlib.pyplot as plt

def measure_distance_distribution(n_points=1000, dims=[2, 10, 100, 768]):
    """测量不同维度下距离的分布"""
    results = {}

    for dim in dims:
        # 生成随机向量
        vectors = np.random.randn(n_points, dim)

        # 计算所有点对的距离
        distances = []
        for i in range(100):  # 采样100对
            v1, v2 = vectors[i], vectors[i+1]
            dist = np.linalg.norm(v1 - v2)
            distances.append(dist)

        results[dim] = {
            'mean': np.mean(distances),
            'std': np.std(distances),
            'cv': np.std(distances) / np.mean(distances)  # 变异系数
        }

    # 打印结果
    print("维度诅咒实验结果：")
    print(f"{'维度':<10} {'平均距离':<12} {'标准差':<12} {'变异系数':<12}")
    print("-" * 50)
    for dim, stats in results.items():
        print(f"{dim:<10} {stats['mean']:<12.3f} {stats['std']:<12.3f} {stats['cv']:<12.3f}")

    return results

# 运行实验
results = measure_distance_distribution()
```

**输出：**
```
维度诅咒实验结果：
维度         平均距离        标准差          变异系数
--------------------------------------------------
2          1.414        0.234        0.165
10         3.162        0.412        0.130
100        10.000       0.523        0.052
768        27.713       0.634        0.023
```

**关键发现**：
- 维度越高，变异系数越小
- 高维空间中，所有点距离趋于相同
- 这就是为什么需要ANN算法而非暴力搜索

---

## 2. 三种主流距离度量

### 2.1 欧氏距离（Euclidean Distance）

**定义**：两点之间的直线距离

**数学公式**：
```
d(v1, v2) = √(Σ(v1ᵢ - v2ᵢ)²)
```

**Python实现**：
```python
def euclidean_distance(v1: np.ndarray, v2: np.ndarray) -> float:
    """欧氏距离"""
    return np.linalg.norm(v1 - v2)

# 示例
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

dist = euclidean_distance(v1, v2)
print(f"欧氏距离: {dist:.3f}")  # 5.196
```

**几何直觉**：
```
2维空间：
  v1 = (1, 2)
  v2 = (4, 6)

  距离 = √((4-1)² + (6-2)²)
       = √(9 + 16)
       = 5
```

**优点**：
- ✅ 直观易懂
- ✅ 符合几何直觉
- ✅ 适合未归一化向量

**缺点**：
- ❌ 受向量长度影响
- ❌ 高维空间区分度下降

**适用场景**：
- 图像检索（像素向量）
- 未归一化的Embedding

---

### 2.2 余弦相似度（Cosine Similarity）

**定义**：两向量夹角的余弦值

**数学公式**：
```
cos(θ) = (v1 · v2) / (||v1|| × ||v2||)
```

**取值范围**：[-1, 1]
- 1：完全相同方向
- 0：垂直
- -1：完全相反方向

**Python实现**：
```python
def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    """余弦相似度"""
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    return dot_product / (norm_v1 * norm_v2)

# 示例
v1 = np.array([1, 2, 3])
v2 = np.array([2, 4, 6])  # v1的2倍

similarity = cosine_similarity(v1, v2)
print(f"余弦相似度: {similarity:.3f}")  # 1.000（完全相同方向）

v3 = np.array([1, 0, 0])
v4 = np.array([0, 1, 0])

similarity2 = cosine_similarity(v3, v4)
print(f"余弦相似度: {similarity2:.3f}")  # 0.000（垂直）
```

**几何直觉**：
```
2维空间：
  v1 = (3, 4)  # 长度5
  v2 = (6, 8)  # 长度10，方向相同

  cos(θ) = (3×6 + 4×8) / (5 × 10)
         = 50 / 50
         = 1.0（完全相同方向）
```

**余弦距离**：
```python
def cosine_distance(v1: np.ndarray, v2: np.ndarray) -> float:
    """余弦距离（用于检索）"""
    return 1 - cosine_similarity(v1, v2)

# 距离范围：[0, 2]
# 0：完全相同
# 1：垂直
# 2：完全相反
```

**优点**：
- ✅ 不受向量长度影响（只看方向）
- ✅ 高维空间区分度好
- ✅ 适合归一化向量

**缺点**：
- ❌ 忽略向量长度信息
- ❌ 计算稍复杂

**适用场景**：
- 文本Embedding（OpenAI、BERT）
- 推荐系统（用户向量）
- RAG系统（最常用）

---

### 2.3 内积（Inner Product / Dot Product）

**定义**：两向量对应元素乘积之和

**数学公式**：
```
<v1, v2> = Σ(v1ᵢ × v2ᵢ)
```

**Python实现**：
```python
def inner_product(v1: np.ndarray, v2: np.ndarray) -> float:
    """内积"""
    return np.dot(v1, v2)

# 示例
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

ip = inner_product(v1, v2)
print(f"内积: {ip}")  # 32
```

**与余弦相似度的关系**：
```python
# 对于归一化向量（长度为1）
# 内积 = 余弦相似度

v1_normalized = v1 / np.linalg.norm(v1)
v2_normalized = v2 / np.linalg.norm(v2)

ip_normalized = np.dot(v1_normalized, v2_normalized)
cos_sim = cosine_similarity(v1, v2)

print(f"归一化内积: {ip_normalized:.3f}")
print(f"余弦相似度: {cos_sim:.3f}")
# 两者相等！
```

**优点**：
- ✅ 计算最快（无需开方）
- ✅ 归一化后等价于余弦相似度
- ✅ 适合GPU加速

**缺点**：
- ❌ 必须归一化才有意义
- ❌ 未归一化时受长度影响

**适用场景**：
- 归一化Embedding检索
- GPU加速场景
- FAISS等库的默认选项

---

## 3. 三种度量的对比

### 3.1 实验对比

```python
import numpy as np

def compare_metrics():
    """对比三种距离度量"""

    # 测试向量
    v1 = np.array([1.0, 2.0, 3.0])
    v2 = np.array([2.0, 4.0, 6.0])  # v1的2倍
    v3 = np.array([1.0, 0.0, 0.0])  # 垂直于v2

    print("=== 向量1 vs 向量2（方向相同，长度不同）===")
    print(f"v1: {v1}")
    print(f"v2: {v2} (v1的2倍)")
    print(f"欧氏距离: {euclidean_distance(v1, v2):.3f}")
    print(f"余弦相似度: {cosine_similarity(v1, v2):.3f}")
    print(f"内积: {inner_product(v1, v2):.3f}")

    print("\n=== 向量1 vs 向量3（方向不同）===")
    print(f"v1: {v1}")
    print(f"v3: {v3}")
    print(f"欧氏距离: {euclidean_distance(v1, v3):.3f}")
    print(f"余弦相似度: {cosine_similarity(v1, v3):.3f}")
    print(f"内积: {inner_product(v1, v3):.3f}")

    # 归一化后的内积
    v1_norm = v1 / np.linalg.norm(v1)
    v2_norm = v2 / np.linalg.norm(v2)

    print("\n=== 归一化后 ===")
    print(f"归一化内积: {np.dot(v1_norm, v2_norm):.3f}")
    print(f"余弦相似度: {cosine_similarity(v1, v2):.3f}")
    print("两者相等！")

compare_metrics()
```

**输出：**
```
=== 向量1 vs 向量2（方向相同，长度不同）===
v1: [1. 2. 3.]
v2: [2. 4. 6.] (v1的2倍)
欧氏距离: 3.742
余弦相似度: 1.000
内积: 28.000

=== 向量1 vs 向量3（方向不同）===
v1: [1. 2. 3.]
v3: [1. 0. 0.]
欧氏距离: 3.606
余弦相似度: 0.267
内积: 1.000

=== 归一化后 ===
归一化内积: 1.000
余弦相似度: 1.000
两者相等！
```

---

### 3.2 选择指南

| 度量方法 | 适用场景 | 优点 | 缺点 |
|---------|---------|------|------|
| **欧氏距离** | 图像、未归一化向量 | 直观、几何意义明确 | 受长度影响、高维区分度差 |
| **余弦相似度** | 文本Embedding、RAG | 不受长度影响、高维区分度好 | 计算稍慢 |
| **内积** | 归一化向量、GPU加速 | 计算最快 | 必须归一化 |

**RAG系统推荐**：
- ✅ 余弦相似度（OpenAI、BERT默认）
- ✅ 内积（归一化后，FAISS推荐）

---

## 4. 在RAG系统中的应用

### 4.1 OpenAI Embedding示例

```python
from openai import OpenAI
import numpy as np

client = OpenAI()

def get_embedding(text: str) -> list[float]:
    """获取文本的Embedding"""
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return response.data[0].embedding

# 获取Embedding
query = "如何优化RAG系统？"
doc1 = "RAG系统优化的10个技巧"
doc2 = "前端开发最佳实践"

query_emb = np.array(get_embedding(query))
doc1_emb = np.array(get_embedding(doc1))
doc2_emb = np.array(get_embedding(doc2))

# 计算相似度（OpenAI推荐余弦相似度）
sim1 = cosine_similarity(query_emb, doc1_emb)
sim2 = cosine_similarity(query_emb, doc2_emb)

print(f"查询 vs 文档1: {sim1:.3f}")  # 高相似度
print(f"查询 vs 文档2: {sim2:.3f}")  # 低相似度
```

---

### 4.2 向量数据库配置

**ChromaDB（余弦相似度）**：
```python
import chromadb

client = chromadb.PersistentClient(path="./db")

collection = client.get_or_create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine"}  # 余弦相似度
)
```

**Milvus（内积）**：
```python
from pymilvus import Collection, connections

connections.connect(host="localhost", port="19530")

# 创建索引
index_params = {
    "metric_type": "IP",  # Inner Product（内积）
    "index_type": "HNSW",
    "params": {"M": 16, "efConstruction": 200}
}

collection.create_index(
    field_name="embedding",
    index_params=index_params
)

# 注意：使用内积时，向量必须归一化！
```

**FAISS（内积）**：
```python
import faiss

dim = 768

# 内积索引
index = faiss.IndexFlatIP(dim)

# 添加向量（必须归一化）
vectors_normalized = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
index.add(vectors_normalized)

# 查询
query_normalized = query / np.linalg.norm(query)
distances, indices = index.search(query_normalized, k=10)
```

---

## 5. 高级话题

### 5.1 归一化的重要性

**为什么归一化？**
```python
# 未归一化
v1 = np.array([1, 2, 3])  # 长度 √14 ≈ 3.74
v2 = np.array([10, 20, 30])  # 长度 √1400 ≈ 37.4

# 内积受长度影响
ip = np.dot(v1, v2)  # 140（很大）

# 归一化后
v1_norm = v1 / np.linalg.norm(v1)
v2_norm = v2 / np.linalg.norm(v2)

ip_norm = np.dot(v1_norm, v2_norm)  # 1.0（方向相同）
```

**归一化方法**：
```python
def normalize(v: np.ndarray) -> np.ndarray:
    """L2归一化"""
    return v / np.linalg.norm(v)

# 批量归一化
def normalize_batch(vectors: np.ndarray) -> np.ndarray:
    """批量L2归一化"""
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / norms

# 示例
vectors = np.random.randn(1000, 768)
vectors_normalized = normalize_batch(vectors)

# 验证：每个向量长度为1
norms = np.linalg.norm(vectors_normalized, axis=1)
print(f"归一化后的长度: {norms[:5]}")  # [1. 1. 1. 1. 1.]
```

---

### 5.2 距离与相似度的转换

**余弦距离 ↔ 余弦相似度**：
```python
# 相似度 → 距离
cosine_distance = 1 - cosine_similarity

# 距离 → 相似度
cosine_similarity = 1 - cosine_distance
```

**欧氏距离 ↔ 相似度**：
```python
# 距离 → 相似度（常用转换）
similarity = 1 / (1 + euclidean_distance)

# 或使用指数衰减
similarity = np.exp(-euclidean_distance)
```

---

### 5.3 2025-2026新趋势

**1. 二进制Embedding**：
```python
# 1bit量化，内存↓32倍
binary_embedding = (embedding > 0).astype(np.uint8)

# 汉明距离（XOR计数）
def hamming_distance(b1, b2):
    return np.sum(b1 != b2)
```

**2. Matryoshka Embedding**：
```python
# 可变维度
full_embedding = model.encode(text)  # 1536维

# 截断到不同维度
embedding_768 = full_embedding[:768]
embedding_384 = full_embedding[:384]
embedding_128 = full_embedding[:128]

# 根据场景选择
# 快速筛选：128维
# 精确排序：768维
```

---

## 6. 实战练习

### 练习1：实现三种距离度量

```python
import numpy as np

class DistanceMetrics:
    """距离度量工具类"""

    @staticmethod
    def euclidean(v1, v2):
        """欧氏距离"""
        return np.linalg.norm(v1 - v2)

    @staticmethod
    def cosine_similarity(v1, v2):
        """余弦相似度"""
        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

    @staticmethod
    def cosine_distance(v1, v2):
        """余弦距离"""
        return 1 - DistanceMetrics.cosine_similarity(v1, v2)

    @staticmethod
    def inner_product(v1, v2):
        """内积"""
        return np.dot(v1, v2)

# 测试
v1 = np.random.randn(768)
v2 = np.random.randn(768)

print(f"欧氏距离: {DistanceMetrics.euclidean(v1, v2):.3f}")
print(f"余弦相似度: {DistanceMetrics.cosine_similarity(v1, v2):.3f}")
print(f"余弦距离: {DistanceMetrics.cosine_distance(v1, v2):.3f}")
print(f"内积: {DistanceMetrics.inner_product(v1, v2):.3f}")
```

---

### 练习2：向量检索

```python
class SimpleVectorSearch:
    """简单向量检索"""

    def __init__(self, metric='cosine'):
        self.vectors = []
        self.metadata = []
        self.metric = metric

    def add(self, vector, meta):
        """添加向量"""
        self.vectors.append(vector)
        self.metadata.append(meta)

    def search(self, query, top_k=5):
        """检索Top-K"""
        if not self.vectors:
            return []

        vectors = np.array(self.vectors)

        # 计算距离/相似度
        if self.metric == 'cosine':
            # 归一化
            query_norm = query / np.linalg.norm(query)
            vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
            # 余弦相似度
            scores = np.dot(vectors_norm, query_norm)
            # 降序排序
            top_indices = np.argsort(scores)[::-1][:top_k]
        elif self.metric == 'euclidean':
            # 欧氏距离
            distances = np.linalg.norm(vectors - query, axis=1)
            # 升序排序
            top_indices = np.argsort(distances)[:top_k]
            scores = 1 / (1 + distances[top_indices])

        results = [
            (scores[i] if self.metric == 'cosine' else scores[i], self.metadata[idx])
            for i, idx in enumerate(top_indices)
        ]

        return results

# 使用
search = SimpleVectorSearch(metric='cosine')

# 添加文档
search.add(np.random.randn(768), {"text": "Python编程"})
search.add(np.random.randn(768), {"text": "机器学习"})
search.add(np.random.randn(768), {"text": "前端开发"})

# 查询
query = np.random.randn(768)
results = search.search(query, top_k=2)

for score, meta in results:
    print(f"得分: {score:.3f}, 文档: {meta['text']}")
```

---

## 总结

### 核心要点

1. **向量空间**：高维空间中的点，支持加法和标量乘法
2. **欧氏距离**：直线距离，适合未归一化向量
3. **余弦相似度**：方向相似度，RAG系统最常用
4. **内积**：归一化后等价于余弦相似度，计算最快
5. **维度诅咒**：高维空间距离失去区分度
6. **归一化**：使用内积时必须归一化

### RAG系统推荐

- ✅ 余弦相似度（OpenAI、BERT默认）
- ✅ 内积（归一化后，FAISS推荐）
- ✅ 向量必须归一化（使用内积时）

### 下一步

学习 `03_核心概念_02_ANN近似最近邻算法概览.md`，了解如何高效检索。
