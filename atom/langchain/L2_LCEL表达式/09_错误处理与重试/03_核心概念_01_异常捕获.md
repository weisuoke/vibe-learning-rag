# 核心概念1：异常捕获

## 概述

**异常捕获是 LCEL 错误处理的基础，理解异常的传播机制是构建可靠 AI 应用的前提。**

在 LCEL 中，异常捕获不仅仅是简单的 try-except，而是一个完整的异常处理体系，包括：
- 异常的传播机制
- Runnable 协议的异常处理
- 自动重试的触发条件
- 异常类型的分类和过滤

---

## 1. LCEL 异常处理机制

### 1.1 Runnable 协议的异常传播

**核心原理：** LCEL 链中的异常会沿着管道向上传播，直到被捕获或到达顶层

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 构建一个简单的链
prompt = ChatPromptTemplate.from_template("告诉我关于{topic}的信息")
llm = ChatOpenAI(model="gpt-4")
parser = StrOutputParser()

chain = prompt | llm | parser

# 异常传播示例
try:
    result = chain.invoke({"topic": "Python"})
except Exception as e:
    print(f"捕获到异常: {type(e).__name__}: {e}")
```

**异常传播路径：**
```
prompt.invoke() → 成功
    ↓
llm.invoke() → RateLimitError ❌
    ↓
异常向上传播（跳过 parser）
    ↓
被顶层 try-except 捕获
```

### 1.2 异常类型层次结构

**LangChain 的异常体系：**

```python
# LangChain 核心异常
from langchain_core.exceptions import (
    OutputParserException,  # 输出解析错误
)

# OpenAI 特定异常
from openai import (
    APIError,              # API 错误
    RateLimitError,        # 速率限制
    APIConnectionError,    # 连接错误
    AuthenticationError,   # 认证错误
    InvalidRequestError,   # 请求参数错误
    Timeout,               # 超时
)

# 异常层次结构
"""
Exception
├── OpenAIError
│   ├── APIError (可重试)
│   ├── RateLimitError (可重试)
│   ├── APIConnectionError (可重试)
│   ├── Timeout (可重试)
│   ├── AuthenticationError (不可重试)
│   └── InvalidRequestError (不可重试)
└── LangChainException
    ├── OutputParserException (可重试)
    └── ...
"""
```

### 1.3 异常捕获的三个层次

**层次1：组件级捕获**
```python
# 在单个组件上捕获异常
try:
    response = llm.invoke("你好")
except RateLimitError:
    print("速率限制，等待后重试")
except APIError as e:
    print(f"API 错误: {e}")
```

**层次2：链级捕获**
```python
# 在整个链上捕获异常
try:
    result = chain.invoke({"topic": "Python"})
except Exception as e:
    print(f"链执行失败: {e}")
```

**层次3：应用级捕获**
```python
# 在应用层捕获所有异常
def safe_invoke(chain, input_data):
    try:
        return chain.invoke(input_data)
    except RateLimitError:
        return {"error": "服务繁忙，请稍后重试"}
    except AuthenticationError:
        return {"error": "认证失败，请检查 API key"}
    except Exception as e:
        return {"error": f"未知错误: {e}"}
```

---

## 2. try-except 在 LCEL 中的应用

### 2.1 基础异常捕获

**示例1：捕获特定异常**

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError
import time

llm = ChatOpenAI(model="gpt-4")

def invoke_with_manual_retry(llm, prompt, max_retries=3):
    """手动实现重试逻辑"""
    for attempt in range(max_retries):
        try:
            return llm.invoke(prompt)
        except RateLimitError as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # 指数退避
                print(f"速率限制，等待 {wait_time} 秒后重试...")
                time.sleep(wait_time)
            else:
                raise  # 最后一次重试失败，抛出异常
        except APIError as e:
            if attempt < max_retries - 1:
                print(f"API 错误，重试中...")
                time.sleep(1)
            else:
                raise

# 使用
try:
    response = invoke_with_manual_retry(llm, "你好")
    print(response.content)
except Exception as e:
    print(f"最终失败: {e}")
```

### 2.2 异常分类处理

**示例2：根据异常类型采取不同策略**

```python
from langchain_openai import ChatOpenAI
from openai import (
    RateLimitError,
    APIError,
    AuthenticationError,
    InvalidRequestError,
    Timeout
)

llm = ChatOpenAI(model="gpt-4")

def smart_invoke(llm, prompt):
    """智能异常处理"""
    try:
        return llm.invoke(prompt)

    except RateLimitError as e:
        # 可重试：速率限制
        print("速率限制，建议使用 with_retry()")
        raise

    except Timeout as e:
        # 可重试：超时
        print("请求超时，建议使用 with_retry()")
        raise

    except APIError as e:
        # 可重试：API 错误
        print("API 错误，建议使用 with_retry()")
        raise

    except AuthenticationError as e:
        # 不可重试：认证错误
        print("认证失败，请检查 API key")
        return {"error": "认证失败"}

    except InvalidRequestError as e:
        # 不可重试：请求参数错误
        print("请求参数错误，请检查输入")
        return {"error": "参数错误"}

    except Exception as e:
        # 未知错误
        print(f"未知错误: {type(e).__name__}: {e}")
        raise

# 使用
result = smart_invoke(llm, "你好")
```

### 2.3 链式异常捕获

**示例3：在 LCEL 链中捕获异常**

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnableLambda

# 自定义异常处理函数
def handle_llm_error(error):
    """处理 LLM 错误"""
    print(f"LLM 错误: {error}")
    return {"error": "LLM 调用失败"}

def handle_parser_error(error):
    """处理解析错误"""
    print(f"解析错误: {error}")
    return {"error": "输出解析失败"}

# 构建链
prompt = ChatPromptTemplate.from_template("以 JSON 格式返回: {query}")
llm = ChatOpenAI(model="gpt-4")
parser = JsonOutputParser()

# 方式1：在链外捕获
try:
    chain = prompt | llm | parser
    result = chain.invoke({"query": "Python 的特点"})
except Exception as e:
    print(f"链执行失败: {e}")

# 方式2：使用 RunnableLambda 包装错误处理
def safe_llm_invoke(input):
    try:
        return llm.invoke(input)
    except Exception as e:
        return handle_llm_error(e)

def safe_parser_invoke(input):
    try:
        return parser.invoke(input)
    except Exception as e:
        return handle_parser_error(e)

chain_with_error_handling = (
    prompt
    | RunnableLambda(safe_llm_invoke)
    | RunnableLambda(safe_parser_invoke)
)
```

---

## 3. RunnableRetry 基础

### 3.1 RunnableRetry 的工作原理

**核心概念：** `with_retry()` 返回一个 `RunnableRetry` 包装器，它会自动捕获异常并重试

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

# with_retry() 返回 RunnableRetry 实例
llm_with_retry = llm.with_retry(stop_after_attempt=3)

print(type(llm_with_retry))  # <class 'langchain_core.runnables.retry.RunnableRetry'>
```

**内部机制：**
```python
# RunnableRetry 的简化实现（概念性）
class RunnableRetry:
    def __init__(self, runnable, retry_config):
        self.runnable = runnable
        self.retry_config = retry_config

    def invoke(self, input):
        for attempt in range(self.retry_config.max_attempts):
            try:
                return self.runnable.invoke(input)
            except Exception as e:
                if self._should_retry(e, attempt):
                    self._wait_before_retry(attempt)
                else:
                    raise
        raise Exception("Max retries exceeded")

    def _should_retry(self, exception, attempt):
        # 检查是否应该重试
        if attempt >= self.retry_config.max_attempts - 1:
            return False
        if self.retry_config.retry_if_exception_type:
            return isinstance(exception, self.retry_config.retry_if_exception_type)
        return True

    def _wait_before_retry(self, attempt):
        # 指数退避
        wait_time = 2 ** attempt
        time.sleep(wait_time)
```

### 3.2 with_retry() 的配置参数

**完整参数列表：**

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError

llm = ChatOpenAI(model="gpt-4").with_retry(
    # 基础参数
    stop_after_attempt=3,              # 最大重试次数

    # 等待策略
    wait_exponential_jitter=True,      # 指数退避 + 随机抖动

    # 异常过滤
    retry_if_exception_type=(          # 只重试这些异常
        RateLimitError,
        APIError
    ),

    # 高级参数（基于 tenacity 库）
    # stop=stop_after_attempt(3),      # 停止条件
    # wait=wait_exponential(multiplier=1, min=1, max=60),  # 等待策略
    # retry=retry_if_exception_type((RateLimitError,)),    # 重试条件
    # before_sleep=before_sleep_log(logger, logging.WARNING),  # 重试前回调
)
```

**参数详解：**

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `stop_after_attempt` | int | 3 | 最大重试次数 |
| `wait_exponential_jitter` | bool | False | 是否使用指数退避 + 抖动 |
| `retry_if_exception_type` | tuple | None | 只重试指定类型的异常 |

### 3.3 RunnableRetry 的实际应用

**示例4：基础重试**

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4")

# 添加重试
llm_with_retry = llm.with_retry(stop_after_attempt=3)

# 使用
response = llm_with_retry.invoke("你好")
print(response.content)

# 执行流程：
# 第1次：尝试调用 → 可能失败
# 第2次：重试 → 可能失败
# 第3次：重试 → 成功或抛出异常
```

**示例5：指数退避重试**

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=4,
    wait_exponential_jitter=True
)

# 执行流程：
# 第1次失败：等待 ~1 秒（1 ± jitter）
# 第2次失败：等待 ~2 秒（2 ± jitter）
# 第3次失败：等待 ~4 秒（4 ± jitter）
# 第4次失败：抛出异常
```

**示例6：异常类型过滤**

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError, AuthenticationError

llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    retry_if_exception_type=(RateLimitError, APIError)
)

# 场景1：RateLimitError → 重试 ✅
# 场景2：APIError → 重试 ✅
# 场景3：AuthenticationError → 立即失败 ❌（不在重试列表中）
```

---

## 4. 实际应用场景

### 4.1 场景1：处理 API 速率限制

**问题：** OpenAI API 有每分钟请求限制

**解决方案：**

```python
from langchain_openai import ChatOpenAI
from openai import RateLimitError
import logging

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 创建带重试的 LLM
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,              # 速率限制可能需要更多重试
    wait_exponential_jitter=True,      # 指数退避避免雪崩
    retry_if_exception_type=(RateLimitError,)  # 只重试速率限制
)

# 批量处理
prompts = ["问题1", "问题2", "问题3", "..."]

results = []
for i, prompt in enumerate(prompts):
    try:
        response = llm.invoke(prompt)
        results.append(response.content)
        logger.info(f"成功处理第 {i+1} 个请求")
    except RateLimitError as e:
        logger.error(f"第 {i+1} 个请求失败（速率限制）: {e}")
        results.append(None)
    except Exception as e:
        logger.error(f"第 {i+1} 个请求失败: {e}")
        results.append(None)

print(f"成功: {len([r for r in results if r])}/{len(prompts)}")
```

### 4.2 场景2：处理网络超时

**问题：** 网络不稳定导致请求超时

**解决方案：**

```python
from langchain_openai import ChatOpenAI
from openai import Timeout, APIConnectionError

# 创建带超时重试的 LLM
llm = ChatOpenAI(
    model="gpt-4",
    timeout=30  # 设置超时时间
).with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True,
    retry_if_exception_type=(Timeout, APIConnectionError)
)

# 使用
try:
    response = llm.invoke("复杂的问题，可能需要较长时间")
    print(response.content)
except Timeout:
    print("请求超时，即使重试也失败了")
except APIConnectionError:
    print("网络连接失败")
```

### 4.3 场景3：处理输出解析错误

**问题：** LLM 输出格式不符合预期，解析失败

**解决方案：**

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException

# 创建链
prompt = ChatPromptTemplate.from_template(
    "以 JSON 格式返回: {query}\n"
    "格式: {{\"answer\": \"...\", \"confidence\": 0.0-1.0}}"
)
llm = ChatOpenAI(model="gpt-4")
parser = JsonOutputParser()

# 为解析器添加重试
parser_with_retry = parser.with_retry(
    stop_after_attempt=2,
    retry_if_exception_type=(OutputParserException,)
)

chain = prompt | llm | parser_with_retry

# 使用
try:
    result = chain.invoke({"query": "Python 的优点"})
    print(result)
except OutputParserException:
    print("解析失败，LLM 输出格式不正确")
```

### 4.4 场景4：RAG 系统的异常处理

**问题：** RAG 系统有多个可能失败的环节

**解决方案：**

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from openai import RateLimitError, APIError, Timeout

# 1. 向量检索器（可能超时）
embeddings = OpenAIEmbeddings().with_retry(
    stop_after_attempt=3,
    retry_if_exception_type=(Timeout, APIConnectionError)
)

vectorstore = Chroma(embedding_function=embeddings)
retriever = vectorstore.as_retriever()

# 2. LLM（可能速率限制）
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,
    wait_exponential_jitter=True,
    retry_if_exception_type=(RateLimitError, APIError)
)

# 3. 构建 RAG 链
prompt = ChatPromptTemplate.from_template(
    "基于以下上下文回答问题:\n{context}\n\n问题: {question}"
)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

# 4. 使用（带应用级异常处理）
def safe_rag_query(question):
    try:
        return rag_chain.invoke(question)
    except Timeout:
        return {"error": "检索超时，请稍后重试"}
    except RateLimitError:
        return {"error": "服务繁忙，请稍后重试"}
    except Exception as e:
        return {"error": f"查询失败: {e}"}

# 测试
result = safe_rag_query("什么是 LangChain？")
print(result)
```

---

## 5. 最佳实践

### 5.1 异常捕获的黄金法则

1. **分层捕获**：组件级 + 链级 + 应用级
2. **异常分类**：区分可重试和不可重试的异常
3. **快速失败**：不可重试的异常应该立即失败
4. **日志记录**：记录所有异常，便于调试
5. **用户友好**：向用户返回友好的错误信息

### 5.2 常见错误类型分类

**可重试的错误（临时故障）：**
- `RateLimitError`：速率限制
- `APIError`：API 服务器错误
- `Timeout`：超时
- `APIConnectionError`：连接错误
- `OutputParserException`：解析错误（可能是 LLM 输出不稳定）

**不可重试的错误（永久故障）：**
- `AuthenticationError`：认证错误
- `InvalidRequestError`：请求参数错误
- `NotFoundError`：资源不存在

### 5.3 异常处理模板

```python
from langchain_openai import ChatOpenAI
from openai import (
    RateLimitError,
    APIError,
    Timeout,
    APIConnectionError,
    AuthenticationError,
    InvalidRequestError
)

# 定义可重试的异常
RETRYABLE_ERRORS = (
    RateLimitError,
    APIError,
    Timeout,
    APIConnectionError
)

# 创建带智能重试的 LLM
llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True,
    retry_if_exception_type=RETRYABLE_ERRORS
)

# 应用级异常处理
def safe_invoke(llm, prompt):
    try:
        return llm.invoke(prompt)
    except AuthenticationError:
        return {"error": "认证失败，请检查 API key"}
    except InvalidRequestError as e:
        return {"error": f"请求参数错误: {e}"}
    except Exception as e:
        return {"error": f"未知错误: {e}"}
```

---

## 总结

**异常捕获的三个关键点：**

1. **理解异常传播**：LCEL 链中的异常会向上传播
2. **分类处理异常**：区分可重试和不可重试的异常
3. **使用 RunnableRetry**：声明式的重试机制

**记住：异常捕获不是"防御性编程"，而是"主动设计"。**
