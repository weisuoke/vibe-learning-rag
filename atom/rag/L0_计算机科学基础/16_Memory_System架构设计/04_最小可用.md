# 最小可用知识

> **20%核心知识解决80%问题**

本文档提炼 Memory System 的核心概念，让你快速掌握 AI Agent 开发中最常用的记忆管理技术。

---

## 学习目标

掌握以下5个核心概念后，你就能：
- ✅ 实现基本的缓存系统
- ✅ 手写 LRU 缓存算法
- ✅ 理解多级缓存架构
- ✅ 实现简单的持久化策略
- ✅ 在 AI Agent 中应用记忆管理

---

## 核心概念 1：缓存的本质

### 一句话理解
**缓存 = 用空间换时间，把常用数据放在快速存储层**

### 为什么需要缓存？

```python
import time

# 模拟慢速数据源（数据库、API）
def slow_data_source(key: str) -> str:
    time.sleep(1)  # 模拟1秒延迟
    return f"Data for {key}"

# 没有缓存：每次都要等1秒
start = time.time()
data1 = slow_data_source("user_123")
data2 = slow_data_source("user_123")  # 重复请求
print(f"无缓存耗时: {time.time() - start:.2f}秒")  # 输出: 2.00秒

# 有缓存：第二次直接返回
cache = {}

def get_data_with_cache(key: str) -> str:
    if key in cache:
        return cache[key]  # 缓存命中，立即返回

    data = slow_data_source(key)
    cache[key] = data
    return data

start = time.time()
data1 = get_data_with_cache("user_123")
data2 = get_data_with_cache("user_123")  # 从缓存读取
print(f"有缓存耗时: {time.time() - start:.2f}秒")  # 输出: 1.00秒
```

### AI Agent 应用场景

```python
from openai import OpenAI

client = OpenAI()
semantic_cache = {}  # 语义缓存

def ask_llm_with_cache(question: str) -> str:
    """带缓存的 LLM 调用"""
    # 简化版：用问题本身作为缓存键
    if question in semantic_cache:
        print("✅ 缓存命中，节省 API 调用")
        return semantic_cache[question]

    # 缓存未命中，调用 LLM
    print("❌ 缓存未命中，调用 LLM")
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": question}]
    )

    answer = response.choices[0].message.content
    semantic_cache[question] = answer
    return answer

# 第一次调用：需要调用 LLM
answer1 = ask_llm_with_cache("什么是 RAG？")

# 第二次调用：直接从缓存返回
answer2 = ask_llm_with_cache("什么是 RAG？")
```

**效果**：
- 第一次调用：耗时 2-3 秒，花费 $0.01
- 第二次调用：耗时 < 1ms，花费 $0
- **成本节省：100%**

---

## 核心概念 2：LRU 缓存算法

### 一句话理解
**LRU = 淘汰最久未使用的数据，保留最近使用的数据**

### 最简单的实现（Python OrderedDict）

```python
from collections import OrderedDict

class LRUCache:
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity

    def get(self, key: str) -> str | None:
        """获取数据，并标记为最近使用"""
        if key not in self.cache:
            return None

        # 移到末尾（标记为最近使用）
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key: str, value: str) -> None:
        """存入数据"""
        if key in self.cache:
            # 更新已有数据
            self.cache.move_to_end(key)
        else:
            # 新增数据
            if len(self.cache) >= self.capacity:
                # 缓存已满，删除最久未使用的（第一个）
                self.cache.popitem(last=False)

        self.cache[key] = value

# 使用示例
cache = LRUCache(capacity=3)

cache.put("a", "数据A")
cache.put("b", "数据B")
cache.put("c", "数据C")
print(list(cache.cache.keys()))  # ['a', 'b', 'c']

cache.get("a")  # 访问 a，a 变成最近使用
print(list(cache.cache.keys()))  # ['b', 'c', 'a']

cache.put("d", "数据D")  # 缓存满了，淘汰 b
print(list(cache.cache.keys()))  # ['c', 'a', 'd']
```

### 时间复杂度
- `get(key)`: O(1)
- `put(key, value)`: O(1)

### AI Agent 应用：对话历史管理

```python
class ConversationMemory:
    """对话历史记忆（LRU 策略）"""
    def __init__(self, max_turns: int = 10):
        self.memory = LRUCache(capacity=max_turns)
        self.turn = 0

    def add_message(self, role: str, content: str):
        """添加对话消息"""
        self.turn += 1
        self.memory.put(f"turn_{self.turn}", {
            "role": role,
            "content": content
        })

    def get_recent_messages(self, n: int = 5) -> list:
        """获取最近 n 条消息"""
        all_messages = list(self.memory.cache.values())
        return all_messages[-n:]

# 使用示例
memory = ConversationMemory(max_turns=5)

memory.add_message("user", "你好")
memory.add_message("assistant", "你好！有什么可以帮你的？")
memory.add_message("user", "什么是 RAG？")
memory.add_message("assistant", "RAG 是检索增强生成...")

# 获取最近3条消息作为上下文
recent = memory.get_recent_messages(n=3)
print(f"最近3条消息: {len(recent)} 条")
```

---

## 核心概念 3：多级缓存架构

### 一句话理解
**多级缓存 = 像计算机的 CPU 缓存一样，分层存储数据**

### 4层架构

```
L1: 内存缓存（最快，最小）  ← 毫秒级，KB-MB
L2: Redis 缓存（快，中等）   ← 10ms级，MB-GB
L3: 向量数据库（中等，大）   ← 100ms级，GB-TB
L4: 持久化存储（慢，最大）   ← 秒级，TB+
```

### 简化实现

```python
from typing import Any

class MultiLevelCache:
    """简化的多级缓存"""
    def __init__(self):
        self.l1_cache = {}  # 内存缓存
        self.l2_cache = {}  # 模拟 Redis
        self.l3_storage = {}  # 模拟持久化存储

    def get(self, key: str) -> Any | None:
        """从缓存获取数据（L1 → L2 → L3）"""
        # L1: 内存缓存
        if key in self.l1_cache:
            print(f"✅ L1 命中: {key}")
            return self.l1_cache[key]

        # L2: Redis 缓存
        if key in self.l2_cache:
            print(f"✅ L2 命中: {key}")
            # 提升到 L1
            self.l1_cache[key] = self.l2_cache[key]
            return self.l2_cache[key]

        # L3: 持久化存储
        if key in self.l3_storage:
            print(f"✅ L3 命中: {key}")
            # 提升到 L2 和 L1
            data = self.l3_storage[key]
            self.l2_cache[key] = data
            self.l1_cache[key] = data
            return data

        print(f"❌ 缓存未命中: {key}")
        return None

    def put(self, key: str, value: Any):
        """写入数据（写穿策略：同时写入所有层）"""
        self.l1_cache[key] = value
        self.l2_cache[key] = value
        self.l3_storage[key] = value
        print(f"✅ 写入所有层: {key}")

# 使用示例
cache = MultiLevelCache()

cache.put("user_123", {"name": "Alice", "age": 25})

# 第一次读取：从 L1 读取
data1 = cache.get("user_123")

# 清空 L1，模拟缓存失效
cache.l1_cache.clear()

# 第二次读取：从 L2 读取，并提升到 L1
data2 = cache.get("user_123")

# 第三次读取：从 L1 读取
data3 = cache.get("user_123")
```

**输出**：
```
✅ 写入所有层: user_123
✅ L1 命中: user_123
✅ L2 命中: user_123
✅ L1 命中: user_123
```

---

## 核心概念 4：持久化策略

### 一句话理解
**持久化 = 把内存数据保存到磁盘，防止数据丢失**

### 两种策略

| 策略 | 特点 | 适用场景 |
|------|------|----------|
| **Write-through**（写穿） | 写入时立即持久化 | 关键数据（用户偏好、支付信息） |
| **Write-back**（写回） | 批量延迟写入 | 非关键数据（访问日志、统计数据） |

### 代码实现

```python
import json
import time
from pathlib import Path

class PersistentCache:
    """带持久化的缓存"""
    def __init__(self, storage_path: str, strategy: str = "write-through"):
        self.cache = {}
        self.storage_path = Path(storage_path)
        self.strategy = strategy
        self.dirty_keys = set()  # 记录未持久化的键

        # 启动时加载数据
        self._load_from_disk()

    def get(self, key: str) -> Any | None:
        return self.cache.get(key)

    def put(self, key: str, value: Any):
        """写入数据"""
        self.cache[key] = value

        if self.strategy == "write-through":
            # 立即持久化
            self._save_to_disk()
            print(f"✅ 立即持久化: {key}")
        else:
            # 标记为脏数据，稍后批量写入
            self.dirty_keys.add(key)
            print(f"⏳ 标记为待持久化: {key}")

    def flush(self):
        """批量持久化（Write-back 策略）"""
        if self.dirty_keys:
            self._save_to_disk()
            print(f"✅ 批量持久化: {len(self.dirty_keys)} 个键")
            self.dirty_keys.clear()

    def _save_to_disk(self):
        """保存到磁盘"""
        with open(self.storage_path, 'w') as f:
            json.dump(self.cache, f)

    def _load_from_disk(self):
        """从磁盘加载"""
        if self.storage_path.exists():
            with open(self.storage_path, 'r') as f:
                self.cache = json.load(f)
            print(f"✅ 从磁盘加载: {len(self.cache)} 个键")

# Write-through 示例
cache1 = PersistentCache("cache_wt.json", strategy="write-through")
cache1.put("key1", "value1")  # 立即写入磁盘
cache1.put("key2", "value2")  # 立即写入磁盘

# Write-back 示例
cache2 = PersistentCache("cache_wb.json", strategy="write-back")
cache2.put("key1", "value1")  # 仅标记
cache2.put("key2", "value2")  # 仅标记
cache2.flush()  # 批量写入磁盘
```

---

## 核心概念 5：语义缓存（2025-2026 新趋势）

### 一句话理解
**语义缓存 = 用向量相似度匹配，而不是精确字符串匹配**

### 为什么需要语义缓存？

传统缓存的问题：
```python
cache = {}
cache["什么是 RAG？"] = "RAG 是检索增强生成..."

# 问题：稍微改变问法就无法命中缓存
result1 = cache.get("什么是 RAG？")  # ✅ 命中
result2 = cache.get("RAG 是什么？")  # ❌ 未命中（但语义相同！）
result3 = cache.get("请解释 RAG")    # ❌ 未命中（但语义相同！）
```

### 语义缓存解决方案

```python
from openai import OpenAI
import numpy as np

client = OpenAI()

class SemanticCache:
    """语义缓存（基于向量相似度）"""
    def __init__(self, similarity_threshold: float = 0.95):
        self.cache = []  # [(embedding, question, answer), ...]
        self.threshold = similarity_threshold

    def _get_embedding(self, text: str) -> np.ndarray:
        """获取文本的向量表示"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding)

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """计算余弦相似度"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    def get(self, question: str) -> str | None:
        """查询缓存（基于语义相似度）"""
        if not self.cache:
            return None

        # 计算问题的向量
        query_embedding = self._get_embedding(question)

        # 查找最相似的缓存项
        best_match = None
        best_similarity = 0.0

        for cached_embedding, cached_question, cached_answer in self.cache:
            similarity = self._cosine_similarity(query_embedding, cached_embedding)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = (cached_question, cached_answer)

        # 如果相似度超过阈值，返回缓存结果
        if best_similarity >= self.threshold:
            print(f"✅ 语义缓存命中 (相似度: {best_similarity:.3f})")
            print(f"   原问题: {best_match[0]}")
            print(f"   新问题: {question}")
            return best_match[1]

        return None

    def put(self, question: str, answer: str):
        """存入缓存"""
        embedding = self._get_embedding(question)
        self.cache.append((embedding, question, answer))

# 使用示例
semantic_cache = SemanticCache(similarity_threshold=0.95)

# 第一次提问
semantic_cache.put("什么是 RAG？", "RAG 是检索增强生成...")

# 不同问法，但语义相同
result1 = semantic_cache.get("RAG 是什么？")  # ✅ 命中！
result2 = semantic_cache.get("请解释 RAG")    # ✅ 命中！
result3 = semantic_cache.get("什么是向量数据库？")  # ❌ 未命中（语义不同）
```

**效果**：
- 传统缓存命中率：~30%（只有完全相同的问题才命中）
- 语义缓存命中率：~80%（相似问题都能命中）
- **成本节省：80% 的 LLM 调用**

---

## 快速上手检查清单

完成以下任务，确保你掌握了核心知识：

- [ ] 实现一个简单的字典缓存
- [ ] 使用 `OrderedDict` 实现 LRU 缓存
- [ ] 理解多级缓存的数据流动（L1 → L2 → L3）
- [ ] 实现 Write-through 持久化策略
- [ ] 理解语义缓存的原理（向量相似度）

---

## 下一步学习

掌握了这 5 个核心概念后，你可以：

1. **深入学习**：阅读 `03_核心概念_*.md` 系列文档
2. **实战练习**：完成 `07_实战代码_*.md` 中的代码示例
3. **面试准备**：学习 `08_面试必问.md` 中的经典问题
4. **生产应用**：参考 2025-2026 最新的 LangGraph 和 Redis 记忆系统

---

## 参考资源

### 2025-2026 最新研究
- [Redis AI Agent Memory](https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis) - 语义缓存实战
- [LangGraph Memory Documentation](https://docs.langchain.com/oss/python/langgraph/add-memory) - 跨会话记忆管理

### 经典资源
- [LeetCode 146: LRU Cache](https://leetcode.com/problems/lru-cache/) - LRU 算法实现
- [Python OrderedDict Documentation](https://docs.python.org/3/library/collections.html#collections.OrderedDict) - 官方文档
