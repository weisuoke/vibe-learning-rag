# æ ¸å¿ƒæ¦‚å¿µ3ï¼šè¯­ä¹‰åˆ†å—ï¼ˆSemantic Chunkingï¼‰

**åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åˆ‡åˆ†ï¼Œåœ¨è¯­ä¹‰å˜åŒ–çš„åœ°æ–¹åˆ‡åˆ†ï¼Œä¿è¯æ¯ä¸ªå—å†…å®¹è¯­ä¹‰ä¸€è‡´ã€‚**

---

## ä¸€å¥è¯å®šä¹‰

**è¯­ä¹‰åˆ†å—æ˜¯é€šè¿‡è®¡ç®—æ–‡æœ¬ç‰‡æ®µçš„ embedding ç›¸ä¼¼åº¦æ¥ç¡®å®šåˆ†å—è¾¹ç•Œçš„æ–¹æ³•ï¼Œåœ¨è¯­ä¹‰å‘ç”Ÿæ˜¾è‘—å˜åŒ–çš„åœ°æ–¹åˆ‡åˆ†ï¼Œç¡®ä¿æ¯ä¸ªå—å†…å®¹ä¸»é¢˜ä¸€è‡´ï¼Œæ˜¯è´¨é‡æœ€é«˜ä½†æˆæœ¬ä¹Ÿæœ€é«˜çš„åˆ†å—ç­–ç•¥ã€‚**

---

## æ ¸å¿ƒåŸç†

### åŸºæœ¬æ€æƒ³

```
ä¼ ç»Ÿåˆ†å—ï¼šæŒ‰å›ºå®šè§„åˆ™åˆ‡åˆ†
â†’ å¯èƒ½åœ¨ä¸»é¢˜ä¸­é—´åˆ‡æ–­

è¯­ä¹‰åˆ†å—ï¼šæŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦åˆ‡åˆ†
â†’ åœ¨ä¸»é¢˜è½¬æ¢ç‚¹åˆ‡åˆ†

ç¤ºä¾‹ï¼š
"Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ã€‚å®ƒç®€å•æ˜“å­¦ã€‚"  â† ç›¸ä¼¼åº¦é«˜ï¼ˆ0.85ï¼‰
                                      â†“ ä¸åˆ‡åˆ†
"æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯ã€‚æ·±åº¦å­¦ä¹ å¾ˆæµè¡Œã€‚" â† ç›¸ä¼¼åº¦é«˜ï¼ˆ0.82ï¼‰
                                      â†“ åˆ‡åˆ†ï¼ï¼ˆç›¸ä¼¼åº¦ä½ 0.45ï¼‰
"ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚æˆ‘ä»¬å»å…¬å›­ç©å§ã€‚"     â† ä¸»é¢˜å˜åŒ–
```

### ç®—æ³•æµç¨‹

```
1. æŒ‰å¥å­åˆ‡åˆ†æ–‡æœ¬
   â†“
2. è®¡ç®—æ¯ä¸ªå¥å­çš„ embedding
   â†“
3. è®¡ç®—ç›¸é‚»å¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦
   â†“
4. åœ¨ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼çš„åœ°æ–¹åˆ‡åˆ†
   â†“
5. åˆå¹¶ç›¸é‚»çš„é«˜ç›¸ä¼¼åº¦å¥å­
   â†“
6. è¿”å›è¯­ä¹‰ä¸€è‡´çš„å—
```

---

## Python å®ç°

### åŸºç¡€å®ç°

```python
import numpy as np
from typing import List, Callable
import re

def semantic_chunk(
    text: str,
    embed_func: Callable[[str], List[float]],
    threshold: float = 0.5,
    min_chunk_size: int = 100
) -> List[str]:
    """
    è¯­ä¹‰åˆ†å—

    åŸç†ï¼š
    1. å…ˆæŒ‰å¥å­åˆ‡åˆ†
    2. è®¡ç®—ç›¸é‚»å¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦
    3. åœ¨ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼çš„åœ°æ–¹åˆ‡åˆ†

    Args:
        text: åŸå§‹æ–‡æœ¬
        embed_func: å‘é‡åŒ–å‡½æ•°
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼åˆ™åˆ‡åˆ†
        min_chunk_size: æœ€å°å—å¤§å°
    """
    # 1. æŒ‰å¥å­åˆ‡åˆ†
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    if len(sentences) <= 1:
        return [text]

    # 2. è®¡ç®—æ¯ä¸ªå¥å­çš„å‘é‡
    embeddings = [embed_func(s) for s in sentences]

    # 3. è®¡ç®—ç›¸é‚»å¥å­çš„ç›¸ä¼¼åº¦
    def cosine_similarity(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    similarities = []
    for i in range(len(embeddings) - 1):
        sim = cosine_similarity(embeddings[i], embeddings[i + 1])
        similarities.append(sim)

    # 4. åœ¨ç›¸ä¼¼åº¦ä½çš„åœ°æ–¹åˆ‡åˆ†
    chunks = []
    current_chunk = sentences[0]

    for i, sim in enumerate(similarities):
        if sim < threshold and len(current_chunk) >= min_chunk_size:
            # ç›¸ä¼¼åº¦ä½ï¼Œåˆ‡åˆ†
            chunks.append(current_chunk)
            current_chunk = sentences[i + 1]
        else:
            # ç›¸ä¼¼åº¦é«˜ï¼Œåˆå¹¶
            current_chunk += " " + sentences[i + 1]

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

# ä½¿ç”¨ç¤ºä¾‹
from openai import OpenAI

client = OpenAI()

def get_embedding(text: str) -> List[float]:
    """è·å–æ–‡æœ¬çš„ embedding"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

text = """
Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ã€‚å®ƒç®€å•æ˜“å­¦ã€‚Pythonæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ã€‚

æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯ã€‚æ·±åº¦å­¦ä¹ å¾ˆæµè¡Œã€‚ç¥ç»ç½‘ç»œæ˜¯æ ¸å¿ƒæŠ€æœ¯ã€‚

ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚æˆ‘ä»¬å»å…¬å›­ç©å§ã€‚
"""

chunks = semantic_chunk(text, get_embedding, threshold=0.6)
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {chunk}")
```

### LangChain å®ç°ï¼ˆæ¨èï¼‰

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# åˆ›å»ºè¯­ä¹‰åˆ†å—å™¨
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# æ–¹æ³•1ï¼šç™¾åˆ†ä½æ•°é˜ˆå€¼ï¼ˆæ¨èï¼‰
semantic_splitter = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="percentile",  # ä½¿ç”¨ç™¾åˆ†ä½æ•°
    breakpoint_threshold_amount=95,          # 95% åˆ†ä½æ•°
)

# æ–¹æ³•2ï¼šæ ‡å‡†å·®é˜ˆå€¼
semantic_splitter_std = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="standard_deviation",
    breakpoint_threshold_amount=1.5,  # 1.5 å€æ ‡å‡†å·®
)

# æ–¹æ³•3ï¼šå››åˆ†ä½æ•°é˜ˆå€¼
semantic_splitter_iqr = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="interquartile",
    breakpoint_threshold_amount=1.5,
)

# åˆ†å—
text = "ä½ çš„é•¿æ–‡æœ¬..."
chunks = semantic_splitter.split_text(text)

for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1} ({len(chunk)} chars): {chunk[:100]}...")
```

---

## 2025-2026 ä¼˜åŒ–æ–¹æ¡ˆ

### ä¼˜åŒ–1ï¼šChroma ClusterSemanticChunkerï¼ˆ91%+ å¬å›ç‡ï¼‰

**ç ”ç©¶æ¥æº**: Chroma 2024-2025

```python
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
from langchain_experimental.text_splitter import SemanticChunker

# Chroma çš„ ClusterSemanticChunker å®ç°
embedding_function = OpenAIEmbeddingFunction(
    api_key="your-api-key",
    model_name="text-embedding-3-small"
)

# ä½¿ç”¨èšç±»æ–¹æ³•ç¡®å®šè¾¹ç•Œ
cluster_splitter = SemanticChunker(
    embeddings=embedding_function,
    breakpoint_threshold_type="percentile",
    breakpoint_threshold_amount=95,
    number_of_chunks=None,  # è‡ªåŠ¨ç¡®å®šå—æ•°
)

chunks = cluster_splitter.split_text(text)
```

**æ•ˆæœ**: Chroma ç ”ç©¶æ˜¾ç¤º ClusterSemanticChunker åœ¨å¤šæ•°æ®é›†ä¸Šè¾¾åˆ° 91%+ å¬å›ç‡ã€‚

### ä¼˜åŒ–2ï¼šMax-Min Semantic Chunking

**æ ¸å¿ƒæ€æƒ³**: å…ˆ embed æ•´ä¸ªæ–‡æ¡£ï¼Œç„¶ååŸºäºç›¸ä¼¼åº¦çŸ©é˜µç¡®å®šæœ€ä¼˜åˆ‡åˆ†ç‚¹ã€‚

```python
import numpy as np
from typing import List

def max_min_semantic_chunk(
    text: str,
    embed_func: Callable,
    target_chunk_size: int = 512
) -> List[str]:
    """
    Max-Min è¯­ä¹‰åˆ†å—

    åŸç†ï¼š
    1. å…ˆ embed æ•´ä¸ªæ–‡æ¡£çš„æ‰€æœ‰å¥å­
    2. æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
    3. æ‰¾åˆ°å±€éƒ¨æœ€å°ç›¸ä¼¼åº¦ç‚¹ä½œä¸ºåˆ‡åˆ†è¾¹ç•Œ
    """
    # 1. æŒ‰å¥å­åˆ‡åˆ†
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    # 2. è®¡ç®—æ‰€æœ‰å¥å­çš„ embedding
    embeddings = np.array([embed_func(s) for s in sentences])

    # 3. æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ
    n = len(embeddings)
    similarity_matrix = np.zeros((n, n))
    for i in range(n):
        for j in range(i+1, n):
            sim = np.dot(embeddings[i], embeddings[j]) / (
                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])
            )
            similarity_matrix[i][j] = sim
            similarity_matrix[j][i] = sim

    # 4. æ‰¾åˆ°å±€éƒ¨æœ€å°ç›¸ä¼¼åº¦ç‚¹
    breakpoints = [0]
    current_size = 0

    for i in range(1, n-1):
        current_size += len(sentences[i])

        # æ£€æŸ¥æ˜¯å¦æ˜¯å±€éƒ¨æœ€å°å€¼
        if (similarity_matrix[i-1][i] < similarity_matrix[i][i+1] and
            current_size >= target_chunk_size * 0.5):
            breakpoints.append(i)
            current_size = 0

    breakpoints.append(n)

    # 5. æ ¹æ®æ–­ç‚¹åˆ‡åˆ†
    chunks = []
    for i in range(len(breakpoints)-1):
        start = breakpoints[i]
        end = breakpoints[i+1]
        chunk = " ".join(sentences[start:end])
        chunks.append(chunk)

    return chunks
```

### ä¼˜åŒ–3ï¼šGrowing Window Strategy

**æ ¸å¿ƒæ€æƒ³**: ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—ç›¸ä¼¼åº¦ï¼Œæå‡è¾¹ç•Œæ£€æµ‹å‡†ç¡®æ€§ã€‚

```python
def growing_window_semantic_chunk(
    text: str,
    embed_func: Callable,
    window_size: int = 3,
    threshold: float = 0.6
) -> List[str]:
    """
    Growing Window è¯­ä¹‰åˆ†å—

    åŸç†ï¼š
    1. ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼ˆå¦‚3ä¸ªå¥å­ï¼‰è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
    2. åœ¨çª—å£ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼æ—¶åˆ‡åˆ†
    3. æ¯”å•å¥ç›¸ä¼¼åº¦æ›´ç¨³å®š
    """
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])', text)
    sentences = [s.strip() for s in sentences if s.strip()]

    if len(sentences) <= window_size:
        return [text]

    # è®¡ç®—æ‰€æœ‰å¥å­çš„ embedding
    embeddings = [embed_func(s) for s in sentences]

    # è®¡ç®—çª—å£ç›¸ä¼¼åº¦
    window_similarities = []
    for i in range(len(embeddings) - window_size):
        window_emb = embeddings[i:i+window_size]
        # è®¡ç®—çª—å£å†…çš„å¹³å‡ç›¸ä¼¼åº¦
        sims = []
        for j in range(len(window_emb)-1):
            sim = np.dot(window_emb[j], window_emb[j+1]) / (
                np.linalg.norm(window_emb[j]) * np.linalg.norm(window_emb[j+1])
            )
            sims.append(sim)
        window_similarities.append(np.mean(sims))

    # åœ¨ç›¸ä¼¼åº¦ä½çš„åœ°æ–¹åˆ‡åˆ†
    chunks = []
    current_chunk = " ".join(sentences[:window_size])

    for i, sim in enumerate(window_similarities):
        if sim < threshold:
            chunks.append(current_chunk)
            current_chunk = sentences[i+window_size]
        else:
            current_chunk += " " + sentences[i+window_size]

    if current_chunk:
        chunks.append(current_chunk)

    return chunks
```

**ç ”ç©¶æ¥æº**: Growing Window Strategy åœ¨æŸäº›æ•°æ®é›†ä¸Šæ¯”ä¼ ç»Ÿæ–¹æ³•æå‡ 4% å‡†ç¡®ç‡ã€‚

---

## ç›¸ä¼¼åº¦é˜ˆå€¼çš„å½±å“

### é˜ˆå€¼é€‰æ‹©æŒ‡å—

| é˜ˆå€¼ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| **0.8-0.9ï¼ˆé«˜ï¼‰** | åˆ‡åˆ†ç‚¹å¤šï¼Œå—æ›´å°ï¼Œè¯­ä¹‰æ›´çº¯ç²¹ | ä¸»é¢˜é¢‘ç¹å˜åŒ–çš„æ–‡æ¡£ |
| **0.6-0.7ï¼ˆä¸­ï¼‰** | å¹³è¡¡åˆ‡åˆ†ï¼Œé€‚ä¸­å—å¤§å° | é€šç”¨åœºæ™¯ï¼ˆæ¨èï¼‰ |
| **0.3-0.5ï¼ˆä½ï¼‰** | åˆ‡åˆ†ç‚¹å°‘ï¼Œå—æ›´å¤§ï¼Œå¯èƒ½æ··åˆä¸»é¢˜ | ä¸»é¢˜è¿è´¯çš„é•¿æ–‡æ¡£ |

### é˜ˆå€¼å¯¹æ¯”å®éªŒ

```python
def compare_thresholds(text: str, embed_func: Callable):
    """å¯¹æ¯”ä¸åŒé˜ˆå€¼çš„æ•ˆæœ"""
    thresholds = [0.3, 0.5, 0.7, 0.9]

    for threshold in thresholds:
        chunks = semantic_chunk(text, embed_func, threshold=threshold)
        avg_size = sum(len(c) for c in chunks) // len(chunks)

        print(f"\né˜ˆå€¼ {threshold}:")
        print(f"  å—æ•°: {len(chunks)}")
        print(f"  å¹³å‡å¤§å°: {avg_size} å­—ç¬¦")
        print(f"  ç¬¬ä¸€å—: {chunks[0][:100]}...")
```

---

## ä¼˜ç¼ºç‚¹åˆ†æ

### ä¼˜ç‚¹

| ä¼˜ç‚¹ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| âœ… **è¯­ä¹‰å®Œæ•´æ€§æœ€å¥½** | åœ¨ä¸»é¢˜è½¬æ¢ç‚¹åˆ‡åˆ†ï¼Œæ¯ä¸ªå—ä¸»é¢˜ä¸€è‡´ | é«˜è´¨é‡è¦æ±‚ |
| âœ… **é€‚åˆä¸»é¢˜å¤šå˜æ–‡æ¡£** | è‡ªåŠ¨è¯†åˆ«ä¸»é¢˜è¾¹ç•Œ | æ–°é—»ã€ç™¾ç§‘ |
| âœ… **æ£€ç´¢å‡†ç¡®ç‡é«˜** | å—å†…å®¹è¯­ä¹‰çº¯ç²¹ï¼ŒåŒ¹é…æ›´ç²¾å‡† | é—®ç­”ç³»ç»Ÿ |
| âœ… **æ— éœ€æ‰‹åŠ¨è°ƒå‚** | è‡ªåŠ¨ç¡®å®šè¾¹ç•Œï¼Œæ— éœ€è°ƒæ•´åˆ†éš”ç¬¦ | å¤šç§æ–‡æ¡£ç±»å‹ |

### ç¼ºç‚¹

| ç¼ºç‚¹ | è¯´æ˜ | å½±å“ |
|------|------|------|
| âŒ **éœ€è¦è°ƒç”¨ Embedding API** | æ¯ä¸ªå¥å­éƒ½éœ€è¦ embedding | æˆæœ¬é«˜ |
| âŒ **å¤„ç†é€Ÿåº¦æ…¢** | API è°ƒç”¨è€—æ—¶ | ä¸é€‚åˆå®æ—¶ |
| âŒ **å—å¤§å°ä¸å¯æ§** | ç”±è¯­ä¹‰å†³å®šï¼Œæ— æ³•ç²¾ç¡®æ§åˆ¶ | å¯èƒ½è¿‡å¤§æˆ–è¿‡å° |
| âŒ **æˆæœ¬è¾ƒé«˜** | å¤§é‡ API è°ƒç”¨ | é¢„ç®—æœ‰é™ä¸é€‚ç”¨ |

---

## æˆæœ¬åˆ†æï¼ˆ2025-2026ï¼‰

### Embedding API æˆæœ¬

| æ¨¡å‹ | ä»·æ ¼ï¼ˆæ¯ç™¾ä¸‡ tokensï¼‰ | 1000 å¥å­æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|---------------------|--------------|---------|
| **text-embedding-3-small** | $0.02 | ~$0.01 | æ¨èï¼ˆæ€§ä»·æ¯”é«˜ï¼‰ |
| **text-embedding-3-large** | $0.13 | ~$0.065 | é«˜è´¨é‡è¦æ±‚ |
| **text-embedding-ada-002** | $0.10 | ~$0.05 | ä¼ ç»Ÿé€‰æ‹© |

**æˆæœ¬ä¼˜åŒ–å»ºè®®ï¼š**
1. ä½¿ç”¨ `text-embedding-3-small`ï¼ˆæ€§ä»·æ¯”æœ€é«˜ï¼‰
2. æ‰¹é‡å¤„ç†å¥å­ï¼ˆå‡å°‘ API è°ƒç”¨æ¬¡æ•°ï¼‰
3. ç¼“å­˜ embedding ç»“æœ
4. åªå¯¹å…³é”®æ–‡æ¡£ä½¿ç”¨è¯­ä¹‰åˆ†å—

### æˆæœ¬å¯¹æ¯”

```python
# å‡è®¾ï¼š10ä¸‡å­—æ–‡æ¡£ï¼Œçº¦2000ä¸ªå¥å­

# è¯­ä¹‰åˆ†å—æˆæœ¬
embedding_cost = 2000 * 0.02 / 1000000 * 50  # ~$0.002

# é€’å½’åˆ†å—æˆæœ¬
recursive_cost = 0  # æ—  API è°ƒç”¨

# ä»£ç†å¼åˆ†å—æˆæœ¬ï¼ˆ2025-2026ï¼‰
agentic_cost = 10 * 0.01  # ~$0.10ï¼ˆLLM è°ƒç”¨ï¼‰

# ç»“è®ºï¼šè¯­ä¹‰åˆ†å—æˆæœ¬é€‚ä¸­ï¼Œæ¯”ä»£ç†å¼åˆ†å—ä¾¿å®œ50å€
```

---

## åœ¨ RAG å¼€å‘ä¸­çš„åº”ç”¨

### é€‚ç”¨åœºæ™¯ï¼ˆâœ… æ¨èï¼‰

1. **ä¸»é¢˜é¢‘ç¹å˜åŒ–çš„æ–‡æ¡£**
   - æ–°é—»èšåˆã€ç™¾ç§‘å…¨ä¹¦
   - å¤šä¸»é¢˜æŠ€æœ¯æ–‡æ¡£
   - ç¤ºä¾‹ï¼šWikipedia æ–‡ç« ã€æ–°é—»ç½‘ç«™

2. **å¯¹æ£€ç´¢è´¨é‡è¦æ±‚æé«˜**
   - åŒ»ç–—ã€æ³•å¾‹æ–‡æ¡£
   - å…³é”®ä¸šåŠ¡åœºæ™¯
   - ç¤ºä¾‹ï¼šåŒ»ç–—è¯Šæ–­çŸ¥è¯†åº“ã€æ³•å¾‹æ¡æ–‡æ£€ç´¢

3. **æ–‡æ¡£ç»“æ„ä¸è§„åˆ™**
   - æ— æ˜æ˜¾æ®µè½åˆ†éš”
   - æ··åˆæ ¼å¼å†…å®¹
   - ç¤ºä¾‹ï¼šç¤¾äº¤åª’ä½“å†…å®¹ã€è®ºå›å¸–å­

4. **é¢„ç®—å……è¶³çš„é¡¹ç›®**
   - å¯ä»¥æ‰¿æ‹… embedding æˆæœ¬
   - è¿½æ±‚æœ€ä½³æ•ˆæœ
   - ç¤ºä¾‹ï¼šä¼ä¸šçº§çŸ¥è¯†åº“ã€é«˜ç«¯äº§å“

### ä¸é€‚ç”¨åœºæ™¯ï¼ˆâŒ ä¸æ¨èï¼‰

1. **å¤§è§„æ¨¡æ–‡æ¡£å¤„ç†**
   - æˆæœ¬è¿‡é«˜
   - å¤„ç†æ—¶é—´é•¿
   - ç¤ºä¾‹ï¼šTB çº§æ–‡æ¡£åº“

2. **å®æ—¶å¤„ç†åœºæ™¯**
   - API è°ƒç”¨å»¶è¿Ÿé«˜
   - æ— æ³•æ»¡è¶³å®æ—¶è¦æ±‚
   - ç¤ºä¾‹ï¼šå®æ—¶èŠå¤©æœºå™¨äºº

3. **é¢„ç®—æœ‰é™çš„é¡¹ç›®**
   - embedding æˆæœ¬ä¸å¯æ¥å—
   - ç¤ºä¾‹ï¼šä¸ªäººé¡¹ç›®ã€MVP

4. **ç»“æ„åŒ–æ–‡æ¡£**
   - å·²æœ‰æ˜ç¡®æ®µè½åˆ†éš”
   - é€’å½’åˆ†å—å·²è¶³å¤Ÿ
   - ç¤ºä¾‹ï¼šæŠ€æœ¯æ–‡æ¡£ã€ä¹¦ç±

---

## æœ€ä½³å®è·µ

### å®è·µ1ï¼šæ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
from typing import List
import asyncio

async def batch_embed(texts: List[str], batch_size: int = 100) -> List[List[float]]:
    """æ‰¹é‡è·å– embeddingï¼ˆé™ä½æˆæœ¬ï¼‰"""
    embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        response = await client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        embeddings.extend([d.embedding for d in response.data])

    return embeddings

# ä½¿ç”¨
sentences = ["å¥å­1", "å¥å­2", ...]
embeddings = await batch_embed(sentences)
```

### å®è·µ2ï¼šç¼“å­˜ Embedding

```python
import hashlib
import json
from pathlib import Path

class EmbeddingCache:
    """Embedding ç¼“å­˜ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰"""

    def __init__(self, cache_dir: str = ".embedding_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode()).hexdigest()

    def get(self, text: str) -> List[float]:
        """è·å–ç¼“å­˜çš„ embedding"""
        key = self.get_cache_key(text)
        cache_file = self.cache_dir / f"{key}.json"

        if cache_file.exists():
            with open(cache_file) as f:
                return json.load(f)
        return None

    def set(self, text: str, embedding: List[float]):
        """ä¿å­˜ embedding åˆ°ç¼“å­˜"""
        key = self.get_cache_key(text)
        cache_file = self.cache_dir / f"{key}.json"

        with open(cache_file, 'w') as f:
            json.dump(embedding, f)

# ä½¿ç”¨
cache = EmbeddingCache()

def get_embedding_cached(text: str) -> List[float]:
    """å¸¦ç¼“å­˜çš„ embedding è·å–"""
    cached = cache.get(text)
    if cached:
        return cached

    embedding = get_embedding(text)
    cache.set(text, embedding)
    return embedding
```

### å®è·µ3ï¼šæ··åˆç­–ç•¥ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰

```python
def hybrid_chunking(text: str, use_semantic: bool = False) -> List[str]:
    """
    æ··åˆåˆ†å—ç­–ç•¥ï¼š
    - é‡è¦æ–‡æ¡£ï¼šä½¿ç”¨è¯­ä¹‰åˆ†å—
    - æ™®é€šæ–‡æ¡£ï¼šä½¿ç”¨é€’å½’åˆ†å—
    """
    if use_semantic:
        # é«˜è´¨é‡ï¼šè¯­ä¹‰åˆ†å—
        return semantic_chunk(text, get_embedding)
    else:
        # æˆæœ¬ä¼˜å…ˆï¼šé€’å½’åˆ†å—
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            chunk_overlap=77
        )
        return splitter.split_text(text)

# æ ¹æ®æ–‡æ¡£é‡è¦æ€§é€‰æ‹©ç­–ç•¥
important_docs = ["doc1.pdf", "doc2.pdf"]
for doc in all_docs:
    is_important = doc in important_docs
    chunks = hybrid_chunking(doc_text, use_semantic=is_important)
```

---

## ä¸å…¶ä»–ç­–ç•¥å¯¹æ¯”

### è¯­ä¹‰åˆ†å— vs é€’å½’å­—ç¬¦

| ç‰¹æ€§ | è¯­ä¹‰åˆ†å— | é€’å½’å­—ç¬¦ |
|------|----------|----------|
| **è¯­ä¹‰å®Œæ•´æ€§** | âœ…âœ… æœ€å¥½ | âœ… å¥½ |
| **API æˆæœ¬** | é«˜ï¼ˆembeddingï¼‰ | æ—  |
| **å¤„ç†é€Ÿåº¦** | ğŸš€ æ…¢ | ğŸš€ğŸš€ å¿« |
| **å—å¤§å°å¯æ§** | âŒ ä¸å¯æ§ | âœ… å¯æ§ |
| **æ¨èåœºæ™¯** | é«˜è´¨é‡è¦æ±‚ | **é€šç”¨åœºæ™¯ï¼ˆé¦–é€‰ï¼‰** |

### è¯­ä¹‰åˆ†å— vs ä»£ç†å¼åˆ†å—ï¼ˆ2025-2026ï¼‰

| ç‰¹æ€§ | è¯­ä¹‰åˆ†å— | ä»£ç†å¼åˆ†å— |
|------|----------|-----------|
| **è¯­ä¹‰å®Œæ•´æ€§** | âœ…âœ… æœ€å¥½ | âœ…âœ…âœ… æœ€å¥½ |
| **API æˆæœ¬** | ä¸­ï¼ˆembeddingï¼‰ | é«˜ï¼ˆLLMï¼‰ |
| **å¤„ç†é€Ÿåº¦** | ğŸš€ æ…¢ | ğŸš€ æ…¢ |
| **æ™ºèƒ½ç¨‹åº¦** | è‡ªåŠ¨ | æ™ºèƒ½ç†è§£ |
| **æ¨èåœºæ™¯** | ä¸»é¢˜å¤šå˜æ–‡æ¡£ | å¤æ‚æ–‡æ¡£ã€é«˜ä»·å€¼åœºæ™¯ |

---

## æ ¸å¿ƒç ”ç©¶æ¥æº

1. **Chroma 2024-2025**: ClusterSemanticChunker è¾¾åˆ° 91%+ å¬å›ç‡
2. **Growing Window Strategy**: æå‡ 4% å‡†ç¡®ç‡
3. **Max-Min Semantic Chunking**: ä¼˜åŒ–è¾¹ç•Œæ£€æµ‹

---

## ä¸‹ä¸€æ­¥å­¦ä¹ 

**åŒçº§æ¦‚å¿µï¼š**
- â† [03_æ ¸å¿ƒæ¦‚å¿µ_02_é€’å½’å­—ç¬¦åˆ†å—](./03_æ ¸å¿ƒæ¦‚å¿µ_02_é€’å½’å­—ç¬¦åˆ†å—.md) - é€šç”¨é¦–é€‰
- â†’ [03_æ ¸å¿ƒæ¦‚å¿µ_04_ä»£ç†å¼åˆ†å—Agentic](./03_æ ¸å¿ƒæ¦‚å¿µ_04_ä»£ç†å¼åˆ†å—Agentic.md) - IBM 2025-2026 æœ€æ–°æŠ€æœ¯

**å®æˆ˜ä»£ç ï¼š**
- â†’ [07_å®æˆ˜ä»£ç _03_è¯­ä¹‰åˆ†å—å®ç°](./07_å®æˆ˜ä»£ç _03_è¯­ä¹‰åˆ†å—å®ç°.md) - å®Œæ•´å®ç°ä»£ç 
