# 面试必问

> **Memory System 相关的高频面试题及答题策略**

---

## 高频题 1：实现 LRU 缓存（LeetCode 146）

### 题目描述

设计并实现一个 LRU（Least Recently Used）缓存机制。

实现 `LRUCache` 类：
- `LRUCache(int capacity)` 以正整数作为容量 capacity 初始化 LRU 缓存
- `int get(int key)` 如果关键字 key 存在于缓存中，则返回关键字的值，否则返回 -1
- `void put(int key, int value)` 如果关键字已经存在，则变更其数据值；如果关键字不存在，则插入该组「关键字-值」。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值

**要求**：`get` 和 `put` 操作的时间复杂度均为 O(1)

### 普通答案（60分）

```python
from collections import OrderedDict

class LRUCache:
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity

    def get(self, key: int) -> int:
        if key not in self.cache:
            return -1
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key: int, value: int) -> None:
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)
```

**问题**：
- 使用了 Python 内置的 `OrderedDict`，没有展示底层实现原理
- 面试官可能追问："如果不用 OrderedDict，你会怎么实现？"

### 优秀答案（100分）

```python
class Node:
    """双向链表节点"""
    def __init__(self, key: int = 0, value: int = 0):
        self.key = key
        self.value = value
        self.prev = None
        self.next = None

class LRUCache:
    """手写 LRU 缓存（双向链表 + 哈希表）"""
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = {}  # key -> Node

        # 虚拟头尾节点（简化边界处理）
        self.head = Node()
        self.tail = Node()
        self.head.next = self.tail
        self.tail.prev = self.head

    def _add_to_head(self, node: Node) -> None:
        """将节点添加到头部（标记为最近使用）"""
        node.prev = self.head
        node.next = self.head.next
        self.head.next.prev = node
        self.head.next = node

    def _remove_node(self, node: Node) -> None:
        """从链表中移除节点"""
        node.prev.next = node.next
        node.next.prev = node.prev

    def _move_to_head(self, node: Node) -> None:
        """将节点移到头部"""
        self._remove_node(node)
        self._add_to_head(node)

    def _remove_tail(self) -> Node:
        """移除尾部节点（最久未使用）"""
        node = self.tail.prev
        self._remove_node(node)
        return node

    def get(self, key: int) -> int:
        """O(1) 获取数据"""
        if key not in self.cache:
            return -1

        node = self.cache[key]
        self._move_to_head(node)  # 标记为最近使用
        return node.value

    def put(self, key: int, value: int) -> None:
        """O(1) 写入数据"""
        if key in self.cache:
            # 更新已有节点
            node = self.cache[key]
            node.value = value
            self._move_to_head(node)
        else:
            # 新增节点
            node = Node(key, value)
            self.cache[key] = node
            self._add_to_head(node)

            if len(self.cache) > self.capacity:
                # 缓存已满，删除尾部节点
                removed = self._remove_tail()
                del self.cache[removed.key]

# 测试
cache = LRUCache(2)
cache.put(1, 1)
cache.put(2, 2)
print(cache.get(1))    # 返回 1
cache.put(3, 3)        # 淘汰 key 2
print(cache.get(2))    # 返回 -1（未找到）
cache.put(4, 4)        # 淘汰 key 1
print(cache.get(1))    # 返回 -1（未找到）
print(cache.get(3))    # 返回 3
print(cache.get(4))    # 返回 4
```

### 答题要点

1. **数据结构选择**：
   - 哈希表：O(1) 查找
   - 双向链表：O(1) 插入/删除
   - 组合使用：哈希表存储 key → Node 映射

2. **时间复杂度分析**：
   - `get(key)`: O(1) - 哈希表查找 + 链表移动
   - `put(key, value)`: O(1) - 哈希表插入 + 链表操作

3. **空间复杂度**：O(capacity) - 哈希表和链表都存储最多 capacity 个节点

4. **边界处理**：
   - 使用虚拟头尾节点简化边界情况
   - 避免空指针判断

### 进阶追问

**Q1: 如果要实现线程安全的 LRU，怎么做？**

```python
import threading

class ThreadSafeLRUCache(LRUCache):
    """线程安全的 LRU 缓存"""
    def __init__(self, capacity: int):
        super().__init__(capacity)
        self.lock = threading.Lock()

    def get(self, key: int) -> int:
        with self.lock:
            return super().get(key)

    def put(self, key: int, value: int) -> None:
        with self.lock:
            super().put(key, value)
```

**Q2: 如果要支持过期时间（TTL），怎么实现？**

```python
import time

class Node:
    def __init__(self, key: int = 0, value: int = 0, ttl: float = None):
        self.key = key
        self.value = value
        self.expire_time = time.time() + ttl if ttl else None
        self.prev = None
        self.next = None

    def is_expired(self) -> bool:
        if self.expire_time is None:
            return False
        return time.time() > self.expire_time

class LRUCacheWithTTL(LRUCache):
    """支持 TTL 的 LRU 缓存"""
    def get(self, key: int) -> int:
        if key not in self.cache:
            return -1

        node = self.cache[key]
        if node.is_expired():
            # 过期数据，删除并返回 -1
            self._remove_node(node)
            del self.cache[key]
            return -1

        self._move_to_head(node)
        return node.value
```

---

## 高频题 2：设计 AI Agent 的记忆系统

### 题目描述

设计一个 AI Agent 的记忆系统，需要满足以下需求：

1. **对话历史管理**：保存最近 N 轮对话
2. **语义缓存**：相似问题共享答案，节省 LLM 调用成本
3. **跨会话记忆**：用户下次登录时能恢复历史偏好
4. **性能要求**：查询延迟 < 100ms，支持 10K+ 并发用户

### 普通答案（60分）

```python
class SimpleMemorySystem:
    """简单的记忆系统"""
    def __init__(self):
        self.conversations = {}  # user_id -> list of messages
        self.cache = {}  # question -> answer

    def add_message(self, user_id: str, role: str, content: str):
        if user_id not in self.conversations:
            self.conversations[user_id] = []
        self.conversations[user_id].append({"role": role, "content": content})

    def get_history(self, user_id: str, n: int = 10):
        return self.conversations.get(user_id, [])[-n:]

    def cache_answer(self, question: str, answer: str):
        self.cache[question] = answer

    def get_cached_answer(self, question: str):
        return self.cache.get(question)
```

**问题**：
- 没有容量限制，内存会无限增长
- 缓存只支持精确匹配，无法处理相似问题
- 没有持久化，重启后数据丢失
- 没有考虑并发和性能

### 优秀答案（100分）

```python
from typing import Optional, List, Dict
import numpy as np
from openai import OpenAI
import redis
import psycopg2
from collections import OrderedDict

client = OpenAI()

class AIAgentMemorySystem:
    """AI Agent 记忆系统（4层架构）"""

    def __init__(
        self,
        redis_client: redis.Redis,
        db_connection: psycopg2.extensions.connection,
        l1_capacity: int = 100,
        l2_ttl: int = 3600,
        semantic_threshold: float = 0.95
    ):
        # L1: 工作内存（进程内缓存）
        self.l1_cache = OrderedDict()
        self.l1_capacity = l1_capacity

        # L2: 会话缓存（Redis）
        self.redis = redis_client
        self.l2_ttl = l2_ttl

        # L3: 语义缓存（Redis + Embedding）
        self.semantic_threshold = semantic_threshold

        # L4: 长期记忆（PostgreSQL + pgvector）
        self.db = db_connection

    # ========== L1: 工作内存 ==========
    def _l1_get(self, key: str) -> Optional[any]:
        """从 L1 获取数据"""
        if key in self.l1_cache:
            self.l1_cache.move_to_end(key)
            return self.l1_cache[key]
        return None

    def _l1_put(self, key: str, value: any):
        """写入 L1"""
        if key in self.l1_cache:
            self.l1_cache.move_to_end(key)
        else:
            if len(self.l1_cache) >= self.l1_capacity:
                self.l1_cache.popitem(last=False)
        self.l1_cache[key] = value

    # ========== L2: 会话缓存 ==========
    def add_conversation_message(
        self,
        user_id: str,
        role: str,
        content: str
    ):
        """添加对话消息（L2 会话缓存）"""
        # L1: 更新工作内存
        key = f"conv:{user_id}"
        messages = self._l1_get(key) or []
        messages.append({"role": role, "content": content})
        self._l1_put(key, messages[-10:])  # 只保留最近10条

        # L2: 写入 Redis（带过期时间）
        self.redis.lpush(f"conv:{user_id}", f"{role}:{content}")
        self.redis.ltrim(f"conv:{user_id}", 0, 99)  # 最多100条
        self.redis.expire(f"conv:{user_id}", self.l2_ttl)

    def get_conversation_history(
        self,
        user_id: str,
        n: int = 10
    ) -> List[Dict]:
        """获取对话历史（L1 → L2）"""
        # L1: 先查工作内存
        key = f"conv:{user_id}"
        messages = self._l1_get(key)
        if messages:
            return messages[-n:]

        # L2: 查 Redis
        raw_messages = self.redis.lrange(f"conv:{user_id}", 0, n - 1)
        messages = []
        for msg in raw_messages:
            role, content = msg.decode().split(":", 1)
            messages.append({"role": role, "content": content})

        # 提升到 L1
        if messages:
            self._l1_put(key, messages)

        return messages

    # ========== L3: 语义缓存 ==========
    def _get_embedding(self, text: str) -> np.ndarray:
        """获取文本的向量表示"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding)

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """计算余弦相似度"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    def get_semantic_cached_answer(
        self,
        question: str
    ) -> Optional[str]:
        """从语义缓存获取答案"""
        # 计算问题的向量
        query_emb = self._get_embedding(question)

        # 从 Redis 获取所有缓存的问题向量
        cached_keys = self.redis.keys("semantic:*")

        best_match = None
        best_similarity = 0.0

        for key in cached_keys:
            cached_data = self.redis.hgetall(key)
            cached_emb = np.frombuffer(
                cached_data[b"embedding"],
                dtype=np.float32
            )

            similarity = self._cosine_similarity(query_emb, cached_emb)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = cached_data[b"answer"].decode()

        if best_similarity >= self.semantic_threshold:
            print(f"✅ 语义缓存命中 (相似度: {best_similarity:.3f})")
            return best_match

        return None

    def cache_semantic_answer(
        self,
        question: str,
        answer: str
    ):
        """缓存语义答案"""
        embedding = self._get_embedding(question)
        key = f"semantic:{hash(question)}"

        self.redis.hset(key, mapping={
            "question": question,
            "answer": answer,
            "embedding": embedding.astype(np.float32).tobytes()
        })
        self.redis.expire(key, 86400)  # 24小时过期

    # ========== L4: 长期记忆 ==========
    def save_user_preference(
        self,
        user_id: str,
        preference_key: str,
        preference_value: str
    ):
        """保存用户偏好（L4 持久化）"""
        cursor = self.db.cursor()
        cursor.execute("""
            INSERT INTO user_preferences (user_id, key, value)
            VALUES (%s, %s, %s)
            ON CONFLICT (user_id, key)
            DO UPDATE SET value = EXCLUDED.value
        """, (user_id, preference_key, preference_value))
        self.db.commit()

    def get_user_preference(
        self,
        user_id: str,
        preference_key: str
    ) -> Optional[str]:
        """获取用户偏好"""
        cursor = self.db.cursor()
        cursor.execute("""
            SELECT value FROM user_preferences
            WHERE user_id = %s AND key = %s
        """, (user_id, preference_key))
        result = cursor.fetchone()
        return result[0] if result else None

# 使用示例
redis_client = redis.Redis(host='localhost', port=6379)
db_conn = psycopg2.connect("dbname=agent_memory")

memory = AIAgentMemorySystem(redis_client, db_conn)

# 对话历史管理
memory.add_conversation_message("user_123", "user", "什么是 RAG？")
memory.add_conversation_message("user_123", "assistant", "RAG 是检索增强生成...")

# 语义缓存
memory.cache_semantic_answer("什么是 RAG？", "RAG 是检索增强生成...")
answer = memory.get_semantic_cached_answer("RAG 是什么？")  # ✅ 命中

# 长期记忆
memory.save_user_preference("user_123", "language", "zh-CN")
lang = memory.get_user_preference("user_123", "language")
```

### 答题要点

1. **架构设计**：
   - L1: 进程内缓存（最快，容量小）
   - L2: Redis 会话缓存（快，会话级）
   - L3: 语义缓存（中等速度，智能匹配）
   - L4: PostgreSQL 长期记忆（持久化）

2. **性能优化**：
   - 多级缓存减少数据库查询
   - LRU 策略控制内存使用
   - Redis 过期时间自动清理

3. **成本优化**：
   - 语义缓存避免重复 LLM 调用
   - 相似问题共享答案

4. **可扩展性**：
   - Redis 支持分布式部署
   - PostgreSQL 支持读写分离

### 进阶追问

**Q1: 如何处理缓存雪崩？**

```python
import random

def get_with_random_ttl(self, key: str, base_ttl: int = 3600):
    """随机 TTL 避免缓存雪崩"""
    # 在基础 TTL 上加随机偏移（±10%）
    ttl = base_ttl + random.randint(-base_ttl // 10, base_ttl // 10)
    self.redis.setex(key, ttl, value)
```

**Q2: 如何监控缓存命中率？**

```python
class MonitoredCache:
    def __init__(self):
        self.hits = 0
        self.misses = 0

    def get(self, key: str):
        result = self.cache.get(key)
        if result:
            self.hits += 1
        else:
            self.misses += 1
        return result

    def hit_rate(self) -> float:
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0
```

---

## 面试技巧总结

### 答题框架

1. **理解需求**（1分钟）
   - 明确题目要求
   - 询问边界条件

2. **设计方案**（2分钟）
   - 画出架构图
   - 说明数据结构选择

3. **编写代码**（10分钟）
   - 先写核心逻辑
   - 再处理边界情况

4. **分析复杂度**（1分钟）
   - 时间复杂度
   - 空间复杂度

5. **测试用例**（1分钟）
   - 正常情况
   - 边界情况

### 加分项

- ✅ 手写底层实现（不依赖库）
- ✅ 考虑线程安全
- ✅ 考虑分布式场景
- ✅ 提出优化方案
- ✅ 联系实际应用（AI Agent）

### 减分项

- ❌ 直接用库（OrderedDict）而不解释原理
- ❌ 忽略边界情况
- ❌ 时间复杂度不是 O(1)
- ❌ 代码有 bug
- ❌ 无法回答追问

---

## 参考资源

### LeetCode 题目
- [146. LRU Cache](https://leetcode.com/problems/lru-cache/) - 中等难度
- [460. LFU Cache](https://leetcode.com/problems/lfu-cache/) - 困难难度
- [Design In-Memory File System](https://leetcode.com/problems/design-in-memory-file-system/) - 困难难度

### 2025-2026 最新研究
- [AI Agents 2026 Architecture](https://andriifurmanets.com/blogs/ai-agents-2026-practical-architecture-tools-memory-evals-guardrails) - 记忆系统设计
- [Redis AI Agent Memory](https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis) - 生产级实现
- [LangGraph Memory Documentation](https://docs.langchain.com/oss/python/langgraph/add-memory) - 跨会话记忆

### 系统设计资源
- [System Design Interview](https://github.com/donnemartin/system-design-primer) - 系统设计指南
- [Designing Data-Intensive Applications](https://dataintensive.net/) - 经典书籍
