# 大模型API调用 - 第一性原理

## 什么是第一性原理？

**第一性原理**：回到事物最基本的真理，从源头思考问题。

不是问"别人怎么调用 API"，而是问"为什么需要 API？"

---

## 大模型 API 的第一性原理

### 1. 最基础的定义

**大模型 API = 远程函数调用**

```
输入：文本（Prompt）
输出：文本（Completion）
```

仅此而已！本质上就是：
- 你发送一段文字
- 云端模型处理
- 返回生成的文字

### 2. 为什么需要 API？

**核心问题：LLM 太大，普通设备跑不动**

| 模型 | 参数量 | 显存需求 | 普通电脑 |
|------|--------|----------|----------|
| GPT-4 | ~1.8T | 数百 GB | ❌ 不可能 |
| Claude 3 | 未公开 | 巨大 | ❌ 不可能 |
| Llama 70B | 70B | ~140 GB | ❌ 很难 |

**推导链：**

```
1. LLM 能力强大，但参数量巨大
   ↓
2. 运行需要昂贵的 GPU 集群
   ↓
3. 个人/企业无法承担硬件成本
   ↓
4. 厂商部署模型，提供 API 服务
   ↓
5. 用户按调用量付费，降低门槛
   ↓
6. API 成为使用 LLM 的标准方式
```

### 3. API 的三层价值

#### 价值1：降低使用门槛

**无需：**
- 购买昂贵 GPU
- 部署复杂环境
- 维护模型更新

**只需：**
- 一个 API Key
- 几行代码
- 按量付费

```python
# 这就是全部！
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "你好"}]
)
```

#### 价值2：标准化交互协议

不同厂商的 API 遵循相似的设计模式：

```python
# OpenAI
openai.chat.completions.create(messages=[...])

# Anthropic
anthropic.messages.create(messages=[...])
```

**好处：**
- 学会一个，触类旁通
- 方便切换模型
- 生态工具通用

#### 价值3：持续获得最新能力

```
厂商更新模型 → API 自动升级 → 你的应用自动变强
```

你不需要重新部署，只需要改一个参数：

```python
model="gpt-4"      # 旧模型
model="gpt-4o"     # 新模型，更快更便宜
model="gpt-4-turbo" # 更强的模型
```

### 4. 从第一性原理推导 RAG 应用

**推理链：**

```
1. RAG 需要 LLM 生成最终答案
   ↓
2. LLM 太大无法本地运行
   ↓
3. 必须通过 API 调用云端模型
   ↓
4. API 调用是 RAG 系统的必备能力
   ↓
5. 掌握 API 调用 = 掌握 RAG 生成模块
```

**RAG 中的 API 调用位置：**

```
用户问题
    ↓
向量检索（找到相关文档）
    ↓
构建 Prompt（问题 + 检索结果）
    ↓
【API 调用】← 这里！
    ↓
返回生成的答案
```

### 5. 一句话总结第一性原理

**大模型 API 是"远程函数调用"，因为 LLM 太大无法本地运行，所以通过网络请求使用云端模型，这是 RAG 系统生成答案的唯一途径。**

---

## 思考题

1. 如果未来个人电脑能运行大模型，API 还有存在的必要吗？
2. 为什么 OpenAI 和 Anthropic 的 API 设计如此相似？

---

**上一节：** [01_30字核心.md](./01_30字核心.md)
**下一节：** [03_核心概念.md](./03_核心概念.md) - 掌握 3 个核心概念
