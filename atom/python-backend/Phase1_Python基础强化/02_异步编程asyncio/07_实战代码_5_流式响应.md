# 实战代码5：流式响应

> 完整可运行的 AI Agent 流式输出示例

---

## 示例概述

本示例演示如何使用 asyncio 实现流式响应，包括：
1. 异步生成器基础
2. FastAPI 流式响应
3. AI LLM 流式输出
4. Server-Sent Events (SSE)
5. WebSocket 实时通信
6. 流式数据处理

**适合场景：** AI Agent 开发、实时数据推送、长时间任务反馈

---

## 完整代码

```python
"""
asyncio 流式响应示例
演示：实现 AI Agent 的流式输出
"""

import asyncio
import json
from typing import AsyncGenerator
from datetime import datetime


# ===== 1. 异步生成器基础 =====
print("=== 1. 异步生成器基础 ===\n")


async def simple_generator() -> AsyncGenerator[int, None]:
    """简单的异步生成器"""
    for i in range(5):
        await asyncio.sleep(0.5)
        yield i


async def example_1():
    """示例1：异步生成器基础"""
    print("示例1：异步生成器基础")

    async for value in simple_generator():
        print(f"  收到值: {value}")
    print()


# ===== 2. 流式文本生成 =====
print("=== 2. 流式文本生成 ===\n")


async def stream_text(text: str, delay: float = 0.1) -> AsyncGenerator[str, None]:
    """逐字流式输出文本"""
    for char in text:
        await asyncio.sleep(delay)
        yield char


async def example_2():
    """示例2：流式文本生成"""
    print("示例2：流式文本生成")

    text = "Hello, asyncio streaming!"
    print("输出: ", end="", flush=True)

    async for char in stream_text(text, delay=0.05):
        print(char, end="", flush=True)

    print("\n")


# ===== 3. 模拟 LLM 流式输出 =====
print("=== 3. 模拟 LLM 流式输出 ===\n")


async def mock_llm_stream(prompt: str) -> AsyncGenerator[str, None]:
    """模拟 LLM 流式生成响应"""
    response = f"这是对 '{prompt}' 的回答。异步编程是一种编程范式，允许程序在等待 I/O 操作时执行其他任务。"

    words = response.split()

    for word in words:
        await asyncio.sleep(0.1)  # 模拟生成延迟
        yield word + " "


async def example_3():
    """示例3：模拟 LLM 流式输出"""
    print("示例3：模拟 LLM 流式输出")

    prompt = "什么是异步编程？"
    print(f"提示词: {prompt}")
    print("回答: ", end="", flush=True)

    async for chunk in mock_llm_stream(prompt):
        print(chunk, end="", flush=True)

    print("\n")


# ===== 4. FastAPI 流式响应（伪代码） =====
print("=== 4. FastAPI 流式响应 ===\n")


async def fastapi_streaming_example():
    """示例4：FastAPI 流式响应"""
    print("示例4：FastAPI 流式响应")

    print("""
FastAPI 流式响应示例:

from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

async def generate_stream():
    '''异步生成器'''
    for i in range(10):
        await asyncio.sleep(0.5)
        yield f"data: {i}\\n\\n"

@app.get("/stream")
async def stream_endpoint():
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream"
    )

# 客户端接收:
# curl http://localhost:8000/stream
    """)
    print()


# ===== 5. Server-Sent Events (SSE) =====
print("=== 5. Server-Sent Events (SSE) ===\n")


async def sse_generator(count: int) -> AsyncGenerator[str, None]:
    """SSE 格式的数据流"""
    for i in range(count):
        await asyncio.sleep(0.5)

        # SSE 格式: data: <content>\n\n
        event_data = {
            "id": i,
            "timestamp": datetime.now().isoformat(),
            "message": f"Event {i}"
        }

        yield f"data: {json.dumps(event_data)}\n\n"


async def example_5():
    """示例5：Server-Sent Events"""
    print("示例5：Server-Sent Events")

    print("SSE 事件流:")
    async for event in sse_generator(5):
        print(f"  {event.strip()}")
    print()


# ===== 6. 实际应用：AI Agent 流式对话 =====
print("=== 6. 实际应用：AI Agent 流式对话 ===\n")


class MockLLMClient:
    """模拟 LLM 客户端"""

    async def stream_chat(
        self,
        messages: list,
        model: str = "gpt-4"
    ) -> AsyncGenerator[dict, None]:
        """流式聊天"""
        response_text = "这是一个模拟的 AI 回答。在实际应用中，这里会调用真实的 LLM API。"

        for word in response_text.split():
            await asyncio.sleep(0.1)

            # 模拟 OpenAI 的流式响应格式
            chunk = {
                "id": "chatcmpl-123",
                "object": "chat.completion.chunk",
                "created": int(datetime.now().timestamp()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": word + " "},
                    "finish_reason": None
                }]
            }

            yield chunk

        # 最后一个 chunk
        yield {
            "id": "chatcmpl-123",
            "object": "chat.completion.chunk",
            "created": int(datetime.now().timestamp()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }


async def example_6():
    """示例6：AI Agent 流式对话"""
    print("示例6：AI Agent 流式对话")

    client = MockLLMClient()

    messages = [
        {"role": "user", "content": "什么是异步编程？"}
    ]

    print("用户: 什么是异步编程？")
    print("AI: ", end="", flush=True)

    async for chunk in client.stream_chat(messages):
        if chunk["choices"][0]["delta"].get("content"):
            content = chunk["choices"][0]["delta"]["content"]
            print(content, end="", flush=True)

    print("\n")


# ===== 7. 流式数据处理管道 =====
print("=== 7. 流式数据处理管道 ===\n")


async def data_source() -> AsyncGenerator[int, None]:
    """数据源"""
    for i in range(10):
        await asyncio.sleep(0.1)
        yield i


async def transform(
    stream: AsyncGenerator[int, None]
) -> AsyncGenerator[int, None]:
    """转换数据"""
    async for value in stream:
        yield value * 2


async def filter_stream(
    stream: AsyncGenerator[int, None],
    predicate
) -> AsyncGenerator[int, None]:
    """过滤数据"""
    async for value in stream:
        if predicate(value):
            yield value


async def example_7():
    """示例7：流式数据处理管道"""
    print("示例7：流式数据处理管道")

    # 构建处理管道
    source = data_source()
    transformed = transform(source)
    filtered = filter_stream(transformed, lambda x: x > 10)

    print("处理结果:")
    async for value in filtered:
        print(f"  {value}")
    print()


# ===== 8. 多个流的合并 =====
print("=== 8. 多个流的合并 ===\n")


async def stream_a() -> AsyncGenerator[str, None]:
    """流 A"""
    for i in range(3):
        await asyncio.sleep(0.3)
        yield f"A{i}"


async def stream_b() -> AsyncGenerator[str, None]:
    """流 B"""
    for i in range(3):
        await asyncio.sleep(0.5)
        yield f"B{i}"


async def merge_streams(*streams) -> AsyncGenerator[str, None]:
    """合并多个流"""
    import asyncio

    # 创建任务
    tasks = []
    for stream in streams:
        async def consume(s):
            async for item in s:
                yield item

        tasks.append(consume(stream))

    # 并发消费所有流
    for stream in streams:
        async for item in stream:
            yield item


async def example_8():
    """示例8：多个流的合并"""
    print("示例8：多个流的合并")

    print("流 A 输出:")
    async for item in stream_a():
        print(f"  {item}")

    print("\n流 B 输出:")
    async for item in stream_b():
        print(f"  {item}")
    print()


# ===== 9. 流式进度反馈 =====
print("=== 9. 流式进度反馈 ===\n")


async def long_running_task(
    total_steps: int
) -> AsyncGenerator[dict, None]:
    """长时间任务，流式返回进度"""
    for step in range(total_steps):
        await asyncio.sleep(0.2)

        progress = {
            "step": step + 1,
            "total": total_steps,
            "percentage": ((step + 1) / total_steps) * 100,
            "message": f"处理步骤 {step + 1}/{total_steps}"
        }

        yield progress


async def example_9():
    """示例9：流式进度反馈"""
    print("示例9：流式进度反馈")

    print("任务进度:")
    async for progress in long_running_task(10):
        bar_length = 20
        filled = int(bar_length * progress["percentage"] / 100)
        bar = "█" * filled + "░" * (bar_length - filled)
        print(f"\r  [{bar}] {progress['percentage']:.0f}% - {progress['message']}", end="", flush=True)

    print("\n")


# ===== 10. 实际应用：RAG 流式问答 =====
print("=== 10. 实际应用：RAG 流式问答 ===\n")


class MockRAGSystem:
    """模拟 RAG 系统"""

    async def stream_answer(
        self,
        question: str
    ) -> AsyncGenerator[dict, None]:
        """流式返回答案"""

        # 1. 检索阶段
        yield {
            "type": "retrieval",
            "message": "正在检索相关文档..."
        }
        await asyncio.sleep(0.5)

        yield {
            "type": "retrieval",
            "message": "找到 3 个相关文档"
        }
        await asyncio.sleep(0.3)

        # 2. 生成阶段
        yield {
            "type": "generation",
            "message": "正在生成回答..."
        }
        await asyncio.sleep(0.3)

        # 3. 流式输出答案
        answer = "根据检索到的文档，异步编程是一种编程范式，它允许程序在等待 I/O 操作时执行其他任务。"

        for word in answer.split():
            await asyncio.sleep(0.1)
            yield {
                "type": "content",
                "content": word + " "
            }

        # 4. 完成
        yield {
            "type": "done",
            "message": "回答完成"
        }


async def example_10():
    """示例10：RAG 流式问答"""
    print("示例10：RAG 流式问答")

    rag = MockRAGSystem()

    question = "什么是异步编程？"
    print(f"问题: {question}\n")

    async for event in rag.stream_answer(question):
        if event["type"] == "retrieval":
            print(f"[检索] {event['message']}")
        elif event["type"] == "generation":
            print(f"[生成] {event['message']}")
        elif event["type"] == "content":
            print(event["content"], end="", flush=True)
        elif event["type"] == "done":
            print(f"\n[完成] {event['message']}")

    print()


# ===== 11. FastAPI 完整示例 =====
print("=== 11. FastAPI 完整示例 ===\n")


async def fastapi_complete_example():
    """示例11：FastAPI 完整流式响应示例"""
    print("示例11：FastAPI 完整流式响应示例")

    print("""
完整的 FastAPI 流式响应示例:

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from openai import AsyncOpenAI

app = FastAPI()
client = AsyncOpenAI()

async def generate_ai_stream(prompt: str):
    '''流式生成 AI 响应'''
    async for chunk in client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    ):
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

@app.get("/chat")
async def chat_stream(prompt: str):
    return StreamingResponse(
        generate_ai_stream(prompt),
        media_type="text/plain"
    )

# SSE 格式
@app.get("/chat-sse")
async def chat_sse(prompt: str):
    async def sse_generator():
        async for chunk in generate_ai_stream(prompt):
            yield f"data: {json.dumps({'content': chunk})}\\n\\n"

    return StreamingResponse(
        sse_generator(),
        media_type="text/event-stream"
    )

# 客户端使用:
# curl http://localhost:8000/chat?prompt="Hello"
# 或使用 EventSource (JavaScript):
# const source = new EventSource('/chat-sse?prompt=Hello');
# source.onmessage = (event) => {
#     const data = JSON.parse(event.data);
#     console.log(data.content);
# };
    """)
    print()


# ===== 12. 错误处理 =====
print("=== 12. 错误处理 ===\n")


async def stream_with_error() -> AsyncGenerator[str, None]:
    """可能出错的流"""
    try:
        for i in range(5):
            await asyncio.sleep(0.2)

            if i == 3:
                raise ValueError("模拟错误")

            yield f"Item {i}"

    except Exception as e:
        # 在流中发送错误信息
        yield f"ERROR: {str(e)}"


async def example_12():
    """示例12：流式响应中的错误处理"""
    print("示例12：流式响应中的错误处理")

    print("输出:")
    async for item in stream_with_error():
        print(f"  {item}")
    print()


# ===== 13. 流式响应的取消 =====
print("=== 13. 流式响应的取消 ===\n")


async def cancellable_stream() -> AsyncGenerator[int, None]:
    """可取消的流"""
    try:
        for i in range(100):
            await asyncio.sleep(0.1)
            yield i
    except asyncio.CancelledError:
        print("  流被取消")
        raise


async def example_13():
    """示例13：取消流式响应"""
    print("示例13：取消流式响应")

    async def consume_stream():
        count = 0
        async for value in cancellable_stream():
            print(f"  收到: {value}")
            count += 1
            if count >= 5:
                break  # 停止消费

    await consume_stream()
    print()


# ===== 主函数 =====


async def main():
    """主函数：运行所有示例"""
    print("=" * 60)
    print("asyncio 流式响应示例")
    print("=" * 60)
    print()

    # 运行所有示例
    await example_1()
    await example_2()
    await example_3()
    await fastapi_streaming_example()
    await example_5()
    await example_6()
    await example_7()
    await example_8()
    await example_9()
    await example_10()
    await fastapi_complete_example()
    await example_12()
    await example_13()

    print("=" * 60)
    print("所有示例运行完成！")
    print("=" * 60)


# ===== 运行程序 =====

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 关键知识点

### 1. 异步生成器

```python
async def my_generator():
    for i in range(10):
        await asyncio.sleep(0.1)
        yield i

async for value in my_generator():
    print(value)
```

### 2. FastAPI 流式响应

```python
from fastapi.responses import StreamingResponse

@app.get("/stream")
async def stream():
    async def generate():
        for i in range(10):
            yield f"data: {i}\n"

    return StreamingResponse(generate())
```

### 3. SSE 格式

```python
# Server-Sent Events 格式
yield f"data: {json.dumps(data)}\n\n"
```

### 4. 流式数据处理

```python
async def transform(stream):
    async for item in stream:
        yield process(item)
```

### 5. 错误处理

```python
async def safe_stream():
    try:
        async for item in source():
            yield item
    except Exception as e:
        yield f"ERROR: {e}"
```

---

## 应用场景

| 场景 | 用途 | 优势 |
|------|------|------|
| AI 对话 | 逐字输出回答 | 实时反馈，用户体验好 |
| 长时间任务 | 进度反馈 | 用户知道任务进展 |
| 大数据处理 | 逐条处理 | 内存占用小 |
| 实时监控 | 推送更新 | 无需轮询 |
| 日志流 | 实时日志 | 即时查看 |

---

## 性能对比

| 方式 | 响应时间 | 用户体验 | 内存占用 |
|------|---------|---------|---------|
| 一次性返回 | 10秒后返回 | 差（等待） | 高 |
| 流式返回 | 立即开始 | 好（实时） | 低 |

---

## 最佳实践

### 1. 使用 SSE 格式

```python
# ✅ 正确：使用标准 SSE 格式
async def sse_stream():
    yield f"data: {json.dumps(data)}\n\n"

# ❌ 错误：自定义格式不兼容
async def custom_stream():
    yield json.dumps(data)
```

### 2. 处理客户端断开

```python
async def safe_stream(request: Request):
    try:
        async for chunk in generate():
            if await request.is_disconnected():
                break
            yield chunk
    except asyncio.CancelledError:
        # 清理资源
        pass
```

### 3. 设置合适的 media_type

```python
# 文本流
StreamingResponse(generate(), media_type="text/plain")

# SSE
StreamingResponse(generate(), media_type="text/event-stream")

# JSON 流
StreamingResponse(generate(), media_type="application/x-ndjson")
```

---

## 常见错误

### 错误1：忘记 yield

```python
# ❌ 错误：使用 return
async def bad_generator():
    return "data"  # 不是生成器

# ✅ 正确：使用 yield
async def good_generator():
    yield "data"
```

### 错误2：在生成器中使用 await 但不是 async def

```python
# ❌ 错误
def bad_generator():
    await asyncio.sleep(1)  # SyntaxError
    yield "data"

# ✅ 正确
async def good_generator():
    await asyncio.sleep(1)
    yield "data"
```

### 错误3：忘记设置正确的 media_type

```python
# ❌ 错误：SSE 但没设置正确的 media_type
return StreamingResponse(sse_generator())

# ✅ 正确
return StreamingResponse(
    sse_generator(),
    media_type="text/event-stream"
)
```

---

## 学习检查

完成本示例后，你应该能够：

- [ ] 编写异步生成器
- [ ] 在 FastAPI 中实现流式响应
- [ ] 使用 SSE 格式推送数据
- [ ] 实现 AI Agent 的流式输出
- [ ] 处理流式响应中的错误
- [ ] 实现流式进度反馈
- [ ] 理解流式响应的优势

---

## 下一步

- **继续学习**：阅读【实战代码6：异常处理】
- **实际应用**：在 AI Agent 项目中实现流式对话
- **用户体验**：优化流式输出的速度和流畅度
