# 化骨绵掌

> 把评估与调优拆成 10 个 2 分钟能看完的知识卡片

---

## 使用说明

每张卡片独立完整，2 分钟内可看完。建议按顺序阅读，10 张卡片形成递进关系。

---

## 卡片1：为什么需要评估 RAG 系统？

**一句话：** 没有评估的 RAG 系统就像没有仪表盘的汽车——你不知道它跑得好不好。

**问题：**
你花了一周搭建了一个 RAG 系统，用户说"回答不太好"。但"不好"是什么意思？
- 检索到的文档不相关？
- 答案编造了信息？
- 答案跑题了？
- 答案遗漏了关键信息？

没有评估指标，你只能猜。有了评估指标，你能精确定位问题。

**类比：**
就像前端开发中的 Lighthouse 评分——不跑一次，你不知道页面性能到底怎么样。同理，不评估一次，你不知道 RAG 系统到底哪里需要改进。

**应用：** 在 RAG 开发中，评估是优化的前提。先量化问题，再针对性解决。没有评估就没有优化方向。

---

## 卡片2：评估指标的数学本质

**一句话：** 所有评估指标的本质都是衡量"实际输出"与"期望输出"之间的距离。

**核心公式：**
```
评估分数 = f(实际输出, 期望输出)
```

**两类距离：**
- **集合距离**：Precision（交集/检索集）、Recall（交集/相关集）
- **语义距离**：Embedding 相似度、LLM 判断

```python
# 集合距离示例
precision = len(retrieved & relevant) / len(retrieved)
recall = len(retrieved & relevant) / len(relevant)

# 语义距离示例
from numpy import dot
from numpy.linalg import norm
similarity = dot(vec_a, vec_b) / (norm(vec_a) * norm(vec_b))
```

**理解本质的好处：** 一旦你理解了"评估 = 衡量距离"，你就能自己设计适合业务的评估指标，而不是只会套用现成框架。

**应用：** 不同业务场景需要不同的"距离函数"。医疗问答看重精确匹配，创意写作看重语义相关。

---

## 卡片3：RAGAS 框架速览

**一句话：** RAGAS 是 RAG 系统的"体检套餐"，4 个核心指标一次检查完。

**4 个核心指标：**

| 指标 | 衡量什么 | 直觉理解 |
|------|----------|----------|
| Faithfulness | 答案忠于上下文 | "有没有编造" |
| Answer Relevancy | 答案回答了问题 | "有没有跑题" |
| Context Precision | 好文档排前面 | "排序好不好" |
| Context Recall | 覆盖了标准答案 | "有没有遗漏" |

**最小使用示例：**

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

# 准备评估数据集（需要包含 question, answer, contexts, ground_truth）
result = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ],
)
print(result)
# 输出: {'faithfulness': 0.85, 'answer_relevancy': 0.92, ...}
```

**为什么选 RAGAS？**
- 开源免费，社区活跃
- 基于 LLM 自动评估，不需要人工标注
- 覆盖检索侧和生成侧

**应用：** 每次修改 RAG 系统后，跑一次 RAGAS 评估，确保改进是真实的，没有引入新问题。

---

## 卡片4：Precision 与 Recall — 检索质量的两个维度

**一句话：** Precision 问"找到的准不准"，Recall 问"该找的全不全"。

**类比：钓鱼**
- **Precision** = 钓上来的鱼中，目标鱼的比例（有没有钓到垃圾）
- **Recall** = 池塘里的目标鱼，被你钓上来的比例（有没有漏掉）

**Trade-off（此消彼长）：**
```
撒大网 → Recall 高，Precision 低（什么都捞上来了，但很多是垃圾）
用鱼竿 → Precision 高，Recall 低（钓到的都是好鱼，但可能漏掉很多）
```

**代码示例：**

```python
def precision_at_k(retrieved: list, relevant: set, k: int) -> float:
    """计算 Precision@K：前 K 个结果中相关文档的比例"""
    top_k = retrieved[:k]
    hits = len(set(top_k) & relevant)
    return hits / k

def recall_at_k(retrieved: list, relevant: set, k: int) -> float:
    """计算 Recall@K：相关文档被检索到的比例"""
    top_k = retrieved[:k]
    hits = len(set(top_k) & relevant)
    return hits / len(relevant)

# 示例
retrieved = ["doc1", "doc3", "doc5", "doc2", "doc7"]
relevant = {"doc1", "doc2", "doc4"}

print(f"Precision@3 = {precision_at_k(retrieved, relevant, 3):.2f}")  # 0.33
print(f"Recall@3 = {recall_at_k(retrieved, relevant, 3):.2f}")        # 0.33
print(f"Precision@5 = {precision_at_k(retrieved, relevant, 5):.2f}")  # 0.40
print(f"Recall@5 = {recall_at_k(retrieved, relevant, 5):.2f}")        # 0.67
```

**场景选择：**
- 医疗场景优先 Recall（不能漏诊，宁可多检索几篇）
- 客服场景平衡两者（既要准确又要全面）
- 精准问答优先 Precision（只要最相关的那一篇）

**应用：** Precision 和 Recall 是检索评估的基石，理解它们的 trade-off 是调优 top_k 参数的关键。

---

## 卡片5：MRR 与 NDCG — 排序质量的衡量

**一句话：** MRR 关注"第一个正确结果排第几"，NDCG 关注"整体排序好不好"。

**MRR（Mean Reciprocal Rank）：**
- 第1个相关结果排第1位 → RR = 1/1 = 1.0
- 第1个相关结果排第3位 → RR = 1/3 = 0.33
- 没有相关结果 → RR = 0.0
- MRR = 所有查询的 RR 平均值

**NDCG（Normalized Discounted Cumulative Gain）：**
- 相关文档排前面 → NDCG 高
- 相关文档排后面 → NDCG 低（因为位置越靠后，"折扣"越大）
- 完美排序 → NDCG = 1.0

**代码示例：**

```python
def reciprocal_rank(retrieved: list, relevant: set) -> float:
    """计算单个查询的 Reciprocal Rank"""
    for i, doc in enumerate(retrieved):
        if doc in relevant:
            return 1.0 / (i + 1)
    return 0.0

def mean_reciprocal_rank(queries_results: list, queries_relevant: list) -> float:
    """计算多个查询的 MRR"""
    rr_sum = sum(
        reciprocal_rank(results, relevant)
        for results, relevant in zip(queries_results, queries_relevant)
    )
    return rr_sum / len(queries_results)

# 示例
results = [["doc3", "doc1", "doc5"], ["doc2", "doc4", "doc1"]]
relevant = [{"doc1", "doc2"}, {"doc1"}]
print(f"MRR = {mean_reciprocal_rank(results, relevant):.2f}")  # 0.42
```

**何时用哪个？**
- **问答系统用 MRR**：用户通常只看第一个答案，关心"最好的结果排第几"
- **搜索系统用 NDCG**：用户会浏览多个结果，关心"整体排序质量"

**应用：** 在 RAG 系统中，MRR 更常用，因为通常只取 top-1 或 top-3 的文档送给 LLM 生成答案。

---

## 卡片6：Faithfulness — 生成忠实度评估

**一句话：** Faithfulness 检查 LLM 有没有"编造"上下文中不存在的信息。

**计算方法（三步走）：**
1. 把答案拆成独立的事实陈述（claims）
2. 逐一检查每个陈述是否有上下文支持
3. 忠实度 = 有支持的陈述数 / 总陈述数

**示例：**
```
上下文: "Python 3.9 于 2020 年 10 月发布，新增了字典合并运算符"
答案: "Python 3.9 于 2020 年发布，新增了字典合并运算符和模式匹配"

拆分陈述：
  陈述1: "Python 3.9 于 2020 年发布" → 有支持 ✅
  陈述2: "新增了字典合并运算符" → 有支持 ✅
  陈述3: "新增了模式匹配" → 上下文没提到 ❌（幻觉！）

Faithfulness = 2/3 = 0.67
```

**关键阈值：**
- Faithfulness >= 0.9：优秀，几乎没有幻觉
- Faithfulness >= 0.8：良好，可接受
- Faithfulness < 0.8：需要关注，可能存在较多幻觉
- Faithfulness < 0.6：严重问题，必须立即修复

**应用：** Faithfulness 是 RAG 系统最重要的生成侧指标。低于 0.8 时，优先考虑加强 Prompt 约束（如"只使用文档中的信息"）或降低 temperature。

---

## 卡片7：Answer Relevancy — 答案相关性评估

**一句话：** Answer Relevancy 检查答案有没有"跑题"——回答的是不是用户问的问题。

**计算方法（RAGAS 的巧妙设计）：**
1. 根据答案反向生成 N 个问题
2. 计算生成的问题与原始问题的 Embedding 相似度
3. 取平均值作为 Answer Relevancy 分数

**为什么这样算？**
如果答案真的在回答"什么是 RAG？"，那从答案反推出的问题应该也是关于 RAG 的。如果反推出的问题是"什么是 Python？"，说明答案跑题了。

**示例：**
```
原始问题: "什么是 RAG？"

情况1（高分 ✅）：
  答案: "RAG 是检索增强生成技术，结合检索和生成两个步骤..."
  反向生成: "什么是检索增强生成？" → 与原始问题相似 → 高分

情况2（低分 ❌）：
  答案: "Python 是一种编程语言，广泛用于数据科学..."
  反向生成: "什么是 Python？" → 与原始问题不相似 → 低分
```

**与 Faithfulness 的区别：**
- **Faithfulness**：答案是否忠于上下文（有没有编造）
- **Answer Relevancy**：答案是否回答了问题（有没有跑题）
- 两者可以同时高或同时低，是独立的维度

**应用：** Relevancy 低时，通常是 Prompt 设计问题——没有清晰引导 LLM 聚焦用户问题。检查 Prompt 中是否明确要求"回答以下问题"。

---

## 卡片8：LLM-as-Judge — 用 LLM 评估 LLM

**一句话：** 用一个强大的 LLM（如 GPT-4）来评估另一个 LLM 的输出质量，实现自动化评估。

**核心思路：**
```
传统方法: 人工阅读 → 打分 → 成本高、速度慢、难以规模化
LLM-as-Judge: 设计评估 Prompt → LLM 打分 → 成本低、速度快、可批量
```

**代码示例：**

```python
from openai import OpenAI

client = OpenAI()

def llm_judge(question: str, answer: str, reference: str) -> dict:
    """用 LLM 评估答案质量"""
    prompt = f"""请评估以下答案的质量（1-5分）。

问题: {question}
参考答案: {reference}
待评估答案: {answer}

评分标准:
- 5分: 完全正确，信息完整
- 4分: 基本正确，略有遗漏
- 3分: 部分正确，有明显遗漏
- 2分: 大部分不正确
- 1分: 完全错误或无关

请以 JSON 格式输出: {{"score": N, "reason": "..."}}"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,  # 降低随机性，提高评估一致性
    )
    return response.choices[0].message.content
```

**关键技巧：**
1. 评估 Prompt 要结构化（明确评分标准和输出格式）
2. 使用低 temperature（0.0）提高评估一致性
3. 多次评估取平均（减少单次偏差）
4. 定期用人工评估校准 LLM 评估结果

**应用：** LLM-as-Judge 是自动化评估的核心方法，但不能完全替代人工。建议 80% 自动评估 + 20% 人工抽检。

---

## 卡片9：评估驱动优化 — 从指标到行动

**一句话：** 评估指标不是目的，而是告诉你"下一步该优化什么"的指南针。

**诊断决策树：**

```
问题: RAG 系统效果不好，怎么办？

Step 1: 看 Context Recall
  └─ 低（< 0.7）→ 检索召回不足
     → 行动: 增大 top_k、换更好的 Embedding 模型、调整 Chunk 大小

Step 2: 看 Context Precision
  └─ 低（< 0.7）→ 检索精度不足，噪声文档太多
     → 行动: 添加 ReRank 重排序、优化 Query 改写、提高相似度阈值

Step 3: 看 Faithfulness
  └─ 低（< 0.8）→ 生成有幻觉
     → 行动: 加强 Prompt 约束、降低 temperature、添加引用要求

Step 4: 看 Answer Relevancy
  └─ 低（< 0.8）→ 答案跑题
     → 行动: 优化 Prompt 设计、改进上下文注入方式、减少无关上下文
```

**优化闭环：**
```
评估 → 诊断瓶颈 → 针对性优化 → 再评估 → 确认改进 → 下一轮...
```

**关键原则：**
- 每次只改一个变量（否则不知道是哪个改动起了作用）
- 每次改动后都要重新评估（确保改进是真实的）
- 记录每次实验的参数和结果（方便回溯）

**应用：** 评估驱动优化是 RAG 系统从"能用"到"好用"的核心方法论。没有这个闭环，优化就是盲人摸象。

---

## 卡片10：构建评估体系的最佳实践

**一句话：** 好的评估体系 = 好的数据集 + 多维度指标 + 持续监控。

**三步构建：**

**第一步：构建评估数据集（最重要！）**
- 准备 50-100 个问答对
- 覆盖常见问题和边缘案例
- 每条数据包含：question、ground_truth、relevant_docs

```python
# 评估数据集格式示例
eval_data = [
    {
        "question": "什么是 RAG？",
        "ground_truth": "RAG 是检索增强生成技术，结合检索和生成...",
        "relevant_docs": ["doc_rag_intro.pdf", "doc_rag_overview.pdf"],
    },
    {
        "question": "如何优化 Chunk 大小？",
        "ground_truth": "Chunk 大小通常在 200-1000 token 之间...",
        "relevant_docs": ["doc_chunking.pdf"],
    },
]
```

**第二步：选择评估指标**
- 检索侧：Precision@5 + Recall@5 + MRR
- 生成侧：Faithfulness + Answer Relevancy
- 端到端：RAGAS 综合评估

**第三步：建立持续评估机制**
- 每次代码修改后自动跑评估（CI/CD 集成）
- 定期人工抽检（每周 10-20 条）
- 收集用户反馈（点赞/点踩）
- 监控指标趋势（发现退化及时告警）

```python
# 最小评估流程示例
def run_evaluation(rag_pipeline, eval_dataset):
    """每次修改后运行的评估流程"""
    results = evaluate(eval_dataset, metrics=[
        context_precision, context_recall,
        faithfulness, answer_relevancy,
    ])

    # 设置告警阈值
    thresholds = {
        "faithfulness": 0.8,
        "answer_relevancy": 0.8,
        "context_recall": 0.7,
        "context_precision": 0.7,
    }

    for metric, threshold in thresholds.items():
        if results[metric] < threshold:
            print(f"[告警] {metric} = {results[metric]:.2f}，低于阈值 {threshold}")

    return results
```

**应用：** 评估不是一次性的工作，而是贯穿 RAG 系统整个生命周期的持续活动。越早建立评估体系，后续优化越高效。

---

## 学习检查清单

完成以上 10 个卡片后，检查你是否掌握了以下要点：

### 基础理解
- [ ] 理解为什么需要评估 RAG 系统（不能靠"感觉"判断好坏）
- [ ] 掌握评估指标的数学本质（衡量实际输出与期望输出的距离）
- [ ] 了解 RAGAS 框架的 4 个核心指标

### 检索侧评估
- [ ] 掌握 Precision 和 Recall 的区别与 trade-off
- [ ] 理解 MRR 和 NDCG 的适用场景
- [ ] 能根据业务场景选择合适的检索指标

### 生成侧评估
- [ ] 理解 Faithfulness 的计算方法（拆分陈述 → 逐一验证）
- [ ] 理解 Answer Relevancy 的计算方法（反向生成问题）
- [ ] 知道 LLM-as-Judge 的基本原理和关键技巧

### 优化实践
- [ ] 能根据指标诊断优化方向（决策树）
- [ ] 了解评估数据集的构建方法
- [ ] 理解持续评估机制的重要性

---

## 快速参考卡

### 核心指标速查

| 指标 | 类型 | 衡量什么 | 理想值 |
|------|------|----------|--------|
| Precision@K | 检索侧 | 检索结果的准确性 | > 0.7 |
| Recall@K | 检索侧 | 检索结果的完整性 | > 0.7 |
| MRR | 检索侧 | 第一个正确结果的排名 | > 0.6 |
| NDCG | 检索侧 | 整体排序质量 | > 0.7 |
| Faithfulness | 生成侧 | 答案忠于上下文 | > 0.8 |
| Answer Relevancy | 生成侧 | 答案回答了问题 | > 0.8 |

### 诊断速查

```
Recall 低 → 增大 top_k / 换 Embedding 模型 / 调 Chunk 大小
Precision 低 → 添加 ReRank / 优化 Query / 提高阈值
Faithfulness 低 → 加强 Prompt 约束 / 降低 temperature
Relevancy 低 → 优化 Prompt 设计 / 减少无关上下文
```

---

## 总结

**通过这 10 个知识卡片，你已经掌握了：**

1. 为什么需要评估 RAG 系统
2. 评估指标的数学本质
3. RAGAS 框架的核心指标
4. Precision 与 Recall 的 trade-off
5. MRR 与 NDCG 的适用场景
6. Faithfulness 忠实度评估
7. Answer Relevancy 相关性评估
8. LLM-as-Judge 自动化评估
9. 评估驱动优化的方法论
10. 构建评估体系的最佳实践

**记住这句话：**

> **评估与调优的核心：用指标量化质量，用数据驱动优化——从"为什么评估"到"怎么优化"，构建完整的 RAG 评估认知体系。**

**下一步行动：**

1. 构建一个包含 50 条问答对的评估数据集
2. 用 RAGAS 跑一次端到端评估
3. 根据指标诊断瓶颈，针对性优化
4. 建立持续评估机制，每次修改后自动评估
5. 深入学习 LangChain/LlamaIndex 内置的评估工具
