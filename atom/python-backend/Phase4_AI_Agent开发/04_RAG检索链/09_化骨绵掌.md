# RAG检索链 - 化骨绵掌

> 10个2分钟知识卡片，全面掌握 RAG 核心知识

---

## 卡片1：RAG 的本质

**一句话：** RAG 是通过检索外部知识、注入 LLM 上下文、让 LLM 基于真实文档生成答案的技术。

**核心公式：**
```
RAG = Retrieval(检索) + Augmented(增强) + Generation(生成)
```

**举例：**
```python
# 传统 LLM
answer = llm("我们公司的休假政策是什么？")
# 输出："我不知道" 或编造答案

# RAG
docs = retriever.search("休假政策")  # 检索
context = inject(docs)  # 增强
answer = llm(context + "休假政策是什么？")  # 生成
# 输出：基于真实文档的准确答案
```

**应用：** 解决 LLM 知识过时和幻觉问题，让 AI 基于最新、准确的知识回答。

---

## 卡片2：Embedding 向量化

**一句话：** Embedding 将文本转换为高维向量，相似的文本映射到向量空间中相近的位置。

**核心原理：**
```
文本 → Embedding模型 → 向量（1536维）
"Python 编程" → [0.2, 0.8, 0.3, ...]
"Python 程序设计" → [0.21, 0.79, 0.31, ...]  # 向量很接近
"今天天气好" → [0.9, 0.1, 0.2, ...]  # 向量很远
```

**举例：**
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
vec1 = embeddings.embed_query("Python 编程")
vec2 = embeddings.embed_query("Python 程序设计")

# 计算相似度
similarity = cosine_similarity(vec1, vec2)  # 0.95（非常相似）
```

**应用：** RAG 的基础，让计算机理解文本的语义相似度。

---

## 卡片3：向量数据库

**一句话：** 向量数据库存储文档向量，支持高效的语义相似度检索。

**核心对比：**
```
传统数据库：WHERE title LIKE '%Python%'  # 关键词匹配
向量数据库：similarity_search("编程语言")  # 语义检索
→ 能找到"Python"、"Java"、"Go"等相关文档
```

**举例：**
```python
# ChromaDB（简单易用）
vectorstore = Chroma.from_texts(documents, embeddings)
results = vectorstore.similarity_search("Python", k=3)

# FAISS（高性能）
vectorstore = FAISS.from_texts(documents, embeddings)
results = vectorstore.similarity_search("Python", k=3)
```

**应用：** 在海量文档中快速找到最相关的内容（毫秒级）。

---

## 卡片4：文档分块 Chunking

**一句话：** 将长文档切成小块，每块独立向量化和检索，适配 LLM 上下文窗口限制。

**核心策略：**
```
长文档（10000字）
    ↓ 分块
块1（500字）→ 向量1
块2（500字）→ 向量2  ← 重叠50字（避免语义截断）
块3（500字）→ 向量3
...
```

**举例：**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # 每块500字符
    chunk_overlap=50,      # 重叠50字符
    separators=["\n\n", "\n", " ", ""]  # 优先按段落分割
)

chunks = splitter.split_text(long_document)
```

**应用：** 处理长文档（如PDF、技术文档），只检索最相关的块。

---

## 卡片5：Top-K 检索

**一句话：** 只返回相似度最高的 K 个文档，平衡精度和召回率。

**核心权衡：**
```
K 太小（k=1）：可能漏掉重要信息
K 太大（k=20）：引入噪音，浪费 token
K 适中（k=3-5）：平衡精度和召回率 ✅
```

**举例：**
```python
# 基础检索
results = vectorstore.similarity_search(query, k=3)

# 带相似度分数
results = vectorstore.similarity_search_with_score(query, k=5)
for doc, score in results:
    if score > 0.7:  # 设置阈值过滤噪音
        print(doc.page_content)
```

**应用：** 控制检索结果数量，避免信息过载。

---

## 卡片6：Prompt 工程

**一句话：** 设计 Prompt 模板，明确告诉 LLM 如何使用检索到的文档。

**核心模板：**
```python
template = """
你是一个专业的问答助手。请严格基于以下文档回答问题。

规则：
1. 只使用文档中的信息
2. 如果文档中没有答案，说"我不知道"
3. 引用文档时注明来源

文档：
{context}

问题：{question}

答案：
"""
```

**举例：**
```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

formatted = prompt.format(
    context="Python 是一门编程语言",
    question="什么是 Python？"
)
```

**应用：** 防止 LLM 编造答案，确保回答基于文档。

---

## 卡片7：混合检索

**一句话：** 结合向量检索（语义相似）和关键词检索（精确匹配），提升检索精度。

**核心原理：**
```
向量检索：找到语义相似的文档
    ↓
关键词检索（BM25）：找到包含关键词的文档
    ↓
融合结果：加权合并，返回 Top-K
```

**举例：**
```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# 向量检索器
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# 关键词检索器
bm25_retriever = BM25Retriever.from_texts(documents)

# 混合检索器
ensemble = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.5, 0.5]  # 各占50%
)

results = ensemble.get_relevant_documents(query)
```

**应用：** 处理包含专有名词、代码、数字的查询（如"GPT-4"）。

---

## 卡片8：上下文窗口管理

**一句话：** 管理 LLM 的上下文窗口限制，确保检索到的文档不超过 token 限制。

**核心限制：**
```
GPT-3.5-turbo：4K tokens（约3000字）
GPT-4：8K tokens（约6000字）
GPT-4-32K：32K tokens（约24000字）
Claude-3：100K tokens（约75000字）
```

**举例：**
```python
import tiktoken

def count_tokens(text: str) -> int:
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(text))

def truncate_context(docs: list[str], max_tokens: int) -> list[str]:
    selected = []
    total_tokens = 0

    for doc in docs:
        doc_tokens = count_tokens(doc)
        if total_tokens + doc_tokens <= max_tokens:
            selected.append(doc)
            total_tokens += doc_tokens
        else:
            break

    return selected
```

**应用：** 避免超出上下文窗口导致错误，优化 token 使用。

---

## 卡片9：引用来源追踪

**一句话：** 追踪答案的来源文档，提供可验证性和可追溯性。

**核心价值：**
```
纯 LLM：答案无法验证 ❌
RAG + 引用：答案可追溯到具体文档 ✅
```

**举例：**
```python
# 创建带元数据的向量库
vectorstore = Chroma.from_texts(
    texts=documents,
    metadatas=[
        {"source": "python_intro.pdf", "page": 1},
        {"source": "fastapi_guide.md", "page": 3}
    ]
)

# 检索时返回来源
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True  # 返回来源
)

result = qa_chain.invoke({"query": "什么是 Python？"})
print(result["result"])  # 答案
print(result["source_documents"])  # 来源文档
```

**应用：** 法律咨询、医疗问答等需要验证来源的场景。

---

## 卡片10：RAG 的常见误区

**一句话：** 避免三大误区：检索越多越好、相似度等于相关性、RAG 万能。

**误区1：检索越多越好 ❌**
```
k=3：精准，少噪音 ✅
k=20：噪音多，干扰 LLM ❌
```

**误区2：相似度 = 相关性 ❌**
```
查询："Python 如何读取文件？"
文档1："Python 文件读取方法"（相似度0.92，相关 ✅）
文档2："Java 如何读取文件？"（相似度0.89，不相关 ❌）
→ 需要混合检索或 ReRank
```

**误区3：RAG 万能 ❌**
```
✅ 适合：知识检索（"公司休假政策"）
❌ 不适合：实时数据（"现在几点？"）
❌ 不适合：创意生成（"写一首诗"）
❌ 不适合：计算任务（"123 * 456"）
```

**应用：** 正确理解 RAG 的能力边界，选择合适的技术方案。

---

## 知识卡片总结

| 卡片 | 核心知识 | 关键技术 |
|------|----------|----------|
| 1 | RAG 本质 | 检索 + 增强 + 生成 |
| 2 | Embedding | 文本 → 向量 |
| 3 | 向量数据库 | ChromaDB、FAISS |
| 4 | 文档分块 | chunk_size、overlap |
| 5 | Top-K 检索 | k=3-5 |
| 6 | Prompt 工程 | 明确指令、规则 |
| 7 | 混合检索 | 向量 + 关键词 |
| 8 | 上下文管理 | token 计数、截断 |
| 9 | 引用追踪 | 元数据、来源 |
| 10 | 常见误区 | 避免过度检索 |

---

## 学习路径建议

### 快速入门（1小时）
```
卡片1 → 卡片2 → 卡片5 → 卡片6
理解 RAG 本质 → Embedding → 检索 → Prompt
```

### 深度学习（3小时）
```
全部10个卡片 + 实战代码
理论 + 实践结合
```

### 面试准备（30分钟）
```
卡片1 + 卡片10 + 面试必问
核心概念 + 常见误区 + 面试题
```

---

## 检查清单

完成学习后，检查是否掌握：

### 理论理解
- [ ] 能解释 RAG 的三个步骤
- [ ] 理解 Embedding 的作用
- [ ] 知道向量检索的原理
- [ ] 明白 Prompt 工程的重要性

### 实战能力
- [ ] 能实现基础的 RAG 链
- [ ] 会处理文档加载和分块
- [ ] 能使用 ChromaDB 或 FAISS
- [ ] 会优化检索效果
- [ ] 能管理上下文窗口

### 进阶理解
- [ ] 知道 RAG 的常见误区
- [ ] 能回答面试问题
- [ ] 理解 RAG 的优化方向
- [ ] 能将 RAG 集成到 FastAPI

---

## 下一步学习

掌握 RAG 检索链后，可以继续学习：

1. **对话记忆管理** - 多轮对话的上下文维护
2. **Agent 执行器** - 自主调用工具的智能 Agent
3. **流式输出集成** - 实时显示 AI 生成内容
4. **RAG 进阶优化** - ReRank、Query改写、长文档处理

---

**总结**：这10个知识卡片覆盖了 RAG 的所有核心知识点，每个卡片2分钟可以看完，总共20分钟就能建立完整的 RAG 知识体系。建议反复阅读，直到能够不看卡片就能解释每个概念。
