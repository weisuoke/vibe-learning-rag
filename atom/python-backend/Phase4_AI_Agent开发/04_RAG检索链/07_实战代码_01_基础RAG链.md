# RAG检索链 - 实战代码1：基础RAG链

> 最简单的 RAG 实现，从零开始构建完整流程

---

## 代码概述

本文档提供一个完整可运行的基础 RAG 实现，包含：
1. 文档加载
2. 向量化
3. 向量存储
4. 检索
5. LLM 生成

**目标**：20 分钟内理解并运行一个完整的 RAG 系统。

---

## 完整代码

```python
"""
基础 RAG 链实现
演示：文档问答的完整流程
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# 加载环境变量
load_dotenv()

# ===== 1. 准备文档 =====
print("=== 步骤1：准备文档 ===")

documents = [
    "Python 是一门高级编程语言，由 Guido van Rossum 于 1991 年创建。",
    "Python 的设计哲学强调代码的可读性和简洁的语法。",
    "Python 支持多种编程范式，包括面向对象、命令式、函数式和过程式编程。",
    "FastAPI 是一个现代、快速（高性能）的 Web 框架，用于构建 API。",
    "FastAPI 基于 Python 3.6+ 的类型提示，提供自动的 API 文档生成。",
    "RAG（检索增强生成）是一种结合检索和生成的技术，用于提升 LLM 的回答质量。",
    "RAG 通过检索外部知识库，为 LLM 提供上下文，从而生成更准确的答案。"
]

print(f"准备了 {len(documents)} 个文档")
for i, doc in enumerate(documents, 1):
    print(f"  文档{i}: {doc[:50]}...")

# ===== 2. 创建 Embedding 模型 =====
print("\n=== 步骤2：创建 Embedding 模型 ===")

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
print("✓ Embedding 模型创建成功")

# ===== 3. 创建向量库 =====
print("\n=== 步骤3：创建向量库 ===")

vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    persist_directory="./chroma_db_basic"  # 持久化存储
)
print(f"✓ 向量库创建成功，存储了 {len(documents)} 个文档")

# ===== 4. 测试检索 =====
print("\n=== 步骤4：测试检索 ===")

test_query = "谁创建了 Python？"
print(f"测试问题: {test_query}")

# 检索相关文档
retrieved_docs = vectorstore.similarity_search(test_query, k=2)
print(f"检索到 {len(retrieved_docs)} 个相关文档:")
for i, doc in enumerate(retrieved_docs, 1):
    print(f"  文档{i}: {doc.page_content}")

# ===== 5. 创建 LLM =====
print("\n=== 步骤5：创建 LLM ===")

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0  # 确定性输出
)
print("✓ LLM 创建成功")

# ===== 6. 定义 Prompt 模板 =====
print("\n=== 步骤6：定义 Prompt 模板 ===")

prompt_template = """
你是一个专业的问答助手。请基于以下文档回答问题。

规则：
1. 只使用文档中的信息
2. 如果文档中没有答案，说"根据提供的文档，我无法回答这个问题"
3. 保持回答简洁准确

文档：
{context}

问题：{question}

答案：
"""

PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)
print("✓ Prompt 模板定义成功")

# ===== 7. 创建 RAG 链 =====
print("\n=== 步骤7：创建 RAG 链 ===")

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # 将所有文档塞入一个 Prompt
    retriever=vectorstore.as_retriever(
        search_kwargs={"k": 2}  # 检索 Top-2 文档
    ),
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True  # 返回引用来源
)
print("✓ RAG 链创建成功")

# ===== 8. 测试问答 =====
print("\n=== 步骤8：测试问答 ===\n")

# 测试问题列表
test_questions = [
    "谁创建了 Python？",
    "Python 的设计哲学是什么？",
    "什么是 FastAPI？",
    "什么是 RAG？",
    "Java 的特点是什么？"  # 文档中没有的问题
]

for question in test_questions:
    print(f"问题: {question}")

    # 调用 RAG 链
    result = qa_chain.invoke({"query": question})

    print(f"答案: {result['result']}")

    # 显示引用来源
    print(f"引用来源:")
    for i, doc in enumerate(result['source_documents'], 1):
        print(f"  [{i}] {doc.page_content[:80]}...")

    print("-" * 80)

# ===== 9. 性能统计 =====
print("\n=== 步骤9：性能统计 ===")

import time

# 测试检索速度
start_time = time.time()
for _ in range(10):
    vectorstore.similarity_search("测试问题", k=2)
retrieval_time = (time.time() - start_time) / 10

print(f"平均检索时间: {retrieval_time*1000:.2f} ms")

# 测试端到端速度
start_time = time.time()
qa_chain.invoke({"query": "什么是 Python？"})
e2e_time = time.time() - start_time

print(f"端到端响应时间: {e2e_time:.2f} 秒")

print("\n✓ 基础 RAG 链演示完成！")
```

---

## 运行输出示例

```
=== 步骤1：准备文档 ===
准备了 7 个文档
  文档1: Python 是一门高级编程语言，由 Guido van Rossum 于 1991 年创建。...
  文档2: Python 的设计哲学强调代码的可读性和简洁的语法。...
  ...

=== 步骤2：创建 Embedding 模型 ===
✓ Embedding 模型创建成功

=== 步骤3：创建向量库 ===
✓ 向量库创建成功，存储了 7 个文档

=== 步骤4：测试检索 ===
测试问题: 谁创建了 Python？
检索到 2 个相关文档:
  文档1: Python 是一门高级编程语言，由 Guido van Rossum 于 1991 年创建。
  文档2: Python 的设计哲学强调代码的可读性和简洁的语法。

=== 步骤5：创建 LLM ===
✓ LLM 创建成功

=== 步骤6：定义 Prompt 模板 ===
✓ Prompt 模板定义成功

=== 步骤7：创建 RAG 链 ===
✓ RAG 链创建成功

=== 步骤8：测试问答 ===

问题: 谁创建了 Python？
答案: Guido van Rossum 于 1991 年创建了 Python。
引用来源:
  [1] Python 是一门高级编程语言，由 Guido van Rossum 于 1991 年创建。...
  [2] Python 的设计哲学强调代码的可读性和简洁的语法。...
--------------------------------------------------------------------------------
问题: Python 的设计哲学是什么？
答案: Python 的设计哲学强调代码的可读性和简洁的语法。
引用来源:
  [1] Python 的设计哲学强调代码的可读性和简洁的语法。...
  [2] Python 是一门高级编程语言，由 Guido van Rossum 于 1991 年创建。...
--------------------------------------------------------------------------------
问题: 什么是 FastAPI？
答案: FastAPI 是一个现代、快速（高性能）的 Web 框架，用于构建 API。
引用来源:
  [1] FastAPI 是一个现代、快速（高性能）的 Web 框架，用于构建 API。...
  [2] FastAPI 基于 Python 3.6+ 的类型提示，提供自动的 API 文档生成。...
--------------------------------------------------------------------------------
问题: 什么是 RAG？
答案: RAG（检索增强生成）是一种结合检索和生成的技术，用于提升 LLM 的回答质量。
引用来源:
  [1] RAG（检索增强生成）是一种结合检索和生成的技术，用于提升 LLM 的回答质量。...
  [2] RAG 通过检索外部知识库，为 LLM 提供上下文，从而生成更准确的答案。...
--------------------------------------------------------------------------------
问题: Java 的特点是什么？
答案: 根据提供的文档，我无法回答这个问题。
引用来源:
  [1] Python 是一门高级编程语言，由 Guido van Rossum 于 1991 年创建。...
  [2] FastAPI 是一个现代、快速（高性能）的 Web 框架，用于构建 API。...
--------------------------------------------------------------------------------

=== 步骤9：性能统计 ===
平均检索时间: 15.32 ms
端到端响应时间: 1.23 秒

✓ 基础 RAG 链演示完成！
```

---

## 代码详解

### 1. 文档准备

```python
documents = [
    "Python 是一门高级编程语言...",
    "Python 的设计哲学...",
    ...
]
```

**说明**：
- 这里使用简单的字符串列表
- 实际项目中，文档来自 PDF、TXT、数据库等
- 后续章节会讲解文档加载

### 2. Embedding 模型

```python
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
```

**说明**：
- `text-embedding-3-small`：1536 维，性价比高
- 也可以用 `text-embedding-3-large`：3072 维，更精确但更贵
- 需要设置 `OPENAI_API_KEY` 环境变量

### 3. 向量库创建

```python
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    persist_directory="./chroma_db_basic"
)
```

**说明**：
- `from_texts`：直接从文本列表创建
- `persist_directory`：持久化存储，下次可以直接加载
- ChromaDB 会自动向量化所有文档

### 4. 检索测试

```python
retrieved_docs = vectorstore.similarity_search(test_query, k=2)
```

**说明**：
- `similarity_search`：基于余弦相似度检索
- `k=2`：返回最相关的 2 个文档
- 返回 `Document` 对象列表

### 5. Prompt 模板

```python
prompt_template = """
你是一个专业的问答助手。请基于以下文档回答问题。

规则：
1. 只使用文档中的信息
2. 如果文档中没有答案，说"根据提供的文档，我无法回答这个问题"
3. 保持回答简洁准确

文档：
{context}

问题：{question}

答案：
"""
```

**说明**：
- `{context}`：检索到的文档会注入这里
- `{question}`：用户问题
- 明确的规则防止 LLM 编造答案

### 6. RAG 链创建

```python
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True
)
```

**说明**：
- `chain_type="stuff"`：将所有文档塞入一个 Prompt（最简单）
- `retriever`：检索器，封装了向量库的检索逻辑
- `return_source_documents=True`：返回引用来源

### 7. 调用 RAG 链

```python
result = qa_chain.invoke({"query": question})

print(result['result'])  # 答案
print(result['source_documents'])  # 引用来源
```

**说明**：
- `invoke`：同步调用
- 返回字典，包含 `result` 和 `source_documents`

---

## 手写简化版（理解原理）

如果你想理解 RAG 的底层原理，可以手写一个简化版：

```python
"""
手写简化版 RAG（理解原理）
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
import numpy as np

# 1. 准备文档
documents = [
    "Python 是一门编程语言，由 Guido van Rossum 创建。",
    "FastAPI 是一个 Web 框架。",
    "RAG 是检索增强生成技术。"
]

# 2. 向量化文档
embeddings = OpenAIEmbeddings()
doc_vectors = embeddings.embed_documents(documents)

print(f"文档向量维度: {len(doc_vectors[0])}")

# 3. 定义检索函数
def retrieve(query: str, k: int = 2):
    """检索最相关的 k 个文档"""
    # 向量化问题
    query_vector = embeddings.embed_query(query)

    # 计算相似度
    similarities = []
    for i, doc_vector in enumerate(doc_vectors):
        # 余弦相似度
        sim = np.dot(query_vector, doc_vector) / (
            np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
        )
        similarities.append((i, sim))

    # 排序，返回 Top-K
    similarities.sort(key=lambda x: x[1], reverse=True)
    top_k = similarities[:k]

    return [documents[i] for i, _ in top_k]

# 4. 定义生成函数
def generate(query: str, context_docs: list[str]) -> str:
    """基于上下文生成答案"""
    llm = ChatOpenAI(temperature=0)

    # 构造 Prompt
    context = "\n\n".join(context_docs)
    prompt = f"""
基于以下文档回答问题。如果文档中没有答案，说"我不知道"。

文档：
{context}

问题：{query}

答案：
"""

    # LLM 生成答案
    response = llm.invoke(prompt)
    return response.content

# 5. RAG 流程
def rag(query: str) -> str:
    """完整的 RAG 流程"""
    # 检索
    context_docs = retrieve(query, k=2)
    print(f"检索到的文档: {context_docs}")

    # 生成
    answer = generate(query, context_docs)
    return answer

# 测试
question = "谁创建了 Python？"
answer = rag(question)

print(f"\n问题: {question}")
print(f"答案: {answer}")
```

**输出**：
```
文档向量维度: 1536
检索到的文档: ['Python 是一门编程语言，由 Guido van Rossum 创建。', 'FastAPI 是一个 Web 框架。']

问题: 谁创建了 Python？
答案: Guido van Rossum 创建了 Python。
```

---

## 常见问题

### Q1: 为什么检索到的文档不相关？

**A**: 可能的原因：
1. 文档太少，没有相关内容
2. Embedding 模型理解有限
3. k 值设置不当

**解决方案**：
```python
# 查看检索结果和相似度
results = vectorstore.similarity_search_with_score(query, k=5)
for doc, score in results:
    print(f"相似度: {score:.4f} - {doc.page_content[:50]}...")

# 如果相似度都很低（<0.7），说明文档中确实没有相关内容
```

### Q2: 如何加载已有的向量库？

**A**: 使用 `persist_directory` 参数。

```python
# 第一次：创建并保存
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 第二次：直接加载
vectorstore = Chroma(
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)
```

### Q3: 如何添加新文档到已有向量库？

**A**: 使用 `add_texts` 方法。

```python
# 加载已有向量库
vectorstore = Chroma(
    embedding_function=embeddings,
    persist_directory="./chroma_db"
)

# 添加新文档
new_documents = ["新文档1", "新文档2"]
vectorstore.add_texts(new_documents)

# 自动持久化
```

### Q4: 如何调试 RAG 链？

**A**: 使用 `verbose=True` 参数。

```python
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    verbose=True  # 打印中间步骤
)

result = qa_chain.invoke({"query": "测试问题"})
# 会打印：检索到的文档、构造的 Prompt、LLM 的输出
```

---

## 优化建议

### 1. 使用缓存

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# 启用缓存
set_llm_cache(InMemoryCache())

# 相同问题会直接返回缓存结果
```

### 2. 批量处理

```python
# ❌ 低效：逐个问题
for question in questions:
    answer = qa_chain.invoke({"query": question})

# ✅ 高效：批量处理
answers = qa_chain.batch([{"query": q} for q in questions])
```

### 3. 异步调用

```python
import asyncio

# 异步调用
async def async_rag(question: str):
    result = await qa_chain.ainvoke({"query": question})
    return result

# 并发处理多个问题
answers = await asyncio.gather(*[
    async_rag(q) for q in questions
])
```

---

## 下一步

掌握基础 RAG 链后，可以学习：

1. **[文档处理](./07_实战代码_02_文档处理.md)** - 加载 PDF/TXT/Markdown
2. **[向量存储](./07_实战代码_03_向量存储.md)** - FAISS、持久化
3. **[检索优化](./07_实战代码_04_检索优化.md)** - 混合检索、ReRank
4. **[上下文管理](./07_实战代码_05_上下文管理.md)** - 窗口管理、引用追踪
5. **[FastAPI集成](./07_实战代码_06_FastAPI集成.md)** - API 服务化

---

**总结**：这是最简单的 RAG 实现，只需 50 行代码就能构建一个完整的文档问答系统。理解这个基础版本后，后续的优化都是在此基础上的改进。
