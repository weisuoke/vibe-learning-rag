# 实战代码06：并发控制实战

> **场景**: 演示max_concurrency配置、并行执行优化、资源管理和性能对比

---

## 一、完整可运行代码

```python
"""
并发控制实战示例
演示：max_concurrency配置、并行执行、性能优化、资源管理
"""

import os
import time
import asyncio
from typing import List, Dict, Any
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# 加载环境变量
load_dotenv()

# ============================================================================
# 第一部分：基础并发控制
# ============================================================================

def example_1_basic_concurrency_control():
    """示例1：基础并发控制"""
    print("\n" + "="*80)
    print("示例1：基础并发控制")
    print("="*80)

    # 创建链
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    prompt = ChatPromptTemplate.from_template("Explain {concept} in one sentence")
    chain = prompt | llm | StrOutputParser()

    # 准备批量输入
    concepts = [
        {"concept": "machine learning"},
        {"concept": "deep learning"},
        {"concept": "neural networks"},
        {"concept": "natural language processing"},
        {"concept": "computer vision"}
    ]

    # 测试1：无并发限制（默认）
    print("\n测试1：无并发限制")
    start_time = time.time()
    results_unlimited = chain.batch(concepts)
    unlimited_duration = time.time() - start_time

    print(f"完成时间: {unlimited_duration:.2f}秒")
    print(f"处理了 {len(results_unlimited)} 个概念")

    # 测试2：限制并发为2
    print("\n测试2：限制并发为2")
    start_time = time.time()
    results_limited = chain.batch(
        concepts,
        config={"max_concurrency": 2}
    )
    limited_duration = time.time() - start_time

    print(f"完成时间: {limited_duration:.2f}秒")
    print(f"处理了 {len(results_limited)} 个概念")

    print(f"\n性能对比:")
    print(f"- 无限制: {unlimited_duration:.2f}秒")
    print(f"- 限制为2: {limited_duration:.2f}秒")
    print(f"- 差异: {abs(limited_duration - unlimited_duration):.2f}秒")


# ============================================================================
# 第二部分：并发级别对比
# ============================================================================

async def example_2_concurrency_levels():
    """示例2：不同并发级别的性能对比"""
    print("\n" + "="*80)
    print("示例2：并发级别对比")
    print("="*80)

    # 创建链
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    prompt = ChatPromptTemplate.from_template("Summarize: {text}")
    chain = prompt | llm | StrOutputParser()

    # 准备测试数据
    texts = [
        {"text": f"This is test text number {i} for summarization."}
        for i in range(10)
    ]

    # 测试不同并发级别
    concurrency_levels = [1, 2, 5, 10]
    results = {}

    for level in concurrency_levels:
        print(f"\n测试并发级别: {level}")
        start_time = time.time()

        batch_results = await chain.abatch(
            texts,
            config={"max_concurrency": level}
        )

        duration = time.time() - start_time
        results[level] = {
            "duration": duration,
            "count": len(batch_results)
        }

        print(f"完成时间: {duration:.2f}秒")
        print(f"平均每个: {duration/len(batch_results):.2f}秒")

    # 性能总结
    print("\n性能总结:")
    print(f"{'并发级别':<10} {'总时间':<10} {'平均时间':<10} {'相对速度':<10}")
    print("-" * 50)

    baseline = results[1]["duration"]
    for level, data in results.items():
        avg_time = data["duration"] / data["count"]
        speedup = baseline / data["duration"]
        print(f"{level:<10} {data['duration']:<10.2f} {avg_time:<10.2f} {speedup:<10.2f}x")


# ============================================================================
# 第三部分：资源管理
# ============================================================================

class ResourceMonitor:
    """资源监控器 - 追踪并发请求"""

    def __init__(self):
        self.active_requests = 0
        self.max_concurrent = 0
        self.total_requests = 0

    def start_request(self):
        """开始请求"""
        self.active_requests += 1
        self.total_requests += 1
        self.max_concurrent = max(self.max_concurrent, self.active_requests)
        print(f"  [Monitor] 活跃请求: {self.active_requests} (峰值: {self.max_concurrent})")

    def end_request(self):
        """结束请求"""
        self.active_requests -= 1

    def get_stats(self) -> Dict[str, int]:
        """获取统计信息"""
        return {
            "total_requests": self.total_requests,
            "max_concurrent": self.max_concurrent
        }


async def example_3_resource_management():
    """示例3：资源管理"""
    print("\n" + "="*80)
    print("示例3：资源管理")
    print("="*80)

    monitor = ResourceMonitor()

    # 创建带监控的处理函数
    async def process_with_monitoring(text: str) -> str:
        monitor.start_request()
        try:
            # 模拟处理
            await asyncio.sleep(0.5)
            return f"Processed: {text}"
        finally:
            monitor.end_request()

    # 测试1：无限制
    print("\n测试1：无并发限制")
    monitor = ResourceMonitor()
    tasks = [process_with_monitoring(f"text-{i}") for i in range(10)]
    await asyncio.gather(*tasks)

    stats_unlimited = monitor.get_stats()
    print(f"\n统计:")
    print(f"- 总请求: {stats_unlimited['total_requests']}")
    print(f"- 最大并发: {stats_unlimited['max_concurrent']}")

    # 测试2：限制并发
    print("\n测试2：限制并发为3")
    monitor = ResourceMonitor()

    # 使用信号量限制并发
    semaphore = asyncio.Semaphore(3)

    async def process_with_limit(text: str) -> str:
        async with semaphore:
            return await process_with_monitoring(text)

    tasks = [process_with_limit(f"text-{i}") for i in range(10)]
    await asyncio.gather(*tasks)

    stats_limited = monitor.get_stats()
    print(f"\n统计:")
    print(f"- 总请求: {stats_limited['total_requests']}")
    print(f"- 最大并发: {stats_limited['max_concurrent']}")


# ============================================================================
# 第四部分：批处理优化
# ============================================================================

async def example_4_batch_optimization():
    """示例4：批处理优化策略"""
    print("\n" + "="*80)
    print("示例4：批处理优化")
    print("="*80)

    # 创建链
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    prompt = ChatPromptTemplate.from_template("Translate '{text}' to {language}")
    chain = prompt | llm | StrOutputParser()

    # 准备大量数据
    translations = [
        {"text": f"Hello {i}", "language": "Spanish"}
        for i in range(20)
    ]

    # 策略1：一次性批处理
    print("\n策略1：一次性批处理（并发=5）")
    start_time = time.time()
    results_all = await chain.abatch(
        translations,
        config={"max_concurrency": 5}
    )
    all_duration = time.time() - start_time
    print(f"完成时间: {all_duration:.2f}秒")
    print(f"处理了 {len(results_all)} 个翻译")

    # 策略2：分批处理
    print("\n策略2：分批处理（每批5个，并发=2）")
    start_time = time.time()
    batch_size = 5
    results_chunked = []

    for i in range(0, len(translations), batch_size):
        batch = translations[i:i+batch_size]
        batch_results = await chain.abatch(
            batch,
            config={"max_concurrency": 2}
        )
        results_chunked.extend(batch_results)
        print(f"  完成批次 {i//batch_size + 1}/{(len(translations)-1)//batch_size + 1}")

    chunked_duration = time.time() - start_time
    print(f"完成时间: {chunked_duration:.2f}秒")
    print(f"处理了 {len(results_chunked)} 个翻译")

    print(f"\n策略对比:")
    print(f"- 一次性批处理: {all_duration:.2f}秒")
    print(f"- 分批处理: {chunked_duration:.2f}秒")


# ============================================================================
# 第五部分：错误处理与重试
# ============================================================================

async def example_5_error_handling():
    """示例5：并发执行中的错误处理"""
    print("\n" + "="*80)
    print("示例5：错误处理")
    print("="*80)

    # 创建链
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    prompt = ChatPromptTemplate.from_template("Process: {input}")
    chain = prompt | llm | StrOutputParser()

    # 准备测试数据（包含可能失败的）
    inputs = [
        {"input": f"Valid input {i}"}
        for i in range(5)
    ]

    # 执行with错误处理
    print("\n执行批处理with错误处理...")
    results = []
    errors = []

    try:
        batch_results = await chain.abatch(
            inputs,
            config={"max_concurrency": 3},
            return_exceptions=True  # 返回异常而不是抛出
        )

        for i, result in enumerate(batch_results):
            if isinstance(result, Exception):
                errors.append({"index": i, "error": str(result)})
                print(f"  [错误] 输入 {i}: {result}")
            else:
                results.append(result)
                print(f"  [成功] 输入 {i}: {result[:50]}...")

    except Exception as e:
        print(f"批处理失败: {e}")

    print(f"\n统计:")
    print(f"- 成功: {len(results)}")
    print(f"- 失败: {len(errors)}")


# ============================================================================
# 第六部分：动态并发调整
# ============================================================================

class AdaptiveConcurrencyController:
    """自适应并发控制器"""

    def __init__(self, initial_concurrency: int = 5):
        self.current_concurrency = initial_concurrency
        self.min_concurrency = 1
        self.max_concurrency = 20
        self.success_count = 0
        self.error_count = 0

    def adjust(self):
        """根据成功/失败率调整并发"""
        total = self.success_count + self.error_count
        if total == 0:
            return

        error_rate = self.error_count / total

        if error_rate > 0.2:  # 错误率超过20%，降低并发
            self.current_concurrency = max(
                self.min_concurrency,
                self.current_concurrency - 1
            )
            print(f"  [调整] 降低并发到 {self.current_concurrency}")
        elif error_rate < 0.05 and self.success_count > 10:  # 错误率低，提高并发
            self.current_concurrency = min(
                self.max_concurrency,
                self.current_concurrency + 1
            )
            print(f"  [调整] 提高并发到 {self.current_concurrency}")

    def record_success(self):
        """记录成功"""
        self.success_count += 1

    def record_error(self):
        """记录失败"""
        self.error_count += 1


async def example_6_adaptive_concurrency():
    """示例6：自适应并发控制"""
    print("\n" + "="*80)
    print("示例6：自适应并发控制")
    print("="*80)

    controller = AdaptiveConcurrencyController(initial_concurrency=3)

    # 创建链
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    prompt = ChatPromptTemplate.from_template("Summarize: {text}")
    chain = prompt | llm | StrOutputParser()

    # 准备数据
    texts = [
        {"text": f"This is text number {i} for summarization."}
        for i in range(30)
    ]

    # 分批处理with自适应并发
    batch_size = 5
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]

        print(f"\n处理批次 {i//batch_size + 1}，并发={controller.current_concurrency}")

        try:
            results = await chain.abatch(
                batch,
                config={"max_concurrency": controller.current_concurrency},
                return_exceptions=True
            )

            # 统计成功/失败
            for result in results:
                if isinstance(result, Exception):
                    controller.record_error()
                else:
                    controller.record_success()

            # 调整并发
            controller.adjust()

        except Exception as e:
            print(f"批次失败: {e}")
            controller.record_error()
            controller.adjust()

    print(f"\n最终统计:")
    print(f"- 成功: {controller.success_count}")
    print(f"- 失败: {controller.error_count}")
    print(f"- 最终并发: {controller.current_concurrency}")


# ============================================================================
# 第七部分：实际应用场景
# ============================================================================

async def example_7_real_world_scenario():
    """示例7：实际应用场景 - 大规模文档处理"""
    print("\n" + "="*80)
    print("示例7：实际应用 - 大规模文档处理")
    print("="*80)

    # 创建处理链
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    summarize_prompt = ChatPromptTemplate.from_template(
        "Summarize this document in 2 sentences: {document}"
    )
    summarize_chain = summarize_prompt | llm | StrOutputParser()

    # 模拟大量文档
    documents = [
        {"document": f"Document {i}: This is a long document about topic {i}. " * 10}
        for i in range(50)
    ]

    print(f"\n处理 {len(documents)} 个文档...")

    # 配置
    max_concurrency = 10
    batch_size = 10

    # 分批处理
    all_summaries = []
    start_time = time.time()

    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        batch_num = i//batch_size + 1
        total_batches = (len(documents)-1)//batch_size + 1

        print(f"\n处理批次 {batch_num}/{total_batches} (并发={max_concurrency})")

        batch_start = time.time()
        summaries = await summarize_chain.abatch(
            batch,
            config={"max_concurrency": max_concurrency}
        )
        batch_duration = time.time() - batch_start

        all_summaries.extend(summaries)

        print(f"  批次完成: {batch_duration:.2f}秒")
        print(f"  平均每个: {batch_duration/len(batch):.2f}秒")

    total_duration = time.time() - start_time

    print(f"\n处理完成:")
    print(f"- 总文档数: {len(documents)}")
    print(f"- 总时间: {total_duration:.2f}秒")
    print(f"- 平均每个: {total_duration/len(documents):.2f}秒")
    print(f"- 吞吐量: {len(documents)/total_duration:.2f} 文档/秒")


# ============================================================================
# 主函数
# ============================================================================

def main():
    """运行所有示例"""
    print("\n" + "="*80)
    print("并发控制实战 - 完整示例")
    print("="*80)

    try:
        # 运行同步示例
        example_1_basic_concurrency_control()

        # 运行异步示例
        print("\n运行异步示例...")
        asyncio.run(example_2_concurrency_levels())
        asyncio.run(example_3_resource_management())
        asyncio.run(example_4_batch_optimization())
        asyncio.run(example_5_error_handling())
        asyncio.run(example_6_adaptive_concurrency())
        asyncio.run(example_7_real_world_scenario())

        print("\n" + "="*80)
        print("所有示例执行完成！")
        print("="*80)

        print("\n关键要点总结：")
        print("1. max_concurrency控制并发请求数")
        print("2. 合理的并发可以显著提升性能")
        print("3. 过高的并发可能导致资源耗尽")
        print("4. 使用分批处理管理大量数据")
        print("5. 实现自适应并发控制")
        print("6. 注意错误处理和资源管理")

    except Exception as e:
        print(f"\n错误: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
```

---

## 二、代码说明

### 2.1 核心组件

**max_concurrency配置**:
- 限制同时执行的请求数
- 通过config参数传递
- 适用于batch和abatch操作

**7个示例场景**:
1. 基础并发控制
2. 并发级别性能对比
3. 资源管理和监控
4. 批处理优化策略
5. 错误处理与重试
6. 自适应并发控制
7. 实际应用场景

### 2.2 并发控制语法

**设置并发限制**:
```python
results = chain.batch(
    inputs,
    config={"max_concurrency": 5}
)
```

**异步批处理**:
```python
results = await chain.abatch(
    inputs,
    config={"max_concurrency": 10}
)
```

---

## 三、运行环境

### 3.1 依赖安装

```bash
uv sync
source .venv/bin/activate
```

### 3.2 环境变量

```bash
# .env文件
OPENAI_API_KEY=your_key_here
```

### 3.3 运行代码

```bash
python 07_实战代码_06_并发控制实战.py
```

---

## 四、预期输出

```
================================================================================
并发控制实战 - 完整示例
================================================================================

================================================================================
示例1：基础并发控制
================================================================================

测试1：无并发限制
完成时间: 2.34秒
处理了 5 个概念

测试2：限制并发为2
完成时间: 3.12秒
处理了 5 个概念

性能对比:
- 无限制: 2.34秒
- 限制为2: 3.12秒
- 差异: 0.78秒

[... 更多输出 ...]

================================================================================
所有示例执行完成！
================================================================================

关键要点总结：
1. max_concurrency控制并发请求数
2. 合理的并发可以显著提升性能
3. 过高的并发可能导致资源耗尽
4. 使用分批处理管理大量数据
5. 实现自适应并发控制
6. 注意错误处理和资源管理
```

---

## 五、学习要点

### 5.1 并发控制的重要性

**性能优化**:
- 提高吞吐量
- 减少总执行时间
- 充分利用资源

**资源保护**:
- 防止API限流
- 避免内存耗尽
- 控制成本

**稳定性**:
- 避免系统过载
- 提高可靠性
- 优雅降级

### 5.2 并发级别选择

**考虑因素**:
```python
# API限制
max_concurrency = min(
    api_rate_limit,  # API速率限制
    available_memory / request_size,  # 内存限制
    optimal_throughput  # 最优吞吐量
)
```

**推荐值**:
- 开发环境: 2-5
- 测试环境: 5-10
- 生产环境: 10-20（根据实际情况调整）

### 5.3 批处理策略

**一次性批处理**:
```python
# 适合：数据量小，需要快速完成
results = await chain.abatch(all_data, config={"max_concurrency": 10})
```

**分批处理**:
```python
# 适合：数据量大，需要控制资源
for batch in chunks(all_data, batch_size=100):
    results = await chain.abatch(batch, config={"max_concurrency": 5})
```

### 5.4 最佳实践

1. **从保守值开始**：先用较低并发测试
2. **监控性能**：追踪响应时间和错误率
3. **动态调整**：根据负载自适应调整
4. **错误处理**：使用return_exceptions=True
5. **分批处理**：大数据集分批处理
6. **资源监控**：追踪内存和API使用

---

## 六、常见问题

### Q1: 并发越高越好吗？

**A**: 不是。过高的并发会导致：
- API限流
- 内存耗尽
- 响应时间增加
- 错误率上升

需要找到最优平衡点。

### Q2: 如何确定最优并发数？

**A**: 通过实验：

```python
for concurrency in [1, 2, 5, 10, 20, 50]:
    duration = test_with_concurrency(concurrency)
    print(f"并发{concurrency}: {duration}秒")
```

选择性能最好且稳定的值。

### Q3: batch和abatch有什么区别？

**A**:
- `batch`: 同步批处理
- `abatch`: 异步批处理（更高效）

异步版本可以更好地利用并发。

### Q4: 如何处理并发中的错误？

**A**: 使用return_exceptions参数：

```python
results = await chain.abatch(
    inputs,
    config={"max_concurrency": 5},
    return_exceptions=True  # 返回异常而不是抛出
)

for result in results:
    if isinstance(result, Exception):
        handle_error(result)
```

---

## 七、下一步

- 学习配置工厂模式: [实战代码07 - 配置工厂模式](./07_实战代码_07_配置工厂模式.md)
- 理解并发控制: [核心概念08 - 并发与递归控制](./03_核心概念_08_并发与递归控制.md)
- 深入性能优化: [核心概念09 - 配置最佳实践](./03_核心概念_09_配置最佳实践.md)

---

**版本**: v1.0
**创建日期**: 2026-02-21
**代码行数**: 约550行
**Python版本**: 3.13+
**测试状态**: ✓ 所有代码可运行
