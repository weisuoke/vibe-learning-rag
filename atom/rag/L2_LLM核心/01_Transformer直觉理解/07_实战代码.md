# 实战代码

> 动手实践：可视化 Transformer 的注意力机制

---

## 概述

本节提供可运行的 Python 代码，帮助你直观理解 Transformer 的注意力机制。

**你将学到：**
- 如何加载预训练模型并获取注意力权重
- 如何可视化注意力（哪些词在关注哪些词）
- 位置对注意力的影响
- RAG 场景下的注意力分析

---

## 环境准备

```bash
# 安装依赖
pip install transformers torch matplotlib seaborn
```

---

## 示例1：获取并可视化注意力权重

```python
"""
Transformer 注意力可视化
演示：查看模型内部的注意力权重
"""

import torch
from transformers import AutoTokenizer, AutoModel
import matplotlib.pyplot as plt
import seaborn as sns

# ===== 1. 加载模型和分词器 =====
print("=== 加载模型 ===")
model_name = "bert-base-chinese"  # 使用中文 BERT
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)

print(f"模型: {model_name}")
print(f"注意力头数: {model.config.num_attention_heads}")
print(f"隐藏层数: {model.config.num_hidden_layers}")

# ===== 2. 准备输入文本 =====
text = "小猫坐在垫子上"
print(f"\n=== 输入文本 ===")
print(f"原文: {text}")

# 分词
inputs = tokenizer(text, return_tensors="pt")
tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
print(f"分词结果: {tokens}")

# ===== 3. 获取注意力权重 =====
print("\n=== 获取注意力权重 ===")
with torch.no_grad():
    outputs = model(**inputs)

# attentions 是一个元组，每层一个张量
# 形状: (batch_size, num_heads, seq_len, seq_len)
attentions = outputs.attentions
print(f"层数: {len(attentions)}")
print(f"每层形状: {attentions[0].shape}")

# ===== 4. 可视化第一层的注意力 =====
def plot_attention(attention, tokens, layer=0, head=0):
    """绘制注意力热力图"""
    # 获取指定层和头的注意力
    att = attention[layer][0, head].numpy()

    plt.figure(figsize=(10, 8))
    sns.heatmap(
        att,
        xticklabels=tokens,
        yticklabels=tokens,
        cmap="YlOrRd",
        annot=True,
        fmt=".2f"
    )
    plt.title(f"注意力热力图 (Layer {layer}, Head {head})")
    plt.xlabel("被关注的词 (Key)")
    plt.ylabel("关注者 (Query)")
    plt.tight_layout()
    plt.savefig("attention_heatmap.png", dpi=150)
    plt.show()
    print(f"图片已保存: attention_heatmap.png")

# 绘制第一层第一个头的注意力
plot_attention(attentions, tokens, layer=0, head=0)

# ===== 5. 分析注意力模式 =====
print("\n=== 注意力分析 ===")
# 取第一层所有头的平均
avg_attention = attentions[0][0].mean(dim=0).numpy()

# 找出每个词最关注的词
for i, token in enumerate(tokens):
    if token in ["[CLS]", "[SEP]"]:
        continue
    top_idx = avg_attention[i].argsort()[-3:][::-1]  # 前3个
    top_tokens = [tokens[j] for j in top_idx]
    top_scores = [avg_attention[i][j] for j in top_idx]
    print(f"'{token}' 最关注: {list(zip(top_tokens, [f'{s:.2f}' for s in top_scores]))}")
```

**运行输出示例：**
```
=== 加载模型 ===
模型: bert-base-chinese
注意力头数: 12
隐藏层数: 12

=== 输入文本 ===
原文: 小猫坐在垫子上
分词结果: ['[CLS]', '小', '猫', '坐', '在', '垫', '子', '上', '[SEP]']

=== 获取注意力权重 ===
层数: 12
每层形状: torch.Size([1, 12, 9, 9])

=== 注意力分析 ===
'小' 最关注: [('猫', '0.45'), ('小', '0.20'), ('坐', '0.12')]
'猫' 最关注: [('小', '0.38'), ('坐', '0.25'), ('在', '0.15')]
'坐' 最关注: [('在', '0.35'), ('猫', '0.22'), ('垫', '0.18')]
...
```

---

## 示例2：对比不同句子的注意力

```python
"""
对比不同句子的注意力模式
演示：词序变化如何影响注意力
"""

import torch
from transformers import AutoTokenizer, AutoModel

# 加载模型
model_name = "bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)

def get_attention_summary(text):
    """获取文本的注意力摘要"""
    inputs = tokenizer(text, return_tensors="pt")
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    with torch.no_grad():
        outputs = model(**inputs)

    # 取最后一层的平均注意力
    avg_attention = outputs.attentions[-1][0].mean(dim=0).numpy()

    return tokens, avg_attention

# ===== 对比两个句子 =====
print("=== 对比词序变化的影响 ===\n")

sentences = [
    "狗咬人",
    "人咬狗"
]

for sent in sentences:
    tokens, attention = get_attention_summary(sent)
    print(f"句子: {sent}")
    print(f"分词: {tokens}")

    # 显示每个词的注意力分布
    for i, token in enumerate(tokens):
        if token in ["[CLS]", "[SEP]"]:
            continue
        scores = attention[i][1:-1]  # 去掉 CLS 和 SEP
        content_tokens = tokens[1:-1]
        print(f"  '{token}' 的注意力: ", end="")
        for t, s in zip(content_tokens, scores):
            print(f"{t}:{s:.2f} ", end="")
        print()
    print()
```

**运行输出示例：**
```
=== 对比词序变化的影响 ===

句子: 狗咬人
分词: ['[CLS]', '狗', '咬', '人', '[SEP]']
  '狗' 的注意力: 狗:0.35 咬:0.40 人:0.25
  '咬' 的注意力: 狗:0.30 咬:0.25 人:0.45
  '人' 的注意力: 狗:0.20 咬:0.50 人:0.30

句子: 人咬狗
分词: ['[CLS]', '人', '咬', '狗', '[SEP]']
  '人' 的注意力: 人:0.35 咬:0.42 狗:0.23
  '咬' 的注意力: 人:0.32 咬:0.23 狗:0.45
  '狗' 的注意力: 人:0.22 咬:0.48 狗:0.30
```

**观察：** 虽然词相同，但位置不同导致注意力模式不同！

---

## 示例3：RAG 场景 - 上下文注入的影响

```python
"""
RAG 场景：观察检索内容如何影响注意力
演示：模型如何"关注"注入的上下文
"""

import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np

# 使用英文模型（更好的演示效果）
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)

def analyze_rag_attention(question, context):
    """分析 RAG 场景下的注意力"""
    # 构造 RAG 风格的输入
    text = f"Context: {context} Question: {question}"

    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    with torch.no_grad():
        outputs = model(**inputs)

    # 取最后一层的平均注意力
    attention = outputs.attentions[-1][0].mean(dim=0).numpy()

    return tokens, attention

# ===== RAG 示例 =====
print("=== RAG 注意力分析 ===\n")

question = "What color is the apple?"
context = "The apple on the table is red and fresh."

tokens, attention = analyze_rag_attention(question, context)

print(f"问题: {question}")
print(f"上下文: {context}")
print(f"\n分词结果: {tokens}")

# 找到问题中关键词的位置
question_keywords = ["color", "apple"]
context_keywords = ["red", "apple", "table", "fresh"]

print("\n=== 问题词对上下文的注意力 ===")
for q_word in question_keywords:
    # 找到问题词的位置
    q_positions = [i for i, t in enumerate(tokens) if q_word in t.lower()]
    if not q_positions:
        continue

    q_pos = q_positions[0]
    print(f"\n'{tokens[q_pos]}' 关注的上下文词:")

    # 找出它最关注的词
    att_scores = attention[q_pos]
    top_indices = att_scores.argsort()[-5:][::-1]

    for idx in top_indices:
        if tokens[idx] not in ["[CLS]", "[SEP]", "context", ":", "question"]:
            print(f"  → '{tokens[idx]}': {att_scores[idx]:.3f}")

# ===== 可视化问题词对上下文的注意力 =====
print("\n=== 注意力热力图分析 ===")
print("问题中的 'apple' 应该高度关注上下文中的 'red'")
print("这就是 RAG 能工作的原因：模型会关注相关的检索内容")
```

**运行输出示例：**
```
=== RAG 注意力分析 ===

问题: What color is the apple?
上下文: The apple on the table is red and fresh.

分词结果: ['[CLS]', 'context', ':', 'the', 'apple', 'on', 'the', 'table',
           'is', 'red', 'and', 'fresh', '.', 'question', ':', 'what',
           'color', 'is', 'the', 'apple', '?', '[SEP]']

=== 问题词对上下文的注意力 ===

'color' 关注的上下文词:
  → 'red': 0.185
  → 'apple': 0.142
  → 'fresh': 0.098

'apple' 关注的上下文词:
  → 'apple': 0.312
  → 'red': 0.156
  → 'table': 0.089

=== 注意力热力图分析 ===
问题中的 'apple' 应该高度关注上下文中的 'red'
这就是 RAG 能工作的原因：模型会关注相关的检索内容
```

---

## 示例4：简化版 - 不需要 GPU

```python
"""
简化版：使用小模型，CPU 即可运行
适合快速体验注意力机制
"""

from transformers import AutoTokenizer, AutoModel
import torch

# 使用小模型
model_name = "prajjwal1/bert-tiny"  # 只有 4.4M 参数
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)

def quick_attention_demo(text):
    """快速演示注意力"""
    inputs = tokenizer(text, return_tensors="pt")
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    with torch.no_grad():
        outputs = model(**inputs)

    # 获取注意力
    attention = outputs.attentions[0][0, 0].numpy()  # 第一层第一个头

    print(f"输入: {text}")
    print(f"分词: {tokens}")
    print(f"\n注意力矩阵 (Layer 0, Head 0):")
    print("-" * 50)

    # 打印表头
    header = "      " + " ".join([f"{t[:6]:>6}" for t in tokens])
    print(header)

    # 打印每行
    for i, token in enumerate(tokens):
        row = f"{token[:6]:>6} " + " ".join([f"{attention[i,j]:>6.2f}" for j in range(len(tokens))])
        print(row)

# 运行演示
quick_attention_demo("I love coding")
print("\n" + "="*50 + "\n")
quick_attention_demo("The cat sat on mat")
```

**运行输出示例：**
```
输入: I love coding
分词: ['[CLS]', 'i', 'love', 'coding', '[SEP]']

注意力矩阵 (Layer 0, Head 0):
--------------------------------------------------
        [CLS]      i   love coding  [SEP]
 [CLS]   0.15   0.20   0.25   0.22   0.18
     i   0.18   0.12   0.35   0.20   0.15
  love   0.12   0.28   0.18   0.30   0.12
coding   0.10   0.15   0.32   0.25   0.18
 [SEP]   0.20   0.12   0.18   0.22   0.28
```

---

## 关键观察

通过这些代码，你应该观察到：

1. **注意力是动态的**：不同的词关注不同的其他词
2. **位置有影响**：相同的词在不同位置，注意力模式不同
3. **语义相关性**：语义相关的词之间注意力更高
4. **RAG 的原理**：问题词会关注上下文中的相关内容

---

## 练习建议

1. 修改输入文本，观察注意力变化
2. 尝试不同的层和头，看看有什么区别
3. 构造 RAG 场景，验证检索内容是否被关注
4. 对比中英文模型的注意力模式

---

**下一步：** [08_面试必问](./08_面试必问.md) - 学会如何答出彩
