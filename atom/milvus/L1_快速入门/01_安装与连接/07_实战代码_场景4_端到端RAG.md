# 实战代码 场景4: 端到端RAG

完整的端到端 RAG 系统实战,从 Milvus 部署到 RAG 应用的完整流程。

---

## 场景概述

**目标**: 构建一个完整的 RAG 系统,包括文档加载、向量化、存储、检索和生成。

**技术栈**:
- Milvus 2.6.11 (向量存储)
- OpenAI API (Embedding + LLM)
- LangChain (RAG 框架)
- Python 3.9+

---

## 完整 RAG 系统架构

```
┌─────────────────────────────────────────────────────────┐
│                    RAG 系统架构                          │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  1. 文档加载 (Document Loading)                          │
│     └── PDF/TXT/Markdown → 文本提取                      │
│                                                          │
│  2. 文本分块 (Text Chunking)                             │
│     └── 长文本 → 固定大小的 Chunks                        │
│                                                          │
│  3. 向量化 (Embedding)                                   │
│     └── 文本 → 向量 (OpenAI text-embedding-3-small)      │
│                                                          │
│  4. 向量存储 (Milvus)                                    │
│     └── 向量 + 元数据 → Milvus Collection                │
│                                                          │
│  5. 用户查询 (Query)                                     │
│     └── 查询文本 → 查询向量                               │
│                                                          │
│  6. 向量检索 (Retrieval)                                 │
│     └── Milvus 检索 Top-K 相似文档                        │
│                                                          │
│  7. 上下文注入 (Context Injection)                       │
│     └── 检索结果 → Prompt 模板                            │
│                                                          │
│  8. LLM 生成 (Generation)                                │
│     └── Prompt → OpenAI GPT-4 → 答案                     │
│                                                          │
└─────────────────────────────────────────────────────────┘
```

---

## 环境准备

### 安装依赖

```bash
# 安装核心依赖
pip install pymilvus>=2.6.0
pip install openai>=1.0.0
pip install langchain>=0.1.0
pip install langchain-openai>=0.0.5
pip install pypdf>=3.0.0
pip install python-dotenv>=1.0.0

# 或使用 requirements.txt
cat > requirements.txt << EOF
pymilvus>=2.6.0
openai>=1.0.0
langchain>=0.1.0
langchain-openai>=0.0.5
pypdf>=3.0.0
python-dotenv>=1.0.0
EOF

pip install -r requirements.txt
```

### 配置 API 密钥

```bash
# 创建 .env 文件
cat > .env << EOF
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1  # 可选
EOF
```

---

## 完整 RAG 系统实现

### 主程序 (rag_system.py)

```python
#!/usr/bin/env python3
"""
端到端 RAG 系统实现
"""

import os
from typing import List, Dict
from dotenv import load_dotenv
from pymilvus import MilvusClient
from openai import OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader

# 加载环境变量
load_dotenv()

class RAGSystem:
    """端到端 RAG 系统"""

    def __init__(
        self,
        milvus_uri: str = "http://localhost:19530",
        collection_name: str = "rag_knowledge_base",
        embedding_model: str = "text-embedding-3-small",
        llm_model: str = "gpt-4",
        chunk_size: int = 512,
        chunk_overlap: int = 50
    ):
        # 初始化 Milvus
        self.milvus_client = MilvusClient(uri=milvus_uri)
        self.collection_name = collection_name

        # 初始化 OpenAI
        self.openai_client = OpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL")
        )
        self.embedding_model = embedding_model
        self.llm_model = llm_model

        # 初始化文本分块器
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len
        )

        # 创建 Collection
        self._create_collection()

    def _create_collection(self):
        """创建 Milvus Collection"""
        if self.milvus_client.has_collection(self.collection_name):
            print(f"Collection '{self.collection_name}' 已存在")
            return

        # 创建 Collection (1536 维 - text-embedding-3-small)
        self.milvus_client.create_collection(
            collection_name=self.collection_name,
            dimension=1536,
            metric_type="COSINE",
            auto_id=True
        )
        print(f"✅ Collection '{self.collection_name}' 创建成功")

    def load_documents(self, file_path: str) -> List[str]:
        """加载文档"""
        print(f"\n[1/5] 加载文档: {file_path}")

        # 根据文件类型选择加载器
        if file_path.endswith('.pdf'):
            loader = PyPDFLoader(file_path)
        elif file_path.endswith('.txt'):
            loader = TextLoader(file_path)
        else:
            raise ValueError(f"不支持的文件类型: {file_path}")

        documents = loader.load()
        print(f"✅ 加载 {len(documents)} 个文档")
        return documents

    def chunk_documents(self, documents: List) -> List[str]:
        """文本分块"""
        print(f"\n[2/5] 文本分块...")

        chunks = self.text_splitter.split_documents(documents)
        print(f"✅ 分块完成,共 {len(chunks)} 个 chunks")
        return chunks

    def embed_and_store(self, chunks: List):
        """向量化并存储到 Milvus"""
        print(f"\n[3/5] 向量化并存储...")

        data = []
        for i, chunk in enumerate(chunks):
            # 向量化
            embedding = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=chunk.page_content
            ).data[0].embedding

            # 准备数据
            data.append({
                "vector": embedding,
                "text": chunk.page_content,
                "source": chunk.metadata.get("source", ""),
                "page": chunk.metadata.get("page", 0)
            })

            if (i + 1) % 10 == 0:
                print(f"   已处理 {i + 1}/{len(chunks)}")

        # 批量插入
        self.milvus_client.insert(
            collection_name=self.collection_name,
            data=data
        )
        print(f"✅ 存储 {len(data)} 条数据到 Milvus")

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """向量检索"""
        print(f"\n[4/5] 向量检索: {query}")

        # 查询向量化
        query_embedding = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=query
        ).data[0].embedding

        # Milvus 检索
        results = self.milvus_client.search(
            collection_name=self.collection_name,
            data=[query_embedding],
            limit=top_k,
            output_fields=["text", "source", "page"]
        )

        print(f"✅ 检索到 {len(results[0])} 条相关文档")
        return results[0]

    def generate_answer(self, query: str, context: str) -> str:
        """LLM 生成答案"""
        print(f"\n[5/5] LLM 生成答案...")

        # 构建 Prompt
        prompt = f"""根据以下上下文回答问题。如果上下文中没有相关信息,请说"我不知道"。

上下文:
{context}

问题: {query}

答案:"""

        # LLM 生成
        response = self.openai_client.chat.completions.create(
            model=self.llm_model,
            messages=[
                {"role": "system", "content": "你是一个专业的问答助手,根据提供的上下文准确回答问题。"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )

        answer = response.choices[0].message.content
        print(f"✅ 答案生成完成")
        return answer

    def query(self, question: str, top_k: int = 5) -> Dict:
        """完整的 RAG 查询流程"""
        # 1. 向量检索
        search_results = self.search(question, top_k=top_k)

        # 2. 构建上下文
        context = "\n\n".join([
            f"[文档 {i+1}] (来源: {hit['entity']['source']}, 页码: {hit['entity']['page']})\n{hit['entity']['text']}"
            for i, hit in enumerate(search_results)
        ])

        # 3. LLM 生成答案
        answer = self.generate_answer(question, context)

        return {
            "question": question,
            "answer": answer,
            "sources": [
                {
                    "text": hit['entity']['text'][:200] + "...",
                    "source": hit['entity']['source'],
                    "page": hit['entity']['page'],
                    "similarity": hit['distance']
                }
                for hit in search_results
            ]
        }

    def ingest_documents(self, file_path: str):
        """完整的文档摄入流程"""
        # 1. 加载文档
        documents = self.load_documents(file_path)

        # 2. 文本分块
        chunks = self.chunk_documents(documents)

        # 3. 向量化并存储
        self.embed_and_store(chunks)

        print(f"\n✅ 文档摄入完成!")

# 使用示例
def main():
    """主函数"""
    print("=" * 60)
    print("端到端 RAG 系统演示")
    print("=" * 60)

    # 1. 初始化 RAG 系统
    rag = RAGSystem(
        milvus_uri="http://localhost:19530",
        collection_name="rag_demo",
        embedding_model="text-embedding-3-small",
        llm_model="gpt-4"
    )

    # 2. 摄入文档 (示例)
    # rag.ingest_documents("path/to/your/document.pdf")

    # 3. 查询
    question = "Milvus 2.6 的新特性有哪些?"
    result = rag.query(question, top_k=3)

    # 4. 输出结果
    print("\n" + "=" * 60)
    print("查询结果")
    print("=" * 60)
    print(f"\n问题: {result['question']}")
    print(f"\n答案:\n{result['answer']}")
    print(f"\n参考来源:")
    for i, source in enumerate(result['sources'], 1):
        print(f"\n[{i}] 相似度: {source['similarity']:.4f}")
        print(f"    来源: {source['source']}")
        print(f"    页码: {source['page']}")
        print(f"    内容: {source['text']}")

if __name__ == "__main__":
    main()
```

---

## 简化版 RAG (无 LangChain)

```python
#!/usr/bin/env python3
"""
简化版 RAG 系统 (无 LangChain 依赖)
"""

import os
from typing import List, Dict
from dotenv import load_dotenv
from pymilvus import MilvusClient
from openai import OpenAI

load_dotenv()

class SimpleRAG:
    """简化版 RAG 系统"""

    def __init__(self):
        self.milvus_client = MilvusClient(uri="http://localhost:19530")
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.collection_name = "simple_rag"

        # 创建 Collection
        if not self.milvus_client.has_collection(self.collection_name):
            self.milvus_client.create_collection(
                collection_name=self.collection_name,
                dimension=1536,
                metric_type="COSINE"
            )

    def add_documents(self, texts: List[str]):
        """添加文档"""
        data = []
        for text in texts:
            embedding = self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            ).data[0].embedding

            data.append({"vector": embedding, "text": text})

        self.milvus_client.insert(
            collection_name=self.collection_name,
            data=data
        )
        print(f"✅ 添加 {len(texts)} 条文档")

    def query(self, question: str, top_k: int = 3) -> str:
        """查询"""
        # 1. 查询向量化
        query_embedding = self.openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=question
        ).data[0].embedding

        # 2. 向量检索
        results = self.milvus_client.search(
            collection_name=self.collection_name,
            data=[query_embedding],
            limit=top_k,
            output_fields=["text"]
        )

        # 3. 构建上下文
        context = "\n\n".join([hit['entity']['text'] for hit in results[0]])

        # 4. LLM 生成
        response = self.openai_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "根据上下文回答问题"},
                {"role": "user", "content": f"上下文:\n{context}\n\n问题: {question}"}
            ]
        )

        return response.choices[0].message.content

# 使用示例
if __name__ == "__main__":
    rag = SimpleRAG()

    # 添加文档
    documents = [
        "Milvus 2.6 引入了 Woodpecker WAL,替代了 Kafka/Pulsar。",
        "Milvus 2.6 支持 100K Collections,适合大规模应用。",
        "Milvus 2.6 引入了 Streaming Node 架构,简化部署。"
    ]
    rag.add_documents(documents)

    # 查询
    answer = rag.query("Milvus 2.6 的新特性有哪些?")
    print(f"\n答案:\n{answer}")
```

---

## 性能优化

### 批量处理

```python
def batch_embed_and_store(self, chunks: List, batch_size: int = 100):
    """批量向量化并存储"""
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]

        # 批量向量化
        texts = [chunk.page_content for chunk in batch]
        embeddings = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=texts
        ).data

        # 准备数据
        data = [
            {
                "vector": emb.embedding,
                "text": chunk.page_content,
                "source": chunk.metadata.get("source", "")
            }
            for emb, chunk in zip(embeddings, batch)
        ]

        # 批量插入
        self.milvus_client.insert(
            collection_name=self.collection_name,
            data=data
        )

        print(f"   已处理 {min(i+batch_size, len(chunks))}/{len(chunks)}")
```

---

## 总结

### 核心要点

1. **完整流程**: 文档加载 → 分块 → 向量化 → 存储 → 检索 → 生成
2. **Milvus 集成**: 使用 pymilvus 2.6+ 进行向量存储和检索
3. **OpenAI 集成**: 使用 text-embedding-3-small + GPT-4
4. **LangChain 集成**: 简化文档加载和分块
5. **性能优化**: 批量处理,提升效率

### 核心代码

```python
# 初始化
rag = RAGSystem()

# 摄入文档
rag.ingest_documents("document.pdf")

# 查询
result = rag.query("你的问题?")
print(result['answer'])
```

---

**参考文献**:
- Milvus Documentation: https://milvus.io/docs
- OpenAI API: https://platform.openai.com/docs
- LangChain: https://python.langchain.com/docs
