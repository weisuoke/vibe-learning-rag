# 实战代码5：生产级实践

## 概述

本文提供生产级错误处理的完整可运行示例，涵盖：
- 断路器模式
- 自愈Agent
- 监控与告警
- 综合生产案例

所有代码都可以直接复制运行。

---

## 示例1：断路器模式

**场景：** 避免对已知故障的服务持续重试

```python
"""
断路器模式
演示：使用circuitbreaker库实现断路器
"""

from langchain_openai import ChatOpenAI
from circuitbreaker import circuit
from dotenv import load_dotenv
import time

load_dotenv()

print("=== 断路器模式 ===\n")

# ===== 1. 创建带断路器的LLM调用 =====
llm = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

@circuit(
    failure_threshold=5,               # 5次失败后打开断路器
    recovery_timeout=60,               # 60秒后尝试恢复
    expected_exception=Exception
)
def call_llm_with_circuit_breaker(prompt):
    """带断路器的LLM调用"""
    return llm.invoke(prompt)

print("✅ 断路器配置完成")
print("  失败阈值: 5次")
print("  恢复超时: 60秒\n")

# ===== 2. 测试断路器 =====
test_prompts = [
    "什么是Python？",
    "什么是JavaScript？",
    "什么是Go？",
    "什么是Rust？",
    "什么是Java？",
    "什么是C++？",
    "什么是C#？"
]

print(f"处理 {len(test_prompts)} 个请求...\n")

success_count = 0
failure_count = 0
circuit_open_count = 0

for i, prompt in enumerate(test_prompts):
    print(f"[{i+1}/{len(test_prompts)}] {prompt}")

    try:
        response = call_llm_with_circuit_breaker(prompt)
        print(f"✅ 成功: {response.content[:30]}...\n")
        success_count += 1
    except Exception as e:
        error_type = type(e).__name__

        if "CircuitBreakerError" in error_type:
            print(f"⚠️  断路器打开，跳过请求\n")
            circuit_open_count += 1
        else:
            print(f"❌ 失败: {error_type}\n")
            failure_count += 1

# ===== 3. 统计结果 =====
print("=== 结果统计 ===")
print(f"成功: {success_count}")
print(f"失败: {failure_count}")
print(f"断路器拦截: {circuit_open_count}")
print(f"\n断路器状态机:")
print("  正常 (Closed) → 连续5次失败 → 断开 (Open)")
print("  断开 (Open) → 60秒后 → 半开 (Half-Open)")
print("  半开 (Half-Open) → 测试成功 → 正常 (Closed)")
```

---

## 示例2：自愈Agent

**场景：** Agent自动检测和修复错误

```python
"""
自愈Agent
演示：Agent自动重试和降级
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv
import logging

# ===== 1. 配置日志 =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

print("=== 自愈Agent ===\n")

# ===== 2. 创建自愈Agent类 =====
class SelfHealingAgent:
    """自愈Agent"""

    def __init__(self):
        # 主LLM + 重试
        self.primary_llm = ChatOpenAI(model="gpt-4").with_retry(
            stop_after_attempt=3,
            wait_exponential_jitter=True
        )

        # 备选LLM + 重试
        self.fallback_llm = ChatOpenAI(model="gpt-3.5-turbo").with_retry(
            stop_after_attempt=2
        )

        # 组合
        self.llm = self.primary_llm.with_fallbacks([self.fallback_llm])

        # 统计
        self.stats = {
            "total_requests": 0,
            "primary_success": 0,
            "fallback_used": 0,
            "total_failures": 0,
            "self_healed": 0
        }

    def invoke(self, prompt):
        """执行请求（带自愈）"""
        self.stats["total_requests"] += 1

        try:
            # 尝试执行
            response = self.llm.invoke(prompt)
            self.stats["primary_success"] += 1
            logger.info(f"成功: {prompt[:30]}...")
            return response.content

        except Exception as e:
            logger.warning(f"主方案失败: {type(e).__name__}")

            # 自愈：尝试简化prompt
            try:
                simplified_prompt = f"简单回答: {prompt}"
                response = self.fallback_llm.invoke(simplified_prompt)
                self.stats["self_healed"] += 1
                logger.info(f"自愈成功: {prompt[:30]}...")
                return response.content

            except Exception as e2:
                self.stats["total_failures"] += 1
                logger.error(f"自愈失败: {type(e2).__name__}")
                return f"抱歉，服务暂时不可用"

    def get_health_status(self):
        """获取健康状态"""
        total = self.stats["total_requests"]
        if total == 0:
            return "未使用"

        success_rate = (self.stats["primary_success"] + self.stats["self_healed"]) / total
        self_heal_rate = self.stats["self_healed"] / total

        if success_rate >= 0.95:
            status = "健康"
        elif success_rate >= 0.80:
            status = "警告"
        else:
            status = "异常"

        return {
            "status": status,
            "success_rate": f"{success_rate:.2%}",
            "self_heal_rate": f"{self_heal_rate:.2%}",
            "stats": self.stats
        }

# ===== 3. 测试自愈Agent =====
agent = SelfHealingAgent()

print("✅ 自愈Agent创建完成\n")

test_prompts = [
    "什么是LangChain？",
    "解释一下LCEL表达式",
    "什么是Runnable协议？",
    "详细分析深度学习的发展历史",
    "什么是Python？"
]

print(f"处理 {len(test_prompts)} 个请求...\n")

for i, prompt in enumerate(test_prompts):
    print(f"[{i+1}/{len(test_prompts)}] {prompt[:30]}...")
    result = agent.invoke(prompt)
    print(f"   {result[:50]}...\n")

# ===== 4. 健康状态 =====
print("=== 健康状态 ===")
health = agent.get_health_status()
print(f"状态: {health['status']}")
print(f"成功率: {health['success_rate']}")
print(f"自愈率: {health['self_heal_rate']}")
print(f"\n详细统计:")
for key, value in health['stats'].items():
    print(f"  {key}: {value}")
```

---

## 示例3：监控与告警

**场景：** 实时监控错误率和性能

```python
"""
监控与告警
演示：实时监控和告警系统
"""

from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
import logging
import time
from collections import deque

# ===== 1. 配置日志 =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

print("=== 监控与告警 ===\n")

# ===== 2. 创建监控类 =====
class MonitoringSystem:
    """监控系统"""

    def __init__(self, window_size=100):
        self.window_size = window_size
        self.requests = deque(maxlen=window_size)

        # 告警阈值
        self.thresholds = {
            "error_rate": 0.10,        # 错误率 > 10%
            "retry_rate": 0.05,        # 重试率 > 5%
            "avg_latency": 5.0         # 平均延迟 > 5秒
        }

    def record_request(self, success, retried, latency):
        """记录请求"""
        self.requests.append({
            "timestamp": time.time(),
            "success": success,
            "retried": retried,
            "latency": latency
        })

        # 检查告警
        self._check_alerts()

    def _check_alerts(self):
        """检查告警"""
        if len(self.requests) < 10:
            return

        # 计算指标
        total = len(self.requests)
        errors = sum(1 for r in self.requests if not r["success"])
        retries = sum(1 for r in self.requests if r["retried"])
        avg_latency = sum(r["latency"] for r in self.requests) / total

        error_rate = errors / total
        retry_rate = retries / total

        # 检查阈值
        if error_rate > self.thresholds["error_rate"]:
            logger.warning(f"⚠️  告警: 错误率过高 {error_rate:.2%}")

        if retry_rate > self.thresholds["retry_rate"]:
            logger.warning(f"⚠️  告警: 重试率过高 {retry_rate:.2%}")

        if avg_latency > self.thresholds["avg_latency"]:
            logger.warning(f"⚠️  告警: 平均延迟过高 {avg_latency:.2f}秒")

    def get_metrics(self):
        """获取指标"""
        if not self.requests:
            return {}

        total = len(self.requests)
        errors = sum(1 for r in self.requests if not r["success"])
        retries = sum(1 for r in self.requests if r["retried"])
        avg_latency = sum(r["latency"] for r in self.requests) / total

        return {
            "total_requests": total,
            "error_rate": f"{errors / total:.2%}",
            "retry_rate": f"{retries / total:.2%}",
            "avg_latency": f"{avg_latency:.2f}s",
            "success_rate": f"{(total - errors) / total:.2%}"
        }

# ===== 3. 创建带监控的LLM =====
monitor = MonitoringSystem()

llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
)

def monitored_invoke(prompt):
    """带监控的调用"""
    start_time = time.time()
    retried = False

    try:
        response = llm.invoke(prompt)
        latency = time.time() - start_time
        monitor.record_request(success=True, retried=retried, latency=latency)
        return response.content

    except Exception as e:
        latency = time.time() - start_time
        monitor.record_request(success=False, retried=retried, latency=latency)
        raise

print("✅ 监控系统创建完成\n")

# ===== 4. 测试 =====
test_prompts = [
    "什么是Python？",
    "什么是JavaScript？",
    "什么是Go？",
    "什么是Rust？",
    "什么是Java？"
] * 4  # 重复4次，共20个请求

print(f"处理 {len(test_prompts)} 个请求...\n")

for i, prompt in enumerate(test_prompts):
    try:
        result = monitored_invoke(prompt)
        if (i + 1) % 5 == 0:
            logger.info(f"已处理 {i+1}/{len(test_prompts)} 个请求")
    except Exception as e:
        logger.error(f"请求失败: {type(e).__name__}")

# ===== 5. 显示指标 =====
print("\n=== 监控指标 ===")
metrics = monitor.get_metrics()
for key, value in metrics.items():
    print(f"{key}: {value}")
```

---

## 示例4：综合生产案例

**场景：** 完整的生产级RAG系统

```python
"""
综合生产案例
演示：完整的生产级RAG系统
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from openai import RateLimitError, APIError, Timeout
from dotenv import load_dotenv
import logging
import time

# ===== 1. 配置日志 =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

print("=== 生产级RAG系统 ===\n")

# ===== 2. 定义可重试的错误 =====
RETRYABLE_ERRORS = (RateLimitError, APIError, Timeout)

# ===== 3. 创建组件（带完整错误处理）=====

# 3.1 Embedding（快速失败）
embeddings = OpenAIEmbeddings().with_retry(
    stop_after_attempt=2,
    retry_if_exception_type=(Timeout,)
)

# 3.2 模拟向量检索器
def vector_retriever(query):
    """向量检索（带错误处理）"""
    logger.info(f"检索: {query[:30]}...")
    try:
        # 模拟检索
        docs = [
            "LangChain是一个用于构建AI应用的框架",
            "LCEL是LangChain的表达式语言",
            "Runnable是LangChain的核心协议"
        ]
        return docs
    except Exception as e:
        logger.error(f"检索失败: {type(e).__name__}")
        return []

# 3.3 LLM（关键组件，多层保护）
primary_llm = ChatOpenAI(model="gpt-4").with_retry(
    stop_after_attempt=5,
    wait_exponential_jitter=True,
    retry_if_exception_type=RETRYABLE_ERRORS
)

fallback_llm = ChatOpenAI(model="gpt-3.5-turbo").with_retry(
    stop_after_attempt=3,
    retry_if_exception_type=RETRYABLE_ERRORS
)

llm = primary_llm.with_fallbacks([fallback_llm])

# ===== 4. 构建RAG链 =====
prompt = ChatPromptTemplate.from_template(
    "基于以下上下文回答问题:\n{context}\n\n问题: {question}"
)

def format_docs(docs):
    """格式化文档"""
    if not docs:
        return "未找到相关文档"
    return "\n".join(docs)

rag_chain = (
    {
        "context": RunnableLambda(vector_retriever) | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
)

logger.info("RAG链构建完成\n")

# ===== 5. 生产级执行包装器 =====
class ProductionRAG:
    """生产级RAG包装器"""

    def __init__(self, chain):
        self.chain = chain
        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "avg_latency": 0.0
        }

    def query(self, question):
        """执行查询"""
        self.stats["total"] += 1
        start_time = time.time()

        try:
            logger.info(f"查询: {question}")
            response = self.chain.invoke(question)
            latency = time.time() - start_time

            self.stats["success"] += 1
            self._update_latency(latency)

            logger.info(f"成功，耗时: {latency:.2f}秒")

            return {
                "success": True,
                "answer": response.content,
                "latency": latency
            }

        except Exception as e:
            latency = time.time() - start_time

            self.stats["failed"] += 1
            self._update_latency(latency)

            logger.error(f"失败: {type(e).__name__}, 耗时: {latency:.2f}秒")

            return {
                "success": False,
                "error": str(e),
                "latency": latency
            }

    def _update_latency(self, latency):
        """更新平均延迟"""
        total = self.stats["total"]
        current_avg = self.stats["avg_latency"]
        self.stats["avg_latency"] = (current_avg * (total - 1) + latency) / total

    def get_stats(self):
        """获取统计信息"""
        total = self.stats["total"]
        if total == 0:
            return self.stats

        return {
            **self.stats,
            "success_rate": f"{self.stats['success'] / total:.2%}",
            "failure_rate": f"{self.stats['failed'] / total:.2%}",
            "avg_latency": f"{self.stats['avg_latency']:.2f}s"
        }

# ===== 6. 测试 =====
rag = ProductionRAG(rag_chain)

print("=== 测试查询 ===\n")

questions = [
    "什么是LangChain？",
    "什么是LCEL？",
    "什么是Runnable？",
    "如何使用LangChain构建RAG？",
    "LangChain的核心概念是什么？"
]

for i, question in enumerate(questions):
    print(f"[{i+1}/{len(questions)}] {question}")
    result = rag.query(question)

    if result["success"]:
        print(f"✅ {result['answer'][:60]}...")
    else:
        print(f"❌ {result['error']}")

    print(f"   耗时: {result['latency']:.2f}秒\n")

# ===== 7. 统计信息 =====
print("=== 系统统计 ===")
stats = rag.get_stats()
for key, value in stats.items():
    print(f"{key}: {value}")

# ===== 8. 健康检查 =====
print("\n=== 健康检查 ===")
success_rate = rag.stats["success"] / rag.stats["total"]
if success_rate >= 0.95:
    print("✅ 系统健康")
elif success_rate >= 0.80:
    print("⚠️  系统警告")
else:
    print("❌ 系统异常")
```

---

## 示例5：完整的错误处理模板

**场景：** 可复用的生产级模板

```python
"""
完整的错误处理模板
演示：可复用的生产级配置
"""

from langchain_openai import ChatOpenAI
from openai import RateLimitError, APIError, Timeout, APIConnectionError
from tenacity import wait_exponential, before_sleep_log, after_log
from circuitbreaker import circuit
from dotenv import load_dotenv
import logging
import time

# ===== 1. 配置 =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()

print("=== 生产级错误处理模板 ===\n")

# ===== 2. 定义配置 =====
RETRYABLE_ERRORS = (RateLimitError, APIError, Timeout, APIConnectionError)

RETRY_CONFIG = {
    "stop_after_attempt": 5,
    "wait": wait_exponential(multiplier=1, min=1, max=60),
    "retry_if_exception_type": RETRYABLE_ERRORS,
    "before_sleep": before_sleep_log(logger, logging.WARNING),
    "after": after_log(logger, logging.INFO)
}

CIRCUIT_BREAKER_CONFIG = {
    "failure_threshold": 5,
    "recovery_timeout": 60,
    "expected_exception": Exception
}

# ===== 3. 创建生产级LLM工厂 =====
def create_production_llm(
    model="gpt-4",
    timeout=30,
    enable_fallback=True,
    enable_circuit_breaker=True
):
    """
    创建生产级LLM

    参数:
        model: 模型名称
        timeout: 超时时间
        enable_fallback: 是否启用降级
        enable_circuit_breaker: 是否启用断路器
    """
    # 主LLM + 重试
    primary = ChatOpenAI(model=model, timeout=timeout).with_retry(**RETRY_CONFIG)

    # 降级
    if enable_fallback:
        fallback = ChatOpenAI(
            model="gpt-3.5-turbo",
            timeout=timeout
        ).with_retry(stop_after_attempt=3)

        llm = primary.with_fallbacks([fallback])
    else:
        llm = primary

    # 断路器
    if enable_circuit_breaker:
        @circuit(**CIRCUIT_BREAKER_CONFIG)
        def invoke_with_circuit_breaker(prompt):
            return llm.invoke(prompt)

        return invoke_with_circuit_breaker
    else:
        return llm.invoke

# ===== 4. 使用模板 =====
print("--- 创建生产级LLM ---\n")

# 完整配置
llm_full = create_production_llm(
    model="gpt-4",
    timeout=30,
    enable_fallback=True,
    enable_circuit_breaker=True
)
print("✅ 完整配置: 重试 + 降级 + 断路器")

# 基础配置
llm_basic = create_production_llm(
    model="gpt-4",
    timeout=30,
    enable_fallback=False,
    enable_circuit_breaker=False
)
print("✅ 基础配置: 仅重试")

# ===== 5. 测试 =====
print("\n=== 测试完整配置 ===")

test_prompts = ["你好", "什么是Python？", "什么是JavaScript？"]

for i, prompt in enumerate(test_prompts):
    logger.info(f"[{i+1}/{len(test_prompts)}] 处理: {prompt}")
    try:
        response = llm_full(prompt)
        logger.info(f"成功: {response.content[:30]}...")
    except Exception as e:
        logger.error(f"失败: {type(e).__name__}")

# ===== 6. 配置建议 =====
print("\n=== 配置建议 ===")
print("1. 开发环境:")
print("   - 重试: 2-3次")
print("   - 降级: 可选")
print("   - 断路器: 不需要")
print("\n2. 测试环境:")
print("   - 重试: 3-5次")
print("   - 降级: 推荐")
print("   - 断路器: 推荐")
print("\n3. 生产环境:")
print("   - 重试: 5-7次")
print("   - 降级: 必需")
print("   - 断路器: 必需")
print("   - 监控: 必需")
print("   - 告警: 必需")
```

---

## 运行环境要求

### 依赖安装

```bash
uv add langchain langchain-openai python-dotenv tenacity circuitbreaker
```

### 环境变量配置

```bash
OPENAI_API_KEY=your_openai_api_key_here
```

---

## 生产级检查清单

### 1. 错误处理

- [ ] 所有LLM调用都有重试（3-5次）
- [ ] 关键服务有降级方案
- [ ] 使用指数退避 + jitter
- [ ] 过滤不可重试的错误
- [ ] 实现断路器模式

### 2. 监控与告警

- [ ] 监控错误率（阈值: 10%）
- [ ] 监控重试率（阈值: 5%）
- [ ] 监控平均延迟（阈值: 5秒）
- [ ] 设置告警通知
- [ ] 记录详细日志

### 3. 性能优化

- [ ] 使用连接池
- [ ] 实现请求缓存
- [ ] 批量处理优化
- [ ] 异步调用

### 4. 安全性

- [ ] API密钥安全存储
- [ ] 请求速率限制
- [ ] 输入验证
- [ ] 输出过滤

### 5. 可观测性

- [ ] 集成LangSmith
- [ ] 分布式追踪
- [ ] 性能指标收集
- [ ] 错误追踪

---

## 最佳实践总结

### 1. 多层防护

```
第1层: 重试（处理临时故障）
第2层: 降级（提供备选方案）
第3层: 断路器（避免雪崩）
第4层: 监控告警（及时发现问题）
```

### 2. 错误分类

| 错误类型 | 处理策略 | 示例 |
|----------|----------|------|
| 临时故障 | 重试 | RateLimitError, Timeout |
| 永久故障 | 立即失败 | AuthenticationError |
| 服务不可用 | 降级 | ServiceUnavailable |
| 频繁失败 | 断路器 | 连续5次失败 |

### 3. 监控指标

| 指标 | 正常 | 警告 | 异常 |
|------|------|------|------|
| 错误率 | < 5% | 5-10% | > 10% |
| 重试率 | < 3% | 3-5% | > 5% |
| 平均延迟 | < 2s | 2-5s | > 5s |
| 成功率 | > 95% | 90-95% | < 90% |

### 4. 告警策略

- **警告**: 错误率 > 5%，发送通知
- **严重**: 错误率 > 10%，立即介入
- **紧急**: 服务完全不可用，紧急响应

---

**记住：生产级错误处理是系统可靠性的基石，不是可选项。**
