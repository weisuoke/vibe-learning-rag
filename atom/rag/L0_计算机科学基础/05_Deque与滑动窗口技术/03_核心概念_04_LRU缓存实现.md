# 核心概念 04：LRU 缓存实现

> 深入理解 LRU 缓存的两种实现方式及其在 AI Agent 中的应用

---

## 概述

**LRU（Least Recently Used）缓存** 是一种缓存淘汰策略，当缓存满时，优先淘汰最久未使用的数据。在 AI Agent 开发中，LRU 缓存常用于缓存 LLM API 响应，节省成本和时间。

**核心特性：**
- O(1) 读取和写入
- 自动淘汰最久未使用的数据
- 固定容量

---

## 1. LRU 缓存原理

### 1.1 基本概念

**LRU 策略：** 当缓存满时，移除最久未被访问的数据。

```
缓存容量：3

操作序列：
1. put(1, "a")  → [1]
2. put(2, "b")  → [1, 2]
3. put(3, "c")  → [1, 2, 3]
4. get(1)       → [2, 3, 1]  ← 1 被访问，移到最近
5. put(4, "d")  → [3, 1, 4]  ← 2 被淘汰（最久未使用）
```

### 1.2 核心操作

| 操作 | 说明 | 时间复杂度 |
|------|------|-----------|
| `get(key)` | 获取值，更新访问顺序 | O(1) |
| `put(key, value)` | 添加/更新值，淘汰最旧的 | O(1) |

---

## 2. 方案1：OrderedDict 实现

### 2.1 基本实现

```python
from collections import OrderedDict

class LRUCache:
    """使用 OrderedDict 实现的 LRU 缓存"""

    def __init__(self, capacity: int):
        """
        Args:
            capacity: 缓存容量
        """
        self.cache = OrderedDict()
        self.capacity = capacity

    def get(self, key: int) -> int:
        """
        获取值，更新访问顺序

        Args:
            key: 键

        Returns:
            值，如果不存在返回 -1
        """
        if key not in self.cache:
            return -1

        # 移到末尾（最近使用）
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key: int, value: int):
        """
        添加/更新值

        Args:
            key: 键
            value: 值
        """
        if key in self.cache:
            # 更新值并移到末尾
            self.cache.move_to_end(key)

        self.cache[key] = value

        # 超过容量，移除最旧的（第一个）
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)

# 测试
cache = LRUCache(2)
cache.put(1, 1)
cache.put(2, 2)
print(cache.get(1))  # 1
cache.put(3, 3)      # 淘汰 key 2
print(cache.get(2))  # -1 (not found)
cache.put(4, 4)      # 淘汰 key 1
print(cache.get(1))  # -1 (not found)
print(cache.get(3))  # 3
print(cache.get(4))  # 4
```

### 2.2 OrderedDict 的优势

**优点：**
- 代码简洁，易于理解
- O(1) 读写性能
- Python 标准库，无需额外依赖

**缺点：**
- 只适用于 Python 3.7+
- 内存开销略高

---

## 3. 方案2：Deque + HashMap 实现

### 3.1 基本实现

```python
from collections import deque

class LRUCache:
    """使用 Deque + HashMap 实现的 LRU 缓存"""

    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = {}  # key -> value
        self.order = deque()  # 记录访问顺序

    def get(self, key: int) -> int:
        """获取值，更新访问顺序"""
        if key not in self.cache:
            return -1

        # 更新访问顺序
        self.order.remove(key)  # O(n)
        self.order.append(key)

        return self.cache[key]

    def put(self, key: int, value: int):
        """添加/更新值"""
        if key in self.cache:
            # 更新访问顺序
            self.order.remove(key)  # O(n)
        elif len(self.cache) >= self.capacity:
            # 移除最旧的
            oldest = self.order.popleft()
            del self.cache[oldest]

        self.cache[key] = value
        self.order.append(key)

# 测试
cache = LRUCache(2)
cache.put(1, 1)
cache.put(2, 2)
print(cache.get(1))  # 1
cache.put(3, 3)      # 淘汰 key 2
print(cache.get(2))  # -1
```

### 3.2 性能问题

**问题：** `deque.remove(key)` 是 O(n) 操作，导致整体性能下降。

**解决方案：** 使用双向链表 + HashMap 实现真正的 O(1) LRU 缓存。

---

## 4. 方案3：双向链表 + HashMap（最优）

### 4.1 完整实现

```python
class Node:
    """双向链表节点"""
    def __init__(self, key: int = 0, value: int = 0):
        self.key = key
        self.value = value
        self.prev = None
        self.next = None

class LRUCache:
    """使用双向链表 + HashMap 实现的 LRU 缓存"""

    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = {}  # key -> Node

        # 虚拟头尾节点
        self.head = Node()
        self.tail = Node()
        self.head.next = self.tail
        self.tail.prev = self.head

    def _add_to_head(self, node: Node):
        """将节点添加到头部（最近使用）"""
        node.prev = self.head
        node.next = self.head.next
        self.head.next.prev = node
        self.head.next = node

    def _remove_node(self, node: Node):
        """从链表中移除节点"""
        node.prev.next = node.next
        node.next.prev = node.prev

    def _move_to_head(self, node: Node):
        """将节点移到头部"""
        self._remove_node(node)
        self._add_to_head(node)

    def _remove_tail(self) -> Node:
        """移除尾部节点（最久未使用）"""
        node = self.tail.prev
        self._remove_node(node)
        return node

    def get(self, key: int) -> int:
        """获取值，更新访问顺序"""
        if key not in self.cache:
            return -1

        node = self.cache[key]
        self._move_to_head(node)
        return node.value

    def put(self, key: int, value: int):
        """添加/更新值"""
        if key in self.cache:
            # 更新值并移到头部
            node = self.cache[key]
            node.value = value
            self._move_to_head(node)
        else:
            # 创建新节点
            node = Node(key, value)
            self.cache[key] = node
            self._add_to_head(node)

            # 超过容量，移除尾部节点
            if len(self.cache) > self.capacity:
                removed = self._remove_tail()
                del self.cache[removed.key]

# 测试
cache = LRUCache(2)
cache.put(1, 1)
cache.put(2, 2)
print(cache.get(1))  # 1
cache.put(3, 3)      # 淘汰 key 2
print(cache.get(2))  # -1
cache.put(4, 4)      # 淘汰 key 1
print(cache.get(1))  # -1
print(cache.get(3))  # 3
print(cache.get(4))  # 4
```

---

## 5. 三种方案对比

| 特性 | OrderedDict | Deque + HashMap | 双向链表 + HashMap |
|------|------------|----------------|-------------------|
| **get 复杂度** | O(1) | O(n) | O(1) |
| **put 复杂度** | O(1) | O(n) | O(1) |
| **代码复杂度** | 简单 | 中等 | 复杂 |
| **内存开销** | 中等 | 低 | 中等 |
| **推荐场景** | 生产环境 | 学习理解 | 面试/性能要求高 |

**推荐：**
- **生产环境**：使用 OrderedDict（代码简洁，性能好）
- **面试**：实现双向链表版本（展示算法能力）
- **学习**：从 Deque 版本开始理解原理

---

## 6. AI Agent 应用场景

### 6.1 LLM API 响应缓存

```python
from collections import OrderedDict
import hashlib
import json

class LLMResponseCache:
    """LLM API 响应缓存"""

    def __init__(self, capacity: int = 100):
        self.cache = OrderedDict()
        self.capacity = capacity

    def _generate_key(self, prompt: str, model: str, **kwargs) -> str:
        """生成缓存键"""
        cache_data = {
            "prompt": prompt,
            "model": model,
            **kwargs
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()

    def get(self, prompt: str, model: str, **kwargs) -> str:
        """获取缓存的响应"""
        key = self._generate_key(prompt, model, **kwargs)

        if key not in self.cache:
            return None

        # 移到末尾（最近使用）
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, prompt: str, model: str, response: str, **kwargs):
        """缓存响应"""
        key = self._generate_key(prompt, model, **kwargs)

        if key in self.cache:
            self.cache.move_to_end(key)

        self.cache[key] = response

        # 超过容量，移除最旧的
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)

# 使用示例
cache = LLMResponseCache(capacity=50)

# 第一次调用 API
prompt = "What is Python?"
model = "gpt-4"
response = "Python is a programming language..."  # 从 API 获取
cache.put(prompt, model, response)

# 第二次相同请求，从缓存获取
cached_response = cache.get(prompt, model)
if cached_response:
    print("从缓存获取")
else:
    print("调用 API")
```

**2025-2026 实际应用：**

**来源**: 生产环境最佳实践
**时间**: 2025-2026
**关键点**: 使用 LRU 缓存减少 LLM API 调用，节省成本

### 6.2 Embedding 缓存

```python
from collections import OrderedDict
import hashlib

class EmbeddingCache:
    """Embedding 缓存"""

    def __init__(self, capacity: int = 1000):
        self.cache = OrderedDict()
        self.capacity = capacity

    def _generate_key(self, text: str, model: str) -> str:
        """生成缓存键"""
        cache_str = f"{model}:{text}"
        return hashlib.md5(cache_str.encode()).hexdigest()

    def get(self, text: str, model: str) -> list:
        """获取缓存的 embedding"""
        key = self._generate_key(text, model)

        if key not in self.cache:
            return None

        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, text: str, model: str, embedding: list):
        """缓存 embedding"""
        key = self._generate_key(text, model)

        if key in self.cache:
            self.cache.move_to_end(key)

        self.cache[key] = embedding

        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)

# 使用示例
cache = EmbeddingCache(capacity=500)

text = "Hello world"
model = "text-embedding-3-small"

# 第一次生成 embedding
embedding = [0.1, 0.2, 0.3]  # 从 API 获取
cache.put(text, model, embedding)

# 第二次相同文本，从缓存获取
cached_embedding = cache.get(text, model)
if cached_embedding:
    print("从缓存获取 embedding")
```

### 6.3 带 TTL 的 LRU 缓存

```python
from collections import OrderedDict
import time

class TTLLRUCache:
    """带 TTL（Time To Live）的 LRU 缓存"""

    def __init__(self, capacity: int, ttl_seconds: int = 3600):
        """
        Args:
            capacity: 缓存容量
            ttl_seconds: 过期时间（秒）
        """
        self.cache = OrderedDict()
        self.capacity = capacity
        self.ttl_seconds = ttl_seconds
        self.timestamps = {}  # key -> timestamp

    def get(self, key: str) -> any:
        """获取值，检查是否过期"""
        if key not in self.cache:
            return None

        # 检查是否过期
        if time.time() - self.timestamps[key] > self.ttl_seconds:
            del self.cache[key]
            del self.timestamps[key]
            return None

        # 移到末尾
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key: str, value: any):
        """添加/更新值"""
        if key in self.cache:
            self.cache.move_to_end(key)

        self.cache[key] = value
        self.timestamps[key] = time.time()

        # 超过容量，移除最旧的
        if len(self.cache) > self.capacity:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            del self.timestamps[oldest_key]

# 使用示例
cache = TTLLRUCache(capacity=100, ttl_seconds=60)

cache.put("key1", "value1")
time.sleep(2)
print(cache.get("key1"))  # "value1"

time.sleep(59)
print(cache.get("key1"))  # None (已过期)
```

---

## 7. 性能测试

### 7.1 OrderedDict vs 双向链表

```python
import time
from collections import OrderedDict

def benchmark_ordereddict():
    """测试 OrderedDict 性能"""
    cache = OrderedDict()
    capacity = 1000

    start = time.time()
    for i in range(10000):
        if len(cache) >= capacity:
            cache.popitem(last=False)
        cache[i] = i
        cache.move_to_end(i)

    return time.time() - start

def benchmark_linkedlist():
    """测试双向链表性能"""
    cache = LRUCache(1000)

    start = time.time()
    for i in range(10000):
        cache.put(i, i)
        cache.get(i)

    return time.time() - start

# 运行测试
ordereddict_time = benchmark_ordereddict()
linkedlist_time = benchmark_linkedlist()

print(f"OrderedDict: {ordereddict_time:.4f}s")
print(f"双向链表:    {linkedlist_time:.4f}s")

# 输出示例：
# OrderedDict: 0.0050s
# 双向链表:    0.0055s
# 性能相近，OrderedDict 略快
```

### 7.2 缓存命中率测试

```python
import random

def test_cache_hit_rate():
    """测试缓存命中率"""
    cache = LRUCache(100)
    hits = 0
    misses = 0

    # 模拟请求
    for _ in range(1000):
        key = random.randint(1, 200)  # 200 个可能的键

        if cache.get(key) != -1:
            hits += 1
        else:
            misses += 1
            cache.put(key, key)

    hit_rate = hits / (hits + misses)
    print(f"命中率: {hit_rate:.2%}")
    print(f"命中: {hits}, 未命中: {misses}")

test_cache_hit_rate()

# 输出示例：
# 命中率: 45.00%
# 命中: 450, 未命中: 550
```

---

## 8. 常见错误与最佳实践

### 8.1 错误：忘记更新访问顺序

```python
# ❌ 错误：get 时忘记更新访问顺序
def get(self, key):
    if key in self.cache:
        return self.cache[key]  # 没有更新顺序
    return -1

# ✅ 正确：get 时更新访问顺序
def get(self, key):
    if key in self.cache:
        self.cache.move_to_end(key)  # 更新顺序
        return self.cache[key]
    return -1
```

### 8.2 错误：put 时不检查容量

```python
# ❌ 错误：put 时不检查容量
def put(self, key, value):
    self.cache[key] = value  # 可能超过容量

# ✅ 正确：put 时检查容量
def put(self, key, value):
    if key in self.cache:
        self.cache.move_to_end(key)

    self.cache[key] = value

    if len(self.cache) > self.capacity:
        self.cache.popitem(last=False)
```

### 8.3 最佳实践：线程安全

```python
import threading
from collections import OrderedDict

class ThreadSafeLRUCache:
    """线程安全的 LRU 缓存"""

    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.lock = threading.Lock()

    def get(self, key: int) -> int:
        with self.lock:
            if key not in self.cache:
                return -1
            self.cache.move_to_end(key)
            return self.cache[key]

    def put(self, key: int, value: int):
        with self.lock:
            if key in self.cache:
                self.cache.move_to_end(key)
            self.cache[key] = value
            if len(self.cache) > self.capacity:
                self.cache.popitem(last=False)
```

---

## 学习检查清单

- [ ] 理解 LRU 缓存的基本原理
- [ ] 能够使用 OrderedDict 实现 LRU 缓存
- [ ] 理解双向链表 + HashMap 的实现方式
- [ ] 知道三种方案的优劣对比
- [ ] 能够实现带 TTL 的 LRU 缓存
- [ ] 理解 LRU 缓存在 AI Agent 中的应用
- [ ] 能够实现线程安全的 LRU 缓存

---

## 下一步学习

### AI Agent 应用
→ **03_核心概念_05_AI_Agent短期记忆.md** - AI Agent 记忆管理

### 实战代码
→ **07_实战代码_03_LRU缓存系统.md** - 完整代码示例

### 面试准备
→ **08_面试必问.md** - LRU 缓存面试题

---

**版本**: v1.0
**最后更新**: 2026-02-13
**适用于**: Python 3.13+, AI Agent 开发, RAG 系统
