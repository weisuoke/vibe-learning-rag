# 核心概念04：距离度量

## 一句话定义

**距离度量（Distance Metric）是衡量向量相似度的数学方法，包括Cosine Similarity、Euclidean Distance和Dot Product三种主流度量，选择标准是匹配embedding模型的训练度量，错误选择会导致检索结果完全不准确。**

---

## 详细原理讲解

### 1. 什么是距离度量？

距离度量是向量空间中衡量两个向量相似度或距离的数学函数。

**核心作用**：
- 将"语义相似"转换为"数值计算"
- 决定向量检索的排序结果
- 影响检索精度和性能

**类比理解**：
```
现实世界的距离：
- 直线距离：两点之间的最短路径（Euclidean）
- 方向相似度：两个箭头的指向是否一致（Cosine）
- 重合程度：两个向量的重叠强度（Dot Product）

向量空间的距离：
- Euclidean Distance：空间中的实际距离
- Cosine Similarity：方向的相似度
- Dot Product：方向和强度的综合
```

---

### 2. 三种主流距离度量

#### 2.1 Cosine Similarity（余弦相似度）

**定义**：计算两个向量夹角的余弦值

**数学公式**：
```
cos(θ) = (A · B) / (||A|| × ||B||)

其中：
- A · B = Σ(A_i × B_i)  # 点积
- ||A|| = sqrt(Σ(A_i²))  # A的模长
- ||B|| = sqrt(Σ(B_i²))  # B的模长
```

**特点**：
- 只看方向，忽略长度
- 范围：-1到1（-1完全相反，0正交，1完全相同）
- 归一化后的向量，Cosine等价于Dot Product

**几何意义**：
```
向量A和B的夹角：
- θ = 0°  → cos(θ) = 1  → 完全相同方向
- θ = 90° → cos(θ) = 0  → 正交（无关）
- θ = 180° → cos(θ) = -1 → 完全相反方向
```

**Python实现**：
```python
import numpy as np

def cosine_similarity(a, b):
    """计算余弦相似度"""
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    return dot_product / (norm_a * norm_b)

# 示例
a = np.array([1, 2, 3])
b = np.array([2, 4, 6])  # b是a的2倍
print(f"Cosine: {cosine_similarity(a, b):.4f}")  # 1.0（方向完全相同）
```

---

#### 2.2 Euclidean Distance（欧几里得距离）

**定义**：两个向量在空间中的直线距离

**数学公式**：
```
d(A, B) = sqrt(Σ(A_i - B_i)²)

或者：
d(A, B) = ||A - B||
```

**特点**：
- 考虑空间距离和向量长度
- 范围：0到+∞（0表示完全相同）
- 受向量长度影响

**几何意义**：
```
二维空间示例：
A = (1, 2)
B = (4, 6)
d = sqrt((4-1)² + (6-2)²) = sqrt(9 + 16) = 5
```

**Python实现**：
```python
def euclidean_distance(a, b):
    """计算欧几里得距离"""
    return np.linalg.norm(a - b)

# 示例
a = np.array([1, 2, 3])
b = np.array([2, 4, 6])
print(f"Euclidean: {euclidean_distance(a, b):.4f}")  # 3.7417（距离较远）
```

---

#### 2.3 Dot Product（点积）

**定义**：两个向量对应元素乘积的和

**数学公式**：
```
A · B = Σ(A_i × B_i)
```

**特点**：
- 同时考虑方向和长度
- 范围：-∞到+∞
- 归一化向量的Dot Product等价于Cosine

**几何意义**：
```
A · B = ||A|| × ||B|| × cos(θ)

- 如果A和B归一化（||A|| = ||B|| = 1）
- 则 A · B = cos(θ)
```

**Python实现**：
```python
def dot_product(a, b):
    """计算点积"""
    return np.dot(a, b)

# 示例
a = np.array([1, 2, 3])
b = np.array([2, 4, 6])
print(f"Dot Product: {dot_product(a, b)}")  # 28
```

---

### 3. 三种度量的对比

#### 3.1 数值对比

```python
import numpy as np

# 测试向量
a = np.array([1, 2, 3])
b = np.array([2, 4, 6])  # b是a的2倍
c = np.array([1, 2, 4])  # 与a略有不同

print("向量a vs 向量b（b是a的2倍）：")
print(f"Cosine: {cosine_similarity(a, b):.4f}")  # 1.0
print(f"Euclidean: {euclidean_distance(a, b):.4f}")  # 3.7417
print(f"Dot Product: {dot_product(a, b)}")  # 28

print("\n向量a vs 向量c（略有不同）：")
print(f"Cosine: {cosine_similarity(a, c):.4f}")  # 0.9925
print(f"Euclidean: {euclidean_distance(a, c):.4f}")  # 1.0
print(f"Dot Product: {dot_product(a, c)}")  # 17
```

**结论**：
- Cosine：b和a方向完全相同（1.0），c和a方向几乎相同（0.9925）
- Euclidean：b和a距离较远（3.7417），c和a距离较近（1.0）
- Dot Product：b和a重合度高（28），c和a重合度中等（17）

#### 3.2 归一化的影响

```python
# 归一化向量
a_norm = a / np.linalg.norm(a)
b_norm = b / np.linalg.norm(b)
c_norm = c / np.linalg.norm(c)

print("归一化后：")
print(f"Cosine(a, b): {cosine_similarity(a_norm, b_norm):.4f}")  # 1.0
print(f"Dot Product(a, b): {dot_product(a_norm, b_norm):.4f}")  # 1.0（等价！）
print(f"Euclidean(a, b): {euclidean_distance(a_norm, b_norm):.4f}")  # 0.0
```

**结论**：归一化后，Cosine和Dot Product等价。

---

### 4. 如何选择距离度量？

#### 4.1 选择标准

**核心原则**：匹配embedding模型的训练度量

```python
from sentence_transformers import SentenceTransformer

# 检查模型训练时使用的度量
model = SentenceTransformer('all-mpnet-base-v2')
print(f"Model similarity function: {model.similarity_fn_name}")  # 'cosine'

# 使用相同度量
embeddings = model.encode(texts, normalize_embeddings=True)
# 此时Cosine和Dot Product等价，Dot Product更快
```

#### 4.2 决策树

```
选择距离度量：
├─ 检查embedding模型文档
│   ├─ 训练时用Cosine → 使用Cosine或Dot Product（归一化后）
│   ├─ 训练时用Euclidean → 使用Euclidean
│   └─ 训练时用Dot Product → 使用Dot Product
│
├─ 如果模型输出已归一化
│   └─ 使用Dot Product（比Cosine快20-30%）
│
└─ 如果不确定
    └─ 默认使用Cosine（最安全）
```

#### 4.3 常见模型的度量

| 模型 | 训练度量 | 推荐度量 |
|------|---------|---------|
| sentence-transformers/* | Cosine | Cosine/Dot Product |
| OpenAI text-embedding-ada-002 | Cosine | Cosine/Dot Product |
| OpenAI text-embedding-3-small | Cosine | Cosine/Dot Product |
| Cohere embed-english-v3.0 | Cosine | Cosine/Dot Product |
| Google PaLM Embedding | Dot Product | Dot Product |
| 自定义模型 | 查看文档 | 匹配训练度量 |

---

### 5. 在RAG中的应用

#### 5.1 ChromaDB配置

```python
import chromadb
from sentence_transformers import SentenceTransformer

# 创建collection with Cosine度量
client = chromadb.Client()
collection = client.create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine"}  # 指定距离度量
)

# 添加文档
model = SentenceTransformer('all-mpnet-base-v2')
documents = ["RAG是检索增强生成", "向量存储用于语义检索"]
embeddings = model.encode(documents, normalize_embeddings=True)

collection.add(
    documents=documents,
    embeddings=embeddings.tolist(),
    ids=["doc1", "doc2"]
)

# 查询
query = "什么是RAG？"
query_embedding = model.encode(query, normalize_embeddings=True)
results = collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=2
)
```

#### 5.2 FAISS配置

```python
import faiss
import numpy as np

dimension = 768

# 方案1：Cosine（通过归一化 + L2）
index = faiss.IndexFlatL2(dimension)
# 添加前归一化
faiss.normalize_L2(embeddings)
index.add(embeddings)

# 查询前归一化
faiss.normalize_L2(query_embedding)
distances, indices = index.search(query_embedding, k=10)

# 方案2：Dot Product
index = faiss.IndexFlatIP(dimension)  # IP = Inner Product
index.add(embeddings)
distances, indices = index.search(query_embedding, k=10)

# 方案3：Euclidean
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)
distances, indices = index.search(query_embedding, k=10)
```

#### 5.3 Milvus配置

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# 连接Milvus
connections.connect("default", host="localhost", port="19530")

# 定义schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
]
schema = CollectionSchema(fields, description="Documents")

# 创建collection
collection = Collection("documents", schema)

# 创建索引with指定度量
index_params = {
    "metric_type": "COSINE",  # 或 "L2"（Euclidean）, "IP"（Dot Product）
    "index_type": "HNSW",
    "params": {"M": 32, "efConstruction": 200}
}
collection.create_index("embedding", index_params)

# 查询
search_params = {"metric_type": "COSINE", "params": {"ef": 100}}
results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param=search_params,
    limit=10
)
```

---

### 6. 性能对比

#### 6.1 计算复杂度

| 度量 | 时间复杂度 | 相对速度 |
|------|-----------|---------|
| Dot Product | O(d) | 1.0x（最快） |
| Cosine | O(d) + 2×sqrt | 1.2-1.3x |
| Euclidean | O(d) + sqrt | 1.1-1.2x |

**结论**：归一化后，Dot Product最快。

#### 6.2 实际性能测试

```python
import time
import numpy as np

# 生成测试数据
n_vectors = 100000
dimension = 768
vectors = np.random.randn(n_vectors, dimension).astype('float32')
query = np.random.randn(dimension).astype('float32')

# 归一化
vectors_norm = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)
query_norm = query / np.linalg.norm(query)

# 测试Dot Product
start = time.time()
scores = np.dot(vectors_norm, query_norm)
dot_time = time.time() - start
print(f"Dot Product: {dot_time:.4f}s")

# 测试Cosine
start = time.time()
dot_products = np.dot(vectors, query)
norms = np.linalg.norm(vectors, axis=1) * np.linalg.norm(query)
scores = dot_products / norms
cosine_time = time.time() - start
print(f"Cosine: {cosine_time:.4f}s")

# 测试Euclidean
start = time.time()
distances = np.linalg.norm(vectors - query, axis=1)
euclidean_time = time.time() - start
print(f"Euclidean: {euclidean_time:.4f}s")

print(f"\n相对速度：")
print(f"Dot Product: 1.0x")
print(f"Cosine: {cosine_time/dot_time:.2f}x")
print(f"Euclidean: {euclidean_time/dot_time:.2f}x")
```

**典型结果**：
```
Dot Product: 0.0050s
Cosine: 0.0065s
Euclidean: 0.0058s

相对速度：
Dot Product: 1.0x
Cosine: 1.30x
Euclidean: 1.16x
```

---

### 7. 常见误区

#### 7.1 误区1：Cosine和Dot Product可以互换

**错误认知**：
```python
# ❌ 错误：非归一化向量，Cosine和Dot Product结果不同
a = np.array([1, 2, 3])
b = np.array([2, 4, 6])

cosine = cosine_similarity(a, b)  # 1.0
dot = dot_product(a, b)  # 28
# 结果完全不同！
```

**正确做法**：
```python
# ✅ 正确：归一化后才等价
a_norm = a / np.linalg.norm(a)
b_norm = b / np.linalg.norm(b)

cosine = cosine_similarity(a_norm, b_norm)  # 1.0
dot = dot_product(a_norm, b_norm)  # 1.0
# 结果相同
```

#### 7.2 误区2：距离度量可以随意选择

**错误认知**：
"反正都是计算相似度，选哪个都一样"

**真实情况**：
```python
# 模型用Cosine训练
model = SentenceTransformer('all-mpnet-base-v2')  # 训练时用Cosine

# ❌ 错误：用Euclidean检索
index = faiss.IndexFlatL2(dimension)
# 结果不准确！

# ✅ 正确：用Cosine检索
index = faiss.IndexFlatIP(dimension)
faiss.normalize_L2(embeddings)
# 结果准确
```

#### 7.3 误区3：Euclidean总是比Cosine好

**错误认知**：
"Euclidean考虑了向量长度，信息更多，应该更准确"

**真实情况**：
- 大多数embedding模型用Cosine训练
- 向量长度通常没有语义意义
- 使用Euclidean反而会降低精度

---

### 8. 2025-2026最新实践

#### 8.1 自适应度量选择

**趋势**：根据查询类型动态选择度量

```python
class AdaptiveMetric:
    """自适应距离度量"""

    def __init__(self):
        self.cosine_index = faiss.IndexFlatIP(dimension)
        self.euclidean_index = faiss.IndexFlatL2(dimension)

    def search(self, query, query_type="semantic"):
        """根据查询类型选择度量"""
        if query_type == "semantic":
            # 语义查询用Cosine
            return self.cosine_index.search(query, k=10)
        elif query_type == "exact":
            # 精确匹配用Euclidean
            return self.euclidean_index.search(query, k=10)
        else:
            # 默认Cosine
            return self.cosine_index.search(query, k=10)
```

#### 8.2 混合度量

**思想**：结合多种度量的优势

```python
def hybrid_similarity(query, vectors, alpha=0.7):
    """混合相似度：Cosine + Euclidean"""
    # Cosine相似度
    cosine_scores = np.dot(vectors, query) / (
        np.linalg.norm(vectors, axis=1) * np.linalg.norm(query)
    )

    # Euclidean距离（归一化到0-1）
    euclidean_distances = np.linalg.norm(vectors - query, axis=1)
    max_dist = np.max(euclidean_distances)
    euclidean_scores = 1 - (euclidean_distances / max_dist)

    # 加权组合
    hybrid_scores = alpha * cosine_scores + (1 - alpha) * euclidean_scores

    return hybrid_scores
```

---

### 9. 实战案例

#### 9.1 对比三种度量的检索结果

```python
from sentence_transformers import SentenceTransformer
import numpy as np

# 初始化模型
model = SentenceTransformer('all-mpnet-base-v2')

# 文档
documents = [
    "RAG是检索增强生成技术",
    "向量存储用于语义检索",
    "HNSW是高效的索引算法",
    "Cosine相似度衡量方向",
    "Euclidean距离衡量空间距离"
]

# 向量化
doc_embeddings = model.encode(documents)

# 查询
query = "什么是RAG？"
query_embedding = model.encode(query)

# 方法1：Cosine
cosine_scores = np.dot(doc_embeddings, query_embedding) / (
    np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding)
)
cosine_ranking = np.argsort(-cosine_scores)

# 方法2：Euclidean
euclidean_distances = np.linalg.norm(doc_embeddings - query_embedding, axis=1)
euclidean_ranking = np.argsort(euclidean_distances)

# 方法3：Dot Product（归一化后）
doc_embeddings_norm = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)
query_embedding_norm = query_embedding / np.linalg.norm(query_embedding)
dot_scores = np.dot(doc_embeddings_norm, query_embedding_norm)
dot_ranking = np.argsort(-dot_scores)

# 打印结果
print("Cosine排序：")
for i, idx in enumerate(cosine_ranking[:3]):
    print(f"{i+1}. {documents[idx]} (score: {cosine_scores[idx]:.4f})")

print("\nEuclidean排序：")
for i, idx in enumerate(euclidean_ranking[:3]):
    print(f"{i+1}. {documents[idx]} (distance: {euclidean_distances[idx]:.4f})")

print("\nDot Product排序（归一化）：")
for i, idx in enumerate(dot_ranking[:3]):
    print(f"{i+1}. {documents[idx]} (score: {dot_scores[idx]:.4f})")
```

---

## 总结

**三种度量的核心区别**：
1. **Cosine**：只看方向，忽略长度，最常用
2. **Euclidean**：考虑空间距离和长度，适合特定场景
3. **Dot Product**：方向和长度都考虑，归一化后等价于Cosine

**选择标准**：
- 匹配embedding模型的训练度量
- 大多数情况用Cosine
- 归一化后用Dot Product（更快）

**性能优化**：
- Dot Product最快（归一化后）
- Cosine慢20-30%
- Euclidean慢10-20%

**2026年最佳实践**：
- 默认使用Cosine
- 归一化后用Dot Product
- 根据查询类型自适应选择
- 混合度量提升精度

---

## 引用来源

1. **距离度量完整指南**：https://medium.com/@thomaspatole19/the-complete-guide-to-distance-metrics...
2. **Pinecone向量相似度**：https://www.pinecone.io/learn/vector-similarity
3. **Weaviate距离度量**：https://weaviate.io/blog/distance-metrics-in-vector-search
4. **Redis向量相似度**：https://redis.io/blog/vector-similarity
5. **FAISS度量选择**：https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances
6. **Milvus度量类型**：https://milvus.io/docs/metric.md

---

**记住**：距离度量的选择直接影响检索精度，必须匹配embedding模型的训练度量，错误选择会导致检索结果完全不准确。
