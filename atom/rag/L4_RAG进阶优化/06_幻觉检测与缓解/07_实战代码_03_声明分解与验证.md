# 实战代码3：声明分解与验证

> **实现完整的声明级幻觉检测系统，包括声明分解、逐一验证和引用溯源**

---

## 场景描述

构建声明级幻觉检测系统，将生成内容拆分为原子声明，逐一验证每个声明是否被检索上下文支持，并为被支持的声明添加引用溯源。

**适用场景：**
- 需要精确定位幻觉位置
- 高风险场景（医疗、法律）
- 需要引用溯源的应用

---

## 环境准备

### 安装依赖

```bash
# 安装所需依赖
pip install openai transformers torch sentence-transformers python-dotenv
```

---

## 完整代码实现

```python
"""
声明分解与验证系统
实现完整的声明级幻觉检测和引用溯源
"""

import os
from typing import List, Dict, Tuple
from dotenv import load_dotenv
from openai import OpenAI
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
import json

# 加载环境变量
load_dotenv()


class ClaimDecomposer:
    """声明分解器"""

    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.client = OpenAI()
        self.model_name = model_name

    def decompose(self, response: str) -> List[str]:
        """
        将回答拆分为原子声明

        Args:
            response: 生成的回答

        Returns:
            原子声明列表
        """
        prompt = f"""
将以下回答拆分为独立的原子声明。

回答：
{response}

要求：
1. 每个声明只包含一个可验证的事实
2. 每个声明独立完整，包含主谓宾结构
3. 不包含推理或解释
4. 不能再拆分而不丢失语义

输出格式：每行一个声明，不要编号或符号
"""

        completion = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )

        claims_text = completion.choices[0].message.content.strip()
        claims = [
            c.strip("- ").strip() for c in claims_text.split("\n") if c.strip()
        ]

        return claims


class ClaimVerifier:
    """声明验证器"""

    def __init__(
        self,
        nli_model_name: str = "microsoft/deberta-v3-base-mnli-fever-anli",
        nli_threshold: float = 0.8,
        similarity_threshold: float = 0.75,
    ):
        # 加载NLI模型
        self.nli_model = pipeline("text-classification", model=nli_model_name, device=-1)
        self.nli_threshold = nli_threshold

        # 加载Embedding模型
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
        self.similarity_threshold = similarity_threshold

    def verify_with_nli(self, claim: str, contexts: List[str]) -> Dict:
        """使用NLI验证声明"""
        best_result = {
            "supported": False,
            "best_score": 0.0,
            "best_context_idx": -1,
        }

        for idx, context in enumerate(contexts):
            input_text = f"{context} [SEP] {claim}"
            result = self.nli_model(input_text)[0]

            if (
                result["label"] == "entailment"
                and result["score"] > best_result["best_score"]
            ):
                best_result = {
                    "supported": True,
                    "best_score": result["score"],
                    "best_context_idx": idx,
                    "best_context": context,
                }

        return best_result

    def verify_with_similarity(self, claim: str, contexts: List[str]) -> Dict:
        """使用语义相似度验证声明"""
        claim_emb = self.embedding_model.encode(claim, convert_to_tensor=True)
        context_embs = self.embedding_model.encode(contexts, convert_to_tensor=True)

        similarities = util.cos_sim(claim_emb, context_embs)[0]
        best_idx = similarities.argmax().item()
        best_score = similarities[best_idx].item()

        return {
            "supported": best_score > self.similarity_threshold,
            "best_score": best_score,
            "best_context_idx": best_idx,
            "best_context": contexts[best_idx] if best_score > self.similarity_threshold else None,
        }

    def verify_claim(self, claim: str, contexts: List[str]) -> Dict:
        """
        综合验证声明

        Args:
            claim: 声明
            contexts: 上下文列表

        Returns:
            验证结果
        """
        # 方法1：NLI验证
        nli_result = self.verify_with_nli(claim, contexts)

        # 方法2：语义相似度
        similarity_result = self.verify_with_similarity(claim, contexts)

        # 投票机制
        votes = [nli_result["supported"], similarity_result["supported"]]
        final_supported = sum(votes) >= 1  # 至少1个方法认为支持

        # 选择最佳上下文
        if nli_result["supported"]:
            best_context_idx = nli_result["best_context_idx"]
            best_context = nli_result["best_context"]
            best_score = nli_result["best_score"]
            method = "NLI"
        elif similarity_result["supported"]:
            best_context_idx = similarity_result["best_context_idx"]
            best_context = similarity_result["best_context"]
            best_score = similarity_result["best_score"]
            method = "Similarity"
        else:
            best_context_idx = -1
            best_context = None
            best_score = 0.0
            method = "None"

        return {
            "claim": claim,
            "supported": final_supported,
            "nli_supported": nli_result["supported"],
            "nli_score": nli_result.get("best_score", 0.0),
            "similarity_supported": similarity_result["supported"],
            "similarity_score": similarity_result["best_score"],
            "best_context_idx": best_context_idx,
            "best_context": best_context,
            "best_score": best_score,
            "method": method,
        }


class AttributionGenerator:
    """引用溯源生成器"""

    def format_inline_citation(self, verified_claims: List[Dict]) -> str:
        """生成行内引用格式"""
        cited_text = ""
        citations = {}

        for claim_info in verified_claims:
            if claim_info["supported"]:
                source_idx = claim_info["best_context_idx"]
                citation_num = source_idx + 1

                cited_text += f"{claim_info['claim']}[{citation_num}] "

                if citation_num not in citations:
                    citations[citation_num] = claim_info["best_context"]
            else:
                cited_text += f"[未验证：{claim_info['claim']}] "

        # 生成引用列表
        if citations:
            citation_list = "\n\n引用来源：\n"
            for num in sorted(citations.keys()):
                citation_list += f"[{num}] {citations[num]}\n"
            cited_text += citation_list

        return cited_text.strip()

    def format_detailed_report(self, verified_claims: List[Dict]) -> str:
        """生成详细验证报告"""
        report = "=" * 60 + "\n"
        report += "声明级验证报告\n"
        report += "=" * 60 + "\n\n"

        supported_count = sum(1 for c in verified_claims if c["supported"])
        support_ratio = supported_count / len(verified_claims) if verified_claims else 0

        report += f"总声明数: {len(verified_claims)}\n"
        report += f"支持声明数: {supported_count}\n"
        report += f"支持率: {support_ratio:.1%}\n\n"

        report += "详细结果：\n"
        report += "-" * 60 + "\n"

        for i, claim_info in enumerate(verified_claims, 1):
            status_icon = "✅" if claim_info["supported"] else "❌"
            report += f"\n{i}. {status_icon} {claim_info['claim']}\n"

            if claim_info["supported"]:
                report += f"   验证方法: {claim_info['method']}\n"
                report += f"   NLI: {'✅' if claim_info['nli_supported'] else '❌'} ({claim_info['nli_score']:.2f})\n"
                report += f"   相似度: {'✅' if claim_info['similarity_supported'] else '❌'} ({claim_info['similarity_score']:.2f})\n"
                report += f"   来源: 上下文{claim_info['best_context_idx']+1}\n"
                report += f"   引用: {claim_info['best_context'][:100]}...\n"
            else:
                report += f"   原因: 未找到支持的上下文\n"

        return report


class ClaimLevelDetector:
    """声明级幻觉检测系统"""

    def __init__(self, threshold: float = 0.8):
        self.decomposer = ClaimDecomposer()
        self.verifier = ClaimVerifier()
        self.attribution = AttributionGenerator()
        self.threshold = threshold

    def detect(self, response: str, contexts: List[str]) -> Dict:
        """
        完整的声明级检测流程

        Args:
            response: 生成的回答
            contexts: 检索上下文列表

        Returns:
            检测结果
        """
        # 步骤1：声明分解
        print("步骤1：声明分解...")
        claims = self.decomposer.decompose(response)
        print(f"  拆分为{len(claims)}个原子声明")

        # 步骤2：逐一验证
        print("\n步骤2：逐一验证...")
        verified_claims = []
        for i, claim in enumerate(claims, 1):
            print(f"  验证声明{i}/{len(claims)}: {claim[:50]}...")
            result = self.verifier.verify_claim(claim, contexts)
            verified_claims.append(result)

        # 步骤3：计算支持率
        support_ratio = (
            sum(1 for c in verified_claims if c["supported"]) / len(verified_claims)
            if verified_claims
            else 0
        )

        # 步骤4：生成引用
        print("\n步骤3：生成引用...")
        cited_answer = self.attribution.format_inline_citation(verified_claims)

        # 步骤5：生成报告
        detailed_report = self.attribution.format_detailed_report(verified_claims)

        return {
            "original_response": response,
            "claims": claims,
            "verified_claims": verified_claims,
            "support_ratio": support_ratio,
            "cited_answer": cited_answer,
            "detailed_report": detailed_report,
            "passed": support_ratio >= self.threshold,
        }


def demo_basic_claim_decomposition():
    """演示基础声明分解"""
    print("=" * 60)
    print("演示1：基础声明分解")
    print("=" * 60)

    decomposer = ClaimDecomposer()

    response = "Python是一种由Guido van Rossum于1991年创建的高级编程语言，广泛用于数据科学和Web开发。"

    print(f"\n原始回答：\n{response}")

    claims = decomposer.decompose(response)

    print(f"\n拆分结果（{len(claims)}个原子声明）：")
    for i, claim in enumerate(claims, 1):
        print(f"  {i}. {claim}")


def demo_claim_verification():
    """演示声明验证"""
    print("\n" + "=" * 60)
    print("演示2：声明验证")
    print("=" * 60)

    verifier = ClaimVerifier()

    contexts = [
        "Python是一种高级编程语言，由Guido van Rossum创建。",
        "Python于1991年首次发布。",
    ]

    claims = [
        "Python是高级编程语言",
        "Python由Guido van Rossum创建",
        "Python于1991年创建",
        "Python广泛用于数据科学",  # 未被支持
    ]

    print(f"\n上下文数量: {len(contexts)}")
    print(f"声明数量: {len(claims)}")

    for claim in claims:
        result = verifier.verify_claim(claim, contexts)

        status_icon = "✅" if result["supported"] else "❌"
        print(f"\n{status_icon} {claim}")
        if result["supported"]:
            print(f"   验证方法: {result['method']}")
            print(f"   置信度: {result['best_score']:.2f}")
            print(f"   来源: 上下文{result['best_context_idx']+1}")


def demo_attribution():
    """演示引用溯源"""
    print("\n" + "=" * 60)
    print("演示3：引用溯源")
    print("=" * 60)

    attribution = AttributionGenerator()

    verified_claims = [
        {
            "claim": "Python是高级编程语言",
            "supported": True,
            "best_context_idx": 0,
            "best_context": "Python是一种高级编程语言。",
            "best_score": 0.95,
            "method": "NLI",
            "nli_supported": True,
            "nli_score": 0.95,
            "similarity_supported": True,
            "similarity_score": 0.89,
        },
        {
            "claim": "Python由Guido van Rossum创建",
            "supported": True,
            "best_context_idx": 0,
            "best_context": "Python由Guido van Rossum创建。",
            "best_score": 0.92,
            "method": "NLI",
            "nli_supported": True,
            "nli_score": 0.92,
            "similarity_supported": True,
            "similarity_score": 0.88,
        },
        {
            "claim": "Python广泛用于数据科学",
            "supported": False,
            "best_context_idx": -1,
            "best_context": None,
            "best_score": 0.0,
            "method": "None",
            "nli_supported": False,
            "nli_score": 0.0,
            "similarity_supported": False,
            "similarity_score": 0.45,
        },
    ]

    # 生成行内引用
    cited_answer = attribution.format_inline_citation(verified_claims)
    print(f"\n带引用的回答：\n{cited_answer}")

    # 生成详细报告
    detailed_report = attribution.format_detailed_report(verified_claims)
    print(f"\n{detailed_report}")


def demo_complete_detection():
    """演示完整检测流程"""
    print("\n" + "=" * 60)
    print("演示4：完整声明级检测")
    print("=" * 60)

    detector = ClaimLevelDetector(threshold=0.8)

    response = "Python是一种由Guido van Rossum于1991年创建的高级编程语言，广泛用于数据科学和Web开发。"

    contexts = [
        "Python是一种高级编程语言，由Guido van Rossum创建。",
        "Python于1991年首次发布。",
        "Python语法简洁，易于学习。",
    ]

    print(f"\n原始回答：\n{response}")
    print(f"\n上下文数量: {len(contexts)}")

    result = detector.detect(response, contexts)

    print(f"\n检测结果：")
    print(f"  支持率: {result['support_ratio']:.1%}")
    print(f"  状态: {'✅ 通过' if result['passed'] else '❌ 未通过'}")

    print(f"\n带引用的回答：\n{result['cited_answer']}")

    print(f"\n{result['detailed_report']}")


def demo_real_world_scenario():
    """演示真实场景"""
    print("\n" + "=" * 60)
    print("演示5：真实场景 - 企业知识库问答")
    print("=" * 60)

    detector = ClaimLevelDetector(threshold=0.8)

    # 场景：员工咨询年假政策
    question = "入职满3年可以享受多少天年假？"

    contexts = [
        "公司年假政策：入职满1年可享受5天年假。",
        "公司年假政策：入职满3年可享受10天年假。",
        "公司年假政策：入职满5年可享受15天年假。",
    ]

    # 模拟生成的回答（包含部分幻觉）
    response = "入职满3年可以享受10天年假，此外还可以享受5天病假和2天事假。"

    print(f"\n问题：{question}")
    print(f"\n生成的回答：\n{response}")

    result = detector.detect(response, contexts)

    print(f"\n检测结果：")
    print(f"  支持率: {result['support_ratio']:.1%}")
    print(f"  状态: {'✅ 通过' if result['passed'] else '❌ 未通过'}")

    print(f"\n修正后的回答：\n{result['cited_answer']}")

    print(f"\n{result['detailed_report']}")


def main():
    """主函数"""
    print("\n" + "=" * 60)
    print("声明分解与验证系统")
    print("=" * 60)

    # 检查API密钥
    if not os.getenv("OPENAI_API_KEY"):
        print("\n❌ 错误：未设置OPENAI_API_KEY环境变量")
        print("请创建.env文件并设置API密钥")
        return

    try:
        # 运行演示
        demo_basic_claim_decomposition()
        demo_claim_verification()
        demo_attribution()
        demo_complete_detection()
        demo_real_world_scenario()

        print("\n" + "=" * 60)
        print("✅ 所有演示完成")
        print("=" * 60)

    except Exception as e:
        print(f"\n❌ 错误: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
```

---

## 代码说明

### 核心类

1. **ClaimDecomposer**：声明分解器
   - 使用LLM将复杂句子拆分为原子声明

2. **ClaimVerifier**：声明验证器
   - 结合NLI和语义相似度验证声明

3. **AttributionGenerator**：引用溯源生成器
   - 生成带引用的回答和详细报告

4. **ClaimLevelDetector**：完整检测系统
   - 集成分解、验证、溯源的完整流程

---

## 运行示例

```bash
python claim_level_detection.py
```

---

## 输出示例

```
============================================================
演示4：完整声明级检测
============================================================

原始回答：
Python是一种由Guido van Rossum于1991年创建的高级编程语言，广泛用于数据科学和Web开发。

上下文数量: 3

步骤1：声明分解...
  拆分为6个原子声明

步骤2：逐一验证...
  验证声明1/6: Python是一种编程语言...
  验证声明2/6: Python是高级编程语言...
  验证声明3/6: Python由Guido van Rossum创建...
  验证声明4/6: Python于1991年创建...
  验证声明5/6: Python广泛用于数据科学...
  验证声明6/6: Python广泛用于Web开发...

步骤3：生成引用...

检测结果：
  支持率: 66.7%
  状态: ❌ 未通过

带引用的回答：
Python是一种编程语言[1] Python是高级编程语言[1] Python由Guido van Rossum创建[1] Python于1991年创建[2] [未验证：Python广泛用于数据科学] [未验证：Python广泛用于Web开发]

引用来源：
[1] Python是一种高级编程语言，由Guido van Rossum创建。
[2] Python于1991年首次发布。
```

---

## 性能优化

### 1. 批量验证

```python
def batch_verify_claims(claims: List[str], contexts: List[str]) -> List[Dict]:
    """批量验证多个声明"""
    # 批量NLI验证
    nli_inputs = [f"{ctx} [SEP] {claim}" for ctx in contexts for claim in claims]
    nli_results = nli_model(nli_inputs)

    # 批量Embedding
    claim_embs = embedding_model.encode(claims)
    context_embs = embedding_model.encode(contexts)

    # ...
```

### 2. 缓存分解结果

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def decompose_cached(response: str) -> tuple:
    claims = decomposer.decompose(response)
    return tuple(claims)
```

### 3. 异步处理

```python
import asyncio

async def verify_claims_async(claims: List[str], contexts: List[str]):
    tasks = [verify_claim_async(claim, contexts) for claim in claims]
    return await asyncio.gather(*tasks)
```

---

## 常见问题

### Q1: 声明分解的准确率如何？

**A:** 使用GPT-4o-mini分解，准确率约90%。建议：
- 使用更强的模型（GPT-4o）提升准确率
- 添加后处理验证分解质量
- 人工抽样检查

### Q2: 如何处理分解失败的情况？

**A:** 3种策略：
1. 回退到句子级验证
2. 使用规则分解作为备选
3. 标记为"无法分解"并人工审核

### Q3: 检测延迟如何优化？

**A:** 优化策略：
- 批量处理（提速3-5倍）
- 缓存分解结果
- 异步验证
- 使用更快的模型

---

## 扩展建议

1. **添加置信度评分**
   ```python
   def calculate_confidence(verified_claims):
       return sum(c["best_score"] for c in verified_claims) / len(verified_claims)
   ```

2. **支持多语言**
   ```python
   def detect_language(text):
       # 自动检测语言并选择对应的NLI模型
       pass
   ```

3. **集成监控**
   ```python
   from prometheus_client import Histogram

   detection_latency = Histogram('detection_latency_seconds', 'Detection latency')
   ```

---

**记住：声明级检测是最精确的幻觉检测方法，适合高风险场景和需要引用溯源的应用。**
