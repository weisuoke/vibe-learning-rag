# 最小可用

掌握以下内容，就能开始构建 Milvus 高可用集群：

### 4.1 理解 Milvus 的分布式架构

**核心概念：** Milvus 由多个独立组件组成，每个组件都可以独立扩展

```
Milvus 集群组件：
┌─────────────────────────────────────────┐
│  Root Coordinator (元数据管理)           │  ← 需要高可用
├─────────────────────────────────────────┤
│  Query Coordinator (查询协调)            │  ← 需要高可用
├─────────────────────────────────────────┤
│  Data Coordinator (数据协调)             │  ← 需要高可用
├─────────────────────────────────────────┤
│  Index Coordinator (索引协调)            │  ← 需要高可用
├─────────────────────────────────────────┤
│  Proxy (客户端接入)                      │  ← 可水平扩展
├─────────────────────────────────────────┤
│  Query Node (查询执行)                   │  ← 可水平扩展
├─────────────────────────────────────────┤
│  Data Node (数据写入)                    │  ← 可水平扩展
├─────────────────────────────────────────┤
│  Index Node (索引构建)                   │  ← 可水平扩展
└─────────────────────────────────────────┘
```

**最小可用配置：**
```yaml
# 生产级最小配置
Coordinator: 3 副本（奇数个，用于选主）
Proxy: 2+ 副本（负载均衡）
Query Node: 2+ 副本（查询冗余）
Data Node: 2+ 副本（写入冗余）
Index Node: 2+ 副本（索引冗余）
```

**为什么这样配置？**
- Coordinator 需要奇数个副本（3/5/7）用于 Raft 选主
- Worker 节点至少 2 个副本，确保单点故障时仍能服务
- 更多副本 = 更高可用性，但成本也更高

---

### 4.2 配置多副本部署

**Kubernetes 部署示例：**

```yaml
# milvus-cluster.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-rootcoord
spec:
  replicas: 3  # 3 个副本
  selector:
    matchLabels:
      app: milvus-rootcoord
  template:
    metadata:
      labels:
        app: milvus-rootcoord
    spec:
      containers:
      - name: rootcoord
        image: milvusdb/milvus:v2.4.0
        command: ["milvus", "run", "rootcoord"]
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-proxy
spec:
  replicas: 2  # 2 个副本
  selector:
    matchLabels:
      app: milvus-proxy
  template:
    metadata:
      labels:
        app: milvus-proxy
    spec:
      containers:
      - name: proxy
        image: milvusdb/milvus:v2.4.0
        command: ["milvus", "run", "proxy"]
        ports:
        - containerPort: 19530
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
```

**关键配置点：**
1. `replicas: 3` - 设置副本数量
2. `resources` - 限制资源使用，防止单个 Pod 占用过多资源
3. `selector` - 用于 Service 负载均衡

---

### 4.3 配置健康检查和自动重启

**Kubernetes 健康检查：**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: milvus-proxy
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: proxy
        image: milvusdb/milvus:v2.4.0
        # 存活探针：检测容器是否存活
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9091
          initialDelaySeconds: 30  # 启动后 30 秒开始检查
          periodSeconds: 10        # 每 10 秒检查一次
          timeoutSeconds: 5        # 超时时间 5 秒
          failureThreshold: 3      # 连续失败 3 次则重启
        # 就绪探针：检测容器是否准备好接收流量
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9091
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
```

**健康检查的作用：**
- **livenessProbe**：如果失败，Kubernetes 会自动重启容器
- **readinessProbe**：如果失败，Kubernetes 会将容器从负载均衡中移除

---

### 4.4 配置持久化存储

**核心原则：** 数据和元数据必须持久化到外部存储，而不是容器内部

```yaml
# 使用外部对象存储（MinIO/S3）
apiVersion: v1
kind: ConfigMap
metadata:
  name: milvus-config
data:
  milvus.yaml: |
    # 对象存储配置（存储向量数据和索引）
    minio:
      address: minio.default.svc.cluster.local
      port: 9000
      accessKeyID: minioadmin
      secretAccessKey: minioadmin
      useSSL: false
      bucketName: milvus-bucket

    # 元数据存储配置（存储 Collection 信息）
    etcd:
      endpoints:
        - etcd-0.etcd.default.svc.cluster.local:2379
        - etcd-1.etcd.default.svc.cluster.local:2379
        - etcd-2.etcd.default.svc.cluster.local:2379

    # 消息队列配置（存储 WAL）
    pulsar:
      address: pulsar-proxy.default.svc.cluster.local
      port: 6650
```

**为什么需要外部存储？**
- 容器重启后，容器内的数据会丢失
- 外部存储（MinIO/S3/etcd）的数据是持久化的
- 新的容器可以从外部存储恢复数据

---

### 4.5 验证高可用性

**测试故障转移：**

```python
"""
验证 Milvus 高可用集群的故障转移能力
"""
from pymilvus import connections, Collection
import time

# 连接到 Milvus 集群（通过 Kubernetes Service）
connections.connect(
    alias="default",
    host="milvus-proxy.default.svc.cluster.local",  # K8s Service 地址
    port="19530"
)

# 获取 Collection
collection = Collection("test_collection")
collection.load()

print("=== 开始故障转移测试 ===")

# 持续查询，观察故障转移过程
for i in range(100):
    try:
        # 执行查询
        results = collection.search(
            data=[[0.1] * 128],
            anns_field="embedding",
            param={"metric_type": "L2", "params": {"nprobe": 10}},
            limit=10
        )
        print(f"查询 {i+1}: 成功，返回 {len(results[0])} 条结果")

        # 在第 20 次查询后，手动删除一个 Proxy Pod
        if i == 20:
            print("\n⚠️  请在另一个终端执行：kubectl delete pod -l app=milvus-proxy --force\n")

        time.sleep(1)

    except Exception as e:
        print(f"查询 {i+1}: 失败 - {e}")
        time.sleep(1)

print("\n=== 测试完成 ===")
print("如果在删除 Pod 后，查询仍能继续成功，说明故障转移正常工作")
```

**预期结果：**
```
查询 1: 成功，返回 10 条结果
查询 2: 成功，返回 10 条结果
...
查询 20: 成功，返回 10 条结果

⚠️  请在另一个终端执行：kubectl delete pod -l app=milvus-proxy --force

查询 21: 失败 - connection refused
查询 22: 成功，返回 10 条结果  ← 自动切换到另一个 Proxy
查询 23: 成功，返回 10 条结果
...
```

**关键观察点：**
- 删除 Pod 后，可能有 1-2 次查询失败（故障检测时间）
- 之后查询自动恢复（切换到健康的 Pod）
- 总中断时间 < 5 秒（符合生产级要求）

---

### 这些知识足以：

- ✅ 理解 Milvus 高可用集群的基本架构
- ✅ 在 Kubernetes 上部署多副本 Milvus 集群
- ✅ 配置健康检查和自动重启机制
- ✅ 配置持久化存储，确保数据不丢失
- ✅ 验证故障转移能力，确保高可用性正常工作
- ✅ 为后续学习灾难恢复和跨数据中心复制打下基础

---

### 快速检查清单

在部署高可用集群前，确认以下事项：

- [ ] 每个 Coordinator 组件至少有 3 个副本
- [ ] 每个 Worker 组件至少有 2 个副本
- [ ] 配置了 livenessProbe 和 readinessProbe
- [ ] 使用外部对象存储（MinIO/S3）存储数据
- [ ] 使用外部 etcd 集群存储元数据
- [ ] 使用外部 Pulsar/Kafka 存储消息队列
- [ ] 测试了故障转移能力（手动删除 Pod）
- [ ] 验证了数据持久化（重启后数据不丢失）
