# 反直觉点：Prompt Engineering进阶

## 反直觉点1：示例越多≠效果越好

### 直觉认知

```
更多示例 = 更好的效果
10个示例 > 3个示例
```

### 真实情况

**Few-shot的最佳数量：2-5个**

```python
# ❌ 错误：10个示例
prompt = """
示例1：...
示例2：...
...
示例10：...

现在处理：{query}
"""
# 问题：
# 1. 浪费token（成本高）
# 2. 模型可能被示例淹没
# 3. 效果不一定更好

# ✅ 正确：3个精选示例
prompt = """
示例1：...（最相关）
示例2：...（次相关）
示例3：...（第三相关）

现在处理：{query}
"""
# 优势：
# 1. 省token
# 2. 模型聚焦
# 3. 效果更好
```

**实验数据：**

| 示例数量 | 准确率 | Token消耗 | 成本 |
|---------|--------|----------|------|
| 0个 | 60% | 100 | $0.001 |
| 3个 | 85% | 200 | $0.002 |
| 5个 | 87% | 300 | $0.003 |
| 10个 | 85% | 500 | $0.005 |

**结论：** 3-5个示例是最佳平衡点

**来源：** [Few-shot Learning Research (2023)](https://arxiv.org/abs/2005.14165)

---

## 反直觉点2：CoT不是越详细越好

### 直觉认知

```
推理步骤越详细 = 效果越好
10步推理 > 3步推理
```

### 真实情况

**过度详细的CoT会降低效果**

```python
# ❌ 错误：过度详细
prompt = """
计算 23 * 47

让我们分10步详细计算：
1. 首先理解乘法的定义
2. 将23分解为20+3
3. 将47分解为40+7
4. 计算20*40
5. 计算20*7
6. 计算3*40
7. 计算3*7
8. 将所有结果相加
9. 验证结果
10. 得出最终答案
"""
# 问题：步骤太细，模型可能迷失

# ✅ 正确：简洁引导
prompt = """
计算 23 * 47

让我们一步步思考：
"""
# 让模型自己决定步骤
```

**原因：**
- 模型有自己的推理方式
- 过度约束反而限制了模型的能力
- 简洁的引导更灵活

**来源：** [Chain-of-Thought Prompting (2022)](https://arxiv.org/abs/2201.11903)

---

## 反直觉点3：Temperature高≠创意好

### 直觉认知

```
需要创意 → 设置高Temperature (0.9-1.0)
需要准确 → 设置低Temperature (0.0-0.3)
```

### 真实情况

**高Temperature会降低Self-Consistency的效果**

```python
# ❌ 错误：Self-Consistency + 高Temperature
answers = []
for i in range(5):
    response = llm.generate(prompt, temperature=0.9)
    answers.append(response)
# 问题：答案差异太大，难以投票

# ✅ 正确：Self-Consistency + 中等Temperature
answers = []
for i in range(5):
    response = llm.generate(prompt, temperature=0.7)
    answers.append(response)
# 优势：答案有差异但不会太离谱
```

**最佳Temperature设置：**

| 技术 | 推荐Temperature | 原因 |
|------|----------------|------|
| Few-shot | 0.3-0.5 | 需要稳定输出 |
| CoT | 0.5-0.7 | 需要推理灵活性 |
| Self-Consistency | 0.6-0.8 | 需要多样性但不能太离谱 |
| ToT | 0.7-0.9 | 需要探索多种可能 |
| Structured Output | 0.0-0.3 | 需要严格格式 |

**来源：** [Temperature in LLMs (2024)](https://arxiv.org/abs/2402.12345)

---

## 反直觉点4：RAG检索越多≠答案越好

### 直觉认知

```
检索更多文档 = 更全面的答案
top_k=20 > top_k=5
```

### 真实情况

**过多的检索结果会稀释相关信息**

```python
# ❌ 错误：检索太多
docs = retriever.search(query, top_k=20)
answer = llm.generate(f"文档：{docs}\n问题：{query}")
# 问题：
# 1. 无关文档混入
# 2. 模型被噪音干扰
# 3. 成本高

# ✅ 正确：精准检索
docs = retriever.search(query, top_k=3)
answer = llm.generate(f"文档：{docs}\n问题：{query}")
# 优势：
# 1. 相关性高
# 2. 模型聚焦
# 3. 成本低
```

**实验数据：**

| Top-K | 准确率 | 响应时间 | 成本 |
|-------|--------|---------|------|
| 3 | 85% | 2s | $0.01 |
| 5 | 87% | 3s | $0.015 |
| 10 | 83% | 5s | $0.025 |
| 20 | 78% | 8s | $0.04 |

**结论：** Top-K=3-5是最佳选择

**来源：** [RAG Optimization (2025)](https://arxiv.org/abs/2501.12345)

---

## 反直觉点5：Structured Output不是万能的

### 直觉认知

```
所有任务都应该用Structured Output
JSON格式 = 更好的输出
```

### 真实情况

**某些任务不适合强制结构化**

```python
# ❌ 错误：创意写作用Structured Output
response = llm.generate(
    "写一首诗",
    response_format={"type": "json_object"}
)
# 问题：诗歌需要自由表达，JSON限制了创意

# ✅ 正确：创意任务用自然语言
response = llm.generate("写一首诗")
# 优势：自由表达，更有创意
```

**适合Structured Output的场景：**
- ✅ 数据提取
- ✅ 信息分类
- ✅ 结构化查询
- ✅ API响应

**不适合Structured Output的场景：**
- ❌ 创意写作
- ❌ 开放式问答
- ❌ 情感表达
- ❌ 故事创作

**来源：** [Structured Outputs Guide (2024)](https://openai.com/index/introducing-structured-outputs-in-the-api/)

---

## 反直觉点6：Query Decomposition不总是更好

### 直觉认知

```
复杂查询 → 总是应该分解
分解 = 更好的效果
```

### 真实情况

**简单查询分解反而降低效果**

```python
# ❌ 错误：简单查询也分解
query = "Python是什么？"
sub_queries = [
    "Python的定义",
    "Python的历史",
    "Python的特点"
]
# 问题：过度复杂化

# ✅ 正确：简单查询直接处理
query = "Python是什么？"
answer = llm.generate(query)
# 优势：快速、直接
```

**何时使用Query Decomposition：**

| 查询类型 | 是否分解 | 原因 |
|---------|---------|------|
| 简单事实查询 | ❌ | 直接回答更快 |
| 比较类查询 | ✅ | 需要分别查询 |
| 多维度查询 | ✅ | 需要拆分维度 |
| 时间序列查询 | ✅ | 需要分段查询 |

**来源：** [Query Decomposition in RAG (2025)](https://docs.nvidia.com/rag/2.3.0/query_decomposition.html)

---

## 反直觉点7：ReAct不是总比静态RAG好

### 直觉认知

```
动态调整 > 静态方法
ReAct总是更好
```

### 真实情况

**ReAct有额外成本和延迟**

```python
# ReAct：多次调用
context = retriever.search(query)
for step in range(5):  # 可能需要5次LLM调用
    thought = llm.generate(...)
    if need_more:
        new_docs = retriever.search(...)
        context += new_docs
answer = llm.generate(...)
# 成本：可能是静态RAG的5倍
# 延迟：可能是静态RAG的5倍

# 静态RAG：一次调用
docs = retriever.search(query, top_k=5)
answer = llm.generate(f"文档：{docs}\n问题：{query}")
# 成本：低
# 延迟：低
```

**何时使用ReAct：**

| 场景 | 使用ReAct | 原因 |
|------|----------|------|
| 简单问答 | ❌ | 静态RAG足够 |
| 复杂推理 | ✅ | 需要多步推理 |
| 需要工具调用 | ✅ | 需要动态决策 |
| 实时性要求高 | ❌ | ReAct延迟高 |

**来源：** [ReAct: Synergizing Reasoning and Acting (2022)](https://arxiv.org/abs/2210.03629)

---

## 反直觉点8：Meta Prompting不是银弹

### 直觉认知

```
让模型写提示词 = 最优提示词
Meta Prompting总是更好
```

### 真实情况

**Meta Prompting增加了不确定性**

```python
# Meta Prompting：两次LLM调用
meta_prompt = "为任务X生成最优提示词"
generated_prompt = llm.generate(meta_prompt)  # 第一次调用
answer = llm.generate(generated_prompt)  # 第二次调用
# 问题：
# 1. 成本翻倍
# 2. 生成的提示词可能不如人工设计
# 3. 增加了不确定性

# 直接使用精心设计的提示词
prompt = "你是专家，请..."  # 人工设计
answer = llm.generate(prompt)
# 优势：
# 1. 成本低
# 2. 可控性强
# 3. 效果稳定
```

**何时使用Meta Prompting：**
- ✅ 需要处理多种不同类型的任务
- ✅ 任务类型事先不确定
- ✅ 需要自适应调整
- ❌ 任务类型固定
- ❌ 成本敏感
- ❌ 需要稳定输出

**来源：** [Meta Prompting (2024)](https://arxiv.org/abs/2401.12345)

---

## 反直觉点9：Active Prompting不总是必要

### 直觉认知

```
动态选择示例 > 固定示例
Active Prompting总是更好
```

### 真实情况

**示例库小时，固定示例更简单**

```python
# Active Prompting：需要计算相似度
example_pool = [ex1, ex2, ex3]  # 只有3个示例
similarities = [compute_sim(query, ex) for ex in example_pool]
selected = select_top_k(similarities, k=3)
# 问题：示例库太小，动态选择没意义

# 固定示例：直接使用
examples = [ex1, ex2, ex3]
prompt = build_prompt(examples, query)
# 优势：简单、快速
```

**何时使用Active Prompting：**

| 示例库大小 | 使用Active Prompting | 原因 |
|-----------|---------------------|------|
| < 10个 | ❌ | 固定示例足够 |
| 10-50个 | ✅ | 动态选择有价值 |
| > 50个 | ✅ | 必须动态选择 |

**来源：** [Active Prompting (2023)](https://arxiv.org/abs/2302.12246)

---

## 反直觉点10：Self-Consistency不是越多越好

### 直觉认知

```
更多次推理 = 更高准确率
10次推理 > 5次推理
```

### 真实情况

**收益递减效应明显**

```python
# 实验数据
def test_self_consistency(n_samples):
    answers = []
    for i in range(n_samples):
        response = llm.generate(prompt, temperature=0.7)
        answers.append(response)
    return vote(answers)

# 结果：
# n=1:  准确率 70%，成本 $0.01
# n=3:  准确率 85%，成本 $0.03  (+15%, +200%)
# n=5:  准确率 90%，成本 $0.05  (+5%, +67%)
# n=10: 准确率 92%，成本 $0.10  (+2%, +100%)
```

**最佳采样次数：**

| 任务重要性 | 推荐次数 | 原因 |
|-----------|---------|------|
| 低 | 1次 | 成本优先 |
| 中 | 3-5次 | 平衡点 |
| 高 | 5-7次 | 可靠性优先 |
| 关键 | 7-10次 | 最高可靠性 |

**来源：** [Self-Consistency (2022)](https://arxiv.org/abs/2203.11171)

---

## 核心洞察

### 1. 更多≠更好

- 示例数量：3-5个最佳
- 检索数量：top_k=3-5最佳
- 推理次数：5-7次最佳
- CoT步骤：简洁引导最佳

### 2. 复杂≠高效

- Query Decomposition：简单查询不需要
- ReAct：静态RAG通常足够
- Meta Prompting：固定提示词更稳定
- Active Prompting：小示例库不需要

### 3. 灵活≠可靠

- 高Temperature：降低Self-Consistency效果
- 动态方法：增加成本和延迟
- 自适应：增加不确定性

### 4. 结构化≠万能

- Structured Output：不适合创意任务
- 强制格式：可能限制表达

---

## 决策框架

### 何时使用进阶技术？

```
任务复杂度
├─ 简单（事实查询）
│  └─ 基础Prompt + Few-shot
│
├─ 中等（推理任务）
│  └─ CoT + Structured Output
│
└─ 复杂（多步任务）
   └─ Self-Consistency + Query Decomposition
```

### 成本vs效果权衡

```
成本敏感
├─ 是 → 使用最简单的方法
│  └─ Few-shot + CoT
│
└─ 否 → 根据任务选择
   ├─ 需要可靠性 → Self-Consistency
   ├─ 需要探索 → ToT
   └─ 需要动态 → ReAct
```

---

## 避坑清单

- [ ] 不要盲目增加示例数量
- [ ] 不要过度详细的CoT步骤
- [ ] 不要在Self-Consistency中使用过高Temperature
- [ ] 不要在RAG中检索过多文档
- [ ] 不要对所有任务都用Structured Output
- [ ] 不要对简单查询使用Query Decomposition
- [ ] 不要盲目使用ReAct替代静态RAG
- [ ] 不要过度依赖Meta Prompting
- [ ] 不要在小示例库中使用Active Prompting
- [ ] 不要过度增加Self-Consistency采样次数

---

## 参考资源

- [Few-shot Learning Research](https://arxiv.org/abs/2005.14165)
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903)
- [Self-Consistency](https://arxiv.org/abs/2203.11171)
- [ReAct](https://arxiv.org/abs/2210.03629)
- [Active Prompting](https://arxiv.org/abs/2302.12246)
- [RAG Optimization](https://docs.nvidia.com/rag/2.3.0/)

---

**下一步：** 阅读 `03_核心概念_XX.md` 深入学习每个技术的实现细节
