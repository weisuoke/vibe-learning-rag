# æœ€å°å¯ç”¨çŸ¥è¯†

> æŒæ¡ä»¥ä¸‹ 20% çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œå°±èƒ½è§£å†³ 80% çš„ç®¡é“æ“ä½œç¬¦ä½¿ç”¨åœºæ™¯

---

## æ ¸å¿ƒç†å¿µ

**æœ€å°å¯ç”¨çŸ¥è¯†** = èƒ½è®©ä½ ç«‹å³å¼€å§‹ä½¿ç”¨ç®¡é“æ“ä½œç¬¦çš„æœ€å°‘å¿…è¦çŸ¥è¯†

å­¦å®Œæœ¬æ–‡æ¡£ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç¼–å†™åŸºç¡€çš„ LCEL ç®¡é“
- âœ… ç†è§£æ•°æ®å¦‚ä½•åœ¨ç®¡é“ä¸­æµè½¬
- âœ… è°ƒè¯•ç®€å•çš„ç®¡é“é—®é¢˜
- âœ… ä¸ºåç»­æ·±å…¥å­¦ä¹ æ‰“ä¸‹åŸºç¡€

---

## 4.1 åŸºç¡€è¯­æ³•ï¼š`a | b | c`

### æ ¸å¿ƒæ¦‚å¿µ

**ç®¡é“æ“ä½œç¬¦ `|` å°†å¤šä¸ª Runnable ä¸²è”æˆä¸€ä¸ªæ‰§è¡Œé“¾**

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# ä¸‰ä¸ªç‹¬ç«‹çš„ Runnable
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# ç”¨ | ç»„åˆæˆä¸€ä¸ªé“¾
chain = prompt | llm | parser

# æ‰§è¡Œ
result = chain.invoke({"topic": "ç¨‹åºå‘˜"})
print(result)
```

### å…³é”®ç‚¹

1. **ä»å·¦åˆ°å³æ‰§è¡Œ**ï¼šæ•°æ®ä» prompt â†’ llm â†’ parser æµåŠ¨
2. **è‡ªåŠ¨ç±»å‹åŒ¹é…**ï¼šå‰ä¸€ä¸ªçš„è¾“å‡ºè‡ªåŠ¨æˆä¸ºåä¸€ä¸ªçš„è¾“å…¥
3. **è¿”å›æ–°çš„ Runnable**ï¼š`chain` æœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ª Runnableï¼Œå¯ä»¥ç»§ç»­ç»„åˆ

### å®é™…åº”ç”¨

**RAG é—®ç­”ç³»ç»Ÿ**ï¼š
```python
# æ£€ç´¢ â†’ æ ¼å¼åŒ– â†’ ç”Ÿæˆ â†’ è§£æ
rag_chain = retriever | context_formatter | prompt | llm | parser
answer = rag_chain.invoke("ä»€ä¹ˆæ˜¯ LCELï¼Ÿ")
```

---

## 4.2 ç†è§£ `__or__` æ–¹æ³•

### æ ¸å¿ƒæ¦‚å¿µ

**`a | b` å®é™…ä¸Šè°ƒç”¨äº† `a.__or__(b)`**

```python
# è¿™ä¸¤ç§å†™æ³•ç­‰ä»·
chain = prompt | llm | parser
chain = prompt.__or__(llm).__or__(parser)

# èƒŒåå‘ç”Ÿçš„äº‹æƒ…
# 1. prompt.__or__(llm) â†’ RunnableSequence(first=prompt, last=llm)
# 2. sequence.__or__(parser) â†’ RunnableSequence(first=sequence, last=parser)
```

### å…³é”®ç‚¹

1. **Python é­”æ³•æ–¹æ³•**ï¼š`__or__` æ˜¯ Python çš„è¿ç®—ç¬¦é‡è½½æœºåˆ¶
2. **åˆ›å»º RunnableSequence**ï¼šæ¯æ¬¡ `|` æ“ä½œéƒ½åˆ›å»ºä¸€ä¸ªæ–°çš„ RunnableSequence
3. **é“¾å¼è°ƒç”¨**ï¼šå¯ä»¥æ— é™ä¸²è”

### å®é™…åº”ç”¨

**è‡ªå®šä¹‰ Runnable**ï¼š
```python
from langchain_core.runnables import Runnable

class MyProcessor(Runnable):
    def invoke(self, input, config=None):
        return f"Processed: {input}"

    # ç»§æ‰¿äº† Runnable çš„ __or__ æ–¹æ³•ï¼Œè‡ªåŠ¨æ”¯æŒ | æ“ä½œ

# å¯ä»¥ç›´æ¥ç»„åˆ
chain = MyProcessor() | another_processor | yet_another
```

---

## 4.3 æ•°æ®æµè½¬è§„åˆ™

### æ ¸å¿ƒæ¦‚å¿µ

**å‰ä¸€ä¸ª Runnable çš„è¾“å‡ºå¿…é¡»åŒ¹é…åä¸€ä¸ª Runnable çš„è¾“å…¥ç±»å‹**

```python
# âœ… ç±»å‹åŒ¹é…
prompt = ChatPromptTemplate.from_template("...")  # è¾“å‡º: ChatPromptValue
llm = ChatOpenAI()                                # è¾“å…¥: ChatPromptValue
parser = StrOutputParser()                        # è¾“å…¥: AIMessage

chain = prompt | llm | parser  # æ­£å¸¸å·¥ä½œ

# âŒ ç±»å‹ä¸åŒ¹é…
chain = llm | prompt  # é”™è¯¯ï¼llm è¾“å‡º AIMessageï¼Œprompt éœ€è¦ dict
```

### å…³é”®ç‚¹

1. **è¾“å…¥è¾“å‡ºç±»å‹**ï¼šæ¯ä¸ª Runnable éƒ½æœ‰æ˜ç¡®çš„è¾“å…¥è¾“å‡ºç±»å‹
2. **è‡ªåŠ¨è½¬æ¢**ï¼šæŸäº›ç±»å‹ä¼šè‡ªåŠ¨è½¬æ¢ï¼ˆå¦‚ str â†’ ChatPromptValueï¼‰
3. **è°ƒè¯•æŠ€å·§**ï¼šé€æ­¥æµ‹è¯•æ¯ä¸ª Runnable çš„è¾“å…¥è¾“å‡º

### å®é™…åº”ç”¨

**è°ƒè¯•ç®¡é“**ï¼š
```python
# é€æ­¥æµ‹è¯•æ¯ä¸ªç¯èŠ‚
step1_output = prompt.invoke({"topic": "AI"})
print(f"Step 1 è¾“å‡º: {step1_output}")

step2_output = llm.invoke(step1_output)
print(f"Step 2 è¾“å‡º: {step2_output}")

step3_output = parser.invoke(step2_output)
print(f"Step 3 è¾“å‡º: {step3_output}")

# ç¡®è®¤ç±»å‹åŒ¹é…åå†ç»„åˆ
chain = prompt | llm | parser
```

---

## 4.4 ä½¿ç”¨ `.invoke()` æ‰§è¡Œ

### æ ¸å¿ƒæ¦‚å¿µ

**`.invoke()` æ˜¯æ‰§è¡Œ Runnable çš„æ ‡å‡†æ–¹æ³•**

```python
# åŸºç¡€ç”¨æ³•
chain = prompt | llm | parser
result = chain.invoke({"topic": "Python"})

# ç­‰ä»·äº
result = parser.invoke(
    llm.invoke(
        prompt.invoke({"topic": "Python"})
    )
)
```

### å…³é”®ç‚¹

1. **åŒæ­¥æ‰§è¡Œ**ï¼š`.invoke()` æ˜¯åŒæ­¥æ–¹æ³•ï¼Œä¼šé˜»å¡ç›´åˆ°å®Œæˆ
2. **ä¼ é€’é…ç½®**ï¼šå¯ä»¥ä¼ é€’ `config` å‚æ•°
3. **é”™è¯¯å¤„ç†**ï¼šå¼‚å¸¸ä¼šå‘ä¸Šä¼ æ’­

### å®é™…åº”ç”¨

**å¸¦é…ç½®çš„æ‰§è¡Œ**ï¼š
```python
# ä¼ é€’é…ç½®
result = chain.invoke(
    {"topic": "AI"},
    config={
        "callbacks": [my_callback],
        "tags": ["production"],
        "metadata": {"user_id": "123"}
    }
)

# å¼‚æ­¥æ‰§è¡Œ
result = await chain.ainvoke({"topic": "AI"})

# æµå¼æ‰§è¡Œ
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)
```

---

## 4.5 å¸¸è§æ¨¡å¼ï¼šPrompt â†’ LLM â†’ Parser

### æ ¸å¿ƒæ¦‚å¿µ

**æœ€å¸¸è§çš„ LCEL æ¨¡å¼æ˜¯ä¸‰æ®µå¼ï¼šæç¤º â†’ æ¨¡å‹ â†’ è§£æ**

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

# 1. å®šä¹‰æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"),
    ("user", "{question}")
])

# 2. å®šä¹‰æ¨¡å‹
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# 3. å®šä¹‰è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

# 4. ç»„åˆæˆé“¾
chain = prompt | llm | parser

# 5. æ‰§è¡Œ
answer = chain.invoke({"question": "ä»€ä¹ˆæ˜¯ LCELï¼Ÿ"})
print(answer)
```

### å…³é”®ç‚¹

1. **Prompt**ï¼šæ ¼å¼åŒ–ç”¨æˆ·è¾“å…¥
2. **LLM**ï¼šè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå“åº”
3. **Parser**ï¼šè§£ææ¨¡å‹è¾“å‡ºä¸ºæ‰€éœ€æ ¼å¼

### å®é™…åº”ç”¨

**ç»“æ„åŒ–è¾“å‡º**ï¼š
```python
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# å®šä¹‰è¾“å‡ºç»“æ„
class Joke(BaseModel):
    setup: str = Field(description="ç¬‘è¯çš„é“ºå«")
    punchline: str = Field(description="ç¬‘è¯çš„ç¬‘ç‚¹")

# åˆ›å»ºè§£æå™¨
parser = PydanticOutputParser(pydantic_object=Joke)

# åœ¨æç¤ºä¸­åŒ…å«æ ¼å¼è¯´æ˜
prompt = ChatPromptTemplate.from_template(
    "è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯\n\n{format_instructions}"
)

# ç»„åˆ
chain = prompt | llm | parser

# æ‰§è¡Œï¼ˆè¿”å› Pydantic å¯¹è±¡ï¼‰
joke = chain.invoke({
    "topic": "ç¨‹åºå‘˜",
    "format_instructions": parser.get_format_instructions()
})
print(f"é“ºå«: {joke.setup}")
print(f"ç¬‘ç‚¹: {joke.punchline}")
```

---

## è¿™äº›çŸ¥è¯†è¶³ä»¥åšä»€ä¹ˆï¼Ÿ

### âœ… èƒ½åšçš„äº‹æƒ…

1. **æ„å»ºåŸºç¡€å¯¹è¯é“¾**
   ```python
   chat_chain = prompt | llm | parser
   ```

2. **æ„å»ºç®€å• RAG ç³»ç»Ÿ**
   ```python
   rag_chain = retriever | context_formatter | prompt | llm | parser
   ```

3. **å®ç°ç»“æ„åŒ–è¾“å‡º**
   ```python
   structured_chain = prompt | llm | pydantic_parser
   ```

4. **è°ƒè¯•ç®¡é“é—®é¢˜**
   ```python
   # é€æ­¥æµ‹è¯•æ¯ä¸ªç¯èŠ‚
   step1 = prompt.invoke(input)
   step2 = llm.invoke(step1)
   step3 = parser.invoke(step2)
   ```

5. **ç»„åˆè‡ªå®šä¹‰é€»è¾‘**
   ```python
   chain = custom_processor | prompt | llm | parser
   ```

### âŒ æš‚æ—¶åšä¸äº†çš„äº‹æƒ…ï¼ˆéœ€è¦è¿›é˜¶çŸ¥è¯†ï¼‰

1. **å¹¶è¡Œæ‰§è¡Œå¤šä¸ªåˆ†æ”¯** â†’ éœ€è¦å­¦ä¹  RunnableParallel
2. **æ¡ä»¶è·¯ç”±** â†’ éœ€è¦å­¦ä¹  RunnableBranch
3. **å¤æ‚æ•°æ®è½¬æ¢** â†’ éœ€è¦å­¦ä¹  RunnablePassthrough å’Œ RunnableLambda
4. **æ€§èƒ½ä¼˜åŒ–** â†’ éœ€è¦å­¦ä¹ æµå¼æ‰§è¡Œã€æ‰¹å¤„ç†
5. **é«˜çº§ç±»å‹æ¨æ–­** â†’ éœ€è¦å­¦ä¹  TypedDict å’Œ Pydantic æ¨¡å‹

---

## å¿«é€Ÿå®æˆ˜ï¼š5åˆ†é’Ÿæ„å»ºä¸€ä¸ªç¿»è¯‘é“¾

### ç›®æ ‡

æ„å»ºä¸€ä¸ªä¸­è‹±äº’è¯‘çš„ LCEL é“¾

### ä»£ç 

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from dotenv import load_dotenv

load_dotenv()

# 1. å®šä¹‰æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_template(
    "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}"
)

# 2. å®šä¹‰æ¨¡å‹
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 3. å®šä¹‰è§£æå™¨
parser = StrOutputParser()

# 4. ç»„åˆæˆé“¾
translation_chain = prompt | llm | parser

# 5. æµ‹è¯•
chinese_text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
english_result = translation_chain.invoke({
    "text": chinese_text,
    "target_language": "è‹±è¯­"
})
print(f"ä¸­æ–‡: {chinese_text}")
print(f"è‹±æ–‡: {english_result}")

english_text = "Machine learning is a subset of AI"
chinese_result = translation_chain.invoke({
    "text": english_text,
    "target_language": "ä¸­æ–‡"
})
print(f"è‹±æ–‡: {english_text}")
print(f"ä¸­æ–‡: {chinese_result}")
```

### è¿è¡Œè¾“å‡º

```
ä¸­æ–‡: äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ
è‹±æ–‡: Artificial intelligence is changing the world

è‹±æ–‡: Machine learning is a subset of AI
ä¸­æ–‡: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†
```

### æ‰©å±•ç»ƒä¹ 

1. **æ·»åŠ è¯­è¨€æ£€æµ‹**ï¼šè‡ªåŠ¨æ£€æµ‹è¾“å…¥è¯­è¨€
2. **æ”¯æŒå¤šè¯­è¨€**ï¼šæ‰©å±•åˆ°æ³•è¯­ã€æ—¥è¯­ç­‰
3. **æ‰¹é‡ç¿»è¯‘**ï¼šä½¿ç”¨ `.batch()` æ–¹æ³•
4. **æµå¼è¾“å‡º**ï¼šä½¿ç”¨ `.stream()` æ–¹æ³•

---

## å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬æ–‡æ¡£å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] ä½¿ç”¨ `|` æ“ä½œç¬¦ç»„åˆ Runnable
- [ ] ç†è§£ `__or__` æ–¹æ³•çš„ä½œç”¨
- [ ] è§£é‡Šæ•°æ®å¦‚ä½•åœ¨ç®¡é“ä¸­æµè½¬
- [ ] ä½¿ç”¨ `.invoke()` æ‰§è¡Œé“¾
- [ ] æ„å»º Prompt â†’ LLM â†’ Parser ä¸‰æ®µå¼é“¾
- [ ] è°ƒè¯•ç®€å•çš„ç±»å‹ä¸åŒ¹é…é—®é¢˜
- [ ] å®ç°åŸºç¡€çš„ç¿»è¯‘ã€é—®ç­”ç­‰åº”ç”¨

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆ `a | b` ä¸æŠ¥é”™ï¼Œä½†æ‰§è¡Œæ—¶å¤±è´¥ï¼Ÿ

**A**: ç±»å‹ä¸åŒ¹é…åœ¨æ‰§è¡Œæ—¶æ‰ä¼šæš´éœ²ã€‚

```python
# å®šä¹‰æ—¶ä¸æŠ¥é”™
chain = llm | prompt  # çœ‹èµ·æ¥æ²¡é—®é¢˜

# æ‰§è¡Œæ—¶æŠ¥é”™
result = chain.invoke({"question": "..."})  # ğŸ’¥ ç±»å‹ä¸åŒ¹é…
```

**è§£å†³æ–¹æ³•**ï¼šé€æ­¥æµ‹è¯•æ¯ä¸ª Runnable çš„è¾“å…¥è¾“å‡ºã€‚

---

### Q2: å¦‚ä½•æŸ¥çœ‹ä¸­é—´ç»“æœï¼Ÿ

**A**: é€æ­¥è°ƒç”¨ `.invoke()`ã€‚

```python
# æ–¹æ³•1ï¼šé€æ­¥è°ƒç”¨
step1 = prompt.invoke(input)
print(f"Step 1: {step1}")

step2 = llm.invoke(step1)
print(f"Step 2: {step2}")

step3 = parser.invoke(step2)
print(f"Step 3: {step3}")

# æ–¹æ³•2ï¼šä½¿ç”¨å›è°ƒï¼ˆè¿›é˜¶ï¼‰
from langchain.callbacks import StdOutCallbackHandler

chain.invoke(input, config={"callbacks": [StdOutCallbackHandler()]})
```

---

### Q3: å¯ä»¥é‡å¤ä½¿ç”¨åŒä¸€ä¸ª Runnable å—ï¼Ÿ

**A**: å¯ä»¥ï¼Runnable æ˜¯æ— çŠ¶æ€çš„ã€‚

```python
# åŒä¸€ä¸ª llm ç”¨äºå¤šä¸ªé“¾
llm = ChatOpenAI(model="gpt-4o-mini")

chain1 = prompt1 | llm | parser
chain2 = prompt2 | llm | parser
chain3 = prompt3 | llm | parser

# å®Œå…¨æ²¡é—®é¢˜
```

---

### Q4: å¦‚ä½•å¤„ç†é”™è¯¯ï¼Ÿ

**A**: ä½¿ç”¨ try-except æ•è·å¼‚å¸¸ã€‚

```python
try:
    result = chain.invoke(input)
except Exception as e:
    print(f"æ‰§è¡Œå¤±è´¥: {e}")
    # å¤„ç†é”™è¯¯
```

---

## ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

### å¦‚æœä½ æƒ³æ·±å…¥ç†è§£åŸç†

â†’ é˜…è¯» `03_æ ¸å¿ƒæ¦‚å¿µ_01_ç®¡é“æ“ä½œç¬¦å®ç°åŸç†.md`

### å¦‚æœä½ æƒ³å­¦ä¹ æ›´å¤šç»„åˆæ¨¡å¼

â†’ é˜…è¯» `03_æ ¸å¿ƒæ¦‚å¿µ_02_æ•°æ®æµè½¬æœºåˆ¶.md`

### å¦‚æœä½ æƒ³å®ç°ç±»å‹å®‰å…¨

â†’ é˜…è¯» `03_æ ¸å¿ƒæ¦‚å¿µ_03_ç±»å‹æ¨æ–­.md`

### å¦‚æœä½ æƒ³çœ‹æ›´å¤šå®æˆ˜ä»£ç 

â†’ é˜…è¯» `07_å®æˆ˜ä»£ç _01_åŸºç¡€ç®¡é“ç»„åˆ.md`

---

## æ€»ç»“

æŒæ¡è¿™ 5 ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œä½ å°±èƒ½å¼€å§‹ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ï¼š

1. **åŸºç¡€è¯­æ³•**ï¼š`a | b | c`
2. **å®ç°åŸç†**ï¼š`__or__` æ–¹æ³•
3. **æ•°æ®æµè½¬**ï¼šç±»å‹åŒ¹é…è§„åˆ™
4. **æ‰§è¡Œæ–¹æ³•**ï¼š`.invoke()`
5. **å¸¸è§æ¨¡å¼**ï¼šPrompt â†’ LLM â†’ Parser

è¿™äº›çŸ¥è¯†è¶³ä»¥è®©ä½ æ„å»º 80% çš„å¸¸è§åº”ç”¨åœºæ™¯ã€‚

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-19
**ç»´æŠ¤è€…**: Claude Code

**ä¸‹ä¸€æ­¥**: é˜…è¯» `05_åŒé‡ç±»æ¯”.md` é€šè¿‡ç±»æ¯”åŠ æ·±ç†è§£
