# 核心概念03：PQ压缩

## 一句话定义

**PQ（Product Quantization，乘积量化）是一种向量压缩技术，通过将高维向量分段量化为短码，实现8-32倍的内存节省，同时保持85-95%的检索精度，是大规模向量存储的关键优化手段。**

---

## 详细原理讲解

### 1. 什么是PQ压缩？

Product Quantization（乘积量化）是一种有损压缩技术，由Hervé Jégou等人在2011年提出。

**核心思想**：
- 将高维向量分成多个子向量
- 每个子向量独立量化（映射到码本）
- 用短码（通常8位）表示每个子向量
- 查询时使用码本快速计算距离

**类比理解**：
```
地图缩略图：
- 完整地图：高分辨率，文件大（原始向量）
- 缩略图：低分辨率，文件小（PQ压缩）
- 损失细节，但足够识别位置
- 需要时可以查看完整地图（精排）

PQ压缩：
- 原始向量：768维，3072字节
- PQ压缩：64段，64字节（压缩48倍）
- 损失精度，但足够粗筛
- 精排时使用原始向量
```

---

### 2. PQ的数学原理

#### 2.1 向量分段

**步骤1：将向量分成M段**

```python
# 原始向量：768维
x = [x_1, x_2, ..., x_768]

# 分成M=64段，每段12维
x = [u_1, u_2, ..., u_64]
其中 u_i = [x_{12i-11}, x_{12i-10}, ..., x_{12i}]
```

**数学表达**：
```
x ∈ R^D  →  [u_1, u_2, ..., u_M]
其中 u_i ∈ R^(D/M)
```

#### 2.2 子向量量化

**步骤2：为每段构建码本**

```python
# 对每段运行K-Means聚类（通常K=256）
# 得到M个码本，每个码本有K个中心点

codebook_1 = [c_1^1, c_1^2, ..., c_1^256]  # 第1段的码本
codebook_2 = [c_2^1, c_2^2, ..., c_2^256]  # 第2段的码本
...
codebook_64 = [c_64^1, c_64^2, ..., c_64^256]  # 第64段的码本
```

**步骤3：量化每个子向量**

```python
# 将每个子向量映射到最近的码本中心
for i in range(M):
    # 找到u_i最近的码本中心
    code_i = argmin_k ||u_i - c_i^k||²
    # 用8位整数（0-255）表示
```

**压缩结果**：
```
原始向量：768维 × 4字节 = 3072字节
PQ压缩：64段 × 1字节 = 64字节
压缩比：48倍
```

#### 2.3 距离计算

**查询时的距离计算**：

```python
# 预计算：查询向量的每段到所有码本中心的距离
# 这是PQ的关键优化

# 1. 将查询向量分段
q = [q_1, q_2, ..., q_M]

# 2. 预计算距离表（M × K）
distance_table = []
for i in range(M):
    distances_i = []
    for k in range(K):
        distances_i.append(||q_i - c_i^k||²)
    distance_table.append(distances_i)

# 3. 计算查询向量到数据库向量的距离
# 对于数据库中的向量x（已编码为[code_1, code_2, ..., code_M]）
distance(q, x) ≈ Σ(i=1 to M) distance_table[i][code_i]
```

**时间复杂度**：
```
预计算：O(M × K × D/M) = O(K × D)
每个向量距离计算：O(M)  ← 非常快！

对比原始距离计算：O(D)
当M << D时，PQ显著更快
```

---

### 3. PQ的实现

#### 3.1 训练阶段

```python
import numpy as np
from sklearn.cluster import KMeans

class ProductQuantizer:
    """乘积量化器"""

    def __init__(self, dimension, n_segments=64, n_clusters=256):
        """
        dimension: 向量维度
        n_segments: 分段数（M）
        n_clusters: 每段的聚类数（K，通常256）
        """
        self.dimension = dimension
        self.n_segments = n_segments
        self.n_clusters = n_clusters
        self.segment_dim = dimension // n_segments

        # M个码本，每个码本有K个中心点
        self.codebooks = []

    def train(self, training_vectors):
        """训练PQ码本"""
        print(f"Training PQ: {self.n_segments} segments, {self.n_clusters} clusters per segment")

        # 对每一段训练码本
        for i in range(self.n_segments):
            # 提取第i段的所有子向量
            start = i * self.segment_dim
            end = (i + 1) * self.segment_dim
            sub_vectors = training_vectors[:, start:end]

            # K-Means聚类
            kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)
            kmeans.fit(sub_vectors)

            # 保存码本
            self.codebooks.append(kmeans.cluster_centers_)

        print("PQ training complete.")

    def encode(self, vectors):
        """编码向量为PQ码"""
        n_vectors = len(vectors)
        codes = np.zeros((n_vectors, self.n_segments), dtype=np.uint8)

        for i in range(self.n_segments):
            # 提取第i段
            start = i * self.segment_dim
            end = (i + 1) * self.segment_dim
            sub_vectors = vectors[:, start:end]

            # 找到最近的码本中心
            distances = np.linalg.norm(
                sub_vectors[:, np.newaxis, :] - self.codebooks[i][np.newaxis, :, :],
                axis=2
            )
            codes[:, i] = np.argmin(distances, axis=1)

        return codes

    def decode(self, codes):
        """解码PQ码为近似向量"""
        n_vectors = len(codes)
        vectors = np.zeros((n_vectors, self.dimension))

        for i in range(self.n_segments):
            start = i * self.segment_dim
            end = (i + 1) * self.segment_dim
            vectors[:, start:end] = self.codebooks[i][codes[:, i]]

        return vectors

    def compute_distance_table(self, query):
        """预计算查询向量到所有码本中心的距离"""
        distance_table = np.zeros((self.n_segments, self.n_clusters))

        for i in range(self.n_segments):
            start = i * self.segment_dim
            end = (i + 1) * self.segment_dim
            query_segment = query[start:end]

            # 计算到所有码本中心的距离
            distances = np.linalg.norm(
                query_segment[np.newaxis, :] - self.codebooks[i],
                axis=1
            )
            distance_table[i] = distances ** 2  # 平方距离

        return distance_table

    def compute_distances(self, query, codes):
        """计算查询向量到编码向量的距离"""
        distance_table = self.compute_distance_table(query)

        # 查表计算距离
        distances = np.sum(
            distance_table[np.arange(self.n_segments), codes.T],
            axis=0
        )

        return np.sqrt(distances)


# 使用示例
if __name__ == "__main__":
    # 1. 生成模拟数据
    dimension = 768
    n_vectors = 100000
    vectors = np.random.randn(n_vectors, dimension).astype('float32')

    # 2. 训练PQ
    pq = ProductQuantizer(dimension, n_segments=64, n_clusters=256)
    pq.train(vectors[:10000])  # 用10%数据训练

    # 3. 编码所有向量
    codes = pq.encode(vectors)
    print(f"Original size: {vectors.nbytes / 1024 / 1024:.2f} MB")
    print(f"Compressed size: {codes.nbytes / 1024 / 1024:.2f} MB")
    print(f"Compression ratio: {vectors.nbytes / codes.nbytes:.1f}x")

    # 4. 查询
    query = np.random.randn(dimension).astype('float32')
    distances = pq.compute_distances(query, codes)

    # 5. 返回Top 10
    top_k = 10
    top_indices = np.argpartition(distances, top_k)[:top_k]
    top_indices = top_indices[np.argsort(distances[top_indices])]

    print(f"\nTop {top_k} nearest neighbors:")
    for idx in top_indices:
        print(f"ID: {idx}, Distance: {distances[idx]:.4f}")
```

---

### 4. PQ的变种

#### 4.1 OPQ（Optimized Product Quantization）

**改进**：在量化前对向量进行旋转，使得分段更加独立。

```python
class OptimizedPQ(ProductQuantizer):
    """优化的乘积量化"""

    def __init__(self, dimension, n_segments=64, n_clusters=256):
        super().__init__(dimension, n_segments, n_clusters)
        self.rotation_matrix = None

    def train(self, training_vectors):
        """训练OPQ：学习旋转矩阵 + 码本"""
        # 1. 初始化旋转矩阵为单位矩阵
        self.rotation_matrix = np.eye(self.dimension)

        # 2. 迭代优化旋转矩阵和码本
        for iteration in range(10):
            # 旋转向量
            rotated_vectors = training_vectors @ self.rotation_matrix

            # 训练PQ码本
            super().train(rotated_vectors)

            # 更新旋转矩阵（使用PCA或其他方法）
            # 这里简化处理，实际需要更复杂的优化
            pass

    def encode(self, vectors):
        """编码前先旋转"""
        rotated_vectors = vectors @ self.rotation_matrix
        return super().encode(rotated_vectors)
```

#### 4.2 Additive Quantization（AQ）

**改进**：用多个码本的和表示向量，而非分段。

```python
# 向量表示
x ≈ c_1 + c_2 + ... + c_M

# 而非PQ的
x ≈ [c_1, c_2, ..., c_M]
```

**优势**：更高的精度，但计算更复杂。

---

### 5. PQ的性能分析

#### 5.1 压缩比 vs 精度

**实验数据（768维向量）**：

| 配置 | 压缩比 | 召回率@10 | 查询延迟 |
|------|--------|-----------|---------|
| 无压缩 | 1x | 100% | 100ms |
| PQ32 | 24x | 92% | 15ms |
| PQ64 | 48x | 88% | 8ms |
| PQ128 | 96x | 82% | 5ms |

**结论**：PQ64是最佳平衡点。

#### 5.2 内存占用对比

```
1000万向量，768维：

无压缩：
10M × 768 × 4字节 = 30.7GB

PQ64：
- 码本：64 × 256 × 12 × 4字节 = 768KB
- 编码：10M × 64 × 1字节 = 640MB
- 总计：640MB（压缩48倍）

PQ32：
- 码本：32 × 256 × 24 × 4字节 = 768KB
- 编码：10M × 32 × 1字节 = 320MB
- 总计：320MB（压缩96倍）
```

---

### 6. PQ在RAG中的应用

#### 6.1 IVF+PQ组合

**最常见的配置**：

```python
import faiss

# 创建IVF+PQ索引
dimension = 768
n_clusters = 1024  # IVF的cluster数
pq_segments = 64   # PQ的段数
pq_bits = 8        # 每段8位（256个码本）

# 1. 创建量化器（用于IVF粗筛）
quantizer = faiss.IndexFlatL2(dimension)

# 2. 创建IVF+PQ索引
index = faiss.IndexIVFPQ(
    quantizer,
    dimension,
    n_clusters,
    pq_segments,
    pq_bits
)

# 3. 训练
training_data = np.random.randn(100000, dimension).astype('float32')
index.train(training_data)

# 4. 添加向量
vectors = np.random.randn(10000000, dimension).astype('float32')
index.add(vectors)

# 5. 查询
query = np.random.randn(1, dimension).astype('float32')
index.nprobe = 10  # 搜索10个cluster
distances, indices = index.search(query, k=10)
```

#### 6.2 两阶段检索

**策略**：PQ粗筛 + 原始向量精排

```python
class TwoStageRetrieval:
    """两阶段检索：PQ粗筛 + 精排"""

    def __init__(self, dimension, n_segments=64):
        self.dimension = dimension
        self.pq = ProductQuantizer(dimension, n_segments)
        self.original_vectors = None
        self.pq_codes = None

    def add(self, vectors):
        """添加向量"""
        # 保存原始向量（用于精排）
        self.original_vectors = vectors

        # PQ编码（用于粗筛）
        self.pq_codes = self.pq.encode(vectors)

    def search(self, query, k=10, rerank_k=100):
        """两阶段检索"""
        # 阶段1：PQ粗筛Top rerank_k
        pq_distances = self.pq.compute_distances(query, self.pq_codes)
        top_candidates = np.argpartition(pq_distances, rerank_k)[:rerank_k]

        # 阶段2：原始向量精排Top k
        refined_distances = np.linalg.norm(
            self.original_vectors[top_candidates] - query,
            axis=1
        )
        top_k_indices = np.argpartition(refined_distances, k)[:k]
        top_k_indices = top_k_indices[np.argsort(refined_distances[top_k_indices])]

        final_indices = top_candidates[top_k_indices]
        final_distances = refined_distances[top_k_indices]

        return final_indices, final_distances
```

---

### 7. 2025-2026最新实践

#### 7.1 学习型量化

**趋势**：用神经网络学习更优的量化方式

```python
import torch
import torch.nn as nn

class LearnedPQ(nn.Module):
    """学习型PQ"""

    def __init__(self, dimension, n_segments, n_clusters):
        super().__init__()
        self.dimension = dimension
        self.n_segments = n_segments
        self.n_clusters = n_clusters
        self.segment_dim = dimension // n_segments

        # 可学习的码本
        self.codebooks = nn.Parameter(
            torch.randn(n_segments, n_clusters, self.segment_dim)
        )

    def forward(self, x):
        """前向传播：编码 + 解码"""
        batch_size = x.size(0)
        x = x.view(batch_size, self.n_segments, self.segment_dim)

        # Soft assignment（训练时）
        distances = torch.cdist(x, self.codebooks)  # (B, M, K)
        soft_codes = torch.softmax(-distances, dim=-1)  # (B, M, K)

        # 重构
        reconstructed = torch.einsum('bmk,mkd->bmd', soft_codes, self.codebooks)
        reconstructed = reconstructed.view(batch_size, self.dimension)

        return reconstructed

    def encode(self, x):
        """编码（推理时）"""
        batch_size = x.size(0)
        x = x.view(batch_size, self.n_segments, self.segment_dim)

        # Hard assignment
        distances = torch.cdist(x, self.codebooks)
        codes = torch.argmin(distances, dim=-1)  # (B, M)

        return codes
```

#### 7.2 混合精度量化

**策略**：重要维度用高精度，次要维度用低精度

```python
class AdaptivePQ:
    """自适应PQ：不同段使用不同精度"""

    def __init__(self, dimension, n_segments=64):
        self.dimension = dimension
        self.n_segments = n_segments

        # 根据重要性分配不同的码本大小
        self.n_clusters = [512, 512, 256, 256, 128, 128, ...]  # 前面段用更多码本

    def train(self, training_vectors):
        """训练自适应PQ"""
        # 1. 计算每段的方差（重要性）
        variances = []
        for i in range(self.n_segments):
            start = i * (self.dimension // self.n_segments)
            end = (i + 1) * (self.dimension // self.n_segments)
            variance = np.var(training_vectors[:, start:end])
            variances.append(variance)

        # 2. 根据方差分配码本大小
        # 方差大的段用更多码本
        # ...
```

---

### 8. 常见问题与解决方案

#### 8.1 精度损失过大

**问题**：PQ64召回率只有80%，目标90%+

**解决方案**：
```python
# 方案1：减少段数（增加每段维度）
pq_segments = 32  # 从64减少到32

# 方案2：增加码本大小（需要更多内存）
pq_bits = 10  # 从8位增加到10位（1024个码本）

# 方案3：使用OPQ
# 旋转向量使得分段更独立

# 方案4：两阶段检索
# PQ粗筛Top 100，原始向量精排Top 10
```

#### 8.2 训练时间过长

**问题**：1000万向量，训练需要数小时

**解决方案**：
```python
# 方案1：采样训练
training_sample = vectors[::10]  # 只用10%数据

# 方案2：并行训练
from joblib import Parallel, delayed

def train_segment(i, sub_vectors):
    kmeans = KMeans(n_clusters=256)
    kmeans.fit(sub_vectors)
    return kmeans.cluster_centers_

codebooks = Parallel(n_jobs=-1)(
    delayed(train_segment)(i, vectors[:, start:end])
    for i, (start, end) in enumerate(segment_ranges)
)

# 方案3：使用MiniBatchKMeans
from sklearn.cluster import MiniBatchKMeans
kmeans = MiniBatchKMeans(n_clusters=256, batch_size=10000)
```

---

## 总结

**PQ的核心优势**：
1. **极致压缩**：8-48倍内存节省
2. **快速查询**：距离计算O(M)，远小于O(D)
3. **可扩展**：支持亿级向量
4. **灵活配置**：通过M和K平衡精度和内存

**适用场景**：
- 超大规模数据（>1000万向量）
- 内存严重受限
- 可接受5-15%精度损失
- 配合IVF使用

**2026年最佳实践**：
- IVF+PQ64：标准配置
- 两阶段检索：PQ粗筛 + 精排
- OPQ：提升5-10%精度
- 学习型量化：神经网络优化码本

---

## 引用来源

1. **PQ论文**：https://hal.inria.fr/inria-00514462v2/document
2. **FAISS PQ实现**：https://github.com/facebookresearch/faiss/wiki/Faiss-indexes
3. **PQ详解**：https://thedataguy.pro/blog/2025/12/...
4. **OPQ论文**：https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ge_Optimized_Product_Quantization_2013_CVPR_paper.pdf
5. **向量压缩对比**：https://kawaldeepsingh.medium.com/vector-databases-in-2026-...

---

**记住**：PQ是大规模向量存储的关键技术，通过牺牲少量精度换取巨大的内存节省，是生产环境的必备优化手段。
