# 化骨绵掌

> 10个2分钟知识卡片，系统掌握生成器与迭代器

---

## 卡片1：直觉理解

**一句话：** 迭代器是"记住位置的书签"，生成器是"可暂停的函数"

**举例：**
```python
# 迭代器 = 书签
class BookReader:
    def __init__(self, pages):
        self.pages = pages
        self.bookmark = 0  # 记住位置

    def __next__(self):
        page = self.pages[self.bookmark]
        self.bookmark += 1  # 移动书签
        return page

# 生成器 = 暂停的函数
def read_book(pages):
    for page in pages:
        yield page  # 暂停，返回当前页
```

**应用：** 处理大文件时，迭代器/生成器记住读到哪一行，不需要一次性加载全部

---

## 卡片2：形式化定义

**一句话：** 迭代器实现 `__iter__` 和 `__next__`，生成器用 `yield` 自动实现

**精确表述：**
- **迭代器协议**：`__iter__()` 返回自己，`__next__()` 返回下一个值
- **生成器函数**：包含 `yield` 的函数，调用时返回生成器对象
- **生成器表达式**：`(expression for item in iterable)`

**关系：**
```
可迭代对象 (Iterable)
    ├─ 实现 __iter__
    └─ 返回迭代器
         ↓
迭代器 (Iterator)
    ├─ 实现 __iter__ 和 __next__
    └─ 生成器是特殊的迭代器
         ↓
生成器 (Generator)
    ├─ 用 yield 自动实现协议
    └─ 更简洁的迭代器
```

**应用：** 在 FastAPI 中，`StreamingResponse` 接受生成器作为参数实现流式响应

---

## 卡片3：内存效率

**一句话：** 列表存储全部数据（O(n)），生成器只存储生成逻辑（O(1)）

**量化对比：**
```python
import sys

# 列表：100万个整数
lst = [x for x in range(1_000_000)]
print(sys.getsizeof(lst))  # ~8MB

# 生成器：100万个整数
gen = (x for x in range(1_000_000))
print(sys.getsizeof(gen))  # ~128字节

# 差异：约 60000 倍！
```

**本质原因：**
- 列表：存储结果（每个元素占内存）
- 生成器：存储方法（如何生成结果）

**应用：** 处理 10GB 日志文件，用生成器逐行读取，内存占用只有几 KB

---

## 卡片4：惰性求值

**一句话：** 生成器不立即计算，只在需要时才计算（Lazy Evaluation）

**对比：**
```python
# 列表推导式：立即计算全部
squares = [x**2 for x in range(1_000_000)]  # 立即计算 100 万次

# 生成器表达式：延迟计算
squares = (x**2 for x in range(1_000_000))  # 不计算任何值
first = next(squares)  # 现在才计算第一个
```

**时间对比：**
- 列表：创建耗时 0.1s，但只需要前 10 个也要计算全部
- 生成器：创建耗时 0.000001s，只计算需要的

**应用：** 数据管道中，每个步骤都是生成器，数据流过时才计算，不产生中间结果

---

## 卡片5：yield 的执行机制

**一句话：** `yield` 是暂停点，保存状态并返回值，下次从暂停处继续

**执行流程：**
```python
def demo():
    print("步骤1")
    yield "值1"  # 暂停点1
    print("步骤2")
    yield "值2"  # 暂停点2
    print("步骤3")

gen = demo()  # 不执行任何代码
next(gen)     # 执行到 yield "值1"，返回 "值1"
next(gen)     # 从 yield 后继续，执行到 yield "值2"
next(gen)     # 从 yield 后继续，函数结束，抛出 StopIteration
```

**状态保存：**
- 局部变量的值
- 执行位置（哪个 yield）
- 函数的栈帧

**应用：** AI 流式响应，每个 token 生成后 yield，保存对话状态，下次继续生成

---

## 卡片6：生成器 vs 列表

**一句话：** 生成器用时间换空间，列表用空间换时间

**选择矩阵：**

| 场景 | 推荐 | 原因 |
|------|------|------|
| 数据量大（>100MB） | 生成器 | 内存高效 |
| 只遍历一次 | 生成器 | 按需计算 |
| 需要多次遍历 | 列表 | 避免重复计算 |
| 需要随机访问 | 列表 | 生成器不支持索引 |
| 需要知道长度 | 列表 | 生成器不支持 len() |
| 小数据集（<1000项） | 列表 | 简单方便 |

**权衡：**
- 生成器：省内存，但不能重复使用
- 列表：占内存，但可以多次访问

**应用：** 日志分析用生成器（大文件），配置数据用列表（小数据，需要多次访问）

---

## 卡片7：生成器表达式

**一句话：** 生成器表达式是列表推导式的惰性版本，只改括号类型

**语法对比：**
```python
# 列表推导式：[]
squares_list = [x**2 for x in range(10)]

# 生成器表达式：()
squares_gen = (x**2 for x in range(10))

# 唯一区别：括号类型
```

**使用场景：**
```python
# 与内置函数结合（括号可省略）
total = sum(x**2 for x in range(100))
maximum = max(x**2 for x in range(100))
has_even = any(x % 2 == 0 for x in range(10))

# 链式处理
result = (x * 2 for x in (x for x in range(10) if x % 2 == 0))
```

**应用：** 数据清洗管道，每个步骤用生成器表达式，内存高效且代码简洁

---

## 卡片8：迭代器协议

**一句话：** `__iter__` 返回迭代器，`__next__` 返回下一个值，结束时抛出 `StopIteration`

**完整实现：**
```python
class Counter:
    def __init__(self, max_count):
        self.max_count = max_count
        self.current = 0

    def __iter__(self):
        return self  # 返回迭代器对象（自己）

    def __next__(self):
        if self.current >= self.max_count:
            raise StopIteration  # 结束信号
        self.current += 1
        return self.current

# for 循环的本质
for num in Counter(5):
    print(num)

# 等价于
it = iter(Counter(5))  # 调用 __iter__
while True:
    try:
        num = next(it)  # 调用 __next__
        print(num)
    except StopIteration:
        break
```

**应用：** 自定义数据结构（如树、图）的遍历，实现迭代器协议即可用 for 循环

---

## 卡片9：流式处理模式

**一句话：** 生成器构建数据管道，每个步骤按需处理，不产生中间结果

**管道模式：**
```python
# 步骤1：读取数据
def read_data(filename):
    with open(filename) as f:
        for line in f:
            yield line.strip()

# 步骤2：过滤数据
def filter_errors(lines):
    for line in lines:
        if 'ERROR' in line:
            yield line

# 步骤3：解析数据
def parse_lines(lines):
    for line in lines:
        yield json.loads(line)

# 构建管道
pipeline = parse_lines(filter_errors(read_data('app.log')))

# 使用：按需处理
for record in pipeline:
    process(record)
```

**优势：**
- 内存高效：不存储中间结果
- 代码清晰：每个步骤职责单一
- 灵活组合：可以自由组合步骤

**应用：** ETL 数据处理、日志分析、数据清洗

---

## 卡片10：实战应用总结

**一句话：** 生成器是 Python 处理大数据和流式数据的核心工具

**核心应用场景：**

1. **大文件处理**
   ```python
   def read_large_file(filename):
       with open(filename) as f:
           for line in f:
               yield line.strip()
   ```

2. **AI 流式响应**
   ```python
   def stream_ai_response(prompt):
       for chunk in openai_stream(prompt):
           yield chunk
   ```

3. **数据管道**
   ```python
   pipeline = transform(filter(read_data()))
   ```

4. **无限序列**
   ```python
   def fibonacci():
       a, b = 0, 1
       while True:
           yield a
           a, b = b, a + b
   ```

5. **批量处理**
   ```python
   def batch(data, size):
       batch = []
       for item in data:
           batch.append(item)
           if len(batch) >= size:
               yield batch
               batch = []
   ```

**记忆口诀：**
- 大文件 → 生成器（内存高效）
- 流式输出 → 生成器（实时反馈）
- 数据管道 → 生成器（链式处理）
- 无限序列 → 生成器（唯一选择）
- 小数据 → 列表（简单方便）

**最佳实践：**
- ✅ 简单场景用生成器表达式
- ✅ 复杂逻辑用生成器函数
- ✅ 需要多次访问用列表
- ✅ 与 itertools 结合使用
- ❌ 不要假设生成器可以重复使用
- ❌ 不要用 list() 转换无限生成器

---

## 学习检查清单

完成以下检查，确保掌握生成器与迭代器：

### 基础理解
- [ ] 能解释迭代器和生成器的区别
- [ ] 理解 `__iter__` 和 `__next__` 的作用
- [ ] 理解 `yield` 的执行机制
- [ ] 知道生成器是一次性的

### 实践能力
- [ ] 能手写一个简单的迭代器类
- [ ] 能用 `yield` 编写生成器函数
- [ ] 能用生成器表达式简化代码
- [ ] 能构建数据处理管道

### 应用场景
- [ ] 能用生成器处理大文件
- [ ] 能实现 AI 流式响应
- [ ] 能构建 ETL 数据管道
- [ ] 能生成无限序列

### 进阶理解
- [ ] 理解惰性求值的优势
- [ ] 知道何时用生成器，何时用列表
- [ ] 能解释内存效率的原因
- [ ] 能避免常见陷阱

---

## 快速参考卡

### 创建生成器的三种方式

```python
# 1. 生成器函数
def gen_func():
    yield 1
    yield 2

# 2. 生成器表达式
gen_exp = (x for x in range(10))

# 3. 迭代器类（手动实现协议）
class GenClass:
    def __iter__(self):
        return self
    def __next__(self):
        ...
```

### 常用操作

```python
# 获取下一个值
next(gen)

# 遍历生成器
for item in gen:
    print(item)

# 转换为列表（小心无限生成器）
list(gen)

# 取前 N 个
import itertools
first_n = list(itertools.islice(gen, n))

# 链接多个生成器
combined = itertools.chain(gen1, gen2)
```

### 常见模式

```python
# 过滤
filtered = (x for x in data if condition(x))

# 转换
transformed = (func(x) for x in data)

# 管道
result = step3(step2(step1(data)))

# 批量处理
def batch(data, size):
    batch = []
    for item in data:
        batch.append(item)
        if len(batch) >= size:
            yield batch
            batch = []
```

---

## 延伸学习

### 相关主题

1. **异步生成器**
   - `async def` + `yield`
   - `async for` 循环
   - 在 Phase1-02 异步编程中学习

2. **itertools 模块**
   - `count()`, `cycle()`, `repeat()`
   - `islice()`, `chain()`, `zip_longest()`
   - 强大的迭代器工具

3. **协程**
   - 基于生成器的协程（旧式）
   - `async/await` 协程（新式）
   - `send()`, `throw()`, `close()` 方法

4. **函数式编程**
   - `map()`, `filter()`, `reduce()`
   - 与生成器结合使用
   - 惰性求值思想

### 推荐资源

- Python 官方文档：Iterators & Generators
- PEP 255: Simple Generators
- PEP 342: Coroutines via Enhanced Generators
- Fluent Python (Chapter 14: Iterables, Iterators, and Generators)

---

**版本：** v1.0
**最后更新：** 2026-02-11
