# 实战代码2：NLI验证实现

> **使用NLI模型实现声明级验证，判断上下文是否蕴含生成声明**

---

## 场景描述

构建基于NLI（自然语言推理）的幻觉检测系统，使用DeBERTa-NLI模型验证生成内容中的每个声明是否被检索上下文逻辑蕴含。

**适用场景：**
- 生产环境大规模部署
- 需要快速响应（<100ms）
- 对准确率有较高要求

---

## 环境准备

### 安装依赖

```bash
# 安装Transformers和相关依赖
pip install transformers torch sentence-transformers python-dotenv

# 可选：安装加速库
pip install accelerate
```

### 模型下载

```python
# 首次运行会自动下载模型（约400MB）
from transformers import pipeline

nli_model = pipeline(
    "text-classification",
    model="microsoft/deberta-v3-base-mnli-fever-anli"
)
```

---

## 完整代码实现

```python
"""
NLI验证实现
使用DeBERTa-NLI模型进行声明级幻觉检测
"""

import os
from typing import List, Dict, Tuple
from dotenv import load_dotenv
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
import torch
from functools import lru_cache
import hashlib
import json

# 加载环境变量
load_dotenv()


class NLIVerifier:
    """基于NLI的幻觉验证器"""

    def __init__(
        self,
        nli_model_name: str = "microsoft/deberta-v3-base-mnli-fever-anli",
        threshold: float = 0.8,
        device: int = -1,  # -1 for CPU, 0 for GPU
    ):
        """
        初始化NLI验证器

        Args:
            nli_model_name: NLI模型名称
            threshold: 蕴含概率阈值
            device: 设备（-1=CPU, 0=GPU）
        """
        self.threshold = threshold
        self.device = device

        # 加载NLI模型
        print(f"加载NLI模型: {nli_model_name}")
        self.nli_model = pipeline(
            "text-classification", model=nli_model_name, device=device
        )

        # 加载Embedding模型（用于相似度计算）
        print("加载Embedding模型: all-MiniLM-L6-v2")
        self.embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

    def verify_single_claim(self, context: str, claim: str) -> Dict:
        """
        验证单个声明

        Args:
            context: 上下文
            claim: 声明

        Returns:
            验证结果字典
        """
        # 构造NLI输入
        input_text = f"{context} [SEP] {claim}"

        # NLI推理
        result = self.nli_model(input_text)[0]

        return {
            "claim": claim,
            "label": result["label"],  # entailment/contradiction/neutral
            "score": result["score"],
            "supported": result["label"] == "entailment"
            and result["score"] > self.threshold,
        }

    def verify_claim_multi_context(
        self, claim: str, contexts: List[str]
    ) -> Dict:
        """
        在多个上下文中验证声明

        Args:
            claim: 声明
            contexts: 上下文列表

        Returns:
            验证结果
        """
        best_result = {
            "claim": claim,
            "supported": False,
            "best_score": 0.0,
            "best_context_idx": -1,
            "best_label": "neutral",
        }

        for idx, context in enumerate(contexts):
            result = self.verify_single_claim(context, claim)

            if (
                result["label"] == "entailment"
                and result["score"] > best_result["best_score"]
            ):
                best_result = {
                    "claim": claim,
                    "supported": True,
                    "best_score": result["score"],
                    "best_context_idx": idx,
                    "best_context": context,
                    "best_label": "entailment",
                }

        return best_result

    def batch_verify_claims(
        self, claims: List[str], contexts: List[str]
    ) -> List[Dict]:
        """
        批量验证多个声明

        Args:
            claims: 声明列表
            contexts: 上下文列表

        Returns:
            验证结果列表
        """
        results = []

        for claim in claims:
            result = self.verify_claim_multi_context(claim, contexts)
            results.append(result)

        return results

    def verify_with_similarity(
        self, claim: str, contexts: List[str], similarity_threshold: float = 0.75
    ) -> Dict:
        """
        结合语义相似度的验证

        Args:
            claim: 声明
            contexts: 上下文列表
            similarity_threshold: 相似度阈值

        Returns:
            验证结果
        """
        # 1. NLI验证
        nli_result = self.verify_claim_multi_context(claim, contexts)

        # 2. 语义相似度验证
        claim_emb = self.embedding_model.encode(claim, convert_to_tensor=True)
        context_embs = self.embedding_model.encode(
            contexts, convert_to_tensor=True
        )

        similarities = util.cos_sim(claim_emb, context_embs)[0]
        max_similarity = similarities.max().item()
        best_sim_idx = similarities.argmax().item()

        # 3. 综合判断
        nli_supported = nli_result["supported"]
        similarity_supported = max_similarity > similarity_threshold

        return {
            "claim": claim,
            "nli_supported": nli_supported,
            "nli_score": nli_result.get("best_score", 0.0),
            "similarity_supported": similarity_supported,
            "similarity_score": max_similarity,
            "final_supported": nli_supported and similarity_supported,
            "best_context_idx": (
                nli_result["best_context_idx"]
                if nli_supported
                else best_sim_idx
            ),
        }


class HybridVerifier:
    """混合验证器：NLI + 语义相似度 + 关键词匹配"""

    def __init__(self, nli_threshold: float = 0.8, similarity_threshold: float = 0.75):
        self.nli_verifier = NLIVerifier(threshold=nli_threshold)
        self.similarity_threshold = similarity_threshold

    def keyword_match(self, claim: str, context: str) -> float:
        """
        关键词匹配分数

        Args:
            claim: 声明
            context: 上下文

        Returns:
            匹配分数（0-1）
        """
        claim_words = set(claim.lower().split())
        context_words = set(context.lower().split())

        if len(claim_words) == 0:
            return 0.0

        overlap = claim_words.intersection(context_words)
        return len(overlap) / len(claim_words)

    def verify_claim(self, claim: str, contexts: List[str]) -> Dict:
        """
        混合验证单个声明

        Args:
            claim: 声明
            contexts: 上下文列表

        Returns:
            验证结果
        """
        # 方法1：NLI验证
        nli_result = self.nli_verifier.verify_claim_multi_context(claim, contexts)

        # 方法2：语义相似度
        claim_emb = self.nli_verifier.embedding_model.encode(
            claim, convert_to_tensor=True
        )
        context_embs = self.nli_verifier.embedding_model.encode(
            contexts, convert_to_tensor=True
        )
        similarities = util.cos_sim(claim_emb, context_embs)[0]
        max_similarity = similarities.max().item()

        # 方法3：关键词匹配
        keyword_scores = [self.keyword_match(claim, ctx) for ctx in contexts]
        max_keyword_score = max(keyword_scores) if keyword_scores else 0.0

        # 投票机制
        votes = [
            nli_result["supported"],
            max_similarity > self.similarity_threshold,
            max_keyword_score > 0.5,
        ]

        return {
            "claim": claim,
            "nli_supported": nli_result["supported"],
            "nli_score": nli_result.get("best_score", 0.0),
            "similarity_score": max_similarity,
            "keyword_score": max_keyword_score,
            "votes": sum(votes),
            "final_supported": sum(votes) >= 2,  # 至少2个方法认为支持
        }


def demo_single_claim_verification():
    """演示单个声明验证"""
    print("=" * 60)
    print("演示1：单个声明NLI验证")
    print("=" * 60)

    verifier = NLIVerifier(threshold=0.8)

    context = "Python是一种高级编程语言，由Guido van Rossum于1991年创建。"

    # 测试不同类型的声明
    test_claims = [
        ("Python是由Guido van Rossum创建的", "entailment"),
        ("Python是由James Gosling创建的", "contradiction"),
        ("Python广泛用于数据科学", "neutral"),
    ]

    for claim, expected_label in test_claims:
        result = verifier.verify_single_claim(context, claim)

        status_icon = "✅" if result["supported"] else "❌"
        print(f"\n{status_icon} 声明: {claim}")
        print(f"   标签: {result['label']} (期望: {expected_label})")
        print(f"   置信度: {result['score']:.2f}")
        print(f"   是否支持: {result['supported']}")


def demo_multi_context_verification():
    """演示多上下文验证"""
    print("\n" + "=" * 60)
    print("演示2：多上下文验证")
    print("=" * 60)

    verifier = NLIVerifier(threshold=0.8)

    contexts = [
        "Python是一种高级编程语言。",
        "Python由Guido van Rossum创建。",
        "Python于1991年发布。",
    ]

    claims = [
        "Python是高级编程语言",
        "Python由Guido创建",
        "Python于1991年发布",
        "Python广泛用于AI开发",  # 未被支持
    ]

    print(f"\n上下文数量: {len(contexts)}")
    print(f"声明数量: {len(claims)}")

    for claim in claims:
        result = verifier.verify_claim_multi_context(claim, contexts)

        status_icon = "✅" if result["supported"] else "❌"
        print(f"\n{status_icon} {claim}")
        if result["supported"]:
            print(f"   来源: 上下文{result['best_context_idx']+1}")
            print(f"   置信度: {result['best_score']:.2f}")


def demo_batch_verification():
    """演示批量验证"""
    print("\n" + "=" * 60)
    print("演示3：批量验证")
    print("=" * 60)

    verifier = NLIVerifier(threshold=0.8)

    contexts = [
        "Python是一种高级编程语言，由Guido van Rossum创建。",
        "Python语法简洁，易于学习。",
    ]

    claims = [
        "Python是高级编程语言",
        "Python由Guido van Rossum创建",
        "Python语法简洁",
        "Python易于学习",
        "Python广泛用于数据科学",  # 未被支持
    ]

    results = verifier.batch_verify_claims(claims, contexts)

    supported_count = sum(1 for r in results if r["supported"])
    support_ratio = supported_count / len(results)

    print(f"\n批量验证结果：")
    print(f"  总声明数: {len(results)}")
    print(f"  支持声明数: {supported_count}")
    print(f"  支持率: {support_ratio:.1%}")

    print(f"\n详细结果：")
    for result in results:
        status_icon = "✅" if result["supported"] else "❌"
        print(f"  {status_icon} {result['claim']}")


def demo_hybrid_verification():
    """演示混合验证"""
    print("\n" + "=" * 60)
    print("演示4：混合验证（NLI + 相似度 + 关键词）")
    print("=" * 60)

    verifier = HybridVerifier(nli_threshold=0.8, similarity_threshold=0.75)

    contexts = [
        "Python是一种高级编程语言，由Guido van Rossum创建。",
        "Python语法简洁，易于学习，广泛用于数据科学和Web开发。",
    ]

    claims = [
        "Python是高级编程语言",
        "Python由Guido创建",
        "Python用于数据科学",
        "Python是最好的编程语言",  # 主观判断
    ]

    print(f"\n混合验证结果：")
    for claim in claims:
        result = verifier.verify_claim(claim, contexts)

        status_icon = "✅" if result["final_supported"] else "❌"
        print(f"\n{status_icon} {claim}")
        print(f"   NLI: {'✅' if result['nli_supported'] else '❌'} ({result['nli_score']:.2f})")
        print(f"   相似度: {'✅' if result['similarity_score'] > 0.75 else '❌'} ({result['similarity_score']:.2f})")
        print(f"   关键词: {'✅' if result['keyword_score'] > 0.5 else '❌'} ({result['keyword_score']:.2f})")
        print(f"   投票: {result['votes']}/3")
        print(f"   最终: {'支持' if result['final_supported'] else '不支持'}")


def demo_performance_comparison():
    """演示性能对比"""
    print("\n" + "=" * 60)
    print("演示5：性能对比（NLI vs 相似度）")
    print("=" * 60)

    import time

    verifier = NLIVerifier(threshold=0.8)

    context = "Python是一种高级编程语言，由Guido van Rossum于1991年创建。"
    claim = "Python是由Guido van Rossum创建的高级编程语言。"

    # NLI验证
    start_time = time.time()
    nli_result = verifier.verify_single_claim(context, claim)
    nli_time = (time.time() - start_time) * 1000

    # 语义相似度
    start_time = time.time()
    claim_emb = verifier.embedding_model.encode(claim)
    context_emb = verifier.embedding_model.encode(context)
    similarity = util.cos_sim(claim_emb, context_emb)[0][0].item()
    similarity_time = (time.time() - start_time) * 1000

    print(f"\n性能对比：")
    print(f"  NLI验证:")
    print(f"    延迟: {nli_time:.1f}ms")
    print(f"    结果: {nli_result['label']} ({nli_result['score']:.2f})")

    print(f"\n  语义相似度:")
    print(f"    延迟: {similarity_time:.1f}ms")
    print(f"    结果: {similarity:.2f}")

    print(f"\n  速度提升: {nli_time / similarity_time:.1f}x")


def main():
    """主函数"""
    print("\n" + "=" * 60)
    print("NLI验证实现")
    print("=" * 60)

    try:
        # 运行演示
        demo_single_claim_verification()
        demo_multi_context_verification()
        demo_batch_verification()
        demo_hybrid_verification()
        demo_performance_comparison()

        print("\n" + "=" * 60)
        print("✅ 所有演示完成")
        print("=" * 60)

    except Exception as e:
        print(f"\n❌ 错误: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
```

---

## 代码说明

### 核心类

1. **NLIVerifier**
   - 封装DeBERTa-NLI模型
   - 支持单个和批量验证
   - 支持多上下文验证

2. **HybridVerifier**
   - 混合验证策略
   - 结合NLI、相似度、关键词匹配
   - 投票机制决定最终结果

### 关键功能

1. **单个声明验证**：`verify_single_claim()`
2. **多上下文验证**：`verify_claim_multi_context()`
3. **批量验证**：`batch_verify_claims()`
4. **混合验证**：`verify_claim()`

---

## 运行示例

```bash
python nli_verification.py
```

---

## 输出示例

```
============================================================
演示1：单个声明NLI验证
============================================================

✅ 声明: Python是由Guido van Rossum创建的
   标签: entailment (期望: entailment)
   置信度: 0.95
   是否支持: True

❌ 声明: Python是由James Gosling创建的
   标签: contradiction (期望: contradiction)
   置信度: 0.92
   是否支持: False

❌ 声明: Python广泛用于数据科学
   标签: neutral (期望: neutral)
   置信度: 0.88
   是否支持: False

============================================================
演示4：混合验证（NLI + 相似度 + 关键词）
============================================================

✅ Python是高级编程语言
   NLI: ✅ (0.94)
   相似度: ✅ (0.89)
   关键词: ✅ (0.75)
   投票: 3/3
   最终: 支持
```

---

## 性能优化

### 1. 批量处理

```python
# 批量NLI验证（提速3-5倍）
inputs = [f"{ctx} [SEP] {claim}" for ctx, claim in pairs]
results = nli_model(inputs)
```

### 2. 模型量化

```python
# INT8量化（速度提升40%）
import torch

quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

### 3. GPU加速

```python
# 使用GPU
verifier = NLIVerifier(device=0)  # 0 for GPU
```

---

## 常见问题

### Q1: NLI模型如何选择？

**A:** 根据场景选择：
- 英文：`microsoft/deberta-v3-base-mnli-fever-anli`
- 中文：`IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese-NLI`
- 多语言：`MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7`

### Q2: 阈值如何设置？

**A:** 场景化阈值：
- 严格场景（医疗、法律）：>0.9
- 一般场景（企业知识库）：>0.8
- 宽松场景（通用问答）：>0.7

### Q3: 如何处理Neutral结果？

**A:** 3种策略：
1. 保守：Neutral视为不支持
2. 宽松：Neutral视为支持
3. 二次验证：用语义相似度再次验证

---

## 扩展建议

1. **添加缓存**
   ```python
   @lru_cache(maxsize=1000)
   def verify_cached(context, claim):
       return verify_single_claim(context, claim)
   ```

2. **异步处理**
   ```python
   import asyncio

   async def verify_async(claims, contexts):
       tasks = [verify_claim_async(c, contexts) for c in claims]
       return await asyncio.gather(*tasks)
   ```

3. **监控指标**
   ```python
   from prometheus_client import Histogram

   nli_latency = Histogram('nli_latency_seconds', 'NLI verification latency')
   ```

---

**记住：NLI验证是生产环境的首选方法，速度快、准确率高，适合大规模部署。**
