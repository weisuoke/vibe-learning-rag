# å®æˆ˜ä»£ç 2ï¼šå¤æ‚åˆ†æ”¯åœºæ™¯

æ·±å…¥æ¢ç´¢ RunnableBranch çš„é«˜çº§åº”ç”¨åœºæ™¯ã€‚

---

## ç¯å¢ƒå‡†å¤‡

```bash
uv sync
source .venv/bin/activate
```

---

## ç¤ºä¾‹1ï¼šåµŒå¥—æ¡ä»¶è·¯ç”±

```python
"""
ç¤ºä¾‹1ï¼šå¤šå±‚åµŒå¥—è·¯ç”±
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

# è‹±æ–‡å­è·¯ç”±
english_router = RunnableBranch(
    (lambda x: "technical" in x["text"].lower(),
     ChatPromptTemplate.from_template("Technical answer: {text}") | ChatOpenAI()),
    (lambda x: "casual" in x["text"].lower(),
     ChatPromptTemplate.from_template("Casual answer: {text}") | ChatOpenAI()),
    ChatPromptTemplate.from_template("General answer: {text}") | ChatOpenAI()
)

# ä¸­æ–‡å­è·¯ç”±
chinese_router = RunnableBranch(
    (lambda x: "æŠ€æœ¯" in x["text"],
     ChatPromptTemplate.from_template("æŠ€æœ¯å›ç­”ï¼š{text}") | ChatOpenAI()),
    ChatPromptTemplate.from_template("é€šç”¨å›ç­”ï¼š{text}") | ChatOpenAI()
)

# ä¸»è·¯ç”±
main_router = RunnableBranch(
    (lambda x: any(c.isascii() and c.isalpha() for c in x["text"]), english_router),
    (lambda x: any('\u4e00' <= c <= '\u9fff' for c in x["text"]), chinese_router),
    RunnableLambda(lambda x: "Unsupported language")
)

# æµ‹è¯•
print("=== ç¤ºä¾‹1ï¼šåµŒå¥—è·¯ç”± ===")
result = main_router.invoke({"text": "technical question about Python"})
print(f"EN-Tech: {result.content[:50]}...")

result = main_router.invoke({"text": "æŠ€æœ¯é—®é¢˜å…³äºPython"})
print(f"ZH-Tech: {result.content[:50]}...")
```

---

## ç¤ºä¾‹2ï¼šåŠ¨æ€æ¡ä»¶ç”Ÿæˆ

```python
"""
ç¤ºä¾‹2ï¼šæ ¹æ®é…ç½®åŠ¨æ€ç”Ÿæˆæ¡ä»¶
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda

def create_keyword_router(keyword_handlers, default_handler):
    """åŠ¨æ€åˆ›å»ºå…³é”®è¯è·¯ç”±"""
    branches = []
    for keywords, handler in keyword_handlers:
        condition = lambda x, kws=keywords: any(kw in x.lower() for kw in kws)
        branches.append((condition, handler))

    return RunnableBranch(*branches, default_handler)

# é…ç½®
keyword_config = [
    (["urgent", "emergency", "critical"], RunnableLambda(lambda x: f"URGENT: {x}")),
    (["question", "help", "how"], RunnableLambda(lambda x: f"HELP: {x}")),
    (["thanks", "thank you"], RunnableLambda(lambda x: f"THANKS: {x}")),
]
default = RunnableLambda(lambda x: f"GENERAL: {x}")

# åˆ›å»ºè·¯ç”±
router = create_keyword_router(keyword_config, default)

# æµ‹è¯•
print("\n=== ç¤ºä¾‹2ï¼šåŠ¨æ€æ¡ä»¶ç”Ÿæˆ ===")
print(router.invoke("This is urgent!"))
print(router.invoke("I have a question"))
print(router.invoke("Thank you for help"))
print(router.invoke("Just a comment"))
```

---

## ç¤ºä¾‹3ï¼šå¤šæ¨¡å‹è·¯ç”±ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰

```python
"""
ç¤ºä¾‹3ï¼šæ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©ä¸åŒæ¨¡å‹
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

def calculate_complexity(x):
    """è®¡ç®—ä»»åŠ¡å¤æ‚åº¦"""
    text = x["text"]
    score = 0

    # é•¿åº¦å› ç´ 
    if len(text) > 500:
        score += 0.3

    # æŠ€æœ¯å…³é”®è¯
    tech_keywords = ["algorithm", "architecture", "optimization", "design pattern"]
    if any(kw in text.lower() for kw in tech_keywords):
        score += 0.4

    # å¤šæ­¥éª¤
    if any(word in text.lower() for word in ["first", "then", "finally", "step"]):
        score += 0.3

    return score

# é¢„å¤„ç†ï¼šè®¡ç®—å¤æ‚åº¦
preprocessor = RunnableLambda(lambda x: {
    **x,
    "complexity": calculate_complexity(x)
})

# æ¨¡å‹è·¯ç”±
model_router = RunnableBranch(
    # é«˜å¤æ‚åº¦ï¼šä½¿ç”¨ GPT-4
    (lambda x: x["complexity"] > 0.7,
     ChatPromptTemplate.from_template("Complex task: {text}") | ChatOpenAI(model="gpt-4o")),
    # ä¸­å¤æ‚åº¦ï¼šä½¿ç”¨ GPT-4o
    (lambda x: x["complexity"] > 0.3,
     ChatPromptTemplate.from_template("Medium task: {text}") | ChatOpenAI(model="gpt-4o")),
    # ä½å¤æ‚åº¦ï¼šä½¿ç”¨ GPT-4o-mini
    ChatPromptTemplate.from_template("Simple task: {text}") | ChatOpenAI(model="gpt-4o-mini")
)

# å®Œæ•´é“¾
chain = preprocessor | model_router

# æµ‹è¯•
print("\n=== ç¤ºä¾‹3ï¼šå¤šæ¨¡å‹è·¯ç”± ===")
result = chain.invoke({"text": "What is 2+2?"})
print(f"Simple (mini): {result.content}")

result = chain.invoke({"text": "Explain the algorithm for sorting a large dataset"})
print(f"Complex (4o): {result.content[:50]}...")
```

---

## ç¤ºä¾‹4ï¼šé£é™©è¯„ä¼°åˆ†æ”¯

```python
"""
ç¤ºä¾‹4ï¼šæ ¹æ®é£é™©ç­‰çº§è·¯ç”±åˆ°ä¸åŒå¤„ç†æµç¨‹
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
import re

def assess_risk(x):
    """è¯„ä¼°è¾“å…¥é£é™©"""
    text = x["text"].lower()
    risk_score = 0

    # æ£€æŸ¥æ•æ„Ÿè¯
    sensitive_words = ["password", "credit card", "ssn", "secret"]
    if any(word in text for word in sensitive_words):
        risk_score += 0.5

    # æ£€æŸ¥å¯ç–‘æ¨¡å¼
    if re.search(r'\b\d{16}\b', text):  # ä¿¡ç”¨å¡å·æ¨¡å¼
        risk_score += 0.5

    if re.search(r'\b\d{3}-\d{2}-\d{4}\b', text):  # SSN æ¨¡å¼
        risk_score += 0.5

    return {"text": text, "risk_score": risk_score}

# é£é™©è¯„ä¼°
risk_assessor = RunnableLambda(assess_risk)

# é£é™©è·¯ç”±
risk_router = RunnableBranch(
    # é«˜é£é™©ï¼šæ‹’ç»å¤„ç†
    (lambda x: x["risk_score"] >= 0.5,
     RunnableLambda(lambda x: "âš ï¸ HIGH RISK: Cannot process sensitive information")),
    # ä¸­é£é™©ï¼šè­¦å‘Šå¤„ç†
    (lambda x: x["risk_score"] > 0,
     RunnableLambda(lambda x: f"âš ï¸ CAUTION: {x['text']}")),
    # ä½é£é™©ï¼šæ­£å¸¸å¤„ç†
    RunnableLambda(lambda x: f"âœ… SAFE: {x['text']}")
)

# å®Œæ•´é“¾
chain = risk_assessor | risk_router

# æµ‹è¯•
print("\n=== ç¤ºä¾‹4ï¼šé£é™©è¯„ä¼° ===")
print(chain.invoke({"text": "What is the weather?"}))
print(chain.invoke({"text": "My password is abc123"}))
print(chain.invoke({"text": "Card: 1234567890123456"}))
```

---

## ç¤ºä¾‹5ï¼šè§’è‰²åŸºç¡€è·¯ç”±

```python
"""
ç¤ºä¾‹5ï¼šæ ¹æ®ç”¨æˆ·è§’è‰²è·¯ç”±åˆ°ä¸åŒæƒé™çš„å¤„ç†å™¨
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda

# ä¸åŒè§’è‰²çš„å¤„ç†å™¨
admin_handler = RunnableLambda(lambda x: {
    "role": "admin",
    "access": "full",
    "data": f"Admin view: {x['query']}",
    "sensitive_data": "visible"
})

moderator_handler = RunnableLambda(lambda x: {
    "role": "moderator",
    "access": "limited",
    "data": f"Moderator view: {x['query']}",
    "sensitive_data": "partial"
})

user_handler = RunnableLambda(lambda x: {
    "role": "user",
    "access": "basic",
    "data": f"User view: {x['query']}",
    "sensitive_data": "hidden"
})

guest_handler = RunnableLambda(lambda x: {
    "role": "guest",
    "access": "public",
    "data": "Limited preview",
    "sensitive_data": "hidden"
})

# è§’è‰²è·¯ç”±
role_router = RunnableBranch(
    (lambda x: x.get("role") == "admin", admin_handler),
    (lambda x: x.get("role") == "moderator", moderator_handler),
    (lambda x: x.get("role") == "user", user_handler),
    guest_handler
)

# æµ‹è¯•
print("\n=== ç¤ºä¾‹5ï¼šè§’è‰²è·¯ç”± ===")
for role in ["admin", "moderator", "user", "guest"]:
    result = role_router.invoke({"role": role, "query": "sensitive data"})
    print(f"{role}: {result}")
```

---

## ç¤ºä¾‹6ï¼šæ„å›¾è¯†åˆ«è·¯ç”±

```python
"""
ç¤ºä¾‹6ï¼šä½¿ç”¨ LLM è¿›è¡Œæ„å›¾è¯†åˆ«å¹¶è·¯ç”±
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# æ„å›¾åˆ†ç±»å™¨
intent_classifier = (
    ChatPromptTemplate.from_template(
        "Classify intent as: greeting, question, command, feedback, or other.\n"
        "Text: {text}\nIntent:"
    )
    | ChatOpenAI(model="gpt-4o-mini", temperature=0)
    | RunnableLambda(lambda x: x.content.strip().lower())
)

# æ„å›¾å¤„ç†å™¨
greeting_handler = RunnableLambda(lambda x: f"ğŸ‘‹ Hello! {x}")
question_handler = RunnableLambda(lambda x: f"â“ Let me answer: {x}")
command_handler = RunnableLambda(lambda x: f"âš¡ Executing: {x}")
feedback_handler = RunnableLambda(lambda x: f"ğŸ’¬ Thanks for feedback: {x}")
other_handler = RunnableLambda(lambda x: f"ğŸ“ Noted: {x}")

# æ„å›¾è·¯ç”±
intent_router = RunnableBranch(
    (lambda x: "greeting" in x, greeting_handler),
    (lambda x: "question" in x, question_handler),
    (lambda x: "command" in x, command_handler),
    (lambda x: "feedback" in x, feedback_handler),
    other_handler
)

# å®Œæ•´é“¾
chain = intent_classifier | intent_router

# æµ‹è¯•
print("\n=== ç¤ºä¾‹6ï¼šæ„å›¾è¯†åˆ« ===")
test_inputs = [
    "Hello there!",
    "What is the weather?",
    "Delete my account",
    "Great service!",
    "Random text"
]
for text in test_inputs:
    result = chain.invoke({"text": text})
    print(f"{text[:20]:20} â†’ {result}")
```

---

## ç¤ºä¾‹7ï¼šä¸ RunnableParallel ç»„åˆ

```python
"""
ç¤ºä¾‹7ï¼šå¹¶è¡Œæ‰§è¡Œå¤šä¸ªè·¯ç”±
"""
from langchain_core.runnables import RunnableBranch, RunnableParallel, RunnableLambda

# æƒ…æ„Ÿè·¯ç”±
sentiment_router = RunnableBranch(
    (lambda x: any(w in x.lower() for w in ["love", "great", "awesome"]),
     RunnableLambda(lambda x: "positive")),
    (lambda x: any(w in x.lower() for w in ["hate", "bad", "terrible"]),
     RunnableLambda(lambda x: "negative")),
    RunnableLambda(lambda x: "neutral")
)

# ç´§æ€¥åº¦è·¯ç”±
urgency_router = RunnableBranch(
    (lambda x: any(w in x.lower() for w in ["urgent", "asap", "emergency"]),
     RunnableLambda(lambda x: "high")),
    (lambda x: any(w in x.lower() for w in ["soon", "quick"]),
     RunnableLambda(lambda x: "medium")),
    RunnableLambda(lambda x: "low")
)

# ç±»åˆ«è·¯ç”±
category_router = RunnableBranch(
    (lambda x: "bug" in x.lower(), RunnableLambda(lambda x: "bug_report")),
    (lambda x: "feature" in x.lower(), RunnableLambda(lambda x: "feature_request")),
    RunnableLambda(lambda x: "general")
)

# å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è·¯ç”±
parallel_router = RunnableParallel(
    sentiment=sentiment_router,
    urgency=urgency_router,
    category=category_router
)

# æµ‹è¯•
print("\n=== ç¤ºä¾‹7ï¼šå¹¶è¡Œè·¯ç”± ===")
result = parallel_router.invoke("I love this feature but there's an urgent bug!")
print(f"Analysis: {result}")
```

---

## ç¤ºä¾‹8ï¼šé”™è¯¯å¤„ç†ä¸é‡è¯•

```python
"""
ç¤ºä¾‹8ï¼šå¸¦é”™è¯¯å¤„ç†çš„è·¯ç”±
"""
from langchain_core.runnables import RunnableBranch, RunnableLambda
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯èƒ½å¤±è´¥çš„å¤„ç†å™¨
def risky_handler(x):
    if "error" in x.lower():
        raise ValueError("Simulated error")
    return f"Success: {x}"

# å®‰å…¨åŒ…è£…
def safe_invoke(handler, x):
    try:
        return handler(x)
    except Exception as e:
        logger.error(f"Handler failed: {e}")
        return f"Error handled: {str(e)}"

# å¸¦é”™è¯¯å¤„ç†çš„è·¯ç”±
safe_router = RunnableBranch(
    (lambda x: "risky" in x.lower(),
     RunnableLambda(lambda x: safe_invoke(risky_handler, x))),
    RunnableLambda(lambda x: f"Safe: {x}")
)

# æµ‹è¯•
print("\n=== ç¤ºä¾‹8ï¼šé”™è¯¯å¤„ç† ===")
print(safe_router.invoke("risky operation"))
print(safe_router.invoke("risky error operation"))
print(safe_router.invoke("safe operation"))
```

---

## è¿è¡Œè¾“å‡ºç¤ºä¾‹

```
=== ç¤ºä¾‹1ï¼šåµŒå¥—è·¯ç”± ===
EN-Tech: As a technical expert, I'll explain Python...
ZH-Tech: ä½œä¸ºæŠ€æœ¯ä¸“å®¶ï¼Œæˆ‘å°†è§£é‡ŠPython...

=== ç¤ºä¾‹2ï¼šåŠ¨æ€æ¡ä»¶ç”Ÿæˆ ===
URGENT: This is urgent!
HELP: I have a question
THANKS: Thank you for help
GENERAL: Just a comment

=== ç¤ºä¾‹3ï¼šå¤šæ¨¡å‹è·¯ç”± ===
Simple (mini): 4
Complex (4o): To sort a large dataset efficiently, we need...

=== ç¤ºä¾‹4ï¼šé£é™©è¯„ä¼° ===
âœ… SAFE: what is the weather?
âš ï¸ HIGH RISK: Cannot process sensitive information
âš ï¸ HIGH RISK: Cannot process sensitive information

=== ç¤ºä¾‹5ï¼šè§’è‰²è·¯ç”± ===
admin: {'role': 'admin', 'access': 'full', ...}
moderator: {'role': 'moderator', 'access': 'limited', ...}
user: {'role': 'user', 'access': 'basic', ...}
guest: {'role': 'guest', 'access': 'public', ...}

=== ç¤ºä¾‹6ï¼šæ„å›¾è¯†åˆ« ===
Hello there!        â†’ ğŸ‘‹ Hello! greeting
What is the weather? â†’ â“ Let me answer: question
Delete my account   â†’ âš¡ Executing: command
Great service!      â†’ ğŸ’¬ Thanks for feedback: feedback
Random text         â†’ ğŸ“ Noted: other

=== ç¤ºä¾‹7ï¼šå¹¶è¡Œè·¯ç”± ===
Analysis: {'sentiment': 'positive', 'urgency': 'high', 'category': 'bug_report'}

=== ç¤ºä¾‹8ï¼šé”™è¯¯å¤„ç† ===
Success: risky operation
Error handled: Simulated error
Safe: safe operation
```

---

## æœ€ä½³å®è·µ

1. **åµŒå¥—æ·±åº¦**ï¼šé¿å…è¶…è¿‡2-3å±‚åµŒå¥—ï¼Œå¦åˆ™ç”¨ RunnableLambda
2. **åŠ¨æ€ç”Ÿæˆ**ï¼šä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºå¯é…ç½®çš„è·¯ç”±
3. **æˆæœ¬ä¼˜åŒ–**ï¼šæ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
4. **é£é™©æ§åˆ¶**ï¼šåœ¨è·¯ç”±å±‚é¢è¿›è¡Œå®‰å…¨æ£€æŸ¥
5. **æƒé™ç®¡ç†**ï¼šä½¿ç”¨è§’è‰²è·¯ç”±å®ç°è®¿é—®æ§åˆ¶
6. **å¹¶è¡Œä¼˜åŒ–**ï¼šç‹¬ç«‹çš„è·¯ç”±å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
7. **é”™è¯¯å¤„ç†**ï¼šåœ¨å…³é”®è·¯ç”±æ·»åŠ é”™è¯¯å¤„ç†
8. **æ—¥å¿—è¿½è¸ª**ï¼šè®°å½•è·¯ç”±å†³ç­–ä¾¿äºè°ƒè¯•
