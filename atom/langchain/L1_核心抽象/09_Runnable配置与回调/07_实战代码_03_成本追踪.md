# å®æˆ˜ä»£ç  03ï¼šæˆæœ¬è¿½è¸ª

> **å®ç° Token è®¡æ•°ã€æˆæœ¬è®¡ç®—ã€é¢„ç®—å‘Šè­¦å’Œæˆæœ¬ä¼˜åŒ–çš„å®Œæ•´æ–¹æ¡ˆ**

---

## åœºæ™¯ 1ï¼šåŸºç¡€æˆæœ¬è¿½è¸ª

```python
from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI

class SimpleCostTracker(BaseCallbackHandler):
    """ç®€å•æˆæœ¬è¿½è¸ª"""

    def __init__(self):
        self.total_cost = 0
        self.total_tokens = 0

    def on_llm_end(self, response, **kwargs):
        # è·å– token ä½¿ç”¨é‡
        usage = response.llm_output.get("token_usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        # è®¡ç®—æˆæœ¬ï¼ˆGPT-4 ä»·æ ¼ï¼‰
        cost = prompt_tokens * 0.00003 + completion_tokens * 0.00006

        self.total_cost += cost
        self.total_tokens += usage.get("total_tokens", 0)

        print(f"æœ¬æ¬¡æˆæœ¬: ${cost:.4f}")
        print(f"ç´¯è®¡æˆæœ¬: ${self.total_cost:.4f}")

# ä½¿ç”¨
tracker = SimpleCostTracker()
llm = ChatOpenAI(model="gpt-4")

llm.invoke("è§£é‡Šé‡å­çº ç¼ ", config={"callbacks": [tracker]})
llm.invoke("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", config={"callbacks": [tracker]})

print(f"\næ€»æˆæœ¬: ${tracker.total_cost:.4f}")
print(f"æ€» tokens: {tracker.total_tokens}")
```

---

## åœºæ™¯ 2ï¼šå¤šæ¨¡å‹æˆæœ¬è¿½è¸ª

```python
from collections import defaultdict
from langchain_core.callbacks import BaseCallbackHandler

class MultiModelCostTracker(BaseCallbackHandler):
    """å¤šæ¨¡å‹æˆæœ¬è¿½è¸ª"""

    # æ¨¡å‹ä»·æ ¼è¡¨ï¼ˆ$/1K tokensï¼‰
    PRICES = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
        "claude-3-opus-20240229": {"input": 0.015, "output": 0.075},
        "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015}
    }

    def __init__(self):
        self.model_costs = defaultdict(float)
        self.model_tokens = defaultdict(int)

    def on_llm_end(self, response, **kwargs):
        # è·å–æ¨¡å‹åç§°
        model = response.llm_output.get("model_name", "unknown")

        # è·å– token ä½¿ç”¨é‡
        usage = response.llm_output.get("token_usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        # è®¡ç®—æˆæœ¬
        if model in self.PRICES:
            cost = (
                prompt_tokens / 1000 * self.PRICES[model]["input"] +
                completion_tokens / 1000 * self.PRICES[model]["output"]
            )
        else:
            cost = 0

        # è®°å½•
        self.model_costs[model] += cost
        self.model_tokens[model] += usage.get("total_tokens", 0)

    def get_report(self):
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        print("\n=== æˆæœ¬æŠ¥å‘Š ===")
        total_cost = 0
        total_tokens = 0

        for model in sorted(self.model_costs.keys()):
            cost = self.model_costs[model]
            tokens = self.model_tokens[model]
            total_cost += cost
            total_tokens += tokens

            print(f"{model}:")
            print(f"  æˆæœ¬: ${cost:.4f}")
            print(f"  Tokens: {tokens}")

        print(f"\næ€»æˆæœ¬: ${total_cost:.4f}")
        print(f"æ€» Tokens: {total_tokens}")

# ä½¿ç”¨
from langchain_openai import ChatOpenAI

tracker = MultiModelCostTracker()

# ä½¿ç”¨ä¸åŒæ¨¡å‹
gpt4 = ChatOpenAI(model="gpt-4")
gpt35 = ChatOpenAI(model="gpt-3.5-turbo")

gpt4.invoke("ä½ å¥½", config={"callbacks": [tracker]})
gpt35.invoke("ä½ å¥½", config={"callbacks": [tracker]})

tracker.get_report()
```

---

## åœºæ™¯ 3ï¼šæŒ‰ç”¨æˆ·å½’å› æˆæœ¬

```python
from collections import defaultdict
from langchain_core.callbacks import BaseCallbackHandler

class UserCostTracker(BaseCallbackHandler):
    """æŒ‰ç”¨æˆ·å½’å› æˆæœ¬"""

    def __init__(self):
        self.user_costs = defaultdict(lambda: {
            "total_cost": 0.0,
            "total_tokens": 0,
            "call_count": 0
        })

    def on_llm_end(self, response, **kwargs):
        # ä» metadata è·å– user_id
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        # è®¡ç®—æˆæœ¬
        usage = response.llm_output.get("token_usage", {})
        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        total_tokens = usage.get("total_tokens", 0)

        cost = prompt_tokens * 0.00003 + completion_tokens * 0.00006

        # æ›´æ–°ç”¨æˆ·ç»Ÿè®¡
        self.user_costs[user_id]["total_cost"] += cost
        self.user_costs[user_id]["total_tokens"] += total_tokens
        self.user_costs[user_id]["call_count"] += 1

    def get_report(self):
        """ç”Ÿæˆç”¨æˆ·æˆæœ¬æŠ¥å‘Š"""
        print("\n=== ç”¨æˆ·æˆæœ¬æŠ¥å‘Š ===")
        for user_id in sorted(self.user_costs.keys()):
            stats = self.user_costs[user_id]
            avg_cost = stats["total_cost"] / stats["call_count"]

            print(f"\nç”¨æˆ·: {user_id}")
            print(f"  æ€»æˆæœ¬: ${stats['total_cost']:.4f}")
            print(f"  æ€» Tokens: {stats['total_tokens']}")
            print(f"  è°ƒç”¨æ¬¡æ•°: {stats['call_count']}")
            print(f"  å¹³å‡æˆæœ¬: ${avg_cost:.4f}")

# ä½¿ç”¨
from langchain_openai import ChatOpenAI

tracker = UserCostTracker()
llm = ChatOpenAI(model="gpt-4")

# æ¨¡æ‹Ÿå¤šä¸ªç”¨æˆ·çš„è°ƒç”¨
for user_id in ["user_001", "user_002", "user_001", "user_003"]:
    llm.invoke(
        "ä½ å¥½",
        config={
            "callbacks": [tracker],
            "metadata": {"user_id": user_id}
        }
    )

tracker.get_report()
```

---

## åœºæ™¯ 4ï¼šé¢„ç®—å‘Šè­¦

```python
from langchain_core.callbacks import BaseCallbackHandler

class BudgetAlerter(BaseCallbackHandler):
    """é¢„ç®—å‘Šè­¦"""

    def __init__(self, daily_limit: float = 10.0, hourly_limit: float = 1.0):
        self.daily_limit = daily_limit
        self.hourly_limit = hourly_limit
        self.daily_cost = 0.0
        self.hourly_cost = 0.0

    def on_llm_end(self, response, **kwargs):
        # è®¡ç®—æˆæœ¬
        usage = response.llm_output.get("token_usage", {})
        cost = (
            usage.get("prompt_tokens", 0) * 0.00003 +
            usage.get("completion_tokens", 0) * 0.00006
        )

        self.daily_cost += cost
        self.hourly_cost += cost

        # æ£€æŸ¥é¢„ç®—
        if self.hourly_cost > self.hourly_limit:
            self.alert(f"âš ï¸ è¶…è¿‡æ¯å°æ—¶é¢„ç®—: ${self.hourly_cost:.4f} > ${self.hourly_limit}")

        if self.daily_cost > self.daily_limit:
            self.alert(f"ğŸš¨ è¶…è¿‡æ¯æ—¥é¢„ç®—: ${self.daily_cost:.4f} > ${self.daily_limit}")
            raise Exception(f"è¶…è¿‡æ¯æ—¥é¢„ç®—é™åˆ¶: ${self.daily_limit}")

    def alert(self, message: str):
        """å‘é€å‘Šè­¦"""
        print(message)
        # å¯ä»¥é›†æˆ Slackã€é’‰é’‰ã€Email ç­‰

    def reset_hourly(self):
        """é‡ç½®æ¯å°æ—¶ç»Ÿè®¡"""
        self.hourly_cost = 0.0

    def reset_daily(self):
        """é‡ç½®æ¯æ—¥ç»Ÿè®¡"""
        self.daily_cost = 0.0
        self.hourly_cost = 0.0

# ä½¿ç”¨
from langchain_openai import ChatOpenAI

alerter = BudgetAlerter(daily_limit=1.0, hourly_limit=0.1)
llm = ChatOpenAI(model="gpt-4")

try:
    for i in range(100):
        llm.invoke("ä½ å¥½", config={"callbacks": [alerter]})
except Exception as e:
    print(f"åœæ­¢è°ƒç”¨: {e}")
```

---

## åœºæ™¯ 5ï¼šæˆæœ¬ä¼˜åŒ–å»ºè®®

```python
from langchain_core.callbacks import BaseCallbackHandler

class CostOptimizer(BaseCallbackHandler):
    """æˆæœ¬ä¼˜åŒ–å»ºè®®"""

    def __init__(self):
        self.calls = []

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.current_call = {
            "model": serialized.get("name"),
            "prompt_length": len(prompts[0])
        }

    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})
        model = response.llm_output.get("model_name")

        self.current_call.update({
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0)
        })

        self.calls.append(self.current_call)

        # å®æ—¶ä¼˜åŒ–å»ºè®®
        self.suggest_optimization()

    def suggest_optimization(self):
        """æä¾›ä¼˜åŒ–å»ºè®®"""
        call = self.calls[-1]

        # å»ºè®® 1ï¼šPrompt è¿‡é•¿
        if call["prompt_tokens"] > 1000:
            print(f"ğŸ’¡ ä¼˜åŒ–å»ºè®®: Prompt è¿‡é•¿ ({call['prompt_tokens']} tokens)")
            print("   è€ƒè™‘ï¼šå‹ç¼© promptã€ä½¿ç”¨æ‘˜è¦ã€ç§»é™¤å†—ä½™å†…å®¹")

        # å»ºè®® 2ï¼šè¾“å‡ºè¿‡é•¿
        if call["completion_tokens"] > 500:
            print(f"ğŸ’¡ ä¼˜åŒ–å»ºè®®: è¾“å‡ºè¿‡é•¿ ({call['completion_tokens']} tokens)")
            print("   è€ƒè™‘ï¼šè®¾ç½® max_tokensã€ä½¿ç”¨æ›´ç²¾ç¡®çš„æŒ‡ä»¤")

        # å»ºè®® 3ï¼šæ¨¡å‹é€‰æ‹©
        if call["model"] == "gpt-4" and call["total_tokens"] < 100:
            print(f"ğŸ’¡ ä¼˜åŒ–å»ºè®®: ç®€å•æŸ¥è¯¢ä½¿ç”¨äº† GPT-4")
            print("   è€ƒè™‘ï¼šä½¿ç”¨ GPT-3.5 å¯èŠ‚çœ 98% æˆæœ¬")

# ä½¿ç”¨
from langchain_openai import ChatOpenAI

optimizer = CostOptimizer()
llm = ChatOpenAI(model="gpt-4")

llm.invoke("ä½ å¥½", config={"callbacks": [optimizer]})
```

---

## åœºæ™¯ 6ï¼šæˆæœ¬æŒä¹…åŒ–

```python
import json
from datetime import datetime
from langchain_core.callbacks import BaseCallbackHandler

class CostPersister(BaseCallbackHandler):
    """æˆæœ¬æŒä¹…åŒ–"""

    def __init__(self, db_file: str = "costs.jsonl"):
        self.db_file = db_file

    def on_llm_end(self, response, **kwargs):
        metadata = kwargs.get("metadata", {})
        usage = response.llm_output.get("token_usage", {})

        # è®¡ç®—æˆæœ¬
        cost = (
            usage.get("prompt_tokens", 0) * 0.00003 +
            usage.get("completion_tokens", 0) * 0.00006
        )

        # æ„é€ è®°å½•
        record = {
            "timestamp": datetime.now().isoformat(),
            "user_id": metadata.get("user_id"),
            "model": response.llm_output.get("model_name"),
            "tokens": usage,
            "cost": cost
        }

        # å†™å…¥æ–‡ä»¶ï¼ˆJSONL æ ¼å¼ï¼‰
        with open(self.db_file, "a") as f:
            f.write(json.dumps(record) + "\n")

    def get_total_cost(self):
        """è¯»å–æ€»æˆæœ¬"""
        total = 0.0
        try:
            with open(self.db_file, "r") as f:
                for line in f:
                    record = json.loads(line)
                    total += record["cost"]
        except FileNotFoundError:
            pass
        return total

# ä½¿ç”¨
from langchain_openai import ChatOpenAI

persister = CostPersister()
llm = ChatOpenAI(model="gpt-4")

llm.invoke("ä½ å¥½", config={
    "callbacks": [persister],
    "metadata": {"user_id": "user_123"}
})

print(f"æ€»æˆæœ¬: ${persister.get_total_cost():.4f}")
```

---

## åœºæ™¯ 7ï¼šå®æ—¶æˆæœ¬ä»ªè¡¨ç›˜

```python
from langchain_core.callbacks import BaseCallbackHandler
import time

class CostDashboard(BaseCallbackHandler):
    """å®æ—¶æˆæœ¬ä»ªè¡¨ç›˜"""

    def __init__(self):
        self.total_cost = 0.0
        self.total_tokens = 0
        self.call_count = 0
        self.start_time = time.time()

    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})
        cost = (
            usage.get("prompt_tokens", 0) * 0.00003 +
            usage.get("completion_tokens", 0) * 0.00006
        )

        self.total_cost += cost
        self.total_tokens += usage.get("total_tokens", 0)
        self.call_count += 1

        # å®æ—¶æ˜¾ç¤º
        self.display()

    def display(self):
        """æ˜¾ç¤ºä»ªè¡¨ç›˜"""
        elapsed = time.time() - self.start_time
        avg_cost = self.total_cost / self.call_count if self.call_count > 0 else 0
        tokens_per_sec = self.total_tokens / elapsed if elapsed > 0 else 0

        print("\n" + "="*50)
        print("ğŸ“Š æˆæœ¬ä»ªè¡¨ç›˜")
        print("="*50)
        print(f"æ€»æˆæœ¬:     ${self.total_cost:.4f}")
        print(f"æ€» Tokens:  {self.total_tokens}")
        print(f"è°ƒç”¨æ¬¡æ•°:   {self.call_count}")
        print(f"å¹³å‡æˆæœ¬:   ${avg_cost:.4f}")
        print(f"Token é€Ÿç‡: {tokens_per_sec:.1f} tokens/s")
        print(f"è¿è¡Œæ—¶é—´:   {elapsed:.1f}s")
        print("="*50 + "\n")

# ä½¿ç”¨
from langchain_openai import ChatOpenAI

dashboard = CostDashboard()
llm = ChatOpenAI(model="gpt-4")

for i in range(5):
    llm.invoke(f"é—®é¢˜{i}", config={"callbacks": [dashboard]})
    time.sleep(1)
```

---

## åœºæ™¯ 8ï¼šå®Œæ•´ç”Ÿäº§æˆæœ¬ç³»ç»Ÿ

```python
import json
import time
from datetime import datetime
from collections import defaultdict
from langchain_core.callbacks import BaseCallbackHandler

class ProductionCostTracker(BaseCallbackHandler):
    """ç”Ÿäº§ç¯å¢ƒæˆæœ¬è¿½è¸ªç³»ç»Ÿ"""

    # æ¨¡å‹ä»·æ ¼è¡¨
    PRICES = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015}
    }

    def __init__(
        self,
        db_file: str = "costs.jsonl",
        daily_limit: float = 100.0,
        alert_webhook: str = None
    ):
        self.db_file = db_file
        self.daily_limit = daily_limit
        self.alert_webhook = alert_webhook

        # å†…å­˜ç»Ÿè®¡
        self.user_costs = defaultdict(float)
        self.model_costs = defaultdict(float)
        self.daily_cost = 0.0

    def on_llm_end(self, response, **kwargs):
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        # è·å– token ä½¿ç”¨é‡
        usage = response.llm_output.get("token_usage", {})
        model = response.llm_output.get("model_name", "unknown")

        # è®¡ç®—æˆæœ¬
        cost = self.calculate_cost(model, usage)

        # æ›´æ–°ç»Ÿè®¡
        self.user_costs[user_id] += cost
        self.model_costs[model] += cost
        self.daily_cost += cost

        # æŒä¹…åŒ–
        self.persist_cost(user_id, model, usage, cost, metadata)

        # æ£€æŸ¥é¢„ç®—
        self.check_budget()

    def calculate_cost(self, model: str, usage: dict) -> float:
        """è®¡ç®—æˆæœ¬"""
        if model not in self.PRICES:
            return 0.0

        prompt_tokens = usage.get("prompt_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)

        cost = (
            prompt_tokens / 1000 * self.PRICES[model]["input"] +
            completion_tokens / 1000 * self.PRICES[model]["output"]
        )

        return cost

    def persist_cost(self, user_id: str, model: str, usage: dict, cost: float, metadata: dict):
        """æŒä¹…åŒ–æˆæœ¬è®°å½•"""
        record = {
            "timestamp": datetime.now().isoformat(),
            "user_id": user_id,
            "model": model,
            "tokens": usage,
            "cost": cost,
            "metadata": metadata
        }

        with open(self.db_file, "a") as f:
            f.write(json.dumps(record) + "\n")

    def check_budget(self):
        """æ£€æŸ¥é¢„ç®—"""
        if self.daily_cost > self.daily_limit:
            self.alert(f"ğŸš¨ è¶…è¿‡æ¯æ—¥é¢„ç®—: ${self.daily_cost:.4f} > ${self.daily_limit}")
            raise Exception(f"è¶…è¿‡æ¯æ—¥é¢„ç®—é™åˆ¶: ${self.daily_limit}")

    def alert(self, message: str):
        """å‘é€å‘Šè­¦"""
        print(message)

        if self.alert_webhook:
            try:
                import requests
                requests.post(
                    self.alert_webhook,
                    json={"text": message},
                    timeout=5
                )
            except Exception as e:
                print(f"å‘Šè­¦å‘é€å¤±è´¥: {e}")

    def get_report(self):
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š æˆæœ¬æŠ¥å‘Š")
        print("="*60)

        # æŒ‰ç”¨æˆ·ç»Ÿè®¡
        print("\næŒ‰ç”¨æˆ·:")
        for user_id in sorted(self.user_costs.keys()):
            print(f"  {user_id}: ${self.user_costs[user_id]:.4f}")

        # æŒ‰æ¨¡å‹ç»Ÿè®¡
        print("\næŒ‰æ¨¡å‹:")
        for model in sorted(self.model_costs.keys()):
            print(f"  {model}: ${self.model_costs[model]:.4f}")

        # æ€»è®¡
        print(f"\næ€»æˆæœ¬: ${self.daily_cost:.4f}")
        print(f"é¢„ç®—ä½¿ç”¨ç‡: {self.daily_cost / self.daily_limit * 100:.1f}%")
        print("="*60 + "\n")

# ä½¿ç”¨
from langchain_openai import ChatOpenAI

tracker = ProductionCostTracker(
    db_file="production_costs.jsonl",
    daily_limit=100.0,
    alert_webhook="https://hooks.slack.com/..."
)

llm = ChatOpenAI(model="gpt-4")

# æ¨¡æ‹Ÿå¤šä¸ªç”¨æˆ·çš„è°ƒç”¨
for user_id in ["user_001", "user_002", "user_003"]:
    for i in range(3):
        llm.invoke(
            f"é—®é¢˜{i}",
            config={
                "callbacks": [tracker],
                "metadata": {
                    "user_id": user_id,
                    "env": "production"
                }
            }
        )

# ç”ŸæˆæŠ¥å‘Š
tracker.get_report()
```

---

## æ€»ç»“

### æˆæœ¬è¿½è¸ªæ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **ç®€å•è¿½è¸ª** | å®ç°ç®€å• | åŠŸèƒ½æœ‰é™ | å¼€å‘è°ƒè¯• |
| **å¤šæ¨¡å‹è¿½è¸ª** | æ”¯æŒå¤šæ¨¡å‹ | éœ€è¦ç»´æŠ¤ä»·æ ¼è¡¨ | å¤šæ¨¡å‹åº”ç”¨ |
| **ç”¨æˆ·å½’å› ** | æŒ‰ç”¨æˆ·ç»Ÿè®¡ | éœ€è¦ä¼ é€’ user_id | å¤šç”¨æˆ·åº”ç”¨ |
| **é¢„ç®—å‘Šè­¦** | å®æ—¶å‘Šè­¦ | éœ€è¦å®šæœŸé‡ç½® | æˆæœ¬æ§åˆ¶ |
| **æˆæœ¬ä¼˜åŒ–** | æä¾›å»ºè®® | å»ºè®®å¯èƒ½ä¸å‡†ç¡® | æˆæœ¬ä¼˜åŒ– |
| **æŒä¹…åŒ–** | æ•°æ®ä¸ä¸¢å¤± | éœ€è¦å­˜å‚¨ç©ºé—´ | ç”Ÿäº§ç¯å¢ƒ |
| **å®Œæ•´ç³»ç»Ÿ** | åŠŸèƒ½å®Œæ•´ | å®ç°å¤æ‚ | ä¼ä¸šçº§åº”ç”¨ |

### æœ€ä½³å®è·µ

1. **å®æ—¶è¿½è¸ª**ï¼šåœ¨å›è°ƒä¸­å®æ—¶è®¡ç®—æˆæœ¬
2. **æŒä¹…åŒ–**ï¼šå°†æˆæœ¬æ•°æ®å†™å…¥æ•°æ®åº“
3. **é¢„ç®—å‘Šè­¦**ï¼šè®¾ç½®æ¯æ—¥/æ¯å°æ—¶é¢„ç®—é™åˆ¶
4. **ç”¨æˆ·å½’å› **ï¼šæŒ‰ç”¨æˆ·ç»Ÿè®¡æˆæœ¬ï¼Œä¾¿äºè®¡è´¹
5. **æˆæœ¬ä¼˜åŒ–**ï¼šå®šæœŸåˆ†ææˆæœ¬æ•°æ®ï¼Œä¼˜åŒ–ä½¿ç”¨

---

## å‚è€ƒèµ„æ–™

- [OpenAI Pricing](https://openai.com/pricing)
- [Anthropic Pricing](https://www.anthropic.com/pricing)
- [LangChain Custom Callbacks](https://python.langchain.com/docs/how_to/custom_callbacks/)
