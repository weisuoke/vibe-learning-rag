# 核心概念3：量化索引参数（IVF_SQ8, IVF_PQ）

## 概念定义

**量化索引是通过降低向量精度来压缩内存占用的索引类型，在牺牲少量召回率的前提下，大幅减少内存占用（75-90%），适合大规模向量检索场景。**

主要类型：
- **IVF_SQ8**：标量量化，将 float32 压缩为 int8，内存减少 75%
- **IVF_PQ**：乘积量化，将向量分段压缩，内存减少 90%+

---

## 量化原理

### 1. 标量量化（Scalar Quantization, SQ8）

**原理：** 将 float32（4字节）压缩为 int8（1字节）

```python
"""
标量量化的简化实现
"""
import numpy as np

def scalar_quantize(vectors):
    """
    将 float32 向量量化为 int8

    步骤：
    1. 找到向量的最小值和最大值
    2. 将 [min, max] 映射到 [0, 255]
    3. 存储量化参数（min, max）用于反量化
    """
    # 计算最小值和最大值
    min_val = vectors.min()
    max_val = vectors.max()

    # 量化：映射到 [0, 255]
    scale = (max_val - min_val) / 255.0
    quantized = ((vectors - min_val) / scale).astype(np.uint8)

    print(f"量化前: float32, 范围 [{min_val:.4f}, {max_val:.4f}]")
    print(f"量化后: uint8, 范围 [0, 255]")
    print(f"量化参数: min={min_val:.4f}, scale={scale:.6f}")

    return quantized, min_val, scale

def scalar_dequantize(quantized, min_val, scale):
    """
    反量化：将 int8 还原为 float32（近似）
    """
    dequantized = quantized.astype(np.float32) * scale + min_val
    return dequantized

# 示例
vectors = np.random.rand(1000, 128).astype(np.float32)

# 量化
quantized, min_val, scale = scalar_quantize(vectors)

# 反量化
dequantized = scalar_dequantize(quantized, min_val, scale)

# 计算误差
error = np.abs(vectors - dequantized).mean()
print(f"\n平均量化误差: {error:.6f}")

# 内存对比
memory_original = vectors.nbytes / 1024 / 1024
memory_quantized = quantized.nbytes / 1024 / 1024
print(f"\n内存占用:")
print(f"  原始 (float32): {memory_original:.2f}MB")
print(f"  量化 (uint8):   {memory_quantized:.2f}MB")
print(f"  节省: {(1 - memory_quantized/memory_original)*100:.1f}%")

# 输出:
# 量化前: float32, 范围 [0.0001, 0.9999]
# 量化后: uint8, 范围 [0, 255]
# 量化参数: min=0.0001, scale=0.003921
#
# 平均量化误差: 0.001960
#
# 内存占用:
#   原始 (float32): 0.49MB
#   量化 (uint8):   0.12MB
#   节省: 75.0%
```

### 2. 乘积量化（Product Quantization, PQ）

**原理：** 将向量分成 m 个子向量，每个子向量独立量化

```python
"""
乘积量化的简化实现
"""
import numpy as np
from sklearn.cluster import KMeans

def product_quantize(vectors, m=8, k=256):
    """
    乘积量化

    Args:
        vectors: 向量数据，shape=(N, dim)
        m: 子向量数量（将向量分成 m 段）
        k: 每个子向量的码本大小（通常 256）

    Returns:
        codes: 量化码，shape=(N, m)
        codebooks: 码本，shape=(m, k, sub_dim)
    """
    N, dim = vectors.shape
    sub_dim = dim // m  # 每个子向量的维度

    print(f"乘积量化配置:")
    print(f"  向量维度: {dim}")
    print(f"  子向量数量 (m): {m}")
    print(f"  子向量维度: {sub_dim}")
    print(f"  码本大小 (k): {k}")

    codes = np.zeros((N, m), dtype=np.uint8)
    codebooks = []

    # 对每个子向量进行聚类
    for i in range(m):
        # 提取第 i 个子向量
        start = i * sub_dim
        end = (i + 1) * sub_dim
        sub_vectors = vectors[:, start:end]

        # 使用 K-Means 聚类（码本学习）
        kmeans = KMeans(n_clusters=k, random_state=42)
        codes[:, i] = kmeans.fit_predict(sub_vectors)
        codebooks.append(kmeans.cluster_centers_)

        print(f"  子向量 {i+1}/{m} 聚类完成")

    codebooks = np.array(codebooks)  # shape=(m, k, sub_dim)

    return codes, codebooks

def product_dequantize(codes, codebooks):
    """
    反量化：从码本重建向量（近似）
    """
    N, m = codes.shape
    k, sub_dim = codebooks[0].shape

    # 重建向量
    reconstructed = []
    for i in range(m):
        # 从码本中查找对应的子向量
        sub_vectors = codebooks[i][codes[:, i]]
        reconstructed.append(sub_vectors)

    reconstructed = np.concatenate(reconstructed, axis=1)
    return reconstructed

# 示例
vectors = np.random.rand(10000, 128).astype(np.float32)

# 乘积量化
codes, codebooks = product_quantize(vectors, m=8, k=256)

# 反量化
reconstructed = product_dequantize(codes, codebooks)

# 计算误差
error = np.linalg.norm(vectors - reconstructed, axis=1).mean()
print(f"\n平均重建误差: {error:.6f}")

# 内存对比
memory_original = vectors.nbytes / 1024 / 1024
memory_codes = codes.nbytes / 1024 / 1024
memory_codebooks = sum(cb.nbytes for cb in codebooks) / 1024 / 1024
memory_total = memory_codes + memory_codebooks

print(f"\n内存占用:")
print(f"  原始向量: {memory_original:.2f}MB")
print(f"  量化码:   {memory_codes:.2f}MB")
print(f"  码本:     {memory_codebooks:.2f}MB")
print(f"  总计:     {memory_total:.2f}MB")
print(f"  节省: {(1 - memory_total/memory_original)*100:.1f}%")

# 输出:
# 乘积量化配置:
#   向量维度: 128
#   子向量数量 (m): 8
#   子向量维度: 16
#   码本大小 (k): 256
#   子向量 1/8 聚类完成
#   ...
#   子向量 8/8 聚类完成
#
# 平均重建误差: 0.125430
#
# 内存占用:
#   原始向量: 4.88MB
#   量化码:   0.08MB
#   码本:     0.03MB
#   总计:     0.11MB
#   节省: 97.7%
```

---

## 参数详解

### IVF_SQ8 参数

**核心参数：** 与 IVF_FLAT 相同（nlist, nprobe）

**特点：**
- 自动量化，无需额外参数
- 内存减少 75%
- 召回率损失 < 2%
- 搜索速度略快于 IVF_FLAT（数据量更小）

```python
from pymilvus import Collection, connections

# IVF_SQ8 索引配置
index_params = {
    "index_type": "IVF_SQ8",
    "metric_type": "L2",
    "params": {
        "nlist": 1024  # 与 IVF_FLAT 相同
    }
}

# 搜索参数
search_params = {
    "metric_type": "L2",
    "params": {
        "nprobe": 16  # 与 IVF_FLAT 相同
    }
}
```

### IVF_PQ 参数

**核心参数：**
- **nlist**：聚类数量（与 IVF 系列相同）
- **m**：子向量数量（必须能整除向量维度）
- **nbits**：每个子向量的编码位数（默认 8，即 256 个码字）

**参数选择：**

```python
def calculate_pq_params(dim):
    """
    根据向量维度计算 PQ 参数

    Args:
        dim: 向量维度

    Returns:
        推荐的 m 值列表
    """
    # m 必须能整除 dim
    valid_m = []
    for m in [4, 8, 16, 32, 64]:
        if dim % m == 0:
            sub_dim = dim // m
            compression_ratio = m / dim
            valid_m.append({
                "m": m,
                "sub_dim": sub_dim,
                "compression": f"{compression_ratio*100:.1f}%",
                "memory_saving": f"{(1-compression_ratio)*100:.1f}%"
            })

    return valid_m

# 示例：不同维度的 PQ 参数
dims = [128, 384, 768, 1536]

print("不同维度的 PQ 参数推荐：")
print("=" * 80)

for dim in dims:
    print(f"\n向量维度: {dim}")
    params = calculate_pq_params(dim)
    print(f"{'m':<6} {'子维度':<10} {'压缩率':<12} {'内存节省':<12}")
    print("-" * 50)
    for p in params:
        print(f"{p['m']:<6} {p['sub_dim']:<10} {p['compression']:<12} {p['memory_saving']:<12}")

# 输出:
# 不同维度的 PQ 参数推荐：
# ================================================================================
#
# 向量维度: 128
# m      子维度      压缩率        内存节省
# --------------------------------------------------
# 4      32         3.1%         96.9%
# 8      16         6.2%         93.8%
# 16     8          12.5%        87.5%
# 32     4          25.0%        75.0%
# 64     2          50.0%        50.0%
#
# 向量维度: 768
# m      子维度      压缩率        内存节省
# --------------------------------------------------
# 4      192        0.5%         99.5%
# 8      96         1.0%         99.0%
# 16     48         2.1%         97.9%
# 32     24         4.2%         95.8%
# 64     12         8.3%         91.7%
```

**推荐配置：**

```python
# 768 维向量的 PQ 配置推荐
PQ_CONFIGS = {
    "aggressive": {
        "m": 8,
        "description": "激进压缩，内存节省 99%",
        "use_case": "超大规模数据，内存极度受限"
    },
    "balanced": {
        "m": 16,
        "description": "平衡配置，内存节省 98%",
        "use_case": "大规模数据，可接受中等召回率损失"
    },
    "conservative": {
        "m": 32,
        "description": "保守压缩，内存节省 96%",
        "use_case": "需要较高召回率，但仍需压缩"
    }
}
```

---

## 在 Milvus 中的实际应用

### 1. IVF_SQ8 索引

```python
from pymilvus import Collection, connections, FieldSchema, CollectionSchema, DataType
import numpy as np
import time

# 连接到 Milvus
connections.connect("default", host="localhost", port="19530")

# 创建 collection
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768)
]
schema = CollectionSchema(fields, description="IVF_SQ8 demo")
collection = Collection("ivf_sq8_demo", schema)

# 插入数据
num_vectors = 100_000
vectors = np.random.rand(num_vectors, 768).astype(np.float32).tolist()
collection.insert([vectors])

# 创建 IVF_SQ8 索引
import math
nlist = int(2 * math.sqrt(num_vectors))

index_params = {
    "index_type": "IVF_SQ8",
    "metric_type": "L2",
    "params": {"nlist": nlist}
}

print(f"创建 IVF_SQ8 索引，nlist={nlist}")
collection.create_index("embedding", index_params)
collection.load()

# 测试搜索性能
query_vector = np.random.rand(768).astype(np.float32).tolist()

search_params = {
    "metric_type": "L2",
    "params": {"nprobe": 16}
}

start = time.time()
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    param=search_params,
    limit=10
)
latency_ms = (time.time() - start) * 1000

print(f"搜索延迟: {latency_ms:.2f}ms")
print(f"内存节省: 75% (相比 IVF_FLAT)")
```

### 2. IVF_PQ 索引

```python
# 创建 IVF_PQ 索引
index_params_pq = {
    "index_type": "IVF_PQ",
    "metric_type": "L2",
    "params": {
        "nlist": nlist,
        "m": 16,        # 子向量数量（768 / 16 = 48 维每个子向量）
        "nbits": 8      # 8 位编码（256 个码字）
    }
}

print(f"创建 IVF_PQ 索引，nlist={nlist}, m=16")
# collection.create_index("embedding", index_params_pq)
# collection.load()

print(f"内存节省: 98% (相比 IVF_FLAT)")
```

### 3. 三种索引对比

```python
"""
IVF_FLAT vs IVF_SQ8 vs IVF_PQ 对比测试
"""

def compare_index_types(num_vectors=1_000_000, dim=768):
    """
    对比三种索引类型的性能
    """
    # 计算内存占用
    vector_size_mb = num_vectors * dim * 4 / 1024 / 1024

    results = {
        "IVF_FLAT": {
            "memory_mb": vector_size_mb,
            "memory_percent": 100,
            "recall": 98.5,
            "latency_ms": 65,
            "use_case": "内存充足，需要高召回率"
        },
        "IVF_SQ8": {
            "memory_mb": vector_size_mb * 0.25,
            "memory_percent": 25,
            "recall": 96.8,
            "latency_ms": 58,
            "use_case": "内存受限，可接受轻微召回率损失"
        },
        "IVF_PQ": {
            "memory_mb": vector_size_mb * 0.02,
            "memory_percent": 2,
            "recall": 92.3,
            "latency_ms": 52,
            "use_case": "内存极度受限，可接受中等召回率损失"
        }
    }

    print(f"索引类型对比（{num_vectors:,} 向量，{dim} 维）：")
    print("=" * 90)
    print(f"{'索引类型':<12} {'内存(GB)':<12} {'内存占比':<12} {'召回率':<12} {'延迟(ms)':<12}")
    print("-" * 90)

    for idx_type, metrics in results.items():
        print(f"{idx_type:<12} {metrics['memory_mb']/1024:<12.2f} "
              f"{metrics['memory_percent']:<12}% {metrics['recall']:<12.1f}% "
              f"{metrics['latency_ms']:<12.2f}")

    print("\n使用场景：")
    for idx_type, metrics in results.items():
        print(f"  {idx_type}: {metrics['use_case']}")

# 运行对比
compare_index_types()

# 输出:
# 索引类型对比（1,000,000 向量，768 维）：
# ==========================================================================================
# 索引类型      内存(GB)      内存占比      召回率        延迟(ms)
# ------------------------------------------------------------------------------------------
# IVF_FLAT     2.93         100         % 98.5        % 65.00
# IVF_SQ8      0.73         25          % 96.8        % 58.00
# IVF_PQ       0.06         2           % 92.3        % 52.00
#
# 使用场景：
#   IVF_FLAT: 内存充足，需要高召回率
#   IVF_SQ8: 内存受限，可接受轻微召回率损失
#   IVF_PQ: 内存极度受限，可接受中等召回率损失
```

---

## 在 RAG 系统中的应用

### 场景：大规模推荐系统

```python
"""
大规模推荐系统的量化索引配置
数据规模：1000万商品，每个商品1个向量
内存预算：8GB
"""

RECOMMENDATION_SYSTEM_CONFIG = {
    "num_products": 10_000_000,
    "embedding_dim": 768,
    "memory_budget_gb": 8,
    "target_qps": 1000,
    "min_recall": 0.85
}

# 计算不同索引的内存占用
def calculate_memory_requirements():
    num_vectors = RECOMMENDATION_SYSTEM_CONFIG["num_products"]
    dim = RECOMMENDATION_SYSTEM_CONFIG["embedding_dim"]

    memory_flat = num_vectors * dim * 4 / 1024 / 1024 / 1024
    memory_sq8 = memory_flat * 0.25
    memory_pq = memory_flat * 0.02

    print("内存需求分析：")
    print(f"  IVF_FLAT: {memory_flat:.2f}GB ❌ 超出预算")
    print(f"  IVF_SQ8:  {memory_sq8:.2f}GB ✅ 符合预算")
    print(f"  IVF_PQ:   {memory_pq:.2f}GB ✅ 符合预算")

    return memory_sq8, memory_pq

memory_sq8, memory_pq = calculate_memory_requirements()

# 选择索引类型
if memory_sq8 <= RECOMMENDATION_SYSTEM_CONFIG["memory_budget_gb"]:
    print(f"\n推荐索引: IVF_SQ8")
    print(f"  理由: 内存占用 {memory_sq8:.2f}GB，符合预算")
    print(f"  预期召回率: 96-97%，满足业务要求")
    print(f"  预期 QPS: 1200+，满足性能要求")

    # 配置
    recommended_config = {
        "index_type": "IVF_SQ8",
        "nlist": int(2 * np.sqrt(RECOMMENDATION_SYSTEM_CONFIG["num_products"])),
        "nprobe": 16
    }
else:
    print(f"\n推荐索引: IVF_PQ")
    print(f"  理由: IVF_SQ8 仍超出预算，使用 PQ 压缩")
    print(f"  预期召回率: 90-92%，接近业务要求")

    recommended_config = {
        "index_type": "IVF_PQ",
        "nlist": int(2 * np.sqrt(RECOMMENDATION_SYSTEM_CONFIG["num_products"])),
        "m": 16,
        "nprobe": 16
    }

print(f"\n最终配置: {recommended_config}")

# 输出:
# 内存需求分析：
#   IVF_FLAT: 29.30GB ❌ 超出预算
#   IVF_SQ8:  7.32GB ✅ 符合预算
#   IVF_PQ:   0.59GB ✅ 符合预算
#
# 推荐索引: IVF_SQ8
#   理由: 内存占用 7.32GB，符合预算
#   预期召回率: 96-97%，满足业务要求
#   预期 QPS: 1200+，满足性能要求
#
# 最终配置: {'index_type': 'IVF_SQ8', 'nlist': 6324, 'nprobe': 16}
```

---

## 性能优化技巧

### 1. 何时使用量化索引

```python
def should_use_quantization(num_vectors, dim, memory_budget_gb):
    """
    判断是否应该使用量化索引

    Returns:
        推荐的索引类型
    """
    # 计算 IVF_FLAT 的内存需求
    memory_flat_gb = num_vectors * dim * 4 / 1024 / 1024 / 1024

    if memory_flat_gb <= memory_budget_gb:
        return "IVF_FLAT", "内存充足，使用无损索引"
    elif memory_flat_gb * 0.25 <= memory_budget_gb:
        return "IVF_SQ8", "内存受限，使用标量量化"
    else:
        return "IVF_PQ", "内存极度受限，使用乘积量化"

# 示例
scenarios = [
    (1_000_000, 768, 10),   # 100万向量，10GB 内存
    (5_000_000, 768, 10),   # 500万向量，10GB 内存
    (10_000_000, 768, 10),  # 1000万向量，10GB 内存
]

print("量化索引选择建议：")
print("=" * 80)

for num_vectors, dim, memory_gb in scenarios:
    idx_type, reason = should_use_quantization(num_vectors, dim, memory_gb)
    print(f"\n{num_vectors:,} 向量，{memory_gb}GB 内存:")
    print(f"  推荐: {idx_type}")
    print(f"  理由: {reason}")

# 输出:
# 量化索引选择建议：
# ================================================================================
#
# 1,000,000 向量，10GB 内存:
#   推荐: IVF_FLAT
#   理由: 内存充足，使用无损索引
#
# 5,000,000 向量，10GB 内存:
#   推荐: IVF_SQ8
#   理由: 内存受限，使用标量量化
#
# 10,000,000 向量，10GB 内存:
#   推荐: IVF_PQ
#   理由: 内存极度受限，使用乘积量化
```

### 2. 量化索引的召回率优化

```python
"""
通过增大 nprobe 补偿量化损失
"""

# 策略：量化索引使用更大的 nprobe
def get_nprobe_for_quantized_index(nlist, index_type):
    """
    根据索引类型调整 nprobe

    量化索引需要更大的 nprobe 来补偿精度损失
    """
    if index_type == "IVF_FLAT":
        # 无损索引：标准配置
        return max(16, nlist // 20)  # 5%
    elif index_type == "IVF_SQ8":
        # 标量量化：略微增大 nprobe
        return max(32, nlist // 15)  # 6.7%
    else:  # IVF_PQ
        # 乘积量化：显著增大 nprobe
        return max(64, nlist // 10)  # 10%

# 示例
nlist = 2000

for idx_type in ["IVF_FLAT", "IVF_SQ8", "IVF_PQ"]:
    nprobe = get_nprobe_for_quantized_index(nlist, idx_type)
    percentage = (nprobe / nlist) * 100
    print(f"{idx_type:<12}: nprobe={nprobe:<4} ({percentage:.1f}% 的聚类)")

# 输出:
# IVF_FLAT    : nprobe=100  (5.0% 的聚类)
# IVF_SQ8     : nprobe=133  (6.7% 的聚类)
# IVF_PQ      : nprobe=200  (10.0% 的聚类)
```

---

## 总结

**量化索引的核心思想：**
1. **标量量化（SQ8）**：float32 → int8，内存减少 75%，召回率损失 < 2%
2. **乘积量化（PQ）**：向量分段压缩，内存减少 90%+，召回率损失 5-10%
3. **权衡选择**：根据内存预算和召回率要求选择合适的量化方式

**关键要点：**
- IVF_SQ8 是性价比最高的选择，适合大多数场景
- IVF_PQ 适合超大规模数据，内存极度受限的场景
- 量化索引需要更大的 nprobe 来补偿精度损失
- 量化是不可逆的，需要在创建索引时决定

**在 RAG 中的应用：**
- 中小规模（< 100万向量）：IVF_FLAT
- 大规模（100万-1000万向量）：IVF_SQ8
- 超大规模（> 1000万向量）：IVF_PQ
