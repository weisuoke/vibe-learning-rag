# 核心概念01：HNSW索引

## 一句话定义

**HNSW（Hierarchical Navigable Small World）是一种基于分层图结构的向量索引算法，通过多层导航网络实现O(log n)复杂度的近似最近邻搜索，是2026年向量数据库的主流索引方案。**

---

## 详细原理讲解

### 1. 什么是HNSW？

HNSW全称Hierarchical Navigable Small World（分层可导航小世界图），由Yury Malkov和Dmitry Yashunin在2016年提出。

**核心思想**：
- 构建多层图结构，每层是一个导航网络
- 顶层稀疏，用于快速定位大致区域
- 底层密集，用于精确查找
- 通过"跳跃"式导航，避免遍历所有节点

**类比理解**：
```
高速公路网络：
顶层（L2）：国道 - 快速到达城市
中层（L1）：省道 - 到达区县
底层（L0）：街道 - 精确到门牌号

查找过程：
1. 从顶层国道快速定位城市
2. 切换到省道找到区县
3. 最后走街道到达目的地
```

---

### 2. HNSW的数学原理

#### 2.1 小世界网络（Small World）

**定义**：大多数节点不直接相连，但任意两个节点之间的最短路径很短。

**六度分离理论**：
- 任意两个人通过平均6个中间人就能建立联系
- HNSW利用这个特性构建导航网络

**数学表达**：
```
平均路径长度：L ∝ log(N)
其中N是节点数量
```

#### 2.2 分层结构（Hierarchical）

**层级分配**：
- 每个节点随机分配到某一层
- 层级越高，节点越少
- 层级分布遵循指数衰减

**概率公式**：
```python
# 节点分配到第l层的概率
P(level = l) = (1/M_L)^l

# M_L是层级归一化因子，通常取1/ln(M)
# M是每个节点的最大连接数
```

**层级示例**：
```
假设M=16，M_L=1/ln(16)≈0.36

L2: ████ (4个节点，稀疏)
L1: ████████████ (12个节点，中等)
L0: ████████████████████████████████ (100个节点，密集)
```

#### 2.3 可导航性（Navigable）

**贪心搜索**：
- 从当前节点出发，选择距离目标最近的邻居
- 重复直到无法找到更近的邻居

**伪代码**：
```python
def greedy_search(entry_point, query, layer):
    current = entry_point
    while True:
        neighbors = get_neighbors(current, layer)
        closest = min(neighbors, key=lambda n: distance(n, query))
        if distance(closest, query) >= distance(current, query):
            break
        current = closest
    return current
```

---

### 3. HNSW的构建过程

#### 3.1 插入算法

**步骤**：
1. 随机分配层级
2. 从顶层开始贪心搜索
3. 在每一层找到最近邻
4. 建立双向连接
5. 修剪连接（保持M个最近邻）

**详细流程**：
```python
def insert(element, M, M_max, ef_construction):
    # 1. 随机分配层级
    level = random_level(M_L)

    # 2. 从顶层开始搜索
    entry_point = get_entry_point()
    for lc in range(top_layer, level, -1):
        # 在当前层贪心搜索
        entry_point = greedy_search(entry_point, element, lc)

    # 3. 在目标层及以下建立连接
    for lc in range(level, -1, -1):
        # 找到ef_construction个候选邻居
        candidates = search_layer(entry_point, element, ef_construction, lc)

        # 选择M个最佳邻居
        neighbors = select_neighbors(element, candidates, M, lc)

        # 建立双向连接
        for neighbor in neighbors:
            add_bidirectional_link(element, neighbor, lc)

            # 修剪邻居的连接（保持M_max个）
            prune_connections(neighbor, M_max, lc)
```

#### 3.2 连接选择策略

**启发式选择**：
- 不仅考虑距离，还考虑多样性
- 避免所有连接都指向同一区域

**选择算法**：
```python
def select_neighbors_heuristic(element, candidates, M):
    """启发式选择M个最佳邻居"""
    selected = []
    candidates = sorted(candidates, key=lambda c: distance(element, c))

    for candidate in candidates:
        if len(selected) >= M:
            break

        # 检查是否与已选邻居过于接近
        is_diverse = True
        for s in selected:
            if distance(candidate, s) < distance(element, candidate):
                is_diverse = False
                break

        if is_diverse:
            selected.append(candidate)

    return selected
```

---

### 4. HNSW的查询过程

#### 4.1 查询算法

**步骤**：
1. 从顶层入口点开始
2. 在当前层贪心搜索到局部最优
3. 下降到下一层
4. 重复直到底层
5. 返回K个最近邻

**详细实现**：
```python
def search(query, K, ef_search):
    """
    query: 查询向量
    K: 返回的最近邻数量
    ef_search: 搜索时的候选数量
    """
    # 1. 从顶层入口点开始
    entry_point = get_entry_point()
    current_layer = top_layer

    # 2. 从顶层到第1层，贪心搜索
    for lc in range(current_layer, 0, -1):
        entry_point = greedy_search(entry_point, query, lc)

    # 3. 在底层（L0）搜索ef_search个候选
    candidates = search_layer(entry_point, query, ef_search, layer=0)

    # 4. 返回Top K
    return sorted(candidates, key=lambda c: distance(query, c))[:K]


def search_layer(entry_points, query, ef, layer):
    """在指定层搜索ef个候选"""
    visited = set()
    candidates = []  # 最小堆，按距离排序
    w = []  # 动态候选集

    # 初始化
    for ep in entry_points:
        dist = distance(query, ep)
        heappush(candidates, (-dist, ep))  # 最大堆
        heappush(w, (dist, ep))  # 最小堆
        visited.add(ep)

    # 迭代搜索
    while w:
        current_dist, current = heappop(w)

        # 如果当前距离大于候选集中的最远距离，停止
        if current_dist > -candidates[0][0]:
            break

        # 遍历当前节点的邻居
        for neighbor in get_neighbors(current, layer):
            if neighbor not in visited:
                visited.add(neighbor)
                dist = distance(query, neighbor)

                # 如果比候选集中最远的还近，或候选集未满
                if dist < -candidates[0][0] or len(candidates) < ef:
                    heappush(candidates, (-dist, neighbor))
                    heappush(w, (dist, neighbor))

                    # 保持候选集大小为ef
                    if len(candidates) > ef:
                        heappop(candidates)

    return [c[1] for c in candidates]
```

#### 4.2 时间复杂度分析

**理论复杂度**：
- 构建：O(N * log(N) * M)
- 查询：O(log(N) * M)

**实际性能**：
```
数据规模 vs 查询时间（M=16, ef_search=100）：
10万向量：5ms
100万向量：8ms
1000万向量：12ms
1亿向量：18ms
```

---

### 5. HNSW的关键参数

#### 5.1 M（连接数）

**定义**：每个节点在每一层的最大连接数

**影响**：
- M越大：召回率越高，内存占用越大，构建越慢
- M越小：内存占用小，但召回率下降

**推荐值**：
```
场景                M值      内存占用    召回率
快速原型            8-12     低          85-90%
生产环境（平衡）    16-32    中          92-96%
高精度场景          48-64    高          96-98%
```

**数学关系**：
```
内存占用 ≈ N * M * dimension * 4字节
查询时间 ∝ M * log(N)
```

#### 5.2 ef_construction（构建质量）

**定义**：构建索引时的候选数量

**影响**：
- ef_construction越大：索引质量越高，构建越慢
- ef_construction越小：构建快，但索引质量下降

**推荐值**：
```
场景                ef_construction    构建时间    索引质量
快速构建            64-100            快          中
生产环境（平衡）    100-200           中          高
高质量索引          200-400           慢          很高
```

**经验公式**：
```
ef_construction ≥ M * 2
通常取 ef_construction = M * 4 到 M * 8
```

#### 5.3 ef_search（查询候选数）

**定义**：查询时的候选数量

**影响**：
- ef_search越大：召回率越高，查询越慢
- ef_search越小：查询快，但召回率下降

**推荐值**：
```
场景                ef_search    查询延迟    召回率
实时查询            10-50        <5ms        85-92%
生产环境（平衡）    50-100       5-15ms      92-96%
高召回率场景        100-200      15-30ms     96-98%
```

**动态调整**：
```python
# 根据召回率要求动态调整ef_search
def adaptive_ef_search(target_recall):
    if target_recall < 0.90:
        return 50
    elif target_recall < 0.95:
        return 100
    else:
        return 200
```

---

### 6. HNSW vs 其他索引算法

#### 6.1 HNSW vs IVF

| 维度 | HNSW | IVF |
|------|------|-----|
| **数据结构** | 多层图 | 聚类中心 + 倒排表 |
| **查询方式** | 图遍历 | 先找cluster，再搜索 |
| **召回率** | 高（95%+） | 中（85-92%） |
| **查询延迟** | 低（<10ms） | 中（10-50ms） |
| **内存占用** | 高 | 低（可配合PQ压缩） |
| **构建时间** | 长 | 短 |
| **适用规模** | <1亿向量 | >1亿向量 |

#### 6.2 HNSW vs Flat（暴力搜索）

| 维度 | HNSW | Flat |
|------|------|------|
| **召回率** | 95-98% | 100% |
| **查询延迟** | O(log n) | O(n) |
| **内存占用** | 高（需存储图） | 低（只存向量） |
| **适用规模** | 百万-千万 | <10万 |

---

### 7. 在RAG中的应用

#### 7.1 文档检索场景

```python
# RAG中的HNSW应用
from sentence_transformers import SentenceTransformer
import hnswlib

# 1. 初始化embedding模型
model = SentenceTransformer('all-mpnet-base-v2')

# 2. 文档向量化
documents = [
    "RAG是检索增强生成技术",
    "向量存储用于语义检索",
    "HNSW是高效的索引算法"
]
doc_embeddings = model.encode(documents)

# 3. 构建HNSW索引
dimension = doc_embeddings.shape[1]  # 768
index = hnswlib.Index(space='cosine', dim=dimension)
index.init_index(
    max_elements=len(documents),
    ef_construction=200,  # 高质量构建
    M=32                  # 平衡配置
)
index.add_items(doc_embeddings, list(range(len(documents))))

# 4. 设置查询参数
index.set_ef(100)  # 查询时的候选数

# 5. 查询
query = "什么是RAG？"
query_embedding = model.encode(query)
labels, distances = index.knn_query(query_embedding, k=3)

# 6. 返回结果
for idx, dist in zip(labels[0], distances[0]):
    print(f"文档: {documents[idx]}, 距离: {dist:.4f}")
```

#### 7.2 性能优化策略

**批量查询**：
```python
# 批量查询提升吞吐量
queries = ["什么是RAG？", "如何使用向量存储？"]
query_embeddings = model.encode(queries)
labels, distances = index.knn_query(query_embeddings, k=3)
```

**增量更新**：
```python
# 动态添加新文档
new_doc = "HNSW支持增量更新"
new_embedding = model.encode(new_doc)
new_id = len(documents)
index.add_items(new_embedding, [new_id])
documents.append(new_doc)
```

---

### 8. 2025-2026最新实践

#### 8.1 GPU加速HNSW

**RAPIDS cuVS**（2025年发布）：
```python
# GPU加速的HNSW（实验性）
import cuvs

# 构建GPU索引
index = cuvs.Index(
    metric='cosine',
    dim=768,
    graph_degree=32,
    intermediate_graph_degree=64
)
index.build(doc_embeddings_gpu)

# GPU查询
labels, distances = index.search(query_embedding_gpu, k=10)
```

**性能提升**：
- 构建速度：10-50倍
- 查询速度：5-10倍
- 适用场景：超大规模数据（>1000万向量）

#### 8.2 分布式HNSW

**Milvus 2.4+**（2026年主流方案）：
```python
from pymilvus import connections, Collection

# 连接Milvus集群
connections.connect("default", host="localhost", port="19530")

# 创建collection with HNSW
collection = Collection("documents")
collection.create_index(
    field_name="embedding",
    index_params={
        "metric_type": "COSINE",
        "index_type": "HNSW",
        "params": {"M": 32, "efConstruction": 200}
    }
)

# 分布式查询
collection.load()
results = collection.search(
    data=[query_embedding],
    anns_field="embedding",
    param={"metric_type": "COSINE", "params": {"ef": 100}},
    limit=10
)
```

**优势**：
- 支持数十亿向量
- 水平扩展
- 高可用性

#### 8.3 混合索引策略

**2026年最佳实践**：
```python
# 热数据用HNSW，冷数据用IVF
class HybridIndex:
    def __init__(self):
        self.hot_index = hnswlib.Index(space='cosine', dim=768)
        self.cold_index = faiss.IndexIVFPQ(...)

    def search(self, query, k=10):
        # 先搜索热数据
        hot_results = self.hot_index.knn_query(query, k=k)

        # 如果热数据不足，搜索冷数据
        if len(hot_results) < k:
            cold_results = self.cold_index.search(query, k=k-len(hot_results))
            return merge(hot_results, cold_results)

        return hot_results
```

---

### 9. 常见问题与解决方案

#### 9.1 内存占用过大

**问题**：1000万向量，M=32，占用60GB内存

**解决方案**：
```python
# 方案1：降低M值
M = 16  # 内存减半，召回率下降2-3%

# 方案2：使用量化
index = hnswlib.Index(space='cosine', dim=768)
index.init_index(max_elements=10000000, M=16, ef_construction=100)
# 使用float16代替float32
embeddings_fp16 = embeddings.astype(np.float16)

# 方案3：分片存储
# 将数据分成多个小索引
```

#### 9.2 召回率不足

**问题**：召回率只有85%，目标90%+

**解决方案**：
```python
# 方案1：提高ef_search
index.set_ef(200)  # 从100提高到200

# 方案2：提高M值
M = 48  # 从32提高到48

# 方案3：提高ef_construction
ef_construction = 400  # 重建索引
```

#### 9.3 查询延迟过高

**问题**：P99延迟超过50ms

**解决方案**：
```python
# 方案1：降低ef_search
index.set_ef(50)  # 从100降到50

# 方案2：批量查询
# 单次查询10个query，而非10次单查询

# 方案3：预过滤
# 先用metadata过滤，再向量检索
```

---

### 10. 手写简化版HNSW

```python
import numpy as np
import heapq
from collections import defaultdict

class SimpleHNSW:
    """简化版HNSW实现（教学用途）"""

    def __init__(self, dim, M=16, ef_construction=200, max_level=5):
        self.dim = dim
        self.M = M
        self.ef_construction = ef_construction
        self.max_level = max_level
        self.ml = 1 / np.log(M)

        # 存储结构
        self.data = []  # 向量数据
        self.graph = defaultdict(lambda: defaultdict(set))  # graph[level][node_id] = {neighbors}
        self.entry_point = None
        self.max_layer = -1

    def _random_level(self):
        """随机分配层级"""
        level = 0
        while np.random.random() < self.ml and level < self.max_level:
            level += 1
        return level

    def _distance(self, a, b):
        """计算余弦距离"""
        return 1 - np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    def _search_layer(self, query, entry_points, ef, layer):
        """在指定层搜索"""
        visited = set()
        candidates = []
        w = []

        for ep in entry_points:
            dist = self._distance(query, self.data[ep])
            heapq.heappush(candidates, (-dist, ep))
            heapq.heappush(w, (dist, ep))
            visited.add(ep)

        while w:
            current_dist, current = heapq.heappop(w)

            if current_dist > -candidates[0][0]:
                break

            for neighbor in self.graph[layer][current]:
                if neighbor not in visited:
                    visited.add(neighbor)
                    dist = self._distance(query, self.data[neighbor])

                    if dist < -candidates[0][0] or len(candidates) < ef:
                        heapq.heappush(candidates, (-dist, neighbor))
                        heapq.heappush(w, (dist, neighbor))

                        if len(candidates) > ef:
                            heapq.heappop(candidates)

        return [(-dist, idx) for dist, idx in candidates]

    def add(self, vector):
        """添加向量"""
        idx = len(self.data)
        self.data.append(vector)
        level = self._random_level()

        if self.entry_point is None:
            self.entry_point = idx
            self.max_layer = level
            return

        # 从顶层搜索到目标层
        entry_points = [self.entry_point]
        for lc in range(self.max_layer, level, -1):
            candidates = self._search_layer(vector, entry_points, 1, lc)
            entry_points = [candidates[0][1]]

        # 在目标层及以下建立连接
        for lc in range(level, -1, -1):
            candidates = self._search_layer(vector, entry_points, self.ef_construction, lc)

            # 选择M个最近邻
            neighbors = sorted(candidates, key=lambda x: x[0])[:self.M]

            # 建立双向连接
            for _, neighbor_idx in neighbors:
                self.graph[lc][idx].add(neighbor_idx)
                self.graph[lc][neighbor_idx].add(idx)

                # 修剪邻居连接
                if len(self.graph[lc][neighbor_idx]) > self.M:
                    # 保留距离最近的M个
                    neighbor_vec = self.data[neighbor_idx]
                    neighbor_connections = list(self.graph[lc][neighbor_idx])
                    distances = [(self._distance(neighbor_vec, self.data[n]), n)
                                for n in neighbor_connections]
                    distances.sort()
                    self.graph[lc][neighbor_idx] = set([n for _, n in distances[:self.M]])

            entry_points = [idx]

        if level > self.max_layer:
            self.max_layer = level
            self.entry_point = idx

    def search(self, query, k=10, ef=50):
        """搜索K个最近邻"""
        if self.entry_point is None:
            return []

        # 从顶层到第1层贪心搜索
        entry_points = [self.entry_point]
        for lc in range(self.max_layer, 0, -1):
            candidates = self._search_layer(query, entry_points, 1, lc)
            entry_points = [candidates[0][1]]

        # 在底层搜索ef个候选
        candidates = self._search_layer(query, entry_points, ef, 0)

        # 返回Top K
        return sorted(candidates, key=lambda x: x[0])[:k]


# 使用示例
if __name__ == "__main__":
    # 创建索引
    index = SimpleHNSW(dim=128, M=16, ef_construction=200)

    # 添加向量
    vectors = np.random.randn(1000, 128)
    for v in vectors:
        v = v / np.linalg.norm(v)  # 归一化
        index.add(v)

    # 查询
    query = np.random.randn(128)
    query = query / np.linalg.norm(query)
    results = index.search(query, k=10, ef=50)

    print("Top 10最近邻:")
    for dist, idx in results:
        print(f"ID: {idx}, 距离: {dist:.4f}")
```

---

## 总结

**HNSW的核心优势**：
1. **高召回率**：95-98%，接近暴力搜索
2. **低延迟**：O(log n)复杂度，毫秒级查询
3. **可扩展**：支持百万-千万级向量
4. **增量更新**：支持动态添加向量

**适用场景**：
- 中小规模RAG系统（<1000万文档）
- 实时查询场景（延迟<20ms）
- 高召回率要求（>90%）

**2026年最佳实践**：
- M=32, ef_construction=200, ef_search=100
- 结合GPU加速（RAPIDS cuVS）
- 分布式部署（Milvus）
- 混合索引策略（热数据HNSW，冷数据IVF）

---

## 引用来源

1. **HNSW论文**：https://arxiv.org/abs/1603.09320
2. **hnswlib官方文档**：https://github.com/nmslib/hnswlib
3. **参数调优指南**：https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md
4. **2026年向量索引趋势**：https://oneuptime.com/blog/post/2026-01-30-vector-indexing/view
5. **OpenSearch HNSW实践**：https://opensearch.org/blog/a-practical-guide-to-selecting-hnsw-hyperparameters
6. **Milvus HNSW实现**：https://milvus.io/docs/index.md
7. **RAPIDS cuVS**：https://docs.rapids.ai/api/cuvs/stable/

---

**记住**：HNSW是2026年向量检索的主流算法，理解其原理和参数调优是构建高性能RAG系统的关键。
