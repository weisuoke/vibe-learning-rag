# 核心概念01：HyDE假设文档生成

> HyDE (Hypothetical Document Embeddings) - 用文档检索文档的革命性技术

---

## 什么是HyDE？

**HyDE (Hypothetical Document Embeddings)** 是2022年12月提出的Query改写技术，核心思想是：**生成假设性文档，用文档检索文档，而非用查询检索文档。**

**论文：** [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496)

**核心洞察：** 文档-文档匹配 > 查询-文档匹配

---

## 原理详解

### 1. 为什么需要HyDE？

**问题：查询-文档语义鸿沟**

```
用户查询：简短、模糊、口语化
   ↓
Embedding：信息密度低
   ↓
文档：详细、标准化、技术化
   ↓
Embedding：信息密度高
   ↓
结果：相似度匹配不准确
```

**示例：**

```python
# 用户查询
query = "Python异步怎么用？"
query_embedding = embed(query)  # [768维]，信息密度低

# 文档
doc = """
Python异步编程使用asyncio库实现。
通过async/await关键字定义协程函数。
使用asyncio.run()运行异步任务。
适用于I/O密集型场景，如网络请求、文件读写、数据库操作。
相比多线程，异步编程开销更小，性能更高。
常见的异步库包括aiohttp（异步HTTP客户端）、aiofiles（异步文件操作）等。
"""
doc_embedding = embed(doc)  # [768维]，信息密度高

# 相似度
similarity = cosine(query_embedding, doc_embedding)  # 可能不高
```

**问题根源：**
- 查询：5-10个词
- 文档：数百个词
- 压缩到同样的768维向量
- 信息密度差异导致匹配不准

### 2. HyDE的解决方案

**核心思想：** 生成假设文档，用文档检索文档

```
用户查询
   ↓
LLM生成假设文档（详细、标准化）
   ↓
Embedding：信息密度高
   ↓
与真实文档匹配
   ↓
相似度更高，检索更准确
```

**示例：**

```python
# 用户查询
query = "Python异步怎么用？"

# HyDE生成假设文档
hypothetical_doc = llm.generate(f"""
请根据以下问题，生成一个假设性的答案文档（200字左右）：

问题：{query}

假设答案：
""")

# 假设文档内容
hypothetical_doc = """
Python异步编程使用asyncio库实现。
通过async/await关键字定义协程函数。
使用asyncio.run()运行异步任务。
适用于I/O密集型场景，如网络请求、文件读写、数据库操作。
相比多线程，异步编程开销更小，性能更高。
常见的异步库包括aiohttp（异步HTTP客户端）、aiofiles（异步文件操作）等。
"""

# 用假设文档检索
hypothetical_embedding = embed(hypothetical_doc)  # [768维]，信息密度高
results = vector_search(hypothetical_embedding)  # 匹配度更高
```

**为什么有效？**
- 假设文档与真实文档表达方式相似
- 信息密度相当
- 都使用标准化、技术化的语言
- Embedding相似度更高

### 3. HyDE的数学直觉

**传统检索：**
```
similarity(query_emb, doc_emb) = cosine(E(query), E(doc))
```

**HyDE检索：**
```
similarity(hyde_emb, doc_emb) = cosine(E(LLM(query)), E(doc))
```

**关键差异：**
- `E(query)`：简短查询的embedding，信息密度低
- `E(LLM(query))`：假设文档的embedding，信息密度高
- `E(LLM(query))` 更接近 `E(doc)`

**实验数据（原论文）：**
- 传统检索：NDCG@10 = 0.52
- HyDE检索：NDCG@10 = 0.67
- 提升：+29%

---

## 实现细节

### 1. Prompt设计

**基础Prompt：**

```python
def generate_hypothetical_doc(query: str) -> str:
    """
    生成假设文档
    """
    prompt = f"""
请根据以下问题，生成一个假设性的答案文档。

问题：{query}

要求：
1. 使用技术文档的表达方式
2. 包含相关的技术术语和关键词
3. 结构清晰，逻辑连贯
4. 长度150-200字
5. 不需要完全准确，重点是表达方式

假设文档：
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5  # 中等创造性
    )

    return response.choices[0].message.content.strip()
```

**进阶Prompt（领域定制）：**

```python
def generate_hypothetical_doc_domain(query: str, domain: str = "技术") -> str:
    """
    生成领域定制的假设文档
    """
    domain_instructions = {
        "技术": "使用技术文档的表达方式，包含代码示例和技术术语",
        "医疗": "使用医学术语，包含症状、诊断、治疗方案",
        "法律": "使用法律术语，包含法条、判例、法律分析",
        "金融": "使用金融术语，包含数据、分析、投资建议"
    }

    instruction = domain_instructions.get(domain, "使用专业的表达方式")

    prompt = f"""
请根据以下问题，生成一个假设性的答案文档。

问题：{query}

要求：
1. {instruction}
2. 结构清晰，逻辑连贯
3. 长度150-200字
4. 不需要完全准确，重点是表达方式

假设文档：
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5
    )

    return response.choices[0].message.content.strip()
```

### 2. 长度控制

**为什么控制长度？**
- 太短：信息密度不够，效果不佳
- 太长：增加成本和延迟，边际效益递减

**最优长度：150-200字**

```python
def generate_hypothetical_doc_controlled(query: str, target_length: int = 200) -> str:
    """
    生成长度可控的假设文档
    """
    prompt = f"""
请根据以下问题，生成一个假设性的答案文档（约{target_length}字）。

问题：{query}

要求：
1. 使用技术文档的表达方式
2. 包含相关的技术术语和关键词
3. 结构清晰，逻辑连贯
4. 长度约{target_length}字
5. 不需要完全准确，重点是表达方式

假设文档：
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=int(target_length * 1.5)  # 预留空间
    )

    hypothetical_doc = response.choices[0].message.content.strip()

    # 长度检查
    if len(hypothetical_doc) < target_length * 0.7:
        # 太短，重新生成
        return generate_hypothetical_doc_controlled(query, target_length)

    return hypothetical_doc
```

### 3. 多样性控制

**问题：** 单一假设文档可能有偏差

**解决方案：** 生成多个假设文档，融合结果

```python
def generate_multiple_hypothetical_docs(query: str, num_docs: int = 3) -> list[str]:
    """
    生成多个假设文档
    """
    hypothetical_docs = []

    for i in range(num_docs):
        prompt = f"""
请根据以下问题，生成一个假设性的答案文档（约200字）。

问题：{query}

要求：
1. 使用技术文档的表达方式
2. 包含相关的技术术语和关键词
3. 结构清晰，逻辑连贯
4. 从不同角度回答问题
5. 不需要完全准确，重点是表达方式

假设文档：
"""

        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7 + i * 0.1  # 增加多样性
        )

        hypothetical_docs.append(response.choices[0].message.content.strip())

    return hypothetical_docs

# 使用多个假设文档检索
def search_with_multiple_hyde(query: str, vector_store, num_docs: int = 3) -> list:
    """
    使用多个假设文档检索
    """
    hypothetical_docs = generate_multiple_hypothetical_docs(query, num_docs)

    all_results = []
    for hyde_doc in hypothetical_docs:
        results = vector_store.similarity_search(hyde_doc, k=5)
        all_results.extend(results)

    # 去重
    unique_results = list({doc.page_content: doc for doc in all_results}.values())

    return unique_results[:10]
```

---

## 在RAG中的应用

### 场景1：技术文档问答

**问题：** 用户查询简短，文档详细

```python
from openai import OpenAI
from chromadb import Client

client = OpenAI()
chroma_client = Client()

def rag_with_hyde(query: str) -> str:
    """
    使用HyDE的RAG系统
    """
    # 1. 生成假设文档
    hypothetical_doc = generate_hypothetical_doc(query)

    print(f"原始查询：{query}")
    print(f"\n假设文档：\n{hypothetical_doc}\n")

    # 2. 用假设文档检索
    collection = chroma_client.get_collection("tech_docs")
    results = collection.query(
        query_texts=[hypothetical_doc],
        n_results=5
    )

    docs = results['documents'][0]

    # 3. LLM生成答案
    context = "\n\n".join(docs)

    prompt = f"""
基于以下文档回答问题：

{context}

问题：{query}

答案：
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

# 使用示例
query = "Python异步怎么用？"
answer = rag_with_hyde(query)
print(f"答案：\n{answer}")
```

**输出示例：**
```
原始查询：Python异步怎么用？

假设文档：
Python异步编程使用asyncio库实现。通过async/await关键字定义协程函数，
使用asyncio.run()运行异步任务。适用于I/O密集型场景，如网络请求、
文件读写、数据库操作。相比多线程，异步编程开销更小，性能更高。
常见的异步库包括aiohttp（异步HTTP客户端）、aiofiles（异步文件操作）等。

答案：
Python异步编程主要通过asyncio库实现。基本步骤如下：

1. 使用async def定义协程函数
2. 在协程函数中使用await调用其他协程
3. 使用asyncio.run()运行主协程

示例代码：
```python
import asyncio

async def main():
    print("Hello")
    await asyncio.sleep(1)
    print("World")

asyncio.run(main())
```

适用于I/O密集型场景，如网络请求、文件读写等。
```

### 场景2：对比传统检索

```python
def compare_traditional_vs_hyde(query: str, vector_store) -> dict:
    """
    对比传统检索和HyDE检索
    """
    # 传统检索
    traditional_results = vector_store.similarity_search(query, k=5)

    # HyDE检索
    hypothetical_doc = generate_hypothetical_doc(query)
    hyde_results = vector_store.similarity_search(hypothetical_doc, k=5)

    return {
        "query": query,
        "hypothetical_doc": hypothetical_doc,
        "traditional_results": [doc.page_content[:100] for doc in traditional_results],
        "hyde_results": [doc.page_content[:100] for doc in hyde_results],
        "traditional_scores": [doc.metadata.get('score', 0) for doc in traditional_results],
        "hyde_scores": [doc.metadata.get('score', 0) for doc in hyde_results]
    }

# 使用示例
query = "FastAPI异步路由"
comparison = compare_traditional_vs_hyde(query, vector_store)

print(f"查询：{comparison['query']}\n")
print(f"假设文档：\n{comparison['hypothetical_doc']}\n")
print(f"传统检索Top 3：")
for i, doc in enumerate(comparison['traditional_results'][:3], 1):
    print(f"{i}. {doc}...")
print(f"\nHyDE检索Top 3：")
for i, doc in enumerate(comparison['hyde_results'][:3], 1):
    print(f"{i}. {doc}...")
```

### 场景3：自适应HyDE

**根据查询特征决定是否使用HyDE**

```python
def adaptive_hyde_search(query: str, vector_store) -> list:
    """
    自适应HyDE检索
    """
    # 判断查询特征
    query_length = len(query)
    has_technical_terms = any(term in query for term in [
        "函数", "参数", "返回值", "类", "方法", "API", "async", "await"
    ])
    has_version = bool(re.search(r'\d+\.\d+', query))

    # 决策逻辑
    if query_length < 20 and not has_technical_terms and not has_version:
        # 简短模糊查询 → 使用HyDE
        print(f"使用HyDE策略（查询简短：{query_length}字）")
        hypothetical_doc = generate_hypothetical_doc(query)
        results = vector_store.similarity_search(hypothetical_doc, k=5)
    else:
        # 精确查询 → 直接检索
        print(f"使用直接检索（查询精确）")
        results = vector_store.similarity_search(query, k=5)

    return results

# 使用示例
queries = [
    "Python异步",  # 简短 → HyDE
    "asyncio.run()函数的参数和返回值",  # 精确 → 直接检索
    "FastAPI async def路由装饰器",  # 精确 → 直接检索
    "怎么让API快？"  # 简短 → HyDE
]

for query in queries:
    print(f"\n查询：{query}")
    results = adaptive_hyde_search(query, vector_store)
    print(f"检索到{len(results)}个文档")
```

---

## 优化策略

### 1. 缓存假设文档

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_generate_hypothetical_doc(query: str) -> str:
    """
    带缓存的假设文档生成
    """
    return generate_hypothetical_doc(query)

# 缓存命中率：30-50%（常见查询）
# 缓存命中延迟：<1ms
```

### 2. 使用快速模型

```python
def generate_hypothetical_doc_fast(query: str) -> str:
    """
    使用快速模型生成假设文档
    """
    prompt = f"""
请根据以下问题，生成一个假设性的答案文档（约200字）：

问题：{query}

假设文档：
"""

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # 使用快速模型
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=300
    )

    return response.choices[0].message.content.strip()

# 延迟：50-100ms（vs gpt-4的150ms）
# 成本：$0.001/query（vs gpt-4的$0.01/query）
```

### 3. 批量生成

```python
def generate_hypothetical_docs_batch(queries: list[str]) -> list[str]:
    """
    批量生成假设文档
    """
    prompts = [
        f"请根据以下问题，生成一个假设性的答案文档（约200字）：\n\n问题：{query}\n\n假设文档："
        for query in queries
    ]

    # 批量调用（如果API支持）
    responses = []
    for prompt in prompts:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=300
        )
        responses.append(response.choices[0].message.content.strip())

    return responses

# 批量处理可以优化API调用
```

---

## 效果评估

### 1. 召回率对比

```python
def evaluate_hyde_recall(test_queries: list[str],
                         ground_truth: dict,
                         vector_store) -> dict:
    """
    评估HyDE召回率
    """
    traditional_recall = []
    hyde_recall = []

    for query in test_queries:
        # 传统检索
        traditional_results = vector_store.similarity_search(query, k=10)
        traditional_docs = [doc.metadata['id'] for doc in traditional_results]

        # HyDE检索
        hypothetical_doc = generate_hypothetical_doc(query)
        hyde_results = vector_store.similarity_search(hypothetical_doc, k=10)
        hyde_docs = [doc.metadata['id'] for doc in hyde_results]

        # 计算召回率
        relevant_docs = ground_truth[query]
        traditional_recall.append(
            len(set(traditional_docs) & set(relevant_docs)) / len(relevant_docs)
        )
        hyde_recall.append(
            len(set(hyde_docs) & set(relevant_docs)) / len(relevant_docs)
        )

    return {
        "traditional_recall": sum(traditional_recall) / len(traditional_recall),
        "hyde_recall": sum(hyde_recall) / len(hyde_recall),
        "improvement": (sum(hyde_recall) - sum(traditional_recall)) / sum(traditional_recall)
    }

# 使用示例
test_queries = [
    "Python异步编程",
    "FastAPI路由",
    "数据库连接池"
]

ground_truth = {
    "Python异步编程": ["doc1", "doc5", "doc12"],
    "FastAPI路由": ["doc3", "doc8"],
    "数据库连接池": ["doc2", "doc7", "doc15"]
}

results = evaluate_hyde_recall(test_queries, ground_truth, vector_store)
print(f"传统检索召回率：{results['traditional_recall']:.2%}")
print(f"HyDE检索召回率：{results['hyde_recall']:.2%}")
print(f"提升：{results['improvement']:.2%}")
```

### 2. 2025-2026年生产数据

| 场景 | 传统检索 | HyDE检索 | 提升 |
|------|---------|---------|------|
| 简短查询（<20字） | 0.65 | 0.82 | +26% |
| 中等查询（20-50字） | 0.78 | 0.85 | +9% |
| 精确查询（>50字） | 0.91 | 0.88 | -3% |
| 技术术语查询 | 0.88 | 0.84 | -5% |
| 口语化查询 | 0.58 | 0.79 | +36% |

**结论：**
- HyDE适合简短、模糊、口语化查询
- 精确查询不需要HyDE
- 平均提升：20-35%

---

## 常见问题

### Q1: HyDE生成的假设文档不准确怎么办？

**A:** HyDE不需要假设文档完全准确，重点是表达方式。

```python
# 假设文档可能包含错误信息
hypothetical_doc = """
Python异步编程使用asyncio库实现。
通过async/await关键字定义协程函数。
使用asyncio.run()运行异步任务。
"""

# 但表达方式与真实文档相似
# 检索时匹配的是表达方式，不是内容准确性
```

### Q2: HyDE的成本如何？

**A:** 成本可控，主要是LLM调用。

```python
# 成本分析
costs = {
    "gpt-4": "$0.01-0.02/query",
    "gpt-3.5-turbo": "$0.001-0.002/query",
    "claude-haiku": "$0.0005-0.001/query"
}

# 优化策略
# 1. 使用快速模型（gpt-3.5-turbo）
# 2. 缓存常见查询（命中率30-50%）
# 3. 批量处理
```

### Q3: HyDE适合所有场景吗？

**A:** 不适合，有明确的适用边界。

```python
# 适合的场景
suitable_scenarios = [
    "简短模糊查询",
    "口语化查询",
    "查询与文档表达差异大"
]

# 不适合的场景
unsuitable_scenarios = [
    "精确查询（包含专业术语、版本号）",
    "查询已经很详细",
    "需要精确关键词匹配"
]
```

---

## 学习检查清单

### 理解层面
- [ ] 理解HyDE的核心原理
- [ ] 理解为什么文档-文档匹配优于查询-文档匹配
- [ ] 理解HyDE的适用场景和边界
- [ ] 理解假设文档的生成策略

### 实践层面
- [ ] 能实现基础HyDE
- [ ] 能设计有效的Prompt
- [ ] 能控制假设文档的长度和质量
- [ ] 能评估HyDE的效果

### 优化层面
- [ ] 能实现缓存优化
- [ ] 能选择合适的模型
- [ ] 能实现自适应HyDE
- [ ] 能对比传统检索和HyDE检索

---

## 参考资料

### 核心论文
- [HyDE: Precise Zero-Shot Dense Retrieval](https://arxiv.org/abs/2212.10496) - Original Paper, 2022.12

### 技术实现
- [LangChain HyDE](https://python.langchain.com/docs/modules/data_connection/retrievers/hyde) - LangChain实现
- [Haystack HyDE](https://haystack.deepset.ai/tutorials/query-expansion) - Haystack实现

### 生产实践
- [Advanced RAG Techniques](https://www.stack-ai.com/blog/advanced-rag-techniques) - Stack AI, 2025.09
- [Query Rewriting Strategies](https://www.elastic.co/search-labs/blog/query-rewriting-with-llms) - Elastic Labs, 2026.01

---

**版本：** v1.0 (2026年标准)
**最后更新：** 2026-02-16
**适用场景：** RAG开发、信息检索、查询优化
