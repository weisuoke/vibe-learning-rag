# 核心概念02：结构化日志设计

## 学习目标

深入理解结构化日志的设计原则，掌握日志schema设计、上下文管理和日志聚合策略。

---

## 第一部分：结构化日志的核心原则

### 1.1 什么是"结构化"？

**定义：** 结构化日志是将日志信息组织成键值对（通常是JSON）的格式，而不是自由文本。

**核心思想：** 日志是数据，不是文本。

#### 对比示例

**非结构化日志：**
```python
print("User john logged in from 192.168.1.1 at 10:30:45")
```

**问题：**
- 如何提取用户名？需要正则表达式
- 如何查询特定IP的登录？需要文本搜索
- 如何统计登录次数？需要解析整个文件

**结构化日志：**
```python
logger.info("user_login",
    user="john",
    ip="192.168.1.1",
    timestamp="10:30:45"
)
```

**输出：**
```json
{
  "event": "user_login",
  "user": "john",
  "ip": "192.168.1.1",
  "timestamp": "10:30:45"
}
```

**优势：**
- 提取用户名：`log.user`
- 查询特定IP：`SELECT * WHERE ip='192.168.1.1'`
- 统计登录次数：`COUNT(*) WHERE event='user_login'`

### 1.2 结构化日志的三个层次

#### 层次1：基本结构化

**特点：** 使用键值对，但没有统一schema

```python
# 不同地方使用不同字段名
logger.info("event1", user="john")
logger.info("event2", user_id="john")
logger.info("event3", username="john")
```

**问题：** 字段名不一致，难以查询

#### 层次2：Schema统一

**特点：** 定义统一的字段命名规范

```python
# 统一使用 user_id
logger.info("event1", user_id="john")
logger.info("event2", user_id="jane")
logger.info("event3", user_id="bob")
```

**优势：** 可以跨事件查询

#### 层次3：语义化Schema

**特点：** 字段有明确的语义和类型

```python
# 定义日志schema
class LogSchema:
    event: str          # 事件名称
    user_id: str        # 用户ID
    timestamp: datetime # 时间戳
    duration_ms: float  # 耗时（毫秒）
    status: str         # 状态（success/error）
```

**优势：** 类型安全，易于验证和分析

### 1.3 结构化日志的设计原则

#### 原则1：一致性（Consistency）

**定义：** 相同的信息在不同地方使用相同的字段名

```python
# ❌ 错误：字段名不一致
logger.info("login", user="john")
logger.info("logout", user_id="john")
logger.info("api_call", username="john")

# ✅ 正确：统一使用 user_id
logger.info("login", user_id="john")
logger.info("logout", user_id="john")
logger.info("api_call", user_id="john")
```

#### 原则2：完整性（Completeness）

**定义：** 日志包含足够的信息来回答问题

```python
# ❌ 错误：信息不完整
logger.error("api_call_failed")

# ✅ 正确：包含完整上下文
logger.error("api_call_failed",
    url="https://api.openai.com",
    method="POST",
    status_code=500,
    error="timeout",
    retry_count=3,
    duration_ms=5000
)
```

#### 原则3：简洁性（Conciseness）

**定义：** 只记录必要信息，避免冗余

```python
# ❌ 错误：记录过多无用信息
logger.info("api_call",
    url="https://api.openai.com",
    full_request_body=large_dict,  # 10MB
    full_response_body=large_dict,  # 10MB
    headers=all_headers
)

# ✅ 正确：只记录关键信息
logger.info("api_call",
    url="https://api.openai.com",
    request_size=len(request_body),
    response_size=len(response_body),
    status_code=200
)
```

#### 原则4：可查询性（Queryability）

**定义：** 字段设计便于查询和聚合

```python
# ❌ 错误：难以查询
logger.info("api_call", message="Called OpenAI API with model gpt-4")

# ✅ 正确：易于查询
logger.info("api_call",
    provider="openai",
    model="gpt-4",
    endpoint="/chat/completions"
)

# 查询示例
# SELECT * WHERE provider='openai' AND model='gpt-4'
```

---

## 第二部分：日志Schema设计

### 2.1 通用字段

**每条日志都应该包含的字段：**

```python
{
    "timestamp": "2024-01-15T10:30:45.123Z",  # ISO 8601格式
    "level": "info",                           # 日志级别
    "event": "user_login",                     # 事件名称
    "request_id": "req_abc123",                # 请求ID
    "user_id": "user_456",                     # 用户ID（如果有）
    "session_id": "session_789"                # 会话ID（如果有）
}
```

### 2.2 AI Agent API的Schema设计

#### HTTP请求日志

```python
{
    # 通用字段
    "timestamp": "2024-01-15T10:30:45.123Z",
    "level": "info",
    "event": "http_request",
    "request_id": "req_abc123",

    # HTTP特定字段
    "method": "POST",
    "path": "/api/chat",
    "status_code": 200,
    "duration_ms": 1500,

    # 用户信息
    "user_id": "user_456",
    "ip": "192.168.1.1",
    "user_agent": "Mozilla/5.0"
}
```

#### LLM调用日志

```python
{
    # 通用字段
    "timestamp": "2024-01-15T10:30:46.123Z",
    "level": "info",
    "event": "llm_call",
    "request_id": "req_abc123",

    # LLM特定字段
    "provider": "openai",
    "model": "gpt-4",
    "prompt_length": 100,
    "response_length": 200,
    "tokens_used": 150,
    "tokens_prompt": 50,
    "tokens_completion": 100,
    "duration_ms": 1200,
    "cost_usd": 0.0045,

    # 状态
    "status": "success"
}
```

#### RAG检索日志

```python
{
    # 通用字段
    "timestamp": "2024-01-15T10:30:45.500Z",
    "level": "info",
    "event": "rag_search",
    "request_id": "req_abc123",

    # RAG特定字段
    "query": "Python",
    "query_length": 6,
    "results_count": 5,
    "top_score": 0.95,
    "avg_score": 0.85,
    "duration_ms": 50,

    # 检索参数
    "k": 5,
    "threshold": 0.7
}
```

#### 错误日志

```python
{
    # 通用字段
    "timestamp": "2024-01-15T10:30:47.123Z",
    "level": "error",
    "event": "llm_call_failed",
    "request_id": "req_abc123",

    # 错误特定字段
    "error": "timeout",
    "error_type": "TimeoutError",
    "error_message": "Request timed out after 5s",
    "stack_trace": "...",

    # 上下文
    "provider": "openai",
    "model": "gpt-4",
    "retry_count": 3,
    "duration_ms": 5000
}
```

### 2.3 Schema定义和验证

#### 使用Pydantic定义Schema

```python
from pydantic import BaseModel, Field
from datetime import datetime
from typing import Optional

class BaseLogSchema(BaseModel):
    """基础日志schema"""
    timestamp: datetime
    level: str
    event: str
    request_id: str
    user_id: Optional[str] = None

class LLMCallLogSchema(BaseLogSchema):
    """LLM调用日志schema"""
    event: str = "llm_call"
    provider: str
    model: str
    tokens_used: int = Field(gt=0)
    duration_ms: float = Field(gt=0)
    cost_usd: float = Field(ge=0)
    status: str  # success/error

class RAGSearchLogSchema(BaseLogSchema):
    """RAG检索日志schema"""
    event: str = "rag_search"
    query: str
    results_count: int = Field(ge=0)
    top_score: float = Field(ge=0, le=1)
    duration_ms: float = Field(gt=0)

# 使用
log_data = LLMCallLogSchema(
    timestamp=datetime.now(),
    level="info",
    event="llm_call",
    request_id="req_123",
    provider="openai",
    model="gpt-4",
    tokens_used=150,
    duration_ms=1200,
    cost_usd=0.0045,
    status="success"
)

logger.info(log_data.event, **log_data.model_dump())
```

---

## 第三部分：上下文管理

### 3.1 什么是日志上下文？

**定义：** 日志上下文是一组在多条日志中共享的信息。

**示例：**
```python
# 一个HTTP请求产生多条日志
logger.info("request_start", request_id="req_123", user_id="user_456")
logger.info("rag_search", request_id="req_123", user_id="user_456")
logger.info("llm_call", request_id="req_123", user_id="user_456")
logger.info("request_end", request_id="req_123", user_id="user_456")
```

**问题：** 每条日志都要手动传递 `request_id` 和 `user_id`

**解决：** 使用上下文管理

### 3.2 contextvars实现上下文管理

#### 基本用法

```python
import structlog
from contextvars import ContextVar

# 定义上下文变量
request_id_var: ContextVar[str] = ContextVar("request_id")
user_id_var: ContextVar[str] = ContextVar("user_id")

# 设置上下文
request_id_var.set("req_123")
user_id_var.set("user_456")

# 获取上下文
request_id = request_id_var.get()
user_id = user_id_var.get()
```

#### structlog集成

```python
import structlog

# 配置structlog使用contextvars
structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,  # 合并上下文变量
        structlog.processors.JSONRenderer()
    ]
)

logger = structlog.get_logger()

# 绑定上下文
structlog.contextvars.bind_contextvars(
    request_id="req_123",
    user_id="user_456"
)

# 所有日志自动包含上下文
logger.info("request_start")  # 自动包含 request_id 和 user_id
logger.info("rag_search")     # 自动包含 request_id 和 user_id
logger.info("llm_call")       # 自动包含 request_id 和 user_id

# 清理上下文
structlog.contextvars.clear_contextvars()
```

### 3.3 上下文的层次结构

#### 请求级别上下文

```python
@app.middleware("http")
async def add_request_context(request: Request, call_next):
    # 绑定请求级别的上下文
    structlog.contextvars.bind_contextvars(
        request_id=str(uuid.uuid4()),
        method=request.method,
        path=request.url.path,
        user_id=request.headers.get("X-User-ID")
    )

    response = await call_next(request)

    structlog.contextvars.clear_contextvars()
    return response
```

#### 操作级别上下文

```python
async def call_llm(prompt: str, model: str):
    # 临时绑定操作级别的上下文
    structlog.contextvars.bind_contextvars(
        operation="llm_call",
        model=model
    )

    logger.info("llm_call_start")
    # ... LLM调用
    logger.info("llm_call_end")

    # 操作结束后，operation和model仍在上下文中
    # 需要手动清理或在请求结束时统一清理
```

### 3.4 异步环境中的上下文传递

#### 问题：异步任务中的上下文丢失

```python
@app.post("/api/chat")
async def chat(message: str, background_tasks: BackgroundTasks):
    # 主请求中有上下文
    logger.info("chat_start")  # ✅ 包含 request_id

    # 后台任务中上下文丢失
    background_tasks.add_task(save_history, message)

    return {"message": "OK"}

def save_history(message: str):
    logger.info("saving_history")  # ❌ 不包含 request_id
```

#### 解决：手动传递上下文

```python
@app.post("/api/chat")
async def chat(message: str, background_tasks: BackgroundTasks):
    # 获取当前上下文
    context = structlog.contextvars.get_contextvars()

    # 传递给后台任务
    background_tasks.add_task(save_history, message, context)

    return {"message": "OK"}

def save_history(message: str, context: dict):
    # 恢复上下文
    structlog.contextvars.bind_contextvars(**context)

    logger.info("saving_history")  # ✅ 包含 request_id

    # 清理上下文
    structlog.contextvars.clear_contextvars()
```

---

## 第四部分：日志聚合策略

### 4.1 什么是日志聚合？

**定义：** 将多条日志合并、统计、分析，生成有价值的信息。

**示例：**
```python
# 原始日志（10万条）
{"event": "llm_call", "duration_ms": 1200}
{"event": "llm_call", "duration_ms": 1500}
{"event": "llm_call", "duration_ms": 900}
...

# 聚合后的指标
{
    "llm_call_count": 100000,
    "llm_call_duration_avg": 1200,
    "llm_call_duration_p95": 2000,
    "llm_call_duration_p99": 3000
}
```

### 4.2 时间窗口聚合

#### 按分钟聚合

```python
# 查询：每分钟的LLM调用次数
SELECT
    DATE_TRUNC('minute', timestamp) as minute,
    COUNT(*) as call_count,
    AVG(duration_ms) as avg_duration
FROM logs
WHERE event = 'llm_call'
GROUP BY minute
ORDER BY minute
```

#### 按小时聚合

```python
# 查询：每小时的Token消耗
SELECT
    DATE_TRUNC('hour', timestamp) as hour,
    SUM(tokens_used) as total_tokens,
    SUM(cost_usd) as total_cost
FROM logs
WHERE event = 'llm_call'
GROUP BY hour
ORDER BY hour
```

### 4.3 维度聚合

#### 按模型聚合

```python
# 查询：不同模型的性能对比
SELECT
    model,
    COUNT(*) as call_count,
    AVG(duration_ms) as avg_duration,
    SUM(tokens_used) as total_tokens,
    SUM(cost_usd) as total_cost
FROM logs
WHERE event = 'llm_call'
GROUP BY model
```

#### 按用户聚合

```python
# 查询：用户的使用情况
SELECT
    user_id,
    COUNT(*) as request_count,
    SUM(tokens_used) as total_tokens,
    SUM(cost_usd) as total_cost
FROM logs
WHERE event = 'llm_call'
GROUP BY user_id
ORDER BY total_cost DESC
LIMIT 10
```

### 4.4 实时聚合

#### 使用Python实时聚合

```python
from collections import defaultdict
import json

class LogAggregator:
    """实时日志聚合器"""

    def __init__(self):
        self.metrics = defaultdict(lambda: {
            "count": 0,
            "total_duration": 0,
            "total_tokens": 0
        })

    def process_log(self, log_line: str):
        """处理一条日志"""
        log = json.loads(log_line)

        if log["event"] == "llm_call":
            model = log["model"]
            self.metrics[model]["count"] += 1
            self.metrics[model]["total_duration"] += log["duration_ms"]
            self.metrics[model]["total_tokens"] += log["tokens_used"]

    def get_metrics(self):
        """获取聚合指标"""
        result = {}
        for model, data in self.metrics.items():
            result[model] = {
                "count": data["count"],
                "avg_duration": data["total_duration"] / data["count"],
                "total_tokens": data["total_tokens"]
            }
        return result

# 使用
aggregator = LogAggregator()

with open("app.log") as f:
    for line in f:
        aggregator.process_log(line)

print(aggregator.get_metrics())
```

---

## 第五部分：日志存储和查询

### 5.1 日志存储选项

#### 选项1：文件存储

**优势：**
- 简单
- 无需额外服务
- 适合小规模应用

**劣势：**
- 查询慢
- 难以聚合
- 不支持分布式

**配置：**
```python
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    "app.log",
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)
```

#### 选项2：Elasticsearch

**优势：**
- 强大的查询能力
- 支持全文搜索
- 可视化（Kibana）

**劣势：**
- 需要额外服务
- 资源消耗大

**配置：**
```python
from elasticsearch import Elasticsearch

es = Elasticsearch(["http://localhost:9200"])

def send_to_elasticsearch(log_dict):
    es.index(index="logs", document=log_dict)
```

#### 选项3：云日志服务

**选项：**
- AWS CloudWatch
- Google Cloud Logging
- Datadog
- Sentry

**优势：**
- 托管服务
- 自动扩展
- 集成告警

### 5.2 日志查询示例

#### 使用jq查询JSON日志

```bash
# 查询特定请求的所有日志
jq 'select(.request_id == "req_123")' app.log

# 查询所有错误
jq 'select(.level == "error")' app.log

# 查询慢请求（>3秒）
jq 'select(.duration_ms > 3000)' app.log

# 统计事件数量
jq -s 'group_by(.event) | map({event: .[0].event, count: length})' app.log

# 计算平均耗时
jq -s 'map(select(.event == "llm_call")) | map(.duration_ms) | add / length' app.log
```

#### 使用Python查询

```python
import json

def query_logs(log_file: str, filter_func):
    """查询日志"""
    results = []

    with open(log_file) as f:
        for line in f:
            log = json.loads(line)
            if filter_func(log):
                results.append(log)

    return results

# 查询特定请求
logs = query_logs("app.log", lambda log: log.get("request_id") == "req_123")

# 查询慢请求
slow_logs = query_logs("app.log", lambda log: log.get("duration_ms", 0) > 3000)

# 统计
from collections import Counter

events = [log["event"] for log in query_logs("app.log", lambda log: True)]
event_counts = Counter(events)
print(event_counts)
```

---

## 总结

### 核心要点

1. **结构化原则**
   - 一致性：统一字段命名
   - 完整性：包含足够信息
   - 简洁性：只记录必要信息
   - 可查询性：便于查询和聚合

2. **Schema设计**
   - 定义通用字段
   - 为不同事件定义特定字段
   - 使用Pydantic验证

3. **上下文管理**
   - 使用contextvars自动传递
   - 区分请求级别和操作级别上下文
   - 处理异步环境中的上下文传递

4. **日志聚合**
   - 时间窗口聚合
   - 维度聚合
   - 实时聚合

### 最佳实践

1. 定义统一的日志schema
2. 使用contextvars管理上下文
3. 选择合适的存储方案
4. 实现日志聚合和分析
5. 定期清理旧日志

### 下一步

- 【核心概念03】：请求链路追踪
- 【实战代码】：完整的日志系统实现
