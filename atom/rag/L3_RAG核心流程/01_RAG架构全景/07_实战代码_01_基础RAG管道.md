# 实战代码 - 基础RAG管道

完整的端到端基础RAG实现，包含文档加载、分块、嵌入、存储、检索、生成全流程。

---

## 代码说明

**演示场景：** 构建一个简单的文档问答系统

**技术栈：**
- OpenAI API（嵌入和生成）
- ChromaDB（向量存储）
- LangChain（文本分块）
- pypdf（PDF解析）

**功能：**
1. 加载PDF文档
2. 文本分块
3. 向量嵌入
4. 存储到ChromaDB
5. 查询检索
6. LLM生成答案

---

## 完整代码

```python
"""
基础RAG管道实现
演示：端到端的文档问答系统
"""

import os
from typing import List, Dict
from dotenv import load_dotenv

# 导入必需的库
from openai import OpenAI
import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pypdf import PdfReader

# 加载环境变量
load_dotenv()

# ===== 1. 初始化组件 =====
print("=== 初始化组件 ===")

# OpenAI客户端
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ChromaDB客户端
chroma_client = chromadb.Client()
collection = chroma_client.create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine"}
)

# 文本分块器
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " ", ""]
)

print("✓ 组件初始化完成")

# ===== 2. 文档加载 =====
print("\n=== 文档加载 ===")

def load_pdf(file_path: str) -> str:
    """加载PDF文档"""
    reader = PdfReader(file_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() + "\n"
    return text

# 示例：使用文本代替PDF（实际使用时替换为PDF路径）
sample_document = """
RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。

RAG的核心组件包括：
1. 文档加载：从各种来源加载文档
2. 文本分块：将长文档切分成小块
3. 向量嵌入：将文本转换为向量
4. 向量存储：存储向量以便检索
5. 检索器：根据查询检索相关文档
6. 生成器：使用LLM生成答案

RAG的优势：
- 可以访问外部知识
- 不需要重新训练模型
- 答案可以追溯来源
- 知识可以实时更新

RAG的应用场景：
- 企业知识库问答
- 客服机器人
- 文档分析
- 研究助手
"""

print(f"✓ 文档加载完成，长度: {len(sample_document)} 字符")

# ===== 3. 文本分块 =====
print("\n=== 文本分块 ===")

chunks = text_splitter.split_text(sample_document)
print(f"✓ 文档分成 {len(chunks)} 个块")

# 显示前两个块
for i, chunk in enumerate(chunks[:2]):
    print(f"\n块 {i+1}:")
    print(f"{chunk[:100]}...")

# ===== 4. 向量嵌入 =====
print("\n=== 向量嵌入 ===")

def embed_texts(texts: List[str]) -> List[List[float]]:
    """批量嵌入文本"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [item.embedding for item in response.data]

embeddings = embed_texts(chunks)
print(f"✓ 生成 {len(embeddings)} 个向量")
print(f"✓ 向量维度: {len(embeddings[0])}")

# ===== 5. 存储到向量数据库 =====
print("\n=== 存储到向量数据库 ===")

collection.add(
    embeddings=embeddings,
    documents=chunks,
    ids=[f"doc_{i}" for i in range(len(chunks))]
)

print(f"✓ 存储 {len(chunks)} 个文档块")

# ===== 6. 查询检索 =====
print("\n=== 查询检索 ===")

def search(query: str, k: int = 3) -> List[Dict]:
    """检索相关文档"""
    # 嵌入查询
    query_embedding = embed_texts([query])[0]

    # 检索
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=k
    )

    # 格式化结果
    documents = []
    for i in range(len(results["documents"][0])):
        documents.append({
            "content": results["documents"][0][i],
            "distance": results["distances"][0][i]
        })

    return documents

# 测试查询
query = "RAG有哪些优势？"
print(f"查询: {query}")

retrieved_docs = search(query, k=3)
print(f"\n✓ 检索到 {len(retrieved_docs)} 个相关文档")

for i, doc in enumerate(retrieved_docs):
    print(f"\n文档 {i+1} (距离: {doc['distance']:.4f}):")
    print(f"{doc['content'][:150]}...")

# ===== 7. LLM生成答案 =====
print("\n=== LLM生成答案 ===")

def generate_answer(query: str, context_docs: List[Dict]) -> str:
    """生成答案"""
    # 构建上下文
    context = "\n\n".join([
        f"文档{i+1}:\n{doc['content']}"
        for i, doc in enumerate(context_docs)
    ])

    # 构建Prompt
    prompt = f"""你是一个专业的AI助手，请基于提供的上下文回答用户问题。

上下文信息：
{context}

用户问题：{query}

回答要求：
1. 仅基于上下文信息回答
2. 如果上下文中没有相关信息，明确说明
3. 保持简洁准确

答案："""

    # 调用LLM
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=300
    )

    return response.choices[0].message.content

answer = generate_answer(query, retrieved_docs)
print(f"\n问题: {query}")
print(f"\n答案:\n{answer}")

# ===== 8. 完整RAG流程封装 =====
print("\n=== 完整RAG流程封装 ===")

class SimpleRAG:
    """简单的RAG系统"""

    def __init__(self, client, collection):
        self.client = client
        self.collection = collection

    def query(self, question: str, k: int = 3) -> Dict:
        """查询RAG系统"""
        # 1. 嵌入查询
        query_embedding = self.embed_texts([question])[0]

        # 2. 检索文档
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )

        retrieved_docs = [
            {"content": results["documents"][0][i]}
            for i in range(len(results["documents"][0]))
        ]

        # 3. 生成答案
        answer = self.generate_answer(question, retrieved_docs)

        return {
            "question": question,
            "answer": answer,
            "sources": retrieved_docs
        }

    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """嵌入文本"""
        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [item.embedding for item in response.data]

    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """生成答案"""
        context = "\n\n".join([
            f"文档{i+1}:\n{doc['content']}"
            for i, doc in enumerate(context_docs)
        ])

        prompt = f"""基于以下上下文回答问题。

上下文：
{context}

问题：{query}

答案："""

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=300
        )

        return response.choices[0].message.content

# 使用封装的RAG系统
rag = SimpleRAG(client, collection)

# 测试多个查询
test_queries = [
    "RAG的核心组件有哪些？",
    "RAG有什么优势？",
    "RAG可以用在哪些场景？"
]

print("\n测试多个查询:")
for query in test_queries:
    result = rag.query(query)
    print(f"\n问题: {result['question']}")
    print(f"答案: {result['answer']}")
    print(f"来源数量: {len(result['sources'])}")

print("\n=== RAG管道演示完成 ===")
```

---

## 运行输出示例

```
=== 初始化组件 ===
✓ 组件初始化完成

=== 文档加载 ===
✓ 文档加载完成，长度: 456 字符

=== 文本分块 ===
✓ 文档分成 4 个块

块 1:
RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。

RAG的核心组件包括：
1. 文档加载：从各种来源加载文档...

块 2:
2. 文本分块：将长文档切分成小块
3. 向量嵌入：将文本转换为向量
4. 向量存储：存储向量以便检索...

=== 向量嵌入 ===
✓ 生成 4 个向量
✓ 向量维度: 1536

=== 存储到向量数据库 ===
✓ 存储 4 个文档块

=== 查询检索 ===
查询: RAG有哪些优势？

✓ 检索到 3 个相关文档

文档 1 (距离: 0.2341):
RAG的优势：
- 可以访问外部知识
- 不需要重新训练模型
- 答案可以追溯来源
- 知识可以实时更新...

=== LLM生成答案 ===

问题: RAG有哪些优势？

答案:
RAG的主要优势包括：
1. 可以访问外部知识，扩展LLM的知识范围
2. 不需要重新训练模型，降低成本
3. 答案可以追溯来源，提高可信度
4. 知识可以实时更新，保持时效性

=== 完整RAG流程封装 ===

测试多个查询:

问题: RAG的核心组件有哪些？
答案: RAG的核心组件包括文档加载、文本分块、向量嵌入、向量存储、检索器和生成器。
来源数量: 3

问题: RAG有什么优势？
答案: RAG的优势包括可以访问外部知识、不需要重新训练模型、答案可以追溯来源、知识可以实时更新。
来源数量: 3

问题: RAG可以用在哪些场景？
答案: RAG可以应用在企业知识库问答、客服机器人、文档分析和研究助手等场景。
来源数量: 3

=== RAG管道演示完成 ===
```

---

## 关键要点

**1. 组件初始化**
- OpenAI客户端（嵌入和生成）
- ChromaDB（向量存储）
- 文本分块器（LangChain）

**2. 数据处理流程**
```
文档 → 分块 → 嵌入 → 存储
```

**3. 查询流程**
```
查询 → 嵌入 → 检索 → 生成答案
```

**4. 最佳实践**
- 分块大小：500字符
- 重叠：50字符
- 检索数量：3个文档
- Temperature：0.7（平衡创造性和准确性）

---

## 扩展建议

**1. 添加文档元数据**
```python
collection.add(
    embeddings=embeddings,
    documents=chunks,
    metadatas=[{"source": "doc.pdf", "page": i} for i in range(len(chunks))],
    ids=[f"doc_{i}" for i in range(len(chunks))]
)
```

**2. 添加缓存**
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_embed(text: str):
    return embed_texts([text])[0]
```

**3. 添加流式输出**
```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

---

## 下一步

学习 **07_实战代码_02_混合检索与重排序.md**，提升检索质量。
