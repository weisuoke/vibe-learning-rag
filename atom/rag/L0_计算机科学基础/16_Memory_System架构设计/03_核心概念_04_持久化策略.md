# 核心概念：持久化策略

> **理解数据持久化的权衡与实现**

---

## 概念定义

**持久化（Persistence）= 将内存中的数据保存到非易失性存储（磁盘），确保数据在系统重启后不丢失**

### 为什么需要持久化？

**内存的问题**：
- 易失性（Volatile）：断电或崩溃后数据丢失
- 容量有限：无法存储所有数据
- 成本高：内存比磁盘贵 100 倍

**持久化的价值**：
- 数据安全：防止数据丢失
- 容量扩展：磁盘容量远大于内存
- 跨会话：数据可以在不同会话间共享

---

## 三种持久化策略

### 1. Write-through（写穿）

**定义**：写入数据时，同时写入缓存和磁盘

```python
class WriteThroughCache:
    """Write-through 缓存"""
    def __init__(self, storage_path: str):
        self.cache = {}
        self.storage_path = Path(storage_path)

    def put(self, key: str, value: any):
        """写入数据（同时写缓存和磁盘）"""
        # 1. 写入缓存
        self.cache[key] = value

        # 2. 立即写入磁盘
        self._save_to_disk(key, value)

    def _save_to_disk(self, key: str, value: any):
        """持久化到磁盘"""
        file_path = self.storage_path / f"{key}.json"
        file_path.write_text(json.dumps(value))

    def get(self, key: str) -> any:
        """读取数据（优先从缓存）"""
        if key in self.cache:
            return self.cache[key]

        # 缓存未命中，从磁盘加载
        file_path = self.storage_path / f"{key}.json"
        if file_path.exists():
            value = json.loads(file_path.read_text())
            self.cache[key] = value
            return value

        return None
```

**优点**：
- 数据安全：写入后立即持久化
- 一致性强：缓存和磁盘始终同步

**缺点**：
- 性能低：每次写入都要等待磁盘 I/O
- 延迟高：磁盘写入通常需要 1-10ms

**适用场景**：
- 关键数据（用户偏好、支付信息）
- 强一致性要求
- 写入频率低

---

### 2. Write-back（写回）

**定义**：写入数据时，只写缓存，延迟批量写入磁盘

```python
import time
from pathlib import Path
import json

class WriteBackCache:
    """Write-back 缓存"""
    def __init__(self, storage_path: str, flush_interval: int = 5):
        self.cache = {}
        self.dirty_keys = set()  # 记录未持久化的键
        self.storage_path = Path(storage_path)
        self.flush_interval = flush_interval
        self.last_flush = time.time()

    def put(self, key: str, value: any):
        """写入数据（只写缓存）"""
        # 1. 写入缓存
        self.cache[key] = value

        # 2. 标记为脏数据
        self.dirty_keys.add(key)

        # 3. 检查是否需要刷新
        if time.time() - self.last_flush > self.flush_interval:
            self.flush()

    def flush(self):
        """批量持久化"""
        if not self.dirty_keys:
            return

        print(f"批量持久化 {len(self.dirty_keys)} 个键")

        for key in self.dirty_keys:
            if key in self.cache:
                self._save_to_disk(key, self.cache[key])

        self.dirty_keys.clear()
        self.last_flush = time.time()

    def _save_to_disk(self, key: str, value: any):
        """持久化到磁盘"""
        file_path = self.storage_path / f"{key}.json"
        file_path.write_text(json.dumps(value))

    def get(self, key: str) -> any:
        """读取数据"""
        if key in self.cache:
            return self.cache[key]

        # 从磁盘加载
        file_path = self.storage_path / f"{key}.json"
        if file_path.exists():
            value = json.loads(file_path.read_text())
            self.cache[key] = value
            return value

        return None

# 使用示例
cache = WriteBackCache("cache_data", flush_interval=5)

# 写入 100 次（只写缓存，不写磁盘）
for i in range(100):
    cache.put(f"key_{i}", f"value_{i}")

# 5 秒后自动批量持久化
time.sleep(6)
cache.put("trigger", "flush")  # 触发刷新
```

**优点**：
- 性能高：写入只需要更新内存
- 延迟低：不需要等待磁盘 I/O
- 批量优化：减少磁盘写入次数

**缺点**：
- 数据风险：崩溃时可能丢失未持久化的数据
- 一致性弱：缓存和磁盘可能不同步

**适用场景**：
- 非关键数据（访问日志、统计数据）
- 高频写入
- 可接受少量数据丢失

---

### 3. Write-ahead Log (WAL)

**定义**：先写日志，再写数据，兼顾性能和安全

```python
import json
from pathlib import Path
from datetime import datetime

class WALCache:
    """Write-ahead Log 缓存"""
    def __init__(self, storage_path: str, wal_path: str):
        self.cache = {}
        self.storage_path = Path(storage_path)
        self.wal_path = Path(wal_path)
        self.wal_file = open(self.wal_path, 'a')

        # 启动时恢复数据
        self._recover_from_wal()

    def put(self, key: str, value: any):
        """写入数据（先写 WAL，再写缓存）"""
        # 1. 先写 WAL（顺序写，快速）
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "operation": "PUT",
            "key": key,
            "value": value
        }
        self.wal_file.write(json.dumps(log_entry) + "\n")
        self.wal_file.flush()  # 强制刷新到磁盘

        # 2. 写入缓存
        self.cache[key] = value

    def flush(self):
        """批量持久化到数据文件"""
        for key, value in self.cache.items():
            file_path = self.storage_path / f"{key}.json"
            file_path.write_text(json.dumps(value))

        # 清空 WAL
        self.wal_file.close()
        self.wal_path.unlink()
        self.wal_file = open(self.wal_path, 'a')

    def _recover_from_wal(self):
        """从 WAL 恢复数据"""
        if not self.wal_path.exists():
            return

        print("从 WAL 恢复数据...")
        with open(self.wal_path, 'r') as f:
            for line in f:
                entry = json.loads(line)
                if entry["operation"] == "PUT":
                    self.cache[entry["key"]] = entry["value"]

        print(f"恢复了 {len(self.cache)} 个键")

    def get(self, key: str) -> any:
        """读取数据"""
        return self.cache.get(key)

# 使用示例
cache = WALCache("cache_data", "wal.log")

cache.put("key1", "value1")
cache.put("key2", "value2")

# 模拟崩溃
del cache

# 重新启动，自动恢复
cache = WALCache("cache_data", "wal.log")
print(cache.get("key1"))  # value1（已恢复）
```

**优点**：
- 性能好：WAL 是顺序写，比随机写快
- 安全性高：崩溃后可以从 WAL 恢复
- 一致性强：保证数据不丢失

**缺点**：
- 实现复杂：需要维护 WAL 和数据文件
- 空间开销：需要额外的 WAL 文件

**适用场景**：
- 数据库系统（PostgreSQL、MySQL）
- 需要高性能和高可靠性
- 关键业务数据

---

## 策略对比

| 策略 | 写入延迟 | 数据安全 | 实现复杂度 | 适用场景 |
|------|----------|----------|-----------|----------|
| **Write-through** | 高（1-10ms） | 高 | 简单 | 关键数据 |
| **Write-back** | 低（< 1ms） | 低 | 中等 | 非关键数据 |
| **WAL** | 中等（1-5ms） | 高 | 复杂 | 数据库系统 |

---

## AI Agent 应用

### 1. 用户偏好持久化（Write-through）

```python
import psycopg2
from psycopg2.extras import Json

class UserPreferenceStore:
    """用户偏好存储（Write-through 策略）"""
    def __init__(self, db_config: dict):
        self.cache = {}
        self.conn = psycopg2.connect(**db_config)

    def save_preference(self, user_id: str, key: str, value: any):
        """保存用户偏好（立即持久化）"""
        # 1. 写入缓存
        cache_key = f"{user_id}:{key}"
        self.cache[cache_key] = value

        # 2. 立即写入数据库
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO user_preferences (user_id, key, value)
            VALUES (%s, %s, %s)
            ON CONFLICT (user_id, key)
            DO UPDATE SET value = EXCLUDED.value
        """, (user_id, key, Json(value)))
        self.conn.commit()

        print(f"✅ 用户偏好已持久化: {user_id}:{key}")

    def load_preference(self, user_id: str, key: str) -> any:
        """加载用户偏好"""
        cache_key = f"{user_id}:{key}"

        # 先查缓存
        if cache_key in self.cache:
            return self.cache[cache_key]

        # 从数据库加载
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT value FROM user_preferences
            WHERE user_id = %s AND key = %s
        """, (user_id, key))
        result = cursor.fetchone()

        if result:
            value = result[0]
            self.cache[cache_key] = value
            return value

        return None

# 使用示例
store = UserPreferenceStore({
    "dbname": "agent_memory",
    "user": "postgres",
    "password": "password"
})

# 保存用户偏好（立即持久化）
store.save_preference("user_123", "language", "zh-CN")
store.save_preference("user_123", "theme", "dark")

# 加载用户偏好
lang = store.load_preference("user_123", "language")
print(f"用户语言: {lang}")
```

### 2. 对话历史持久化（Write-back）

```python
import time
from collections import deque

class ConversationStore:
    """对话历史存储（Write-back 策略）"""
    def __init__(self, db_config: dict, flush_interval: int = 60):
        self.cache = {}  # user_id → deque of messages
        self.dirty_users = set()
        self.conn = psycopg2.connect(**db_config)
        self.flush_interval = flush_interval
        self.last_flush = time.time()

    def add_message(self, user_id: str, role: str, content: str):
        """添加对话消息（只写缓存）"""
        # 1. 写入缓存
        if user_id not in self.cache:
            self.cache[user_id] = deque(maxlen=100)

        self.cache[user_id].append({
            "role": role,
            "content": content,
            "timestamp": time.time()
        })

        # 2. 标记为脏数据
        self.dirty_users.add(user_id)

        # 3. 定期刷新
        if time.time() - self.last_flush > self.flush_interval:
            self.flush()

    def flush(self):
        """批量持久化"""
        if not self.dirty_users:
            return

        print(f"批量持久化 {len(self.dirty_users)} 个用户的对话")

        cursor = self.conn.cursor()
        for user_id in self.dirty_users:
            messages = list(self.cache[user_id])
            cursor.execute("""
                INSERT INTO conversations (user_id, messages, updated_at)
                VALUES (%s, %s, NOW())
                ON CONFLICT (user_id)
                DO UPDATE SET messages = EXCLUDED.messages, updated_at = NOW()
            """, (user_id, Json(messages)))

        self.conn.commit()
        self.dirty_users.clear()
        self.last_flush = time.time()

    def get_history(self, user_id: str, n: int = 10) -> list:
        """获取对话历史"""
        if user_id in self.cache:
            return list(self.cache[user_id])[-n:]

        # 从数据库加载
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT messages FROM conversations WHERE user_id = %s
        """, (user_id,))
        result = cursor.fetchone()

        if result:
            messages = result[0]
            self.cache[user_id] = deque(messages, maxlen=100)
            return messages[-n:]

        return []

# 使用示例
store = ConversationStore(db_config, flush_interval=60)

# 添加对话（只写缓存）
store.add_message("user_123", "user", "你好")
store.add_message("user_123", "assistant", "你好！")
store.add_message("user_123", "user", "什么是 RAG？")

# 60 秒后自动批量持久化
time.sleep(61)
store.add_message("user_123", "assistant", "RAG 是...")  # 触发刷新
```

### 3. LangGraph Checkpointer（WAL 模式）

```python
from langgraph.checkpoint.postgres import PostgresSaver
from psycopg import AsyncConnection

class LangGraphMemory:
    """LangGraph 跨会话记忆（WAL 模式）"""
    def __init__(self, db_conn: AsyncConnection):
        # PostgresSaver 内部使用 WAL 模式
        self.checkpointer = PostgresSaver(db_conn)

    async def save_state(self, thread_id: str, state: dict):
        """保存状态（WAL 模式）"""
        # 1. 先写 WAL（事务日志）
        # 2. 再写入数据表
        await self.checkpointer.aput(
            config={"configurable": {"thread_id": thread_id}},
            checkpoint=state
        )

    async def load_state(self, thread_id: str) -> dict:
        """加载状态"""
        checkpoint = await self.checkpointer.aget(
            config={"configurable": {"thread_id": thread_id}}
        )
        return checkpoint if checkpoint else {}

# 使用示例
async def main():
    db_conn = await AsyncConnection.connect("postgresql://...")
    memory = LangGraphMemory(db_conn)

    # 保存状态
    await memory.save_state("user_123", {
        "messages": [
            {"role": "user", "content": "你好"},
            {"role": "assistant", "content": "你好！"}
        ]
    })

    # 加载状态
    state = await memory.load_state("user_123")
    print(state)
```

---

## 混合策略

### 分层持久化

```python
class HybridPersistenceCache:
    """混合持久化策略"""
    def __init__(self, db_config: dict):
        self.cache = {}
        self.dirty_keys = set()
        self.conn = psycopg2.connect(**db_config)

    def put(self, key: str, value: any, critical: bool = False):
        """写入数据（根据重要性选择策略）"""
        # 1. 写入缓存
        self.cache[key] = value

        if critical:
            # 关键数据：Write-through（立即持久化）
            self._save_to_db(key, value)
            print(f"✅ 关键数据立即持久化: {key}")
        else:
            # 非关键数据：Write-back（延迟持久化）
            self.dirty_keys.add(key)
            print(f"⏳ 非关键数据标记为待持久化: {key}")

    def _save_to_db(self, key: str, value: any):
        """持久化到数据库"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO cache_data (key, value)
            VALUES (%s, %s)
            ON CONFLICT (key)
            DO UPDATE SET value = EXCLUDED.value
        """, (key, Json(value)))
        self.conn.commit()

    def flush(self):
        """批量持久化非关键数据"""
        if not self.dirty_keys:
            return

        print(f"批量持久化 {len(self.dirty_keys)} 个非关键数据")

        cursor = self.conn.cursor()
        for key in self.dirty_keys:
            if key in self.cache:
                cursor.execute("""
                    INSERT INTO cache_data (key, value)
                    VALUES (%s, %s)
                    ON CONFLICT (key)
                    DO UPDATE SET value = EXCLUDED.value
                """, (key, Json(self.cache[key])))

        self.conn.commit()
        self.dirty_keys.clear()

    def get(self, key: str) -> any:
        """读取数据"""
        return self.cache.get(key)

# 使用示例
cache = HybridPersistenceCache(db_config)

# 关键数据：立即持久化
cache.put("user_payment", {"amount": 100}, critical=True)

# 非关键数据：延迟持久化
cache.put("access_log", {"ip": "1.2.3.4"}, critical=False)
cache.put("view_count", 123, critical=False)

# 批量持久化非关键数据
cache.flush()
```

---

## 性能优化

### 1. 批量写入

```python
def batch_write(cache: dict, batch_size: int = 100):
    """批量写入（减少磁盘 I/O）"""
    keys = list(cache.keys())

    for i in range(0, len(keys), batch_size):
        batch = keys[i:i + batch_size]

        # 批量写入
        cursor = conn.cursor()
        cursor.executemany("""
            INSERT INTO cache_data (key, value)
            VALUES (%s, %s)
            ON CONFLICT (key)
            DO UPDATE SET value = EXCLUDED.value
        """, [(k, Json(cache[k])) for k in batch])

        conn.commit()
        print(f"批量写入 {len(batch)} 个键")
```

### 2. 异步持久化

```python
import asyncio
import aiofiles

class AsyncPersistenceCache:
    """异步持久化缓存"""
    def __init__(self, storage_path: str):
        self.cache = {}
        self.storage_path = Path(storage_path)

    async def put(self, key: str, value: any):
        """异步写入"""
        self.cache[key] = value

        # 异步持久化（不阻塞）
        asyncio.create_task(self._save_async(key, value))

    async def _save_async(self, key: str, value: any):
        """异步保存到磁盘"""
        file_path = self.storage_path / f"{key}.json"
        async with aiofiles.open(file_path, 'w') as f:
            await f.write(json.dumps(value))

# 使用示例
async def main():
    cache = AsyncPersistenceCache("cache_data")

    # 异步写入（不阻塞）
    await cache.put("key1", "value1")
    await cache.put("key2", "value2")

    # 继续其他操作
    print("写入完成，继续其他操作")

asyncio.run(main())
```

### 3. 压缩存储

```python
import gzip

class CompressedPersistenceCache:
    """压缩持久化缓存"""
    def __init__(self, storage_path: str):
        self.cache = {}
        self.storage_path = Path(storage_path)

    def put(self, key: str, value: any):
        """写入数据（压缩存储）"""
        self.cache[key] = value

        # 压缩后保存
        file_path = self.storage_path / f"{key}.json.gz"
        data = json.dumps(value).encode('utf-8')
        compressed = gzip.compress(data)
        file_path.write_bytes(compressed)

    def get(self, key: str) -> any:
        """读取数据（解压）"""
        if key in self.cache:
            return self.cache[key]

        file_path = self.storage_path / f"{key}.json.gz"
        if file_path.exists():
            compressed = file_path.read_bytes()
            data = gzip.decompress(compressed)
            value = json.loads(data.decode('utf-8'))
            self.cache[key] = value
            return value

        return None
```

---

## 最佳实践

### 1. 根据数据重要性选择策略

```python
def choose_strategy(data_type: str) -> str:
    """根据数据类型选择持久化策略"""
    critical_data = ["user_payment", "user_preference", "auth_token"]
    non_critical_data = ["access_log", "view_count", "cache_hit"]

    if data_type in critical_data:
        return "write-through"  # 立即持久化
    elif data_type in non_critical_data:
        return "write-back"  # 延迟持久化
    else:
        return "wal"  # 默认使用 WAL
```

### 2. 定期检查点

```python
import threading

class CheckpointCache:
    """带检查点的缓存"""
    def __init__(self, checkpoint_interval: int = 300):
        self.cache = {}
        self.dirty_keys = set()
        self.checkpoint_interval = checkpoint_interval

        # 启动后台线程定期创建检查点
        self.checkpoint_thread = threading.Thread(
            target=self._checkpoint_loop,
            daemon=True
        )
        self.checkpoint_thread.start()

    def _checkpoint_loop(self):
        """定期创建检查点"""
        while True:
            time.sleep(self.checkpoint_interval)
            self.flush()
            print(f"✅ 检查点已创建: {len(self.cache)} 个键")

    def flush(self):
        """创建检查点"""
        # 批量持久化所有脏数据
        for key in self.dirty_keys:
            if key in self.cache:
                self._save_to_disk(key, self.cache[key])
        self.dirty_keys.clear()
```

### 3. 崩溃恢复

```python
class RecoverableCache:
    """可恢复的缓存"""
    def __init__(self, storage_path: str, wal_path: str):
        self.cache = {}
        self.storage_path = Path(storage_path)
        self.wal_path = Path(wal_path)

        # 启动时恢复数据
        self._recover()

    def _recover(self):
        """从 WAL 和数据文件恢复"""
        # 1. 加载数据文件
        if self.storage_path.exists():
            for file_path in self.storage_path.glob("*.json"):
                key = file_path.stem
                value = json.loads(file_path.read_text())
                self.cache[key] = value

        # 2. 重放 WAL
        if self.wal_path.exists():
            with open(self.wal_path, 'r') as f:
                for line in f:
                    entry = json.loads(line)
                    if entry["operation"] == "PUT":
                        self.cache[entry["key"]] = entry["value"]

        print(f"✅ 恢复了 {len(self.cache)} 个键")
```

---

## 参考资源

### 2025-2026 最新研究
- [Google ADK Context Engineering](https://medium.com/@juanc.olamendy/context-engineering-in-google-adk-the-ultimate-guide-to-building-scalable-ai-agents-f8d7683f9c60) - 持久化策略
- [2026 Memory Stack](https://alok-mishra.com/2026/01/07/a-2026-memory-stack-for-enterprise-agents) - 企业级记忆栈
- [LangGraph PostgresStore](https://docs.langchain.com/oss/python/langgraph/add-memory) - WAL 模式实现

### 经典资源
- [PostgreSQL WAL](https://www.postgresql.org/docs/current/wal-intro.html) - WAL 原理
- [Redis Persistence](https://redis.io/docs/management/persistence/) - RDB vs AOF
- [SQLite WAL Mode](https://www.sqlite.org/wal.html) - WAL 实现
