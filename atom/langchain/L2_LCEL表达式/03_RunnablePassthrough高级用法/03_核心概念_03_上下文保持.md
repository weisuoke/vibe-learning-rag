# 核心概念：上下文保持

> **理解 RunnablePassthrough 在上下文管理中的核心作用**

---

## 概述

上下文保持是 RunnablePassthrough 最重要的应用场景。在 AI 应用中，特别是 RAG 管道，上下文保持决定了系统能否正确理解和处理用户请求。

**本文内容**：
- ✅ 上下文保持的原理
- ✅ 常见的上下文保持模式
- ✅ 2025-2026 最佳实践
- ✅ 实战案例分析

---

## 原理讲解

### 什么是上下文？

在 AI 应用中，上下文包括：
1. **用户输入**：问题、指令、查询
2. **中间结果**：检索结果、分析结果
3. **元数据**：时间戳、用户信息、会话ID
4. **历史记录**：之前的对话、操作记录

---

### 为什么需要保持上下文？

**问题场景**：RAG 管道中的上下文丢失

```python
# ❌ 问题：上下文丢失
question = "LangChain 是什么？"
context = retriever.invoke(question)  # 检索上下文
answer = model.invoke(context)  # ❌ 问题丢失了

# Prompt 需要：
# "根据上下文回答问题\n上下文: {context}\n问题: {question}"
# 但我们只有 context，没有 question
```

**解决方案**：使用 RunnablePassthrough 保持上下文

```python
# ✅ 解决：保持上下文
chain = RunnablePassthrough.assign(
    context=itemgetter("question") | retriever
) | prompt | model

# 现在 prompt 可以访问 question 和 context
```

---

### 上下文保持的三个层次

**层次1：保持原始输入**
```python
# 最基础：保持用户的原始问题
chain = RunnablePassthrough.assign(
    context=retriever
)
# 输出: {"question": "...", "context": [...]}
```

**层次2：保持中间结果**
```python
# 进阶：保持所有中间结果
chain = (
    RunnablePassthrough.assign(context=retriever)
    | RunnablePassthrough.assign(answer=prompt | model)
)
# 输出: {"question": "...", "context": [...], "answer": "..."}
```

**层次3：保持完整历史**
```python
# 高级：保持完整的处理历史
chain = (
    RunnablePassthrough.assign(context=retriever)
    | RunnablePassthrough.assign(answer=prompt | model)
    | RunnablePassthrough.assign(score=evaluation_chain)
    | RunnablePassthrough.assign(metadata=metadata_chain)
)
# 输出: 完整的上下文历史
```

---

## 常见的上下文保持模式

### 模式1：RAG 上下文保持

**场景**：在 RAG 管道中保持问题和上下文

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from operator import itemgetter
from dotenv import load_dotenv

load_dotenv()

# 准备向量存储
vectorstore = Chroma(embedding_function=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()

# RAG 链：保持问题 + 检索上下文
rag_chain = (
    RunnablePassthrough.assign(
        context=itemgetter("question") | retriever
    )
    | ChatPromptTemplate.from_template(
        "根据上下文回答问题\n\n上下文: {context}\n\n问题: {question}"
    )
    | ChatOpenAI(model="gpt-4")
)

result = rag_chain.invoke({"question": "LangChain 是什么？"})
```

**关键点**：
- 使用 `itemgetter("question")` 提取问题
- 使用 `assign` 添加检索结果
- Prompt 可以同时访问 question 和 context

---

### 模式2：多步骤上下文累积

**场景**：逐步构建完整的上下文

```python
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

model = ChatOpenAI(model="gpt-4")

# 多步骤链：逐步累积上下文
chain = (
    # 步骤1: 添加检索结果
    RunnablePassthrough.assign(
        context=itemgetter("question") | retriever
    )
    # 步骤2: 添加答案
    | RunnablePassthrough.assign(
        answer=ChatPromptTemplate.from_template(
            "根据上下文回答问题\n\n上下文: {context}\n\n问题: {question}"
        ) | model
    )
    # 步骤3: 添加评分
    | RunnablePassthrough.assign(
        score=RunnableLambda(lambda x:
            0.9 if len(x["answer"].content) > 100 else 0.7
        )
    )
)

result = chain.invoke({"question": "..."})
# {
#     "question": "...",  # 保持
#     "context": [...],   # 步骤1
#     "answer": "...",    # 步骤2
#     "score": 0.9        # 步骤3
# }
```

---

### 模式3：并行处理 + 上下文保持

**场景**：并行执行多个任务，同时保持原始输入

```python
# 并行分析 + 保持原始输入
analysis_chain = {
    "original": RunnablePassthrough(),  # 保持原始输入
    "summary": summary_chain,           # 并行任务1
    "keywords": keyword_chain,          # 并行任务2
    "sentiment": sentiment_chain,       # 并行任务3
}

result = analysis_chain.invoke({"text": "..."})
# {
#     "original": {"text": "..."},  # 原始输入
#     "summary": "...",
#     "keywords": [...],
#     "sentiment": "positive"
# }
```

---

### 模式4：对话历史保持

**场景**：在对话式 RAG 中保持对话历史

```python
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

def format_chat_history(data):
    """格式化对话历史"""
    history = data.get("chat_history", [])
    return "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])

# 对话式 RAG：保持对话历史
conversational_rag = (
    # 步骤1: 格式化对话历史
    RunnablePassthrough.assign(
        formatted_history=RunnableLambda(format_chat_history)
    )
    # 步骤2: 检索上下文
    | RunnablePassthrough.assign(
        context=itemgetter("question") | retriever
    )
    # 步骤3: 生成答案
    | RunnablePassthrough.assign(
        answer=ChatPromptTemplate.from_template(
            "对话历史:\n{formatted_history}\n\n"
            "上下文:\n{context}\n\n"
            "问题: {question}"
        ) | model
    )
)

result = conversational_rag.invoke({
    "question": "它有什么优势？",
    "chat_history": [
        {"role": "user", "content": "LangChain 是什么？"},
        {"role": "assistant", "content": "LangChain 是一个 AI 框架..."}
    ]
})
```

---

## 2025-2026 最佳实践

### 实践1：最小上下文原则

**原则**：只保持必要的上下文，避免数据冗余

```python
# ❌ 不推荐：保持所有中间结果
chain = (
    RunnablePassthrough.assign(step1=runnable1)
    | RunnablePassthrough.assign(step2=runnable2)
    | RunnablePassthrough.assign(step3=runnable3)
    | RunnablePassthrough.assign(step4=runnable4)
    | RunnablePassthrough.assign(step5=runnable5)
)
# 数据字典越来越大

# ✅ 推荐：只保持必要的上下文
chain = (
    RunnablePassthrough.assign(step1=runnable1)
    | RunnablePassthrough.assign(step2=runnable2)
    | RunnableLambda(lambda x: {
        "input": x["input"],
        "step1": x["step1"],
        "step2": x["step2"]
        # 只保留必要字段
    })
)
```

---

### 实践2：结构化上下文

**原则**：使用结构化的方式组织上下文

```python
# ❌ 不推荐：扁平化的上下文
result = {
    "question": "...",
    "context": [...],
    "answer": "...",
    "score": 0.9,
    "timestamp": "...",
    "user_id": "...",
    # 所有字段混在一起
}

# ✅ 推荐：结构化的上下文
result = {
    "input": {
        "question": "...",
        "user_id": "...",
    },
    "processing": {
        "context": [...],
        "answer": "...",
    },
    "metadata": {
        "score": 0.9,
        "timestamp": "...",
    }
}
```

---

### 实践3：上下文压缩

**原则**：对于大型上下文，进行压缩以节省内存

```python
from langchain_core.runnables import RunnableLambda

def compress_context(data):
    """压缩上下文：只保留摘要"""
    return {
        "question": data["question"],
        "context_summary": summarize(data["context"]),  # 压缩
        "answer": data["answer"]
    }

# 压缩上下文
chain = (
    RunnablePassthrough.assign(context=retriever)
    | RunnablePassthrough.assign(answer=prompt | model)
    | RunnableLambda(compress_context)  # 压缩
)
```

---

### 实践4：上下文监控

**原则**：在生产环境中监控上下文大小和质量

```python
import sys

def monitor_context(data):
    """监控上下文大小"""
    size = sys.getsizeof(str(data))
    print(f"上下文大小: {size} bytes")

    if size > 1000000:  # 1MB
        print("警告：上下文过大")

    return data

# 添加监控
chain = (
    RunnablePassthrough.assign(context=retriever)
    | RunnableLambda(monitor_context)  # 监控
    | RunnablePassthrough.assign(answer=prompt | model)
)
```

---

## 实战案例分析

### 案例1：生产级 RAG 管道

**需求**：构建一个完整的 RAG 管道，保持所有上下文用于调试和监控

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from operator import itemgetter
import time
from dotenv import load_dotenv

load_dotenv()

# 准备组件
vectorstore = Chroma(embedding_function=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()
model = ChatOpenAI(model="gpt-4")

# 辅助函数
def add_timestamp(data):
    return time.time()

def evaluate_quality(data):
    answer_length = len(data["answer"].content)
    return {
        "score": 0.9 if answer_length > 100 else 0.7,
        "reason": "详细" if answer_length > 100 else "简短"
    }

# 完整的 RAG 管道
production_rag = (
    # 步骤1: 添加开始时间戳
    RunnablePassthrough.assign(
        start_time=RunnableLambda(add_timestamp)
    )
    # 步骤2: 检索上下文
    | RunnablePassthrough.assign(
        context=itemgetter("question") | retriever
    )
    # 步骤3: 生成答案
    | RunnablePassthrough.assign(
        answer=ChatPromptTemplate.from_template(
            "根据上下文回答问题\n\n上下文: {context}\n\n问题: {question}"
        ) | model
    )
    # 步骤4: 添加结束时间戳
    | RunnablePassthrough.assign(
        end_time=RunnableLambda(add_timestamp)
    )
    # 步骤5: 评估质量
    | RunnablePassthrough.assign(
        quality=RunnableLambda(evaluate_quality)
    )
    # 步骤6: 计算耗时
    | RunnableLambda(lambda x: {
        **x,
        "duration": x["end_time"] - x["start_time"]
    })
)

# 使用
result = production_rag.invoke({"question": "LangChain 是什么？"})
print(f"问题: {result['question']}")
print(f"答案: {result['answer'].content}")
print(f"质量: {result['quality']}")
print(f"耗时: {result['duration']:.2f}秒")
```

**关键点**：
- 保持完整的上下文历史
- 添加时间戳用于性能监控
- 评估答案质量
- 便于调试和追踪

---

### 案例2：多轮对话 RAG

**需求**：在多轮对话中保持对话历史和上下文

```python
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

model = ChatOpenAI(model="gpt-4")

def format_history(data):
    """格式化对话历史"""
    history = data.get("history", [])
    if not history:
        return "无对话历史"
    return "\n".join([
        f"{'用户' if msg['role'] == 'user' else '助手'}: {msg['content']}"
        for msg in history
    ])

def update_history(data):
    """更新对话历史"""
    history = data.get("history", [])
    history.append({"role": "user", "content": data["question"]})
    history.append({"role": "assistant", "content": data["answer"].content})
    return history

# 多轮对话 RAG
conversational_rag = (
    # 步骤1: 格式化历史
    RunnablePassthrough.assign(
        formatted_history=RunnableLambda(format_history)
    )
    # 步骤2: 检索上下文
    | RunnablePassthrough.assign(
        context=itemgetter("question") | retriever
    )
    # 步骤3: 生成答案
    | RunnablePassthrough.assign(
        answer=ChatPromptTemplate.from_template(
            "对话历史:\n{formatted_history}\n\n"
            "上下文:\n{context}\n\n"
            "当前问题: {question}\n\n"
            "请根据对话历史和上下文回答问题。"
        ) | model
    )
    # 步骤4: 更新历史
    | RunnablePassthrough.assign(
        updated_history=RunnableLambda(update_history)
    )
)

# 第一轮对话
result1 = conversational_rag.invoke({
    "question": "LangChain 是什么？",
    "history": []
})

# 第二轮对话（使用更新后的历史）
result2 = conversational_rag.invoke({
    "question": "它有什么优势？",
    "history": result1["updated_history"]
})
```

---

### 案例3：动态上下文管理

**需求**：根据任务类型动态管理上下文

```python
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

def classify_task(data):
    """分类任务类型"""
    question = data["question"].lower()
    if "是什么" in question or "定义" in question:
        return "definition"
    elif "怎么" in question or "如何" in question:
        return "howto"
    else:
        return "general"

def select_context_strategy(data):
    """根据任务类型选择上下文策略"""
    task_type = data["task_type"]

    if task_type == "definition":
        # 定义类问题：需要精确的上下文
        return retriever.invoke(data["question"], k=2)
    elif task_type == "howto":
        # 操作类问题：需要详细的步骤
        return retriever.invoke(data["question"], k=5)
    else:
        # 一般问题：标准检索
        return retriever.invoke(data["question"], k=3)

# 动态上下文管理
dynamic_rag = (
    # 步骤1: 分类任务
    RunnablePassthrough.assign(
        task_type=RunnableLambda(classify_task)
    )
    # 步骤2: 动态选择上下文策略
    | RunnablePassthrough.assign(
        context=RunnableLambda(select_context_strategy)
    )
    # 步骤3: 生成答案
    | RunnablePassthrough.assign(
        answer=prompt | model
    )
)
```

---

## 核心要点总结

1. **上下文保持的重要性**：
   - RAG 管道的核心需求
   - 多步骤处理的基础
   - 调试和监控的关键

2. **常见模式**：
   - RAG 上下文保持
   - 多步骤上下文累积
   - 并行处理 + 上下文保持
   - 对话历史保持

3. **2025-2026 最佳实践**：
   - 最小上下文原则
   - 结构化上下文
   - 上下文压缩
   - 上下文监控

4. **实战要点**：
   - 保持完整的处理历史
   - 添加时间戳和元数据
   - 评估和监控
   - 动态上下文管理

---

## 参考资源

**官方文档（2025-2026）**：
- [RunnablePassthrough API Reference](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)
- [LCEL Concepts](https://python.langchain.com/docs/concepts/lcel)

**2025-2026 最佳实践**：
- [Building Production-Ready AI Pipelines](https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557)
- [Master LangChain in 2025](https://towardsai.net/p/machine-learning/master-langchain-in-2025-from-rag-to-tools-complete-guide)
- [Context Engineering for Agents](https://blog.langchain.dev/context-engineering-for-agents)

---

**版本**: v1.0
**最后更新**: 2026-02-19
**适用**: LangChain 0.3+, Python 3.13+
