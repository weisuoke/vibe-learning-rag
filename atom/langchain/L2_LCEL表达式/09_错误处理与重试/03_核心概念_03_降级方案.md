# 核心概念3：降级方案

## 概述

**降级方案（Fallback）是 LCEL 错误处理的高级策略，通过提供备选方案确保服务在主方案失败时仍能可用。**

本文深入讲解：
- `with_fallbacks()` 方法的完整配置
- RunnableWithFallbacks 内部机制
- 多级降级设计模式
- 优雅降级最佳实践
- 实际场景应用

---

## 1. with_fallbacks() 方法详解

### 1.1 基础用法

**最简单的降级：**

```python
from langchain_openai import ChatOpenAI

# 主模型
primary_llm = ChatOpenAI(model="gpt-4")

# 备选模型
fallback_llm = ChatOpenAI(model="gpt-3.5-turbo")

# 添加降级
llm_with_fallback = primary_llm.with_fallbacks([fallback_llm])

# 使用
response = llm_with_fallback.invoke("你好")
```

**执行流程：**
```
尝试 GPT-4 → 失败
    ↓
切换到 GPT-3.5 → 成功 ✅
```

### 1.2 完整参数列表

**with_fallbacks() 的所有参数：**

```python
from langchain_openai import ChatOpenAI

primary = ChatOpenAI(model="gpt-4")
fallback1 = ChatOpenAI(model="gpt-3.5-turbo")
fallback2 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

llm = primary.with_fallbacks(
    # ===== 必需参数 =====
    fallbacks=[fallback1, fallback2],  # 备选 Runnable 列表

    # ===== 可选参数 =====
    exceptions_to_handle=(Exception,),  # 触发降级的异常类型
    # 默认：所有异常都触发降级
)
```

### 1.3 参数详解

#### 1.3.1 fallbacks：备选方案列表

```python
# 单个备选方案
llm = primary.with_fallbacks([fallback1])

# 多个备选方案（按顺序尝试）
llm = primary.with_fallbacks([fallback1, fallback2, fallback3])

# 执行流程：
# primary → 失败
# fallback1 → 失败
# fallback2 → 失败
# fallback3 → 成功 ✅
```

**备选方案可以是任何 Runnable：**
```python
from langchain_core.runnables import RunnableLambda

# 备选方案1：另一个模型
fallback1 = ChatOpenAI(model="gpt-3.5-turbo")

# 备选方案2：自定义函数
def simple_response(input):
    return "抱歉，服务暂时不可用"

fallback2 = RunnableLambda(simple_response)

# 组合
llm = primary.with_fallbacks([fallback1, fallback2])
```

#### 1.3.2 exceptions_to_handle：异常过滤

```python
from openai import RateLimitError, APIError, AuthenticationError

# 只对特定异常降级
llm = primary.with_fallbacks(
    [fallback],
    exceptions_to_handle=(RateLimitError, APIError)
)

# 场景1：RateLimitError → 降级 ✅
# 场景2：APIError → 降级 ✅
# 场景3：AuthenticationError → 不降级，直接抛出 ❌
```

**默认行为：**
```python
# 默认：所有异常都触发降级
llm = primary.with_fallbacks([fallback])
# 等价于
llm = primary.with_fallbacks(
    [fallback],
    exceptions_to_handle=(Exception,)
)
```

---

## 2. RunnableWithFallbacks 内部机制

### 2.1 工作原理

**核心概念：** `with_fallbacks()` 返回一个 `RunnableWithFallbacks` 包装器

```python
from langchain_openai import ChatOpenAI

primary = ChatOpenAI(model="gpt-4")
fallback = ChatOpenAI(model="gpt-3.5-turbo")

llm_with_fallback = primary.with_fallbacks([fallback])

print(type(llm_with_fallback))
# <class 'langchain_core.runnables.fallbacks.RunnableWithFallbacks'>
```

**内部机制（简化版）：**

```python
# RunnableWithFallbacks 的简化实现（概念性）
class RunnableWithFallbacks:
    def __init__(self, runnable, fallbacks, exceptions_to_handle):
        self.runnable = runnable
        self.fallbacks = fallbacks
        self.exceptions_to_handle = exceptions_to_handle

    def invoke(self, input):
        # 尝试主 Runnable
        try:
            return self.runnable.invoke(input)
        except self.exceptions_to_handle as e:
            # 主 Runnable 失败，尝试备选方案
            for fallback in self.fallbacks:
                try:
                    return fallback.invoke(input)
                except self.exceptions_to_handle:
                    continue  # 继续尝试下一个备选方案
            # 所有备选方案都失败
            raise Exception("All fallbacks failed")
```

### 2.2 降级触发条件

**什么时候会触发降级？**

1. **主 Runnable 抛出异常**
   ```python
   # GPT-4 调用失败 → 触发降级
   ```

2. **异常类型匹配 exceptions_to_handle**
   ```python
   # RateLimitError 在 exceptions_to_handle 中 → 触发降级
   # AuthenticationError 不在 exceptions_to_handle 中 → 不触发降级
   ```

3. **按顺序尝试所有备选方案**
   ```python
   # fallback1 失败 → 尝试 fallback2
   # fallback2 失败 → 尝试 fallback3
   # fallback3 成功 → 返回结果
   ```

### 2.3 降级与重试的组合

**降级和重试可以组合使用：**

```python
from langchain_openai import ChatOpenAI

# 主模型 + 重试
primary = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

# 备选模型 + 重试
fallback = ChatOpenAI(model="gpt-3.5-turbo").with_retry(stop_after_attempt=2)

# 组合
llm = primary.with_fallbacks([fallback])

# 执行流程：
# 1. 尝试 GPT-4（最多 3 次）
# 2. 如果仍失败，尝试 GPT-3.5（最多 2 次）
# 3. 如果仍失败，抛出异常
```

**执行时间线：**
```
时间 0s：GPT-4 第1次尝试 → 失败
时间 1s：GPT-4 第2次尝试 → 失败
时间 3s：GPT-4 第3次尝试 → 失败
时间 7s：切换到 GPT-3.5
时间 7s：GPT-3.5 第1次尝试 → 失败
时间 8s：GPT-3.5 第2次尝试 → 成功 ✅
```

---

## 3. 多级降级设计模式

### 3.1 模式1：质量降级（GPT-4 → GPT-3.5 → Claude）

**场景：** 优先使用最好的模型，失败后降级到次好的模型

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# 第1级：GPT-4（最好）
primary = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

# 第2级：GPT-3.5（次好）
fallback1 = ChatOpenAI(model="gpt-3.5-turbo").with_retry(stop_after_attempt=2)

# 第3级：Claude（保底）
fallback2 = ChatAnthropic(model="claude-3-sonnet-20240229").with_retry(stop_after_attempt=2)

# 组合
llm = primary.with_fallbacks([fallback1, fallback2])

# 执行流程：
# GPT-4（重试3次）→ GPT-3.5（重试2次）→ Claude（重试2次）
```

**适用场景：**
- 需要最高质量的输出
- 成本不是主要考虑因素
- 高可用性要求

### 3.2 模式2：成本优化降级（便宜 → 贵）

**场景：** 先试便宜的模型，失败再用贵的模型

```python
from langchain_openai import ChatOpenAI

# 第1级：GPT-3.5（便宜）
primary = ChatOpenAI(model="gpt-3.5-turbo").with_retry(stop_after_attempt=2)

# 第2级：GPT-4（贵）
fallback = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

# 组合
llm = primary.with_fallbacks([fallback])

# 执行流程：
# GPT-3.5（重试2次）→ GPT-4（重试3次）
```

**适用场景：**
- 成本敏感的应用
- 大多数任务不需要最强模型
- 批量处理

### 3.3 模式3：多云部署降级（OpenAI → Anthropic → Google）

**场景：** 跨云服务商的高可用部署

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# 第1级：OpenAI
primary = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

# 第2级：Anthropic
fallback1 = ChatAnthropic(model="claude-3-opus-20240229").with_retry(stop_after_attempt=2)

# 第3级：Google
fallback2 = ChatGoogleGenerativeAI(model="gemini-pro").with_retry(stop_after_attempt=2)

# 组合
llm = primary.with_fallbacks([fallback1, fallback2])
```

**适用场景：**
- 关键业务应用
- 需要极高可用性
- 避免单点故障

### 3.4 模式4：功能降级（复杂 → 简单）

**场景：** 主方案失败时，使用简化的功能

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.runnables import RunnableLambda

# 主链：复杂的 JSON 输出
primary_prompt = ChatPromptTemplate.from_template(
    "以 JSON 格式返回详细分析: {query}\n"
    "格式: {{\"summary\": \"...\", \"details\": [...], \"confidence\": 0.0-1.0}}"
)
primary_llm = ChatOpenAI(model="gpt-4")
primary_parser = JsonOutputParser()
primary_chain = primary_prompt | primary_llm | primary_parser

# 备选链：简单的文本输出
fallback_prompt = ChatPromptTemplate.from_template("简单回答: {query}")
fallback_llm = ChatOpenAI(model="gpt-3.5-turbo")
fallback_parser = StrOutputParser()
fallback_chain = fallback_prompt | fallback_llm | fallback_parser

# 组合
chain = primary_chain.with_fallbacks([fallback_chain])

# 执行流程：
# 尝试复杂 JSON 输出 → 失败 → 降级到简单文本输出
```

**适用场景：**
- 输出格式要求灵活
- 可以接受降级的功能
- 用户体验优先

### 3.5 模式5：本地降级（云 → 本地）

**场景：** 云服务不可用时，降级到本地模型

```python
from langchain_openai import ChatOpenAI
from langchain_community.llms import Ollama

# 第1级：云端 GPT-4
primary = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)

# 第2级：本地 Llama
fallback = Ollama(model="llama2")

# 组合
llm = primary.with_fallbacks([fallback])
```

**适用场景：**
- 离线场景
- 数据隐私要求
- 成本控制

---

## 4. 优雅降级最佳实践

### 4.1 原则1：降级应该对用户透明

**错误示例：**
```python
# ❌ 用户能感知到降级
def bad_fallback(input):
    return "主服务不可用，使用了备用服务"
```

**正确示例：**
```python
# ✅ 用户无感知
primary = ChatOpenAI(model="gpt-4")
fallback = ChatOpenAI(model="gpt-3.5-turbo")

llm = primary.with_fallbacks([fallback])
# 用户只看到正常的回答，不知道发生了降级
```

### 4.2 原则2：降级方案应该经过测试

**测试降级方案：**

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

# 模拟主服务失败
def failing_primary(input):
    raise Exception("Primary service failed")

primary = RunnableLambda(failing_primary)
fallback = ChatOpenAI(model="gpt-3.5-turbo")

llm = primary.with_fallbacks([fallback])

# 测试
try:
    response = llm.invoke("测试")
    print("降级成功:", response.content)
except Exception as e:
    print("降级失败:", e)
```

### 4.3 原则3：记录降级事件

**添加降级日志：**

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda
import logging

logger = logging.getLogger(__name__)

def log_fallback(input):
    """记录降级事件"""
    logger.warning(f"Primary service failed, using fallback for input: {input}")
    return input

primary = ChatOpenAI(model="gpt-4")
fallback = RunnableLambda(log_fallback) | ChatOpenAI(model="gpt-3.5-turbo")

llm = primary.with_fallbacks([fallback])
```

### 4.4 原则4：设置合理的降级层级

**推荐层级：**
- **2-3 层**：大多数应用
- **3-5 层**：关键业务应用
- **1 层**：非关键应用

**过多层级的问题：**
```python
# ❌ 过多层级（7层）
llm = primary.with_fallbacks([
    fallback1, fallback2, fallback3,
    fallback4, fallback5, fallback6
])
# 问题：
# 1. 增加复杂度
# 2. 增加延迟
# 3. 难以维护
```

### 4.5 原则5：考虑降级的成本

**成本感知的降级：**

```python
from langchain_openai import ChatOpenAI

# 成本：GPT-4 > GPT-3.5 > GPT-3.5-mini
primary = ChatOpenAI(model="gpt-4")  # $0.03/1K tokens
fallback1 = ChatOpenAI(model="gpt-3.5-turbo")  # $0.002/1K tokens
fallback2 = ChatOpenAI(model="gpt-3.5-turbo-mini")  # $0.0005/1K tokens

llm = primary.with_fallbacks([fallback1, fallback2])

# 大多数情况：使用 GPT-4（高质量）
# 偶尔降级：使用 GPT-3.5（平衡）
# 极少降级：使用 GPT-3.5-mini（保底）
```

---

## 5. 实际场景应用

### 5.1 场景1：RAG 问答系统的降级

**问题：** RAG 系统有多个可能失败的环节

**解决方案：**

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

# 1. 向量检索器降级
primary_embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
fallback_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# 创建两个向量存储
primary_vectorstore = Chroma(embedding_function=primary_embeddings)
fallback_vectorstore = Chroma(embedding_function=fallback_embeddings)

# 检索器降级
primary_retriever = primary_vectorstore.as_retriever()
fallback_retriever = fallback_vectorstore.as_retriever()

retriever = primary_retriever.with_fallbacks([fallback_retriever])

# 2. LLM 降级
primary_llm = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)
fallback_llm = ChatOpenAI(model="gpt-3.5-turbo").with_retry(stop_after_attempt=2)

llm = primary_llm.with_fallbacks([fallback_llm])

# 3. 构建 RAG 链
prompt = ChatPromptTemplate.from_template(
    "基于以下上下文回答问题:\n{context}\n\n问题: {question}"
)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

# 使用
result = rag_chain.invoke("什么是 LangChain？")
print(result.content)
```

### 5.2 场景2：多模型对比与降级

**问题：** 需要对比多个模型的输出，主模型失败时自动降级

**解决方案：**

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnableParallel

# 主模型：GPT-4
primary = ChatOpenAI(model="gpt-4")

# 备选模型：Claude
fallback = ChatAnthropic(model="claude-3-opus-20240229")

# 方案1：并行调用（用于对比）
parallel_chain = RunnableParallel(
    gpt4=primary,
    claude=fallback
)

# 方案2：降级调用（用于容错）
fallback_chain = primary.with_fallbacks([fallback])

# 使用
try:
    # 尝试并行对比
    results = parallel_chain.invoke("分析这段文本")
    print("GPT-4:", results["gpt4"].content)
    print("Claude:", results["claude"].content)
except Exception:
    # 并行失败，使用降级
    result = fallback_chain.invoke("分析这段文本")
    print("降级结果:", result.content)
```

### 5.3 场景3：批量处理的降级策略

**问题：** 批量处理时，部分失败不应影响整体

**解决方案：**

```python
from langchain_openai import ChatOpenAI
import logging

logger = logging.getLogger(__name__)

# 主模型 + 降级
primary = ChatOpenAI(model="gpt-4").with_retry(stop_after_attempt=3)
fallback = ChatOpenAI(model="gpt-3.5-turbo").with_retry(stop_after_attempt=2)

llm = primary.with_fallbacks([fallback])

# 批量处理
items = ["问题1", "问题2", "问题3", "..."]

results = []
stats = {"primary": 0, "fallback": 0, "failed": 0}

for i, item in enumerate(items):
    try:
        # 尝试处理
        result = llm.invoke(item)
        results.append(result.content)

        # 统计（简化版，实际需要更复杂的追踪）
        stats["primary"] += 1

    except Exception as e:
        logger.error(f"第 {i+1} 个项目失败: {e}")
        results.append(None)
        stats["failed"] += 1

print(f"成功: {len([r for r in results if r])}/{len(items)}")
print(f"统计: {stats}")
```

### 5.4 场景4：实时对话的降级

**问题：** 实时对话不能等待太久，需要快速降级

**解决方案：**

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda
import time

# 主模型：GPT-4（可能较慢）
primary = ChatOpenAI(model="gpt-4", timeout=5)  # 5秒超时

# 备选模型：GPT-3.5（更快）
fallback = ChatOpenAI(model="gpt-3.5-turbo", timeout=3)

# 最后的保底：预设回复
def emergency_response(input):
    return "抱歉，服务暂时繁忙，请稍后重试"

emergency = RunnableLambda(emergency_response)

# 组合
llm = primary.with_fallbacks([fallback, emergency])

# 使用
start_time = time.time()
response = llm.invoke("你好")
elapsed_time = time.time() - start_time

print(f"响应: {response.content if hasattr(response, 'content') else response}")
print(f"耗时: {elapsed_time:.2f}秒")
```

### 5.5 场景5：Agent 系统的降级

**问题：** Agent 的工具调用可能失败，需要降级策略

**解决方案：**

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.runnables import RunnableLambda

# 主工具：复杂的数据库查询
@tool
def complex_database_query(query: str) -> str:
    """复杂的数据库查询"""
    # 可能失败的复杂查询
    raise Exception("Database connection failed")

# 备选工具：简单的缓存查询
@tool
def simple_cache_query(query: str) -> str:
    """简单的缓存查询"""
    return "从缓存返回的结果"

# 工具降级
tool_with_fallback = RunnableLambda(complex_database_query).with_fallbacks([
    RunnableLambda(simple_cache_query)
])

# Agent LLM 降级
primary_llm = ChatOpenAI(model="gpt-4")
fallback_llm = ChatOpenAI(model="gpt-3.5-turbo")

agent_llm = primary_llm.with_fallbacks([fallback_llm])

# 使用
try:
    result = tool_with_fallback.invoke("查询数据")
    print(result)
except Exception as e:
    print(f"工具调用失败: {e}")
```

---

## 6. 降级方案的监控与优化

### 6.1 监控降级率

**关键指标：**

```python
from langchain_openai import ChatOpenAI
import logging

logger = logging.getLogger(__name__)

class MonitoredFallback:
    """带监控的降级包装器"""
    def __init__(self, primary, fallbacks):
        self.primary = primary
        self.fallbacks = fallbacks
        self.stats = {
            "total": 0,
            "primary_success": 0,
            "fallback_used": 0,
            "all_failed": 0
        }

    def invoke(self, input):
        self.stats["total"] += 1

        # 尝试主方案
        try:
            result = self.primary.invoke(input)
            self.stats["primary_success"] += 1
            return result
        except Exception as e:
            logger.warning(f"Primary failed: {e}")

        # 尝试备选方案
        for i, fallback in enumerate(self.fallbacks):
            try:
                result = fallback.invoke(input)
                self.stats["fallback_used"] += 1
                logger.info(f"Fallback {i+1} succeeded")
                return result
            except Exception as e:
                logger.warning(f"Fallback {i+1} failed: {e}")

        # 所有方案都失败
        self.stats["all_failed"] += 1
        raise Exception("All fallbacks failed")

    def get_stats(self):
        """获取统计信息"""
        total = self.stats["total"]
        if total == 0:
            return self.stats

        return {
            **self.stats,
            "primary_success_rate": self.stats["primary_success"] / total,
            "fallback_rate": self.stats["fallback_used"] / total,
            "failure_rate": self.stats["all_failed"] / total
        }

# 使用
primary = ChatOpenAI(model="gpt-4")
fallback = ChatOpenAI(model="gpt-3.5-turbo")

monitored_llm = MonitoredFallback(primary, [fallback])

# 处理多个请求
for i in range(10):
    try:
        result = monitored_llm.invoke(f"问题 {i+1}")
    except Exception:
        pass

# 查看统计
print(monitored_llm.get_stats())
```

### 6.2 优化降级策略

**基于监控数据优化：**

```python
# 如果降级率 > 10%，考虑：
# 1. 增加主方案的重试次数
# 2. 优化主方案的配置
# 3. 升级备选方案的质量

# 如果降级率 < 1%，考虑：
# 1. 简化降级方案（降低成本）
# 2. 减少降级层级
```

---

## 总结

**降级方案的三个关键点：**

1. **透明性**：降级应该对用户透明
2. **可靠性**：降级方案本身要可靠
3. **监控性**：需要监控降级率和成功率

**记住：降级不是"备胎"，而是"保险"。**
