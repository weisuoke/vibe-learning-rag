# RAG检索链 - 最小可用知识

> 掌握 20% 的核心知识，解决 80% 的 RAG 问题

---

## 核心理念

你不需要掌握 RAG 的所有细节，只需要掌握以下 5 个核心知识点，就能开始构建 RAG 应用。

---

## 4.1 RAG 的三步流程

**核心概念**：RAG = 检索 + 增强 + 生成

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# 1. 检索：从向量库检索相关文档
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 2. 增强：把检索到的文档注入到 LLM 上下文
# 3. 生成：LLM 基于上下文生成答案
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=retriever
)

# 一行代码完成 RAG
answer = qa_chain.invoke({"query": "什么是 RAG？"})
```

**记住**：
- 检索：找到相关文档
- 增强：注入到上下文
- 生成：LLM 生成答案

**类比**：
- **前端**：像 API 调用 → 数据处理 → 渲染页面
- **日常**：像查字典 → 阅读定义 → 理解含义

---

## 4.2 Embedding：文本转向量

**核心概念**：把文本转换为数字向量，相似的文本 → 相似的向量

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# 把文本转换为向量
text1 = "Python 是一门编程语言"
text2 = "Python 是一种程序设计语言"
text3 = "今天天气真好"

vec1 = embeddings.embed_query(text1)
vec2 = embeddings.embed_query(text2)
vec3 = embeddings.embed_query(text3)

# vec1 和 vec2 很相似（语义相近）
# vec1 和 vec3 不相似（语义不同）
```

**记住**：
- Embedding 是 RAG 的基础
- 相似的文本 → 相似的向量
- 用向量相似度判断文本相关性

**类比**：
- **前端**：像图片的哈希值，相似的图片 → 相似的哈希
- **日常**：像给书打标签，相似的书 → 相似的标签

---

## 4.3 向量数据库：存储和检索

**核心概念**：向量数据库存储文档向量，支持相似度检索

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# 创建向量数据库
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(
    embedding_function=embeddings,
    persist_directory="./chroma_db"  # 持久化存储
)

# 添加文档
documents = [
    "Python 是一门编程语言",
    "FastAPI 是一个 Web 框架",
    "RAG 是检索增强生成"
]
vectorstore.add_texts(documents)

# 检索相似文档
results = vectorstore.similarity_search("什么是 Python？", k=2)
# 返回最相似的 2 个文档
```

**记住**：
- 向量数据库 = 文档的"搜索引擎"
- 支持语义检索（不是关键词匹配）
- 常用：ChromaDB（简单）、FAISS（快速）

**类比**：
- **前端**：像 localStorage，存储数据并支持查询
- **日常**：像图书馆，存书并支持按主题查找

---

## 4.4 文档分块：切成小片段

**核心概念**：把长文档切成小块，每块独立向量化和检索

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 创建分块器
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,        # 每块 500 字符
    chunk_overlap=50,      # 块之间重叠 50 字符
    separators=["\n\n", "\n", " ", ""]  # 优先按段落分割
)

# 分块
long_text = "这是一篇很长的文档..." * 100
chunks = text_splitter.split_text(long_text)

# 每个块独立向量化
for chunk in chunks:
    vectorstore.add_texts([chunk])
```

**记住**：
- 文档太长无法全部放入上下文
- 分块后只检索最相关的块
- chunk_size 通常 500-1000 字符

**类比**：
- **前端**：像懒加载，把长列表分页加载
- **日常**：像把书分成章节，只读相关章节

---

## 4.5 Prompt 模板：构造上下文

**核心概念**：把检索到的文档注入到 Prompt 中

```python
from langchain.prompts import PromptTemplate

# 定义 Prompt 模板
template = """
基于以下文档回答问题。如果文档中没有相关信息，请说"我不知道"。

文档：
{context}

问题：{question}

答案：
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

# 使用模板
context = "Python 是一门编程语言，由 Guido van Rossum 创建。"
question = "谁创建了 Python？"

formatted_prompt = prompt.format(context=context, question=question)
# 输出：
# 基于以下文档回答问题...
# 文档：Python 是一门编程语言，由 Guido van Rossum 创建。
# 问题：谁创建了 Python？
# 答案：
```

**记住**：
- Prompt 模板控制 LLM 如何使用文档
- 明确告诉 LLM"基于文档回答"
- 避免 LLM 编造答案

**类比**：
- **前端**：像模板字符串，`Hello ${name}`
- **日常**：像填空题，把答案填入模板

---

## 最小可用 RAG 完整示例

把以上 5 个知识点组合起来：

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA

# 1. 准备文档
documents = [
    "Python 是一门编程语言，由 Guido van Rossum 于 1991 年创建。",
    "FastAPI 是一个现代、快速的 Web 框架，用于构建 API。",
    "RAG 是检索增强生成，结合了检索和生成两种技术。"
]

# 2. 分块（这里文档很短，不需要分块）
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)
chunks = text_splitter.split_text("\n".join(documents))

# 3. 向量化并存储
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(chunks, embeddings)

# 4. 创建 RAG 链
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2})
)

# 5. 提问
answer = qa_chain.invoke({"query": "谁创建了 Python？"})
print(answer["result"])
# 输出：Guido van Rossum 于 1991 年创建了 Python。
```

**这 20 行代码就是一个完整的 RAG 系统！**

---

## 这些知识足以做什么？

掌握以上 5 个核心知识点，你可以：

### ✅ 能做的事情

1. **构建基础的文档问答系统**
   - 上传 PDF/TXT 文档
   - 用户提问，AI 基于文档回答

2. **实现知识库检索**
   - 把公司文档存入向量库
   - 员工查询公司政策、流程

3. **集成到 FastAPI**
   - 创建 `/ask` 端点
   - 接收问题，返回答案

### ❌ 暂时做不了的事情

1. **高级检索优化**
   - 混合检索（向量 + 关键词）
   - ReRank 重排序
   - Query 改写

2. **复杂文档处理**
   - 表格提取
   - 图片识别
   - 多语言支持

3. **生产级优化**
   - 缓存策略
   - 流式输出
   - 错误处理

**但这些都是后续优化，不影响你快速上手！**

---

## 快速上手检查清单

完成以下任务，确保你掌握了最小可用知识：

- [ ] 运行上面的完整示例代码
- [ ] 修改文档内容，观察答案变化
- [ ] 修改 `k` 参数（检索文档数量），观察效果
- [ ] 尝试不同的问题，看 RAG 如何回答
- [ ] 把代码集成到 FastAPI 端点

---

## 下一步学习

掌握最小可用知识后，可以深入学习：

1. **[核心概念](./03_核心概念_01_向量化Embedding.md)** - 深入理解 Embedding、检索、生成
2. **[实战代码](./07_实战代码_01_基础RAG链.md)** - 更多实战场景
3. **[反直觉点](./06_反直觉点.md)** - 避免常见误区

---

**记住**：不要追求完美，先让 RAG 跑起来，再逐步优化！
