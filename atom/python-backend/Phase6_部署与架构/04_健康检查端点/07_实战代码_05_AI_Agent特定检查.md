# å®æˆ˜ä»£ç 5ï¼šAI Agentç‰¹å®šæ£€æŸ¥

> LLM APIã€å‘é‡æ•°æ®åº“ã€Embeddingæ¨¡å‹ã€Agentä»»åŠ¡é˜Ÿåˆ—çš„å¥åº·æ£€æŸ¥

---

## æ¦‚è¿°

æœ¬æ–‡æä¾› AI Agent ç‰¹å®šçš„å¥åº·æ£€æŸ¥å®ç°ï¼ŒåŒ…æ‹¬ï¼š
- LLM API å¯ç”¨æ€§æ£€æŸ¥ï¼ˆOpenAI/Anthropicï¼‰
- å‘é‡æ•°æ®åº“è¿æ¥æ£€æŸ¥ï¼ˆpgvectorï¼‰
- Embedding æ¨¡å‹åŠ è½½çŠ¶æ€æ£€æŸ¥
- Agent ä»»åŠ¡é˜Ÿåˆ—å¥åº·æ£€æŸ¥
- RAG ç³»ç»Ÿç«¯åˆ°ç«¯å¥åº·æ£€æŸ¥

---

## å®Œæ•´ä»£ç 

```python
"""
AI Agent ç‰¹å®šå¥åº·æ£€æŸ¥å®ç°
æ¼”ç¤ºï¼šLLM APIã€å‘é‡æ•°æ®åº“ã€Embedding æ¨¡å‹ã€ä»»åŠ¡é˜Ÿåˆ—æ£€æŸ¥
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Dict, Optional, List
import asyncio
import time
import os
from datetime import datetime

# ===== 1. ä¾èµ–å¯¼å…¥ =====

# LLM å®¢æˆ·ç«¯
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic

# æ•°æ®åº“
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy import text

# Redis
import redis.asyncio as redis

# Embedding æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# ===== 2. é…ç½® =====

# OpenAI é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")

# Anthropic é…ç½®
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")

# æ•°æ®åº“é…ç½®
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql+asyncpg://user:password@localhost:5432/dbname")

# Redis é…ç½®
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# ===== 3. å®¢æˆ·ç«¯åˆå§‹åŒ– =====

# OpenAI å®¢æˆ·ç«¯
openai_client = AsyncOpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# Anthropic å®¢æˆ·ç«¯
anthropic_client = AsyncAnthropic(api_key=ANTHROPIC_API_KEY)

# æ•°æ®åº“å¼•æ“
engine = create_async_engine(DATABASE_URL, pool_size=10, max_overflow=20)

# Redis å®¢æˆ·ç«¯
redis_client = redis.from_url(REDIS_URL, decode_responses=True)

# Embedding æ¨¡å‹ï¼ˆå…¨å±€å˜é‡ï¼‰
embedding_model = None

# ===== 4. FastAPI åº”ç”¨ =====

app = FastAPI(title="AI Agent Health Check")

# ===== 5. å“åº”æ¨¡å‹ =====

class LLMHealthResponse(BaseModel):
    """LLM API å¥åº·æ£€æŸ¥å“åº”"""
    healthy: bool
    duration_ms: int
    provider: str
    model: Optional[str] = None
    error: Optional[str] = None

class VectorDBHealthResponse(BaseModel):
    """å‘é‡æ•°æ®åº“å¥åº·æ£€æŸ¥å“åº”"""
    healthy: bool
    duration_ms: int
    extension_installed: bool
    table_exists: bool
    error: Optional[str] = None

class EmbeddingHealthResponse(BaseModel):
    """Embedding æ¨¡å‹å¥åº·æ£€æŸ¥å“åº”"""
    healthy: bool
    loaded: bool
    model_name: Optional[str] = None
    error: Optional[str] = None

class RAGHealthResponse(BaseModel):
    """RAG ç³»ç»Ÿå¥åº·æ£€æŸ¥å“åº”"""
    healthy: bool
    duration_ms: int
    checks: Dict[str, bool]
    error: Optional[str] = None

# ===== 6. LLM API å¥åº·æ£€æŸ¥ =====

async def check_openai_api() -> LLMHealthResponse:
    """
    æ£€æŸ¥ OpenAI API å¯ç”¨æ€§

    ä½¿ç”¨æœ€å°çš„è¯·æ±‚æ¥æ£€æŸ¥ API æ˜¯å¦å¯ç”¨
    """
    start_time = time.time()

    try:
        # å‘é€æœ€å°çš„è¯·æ±‚ï¼ˆ1 tokenï¼‰
        response = await asyncio.wait_for(
            openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": "hi"}],
                max_tokens=1
            ),
            timeout=10.0
        )

        duration_ms = int((time.time() - start_time) * 1000)

        return LLMHealthResponse(
            healthy=True,
            duration_ms=duration_ms,
            provider="openai",
            model=response.model
        )

    except asyncio.TimeoutError:
        duration_ms = int((time.time() - start_time) * 1000)
        return LLMHealthResponse(
            healthy=False,
            duration_ms=duration_ms,
            provider="openai",
            error="Request timeout"
        )
    except Exception as e:
        duration_ms = int((time.time() - start_time) * 1000)
        return LLMHealthResponse(
            healthy=False,
            duration_ms=duration_ms,
            provider="openai",
            error=str(e)
        )

async def check_anthropic_api() -> LLMHealthResponse:
    """
    æ£€æŸ¥ Anthropic API å¯ç”¨æ€§

    ä½¿ç”¨æœ€å°çš„è¯·æ±‚æ¥æ£€æŸ¥ API æ˜¯å¦å¯ç”¨
    """
    start_time = time.time()

    try:
        # å‘é€æœ€å°çš„è¯·æ±‚ï¼ˆ1 tokenï¼‰
        response = await asyncio.wait_for(
            anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=1,
                messages=[{"role": "user", "content": "hi"}]
            ),
            timeout=10.0
        )

        duration_ms = int((time.time() - start_time) * 1000)

        return LLMHealthResponse(
            healthy=True,
            duration_ms=duration_ms,
            provider="anthropic",
            model=response.model
        )

    except asyncio.TimeoutError:
        duration_ms = int((time.time() - start_time) * 1000)
        return LLMHealthResponse(
            healthy=False,
            duration_ms=duration_ms,
            provider="anthropic",
            error="Request timeout"
        )
    except Exception as e:
        duration_ms = int((time.time() - start_time) * 1000)
        return LLMHealthResponse(
            healthy=False,
            duration_ms=duration_ms,
            provider="anthropic",
            error=str(e)
        )

@app.get("/health/llm/openai", response_model=LLMHealthResponse)
async def health_llm_openai():
    """OpenAI API å¥åº·æ£€æŸ¥"""
    result = await check_openai_api()

    if not result.healthy:
        raise HTTPException(503, detail=result.dict())

    return result

@app.get("/health/llm/anthropic", response_model=LLMHealthResponse)
async def health_llm_anthropic():
    """Anthropic API å¥åº·æ£€æŸ¥"""
    result = await check_anthropic_api()

    if not result.healthy:
        raise HTTPException(503, detail=result.dict())

    return result

# ===== 7. å‘é‡æ•°æ®åº“å¥åº·æ£€æŸ¥ =====

async def check_vector_db() -> VectorDBHealthResponse:
    """
    æ£€æŸ¥å‘é‡æ•°æ®åº“ï¼ˆpgvectorï¼‰

    æ£€æŸ¥ pgvector æ‰©å±•æ˜¯å¦å®‰è£…ï¼Œembeddings è¡¨æ˜¯å¦å­˜åœ¨
    """
    start_time = time.time()

    try:
        async with AsyncSession(engine) as session:
            # 1. æ£€æŸ¥ pgvector æ‰©å±•æ˜¯å¦å®‰è£…
            result = await session.execute(text("""
                SELECT EXISTS (
                    SELECT FROM pg_extension
                    WHERE extname = 'vector'
                )
            """))
            extension_installed = result.scalar()

            # 2. æ£€æŸ¥ embeddings è¡¨æ˜¯å¦å­˜åœ¨
            result = await session.execute(text("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_name = 'embeddings'
                )
            """))
            table_exists = result.scalar()

            # 3. å¦‚æœè¡¨å­˜åœ¨ï¼Œæ‰§è¡Œç®€å•çš„å‘é‡æŸ¥è¯¢
            if table_exists:
                await asyncio.wait_for(
                    session.execute(text("""
                        SELECT id FROM embeddings
                        ORDER BY embedding <-> '[0,0,0]'::vector
                        LIMIT 1
                    """)),
                    timeout=3.0
                )

        duration_ms = int((time.time() - start_time) * 1000)

        return VectorDBHealthResponse(
            healthy=extension_installed and table_exists,
            duration_ms=duration_ms,
            extension_installed=extension_installed,
            table_exists=table_exists
        )

    except asyncio.TimeoutError:
        duration_ms = int((time.time() - start_time) * 1000)
        return VectorDBHealthResponse(
            healthy=False,
            duration_ms=duration_ms,
            extension_installed=False,
            table_exists=False,
            error="Query timeout"
        )
    except Exception as e:
        duration_ms = int((time.time() - start_time) * 1000)
        return VectorDBHealthResponse(
            healthy=False,
            duration_ms=duration_ms,
            extension_installed=False,
            table_exists=False,
            error=str(e)
        )

@app.get("/health/vector_db", response_model=VectorDBHealthResponse)
async def health_vector_db():
    """å‘é‡æ•°æ®åº“å¥åº·æ£€æŸ¥"""
    result = await check_vector_db()

    if not result.healthy:
        raise HTTPException(503, detail=result.dict())

    return result

# ===== 8. Embedding æ¨¡å‹å¥åº·æ£€æŸ¥ =====

async def check_embedding_model() -> EmbeddingHealthResponse:
    """
    æ£€æŸ¥ Embedding æ¨¡å‹æ˜¯å¦å·²åŠ è½½

    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨ï¼Œå¹¶æµ‹è¯•ç¼–ç åŠŸèƒ½
    """
    global embedding_model

    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if embedding_model is None:
            return EmbeddingHealthResponse(
                healthy=False,
                loaded=False,
                error="Embedding model not loaded"
            )

        # æµ‹è¯•æ¨¡å‹ç¼–ç åŠŸèƒ½
        test_text = "test"
        embedding = embedding_model.encode(test_text)

        return EmbeddingHealthResponse(
            healthy=True,
            loaded=True,
            model_name=embedding_model.get_sentence_embedding_dimension() if hasattr(embedding_model, 'get_sentence_embedding_dimension') else None
        )

    except Exception as e:
        return EmbeddingHealthResponse(
            healthy=False,
            loaded=embedding_model is not None,
            error=str(e)
        )

@app.get("/health/embedding", response_model=EmbeddingHealthResponse)
async def health_embedding():
    """Embedding æ¨¡å‹å¥åº·æ£€æŸ¥"""
    result = await check_embedding_model()

    if not result.healthy:
        raise HTTPException(503, detail=result.dict())

    return result

# ===== 9. Agent ä»»åŠ¡é˜Ÿåˆ—å¥åº·æ£€æŸ¥ =====

async def check_agent_task_queue() -> Dict:
    """
    æ£€æŸ¥ Agent ä»»åŠ¡é˜Ÿåˆ—

    æ£€æŸ¥ Redis é˜Ÿåˆ—é•¿åº¦ï¼Œåˆ¤æ–­æ˜¯å¦ç§¯å‹
    """
    try:
        # æ£€æŸ¥ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦
        queue_length = await redis_client.llen("agent_tasks")

        # åˆ¤æ–­æ˜¯å¦ç§¯å‹
        if queue_length > 1000:
            return {
                "healthy": False,
                "queue_length": queue_length,
                "error": "Task queue backlog"
            }

        return {
            "healthy": True,
            "queue_length": queue_length
        }

    except Exception as e:
        return {
            "healthy": False,
            "error": str(e)
        }

@app.get("/health/agent/queue")
async def health_agent_queue():
    """Agent ä»»åŠ¡é˜Ÿåˆ—å¥åº·æ£€æŸ¥"""
    result = await check_agent_task_queue()

    if not result["healthy"]:
        raise HTTPException(503, detail=result)

    return result

# ===== 10. RAG ç³»ç»Ÿç«¯åˆ°ç«¯å¥åº·æ£€æŸ¥ =====

async def check_rag_system() -> RAGHealthResponse:
    """
    RAG ç³»ç»Ÿç«¯åˆ°ç«¯å¥åº·æ£€æŸ¥

    æ£€æŸ¥ RAG ç³»ç»Ÿçš„æ‰€æœ‰ç»„ä»¶
    """
    start_time = time.time()

    try:
        # å¹¶å‘æ£€æŸ¥æ‰€æœ‰ç»„ä»¶
        results = await asyncio.gather(
            check_openai_api(),
            check_vector_db(),
            check_embedding_model(),
            return_exceptions=True
        )

        # è§£æç»“æœ
        checks = {
            "llm_api": results[0].healthy if not isinstance(results[0], Exception) else False,
            "vector_db": results[1].healthy if not isinstance(results[1], Exception) else False,
            "embedding_model": results[2].healthy if not isinstance(results[2], Exception) else False,
        }

        duration_ms = int((time.time() - start_time) * 1000)

        # åˆ¤æ–­æ•´ä½“å¥åº·çŠ¶æ€
        healthy = all(checks.values())

        return RAGHealthResponse(
            healthy=healthy,
            duration_ms=duration_ms,
            checks=checks
        )

    except Exception as e:
        duration_ms = int((time.time() - start_time) * 1000)
        return RAGHealthResponse(
            healthy=False,
            duration_ms=duration_ms,
            checks={},
            error=str(e)
        )

@app.get("/health/rag", response_model=RAGHealthResponse)
async def health_rag():
    """RAG ç³»ç»Ÿç«¯åˆ°ç«¯å¥åº·æ£€æŸ¥"""
    result = await check_rag_system()

    if not result.healthy:
        raise HTTPException(503, detail=result.dict())

    return result

# ===== 11. å®Œæ•´çš„ AI Agent å¥åº·æ£€æŸ¥ =====

class AIAgentHealthResponse(BaseModel):
    """AI Agent å¥åº·æ£€æŸ¥å“åº”"""
    status: str  # healthy, degraded, unhealthy
    duration_ms: int
    checks: Dict[str, Dict]
    message: Optional[str] = None

async def check_ai_agent_complete() -> AIAgentHealthResponse:
    """
    å®Œæ•´çš„ AI Agent å¥åº·æ£€æŸ¥

    æ£€æŸ¥æ‰€æœ‰ AI Agent ç›¸å…³ç»„ä»¶
    """
    start_time = time.time()

    try:
        # å¹¶å‘æ£€æŸ¥æ‰€æœ‰ç»„ä»¶
        results = await asyncio.gather(
            check_openai_api(),
            check_vector_db(),
            check_embedding_model(),
            check_agent_task_queue(),
            return_exceptions=True
        )

        # è§£æç»“æœ
        checks = {
            "llm_api": {
                "healthy": results[0].healthy if not isinstance(results[0], Exception) else False,
                "duration_ms": results[0].duration_ms if not isinstance(results[0], Exception) else 0,
                "error": results[0].error if not isinstance(results[0], Exception) else str(results[0])
            },
            "vector_db": {
                "healthy": results[1].healthy if not isinstance(results[1], Exception) else False,
                "duration_ms": results[1].duration_ms if not isinstance(results[1], Exception) else 0,
                "error": results[1].error if not isinstance(results[1], Exception) else str(results[1])
            },
            "embedding_model": {
                "healthy": results[2].healthy if not isinstance(results[2], Exception) else False,
                "loaded": results[2].loaded if not isinstance(results[2], Exception) else False,
                "error": results[2].error if not isinstance(results[2], Exception) else str(results[2])
            },
            "task_queue": results[3] if not isinstance(results[3], Exception) else {"healthy": False, "error": str(results[3])}
        }

        duration_ms = int((time.time() - start_time) * 1000)

        # åˆ¤æ–­æ•´ä½“çŠ¶æ€
        all_healthy = all(check["healthy"] for check in checks.values())
        core_healthy = checks["llm_api"]["healthy"] and checks["vector_db"]["healthy"]

        if all_healthy:
            status = "healthy"
            message = "All AI Agent components are healthy"
        elif core_healthy:
            status = "degraded"
            failed = [k for k, v in checks.items() if not v["healthy"]]
            message = f"Running in degraded mode: {', '.join(failed)} unavailable"
        else:
            status = "unhealthy"
            message = "Core AI Agent components are unhealthy"

        return AIAgentHealthResponse(
            status=status,
            duration_ms=duration_ms,
            checks=checks,
            message=message
        )

    except Exception as e:
        duration_ms = int((time.time() - start_time) * 1000)
        return AIAgentHealthResponse(
            status="unhealthy",
            duration_ms=duration_ms,
            checks={},
            message=str(e)
        )

@app.get("/health/ai_agent", response_model=AIAgentHealthResponse)
async def health_ai_agent():
    """å®Œæ•´çš„ AI Agent å¥åº·æ£€æŸ¥"""
    result = await check_ai_agent_complete()

    if result.status == "unhealthy":
        raise HTTPException(503, detail=result.dict())

    return result

# ===== 12. å¯åŠ¨å’Œå…³é—­äº‹ä»¶ =====

@app.on_event("startup")
async def startup():
    """åº”ç”¨å¯åŠ¨"""
    global embedding_model

    print("ğŸš€ Starting AI Agent API...")

    # åŠ è½½ Embedding æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            print("ğŸ“¦ Loading Embedding model...")
            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            print("âœ… Embedding model loaded")
        except Exception as e:
            print(f"âŒ Failed to load Embedding model: {e}")
    else:
        print("âš ï¸  sentence-transformers not available")

    # æµ‹è¯• AI Agent å¥åº·æ£€æŸ¥
    try:
        result = await check_ai_agent_complete()
        print(f"ğŸ“Š AI Agent status: {result.status}")
    except Exception as e:
        print(f"âŒ AI Agent health check failed: {e}")

@app.on_event("shutdown")
async def shutdown():
    """åº”ç”¨å…³é—­"""
    print("ğŸ‘‹ Shutting down AI Agent API...")

    # å…³é—­è¿æ¥
    await engine.dispose()
    await redis_client.close()

    print("âœ… Connections closed")

# ===== 13. è¿è¡Œè¯´æ˜ =====

if __name__ == "__main__":
    import uvicorn

    print("=" * 50)
    print("AI Agent ç‰¹å®šå¥åº·æ£€æŸ¥å®ç°")
    print("=" * 50)
    print()
    print("ç«¯ç‚¹ï¼š")
    print("  /health/llm/openai     - OpenAI API æ£€æŸ¥")
    print("  /health/llm/anthropic  - Anthropic API æ£€æŸ¥")
    print("  /health/vector_db      - å‘é‡æ•°æ®åº“æ£€æŸ¥")
    print("  /health/embedding      - Embedding æ¨¡å‹æ£€æŸ¥")
    print("  /health/agent/queue    - Agent ä»»åŠ¡é˜Ÿåˆ—æ£€æŸ¥")
    print("  /health/rag            - RAG ç³»ç»Ÿç«¯åˆ°ç«¯æ£€æŸ¥")
    print("  /health/ai_agent       - å®Œæ•´çš„ AI Agent æ£€æŸ¥")
    print()
    print("ç¯å¢ƒå˜é‡ï¼š")
    print("  OPENAI_API_KEY         - OpenAI API å¯†é’¥")
    print("  ANTHROPIC_API_KEY      - Anthropic API å¯†é’¥")
    print("  DATABASE_URL           - æ•°æ®åº“è¿æ¥ URL")
    print("  REDIS_URL              - Redis è¿æ¥ URL")
    print()
    print("=" * 50)

    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## ç¯å¢ƒé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨ uv å®‰è£…ä¾èµ–
uv add fastapi uvicorn[standard] \
  openai anthropic \
  sqlalchemy[asyncio] asyncpg \
  redis sentence-transformers
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# LLM API
OPENAI_API_KEY=sk-...
OPENAI_BASE_URL=https://api.openai.com/v1

ANTHROPIC_API_KEY=sk-ant-...

# æ•°æ®åº“
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/dbname

# Redis
REDIS_URL=redis://localhost:6379/0
```

---

## è¿è¡Œç¤ºä¾‹

### 1. å¯åŠ¨æœåŠ¡

```bash
python main.py
```

### 2. æµ‹è¯•ç«¯ç‚¹

**OpenAI API æ£€æŸ¥ï¼š**

```bash
curl http://localhost:8000/health/llm/openai
```

**è¾“å‡ºï¼š**

```json
{
  "healthy": true,
  "duration_ms": 1500,
  "provider": "openai",
  "model": "gpt-3.5-turbo-0125",
  "error": null
}
```

**å‘é‡æ•°æ®åº“æ£€æŸ¥ï¼š**

```bash
curl http://localhost:8000/health/vector_db
```

**è¾“å‡ºï¼š**

```json
{
  "healthy": true,
  "duration_ms": 50,
  "extension_installed": true,
  "table_exists": true,
  "error": null
}
```

**Embedding æ¨¡å‹æ£€æŸ¥ï¼š**

```bash
curl http://localhost:8000/health/embedding
```

**è¾“å‡ºï¼š**

```json
{
  "healthy": true,
  "loaded": true,
  "model_name": "384",
  "error": null
}
```

**RAG ç³»ç»Ÿæ£€æŸ¥ï¼š**

```bash
curl http://localhost:8000/health/rag
```

**è¾“å‡ºï¼š**

```json
{
  "healthy": true,
  "duration_ms": 1600,
  "checks": {
    "llm_api": true,
    "vector_db": true,
    "embedding_model": true
  },
  "error": null
}
```

**å®Œæ•´çš„ AI Agent æ£€æŸ¥ï¼š**

```bash
curl http://localhost:8000/health/ai_agent
```

**è¾“å‡ºï¼š**

```json
{
  "status": "healthy",
  "duration_ms": 1650,
  "checks": {
    "llm_api": {
      "healthy": true,
      "duration_ms": 1500,
      "error": null
    },
    "vector_db": {
      "healthy": true,
      "duration_ms": 50,
      "error": null
    },
    "embedding_model": {
      "healthy": true,
      "loaded": true,
      "error": null
    },
    "task_queue": {
      "healthy": true,
      "queue_length": 0
    }
  },
  "message": "All AI Agent components are healthy"
}
```

---

## å…³é”®è¦ç‚¹

### 1. LLM API æ£€æŸ¥ç­–ç•¥

**æœ€å°è¯·æ±‚ï¼š**
- ä½¿ç”¨æœ€å°çš„ token æ•°ï¼ˆ1 tokenï¼‰
- è¶…æ—¶æ—¶é—´ 10 ç§’ï¼ˆLLM API è¾ƒæ…¢ï¼‰
- ç¼“å­˜æ—¶é—´ 5 åˆ†é’Ÿï¼ˆé¿å…é¢‘ç¹è°ƒç”¨ï¼‰

### 2. å‘é‡æ•°æ®åº“æ£€æŸ¥

**æ£€æŸ¥å†…å®¹ï¼š**
- pgvector æ‰©å±•æ˜¯å¦å®‰è£…
- embeddings è¡¨æ˜¯å¦å­˜åœ¨
- ç®€å•çš„å‘é‡æŸ¥è¯¢æ˜¯å¦æ­£å¸¸

### 3. Embedding æ¨¡å‹æ£€æŸ¥

**æ£€æŸ¥å†…å®¹ï¼š**
- æ¨¡å‹æ˜¯å¦å·²åŠ è½½
- æ¨¡å‹ç¼–ç åŠŸèƒ½æ˜¯å¦æ­£å¸¸

### 4. ä»»åŠ¡é˜Ÿåˆ—æ£€æŸ¥

**æ£€æŸ¥å†…å®¹ï¼š**
- é˜Ÿåˆ—é•¿åº¦æ˜¯å¦æ­£å¸¸
- æ˜¯å¦æœ‰ç§¯å‹ï¼ˆ> 1000ï¼‰

### 5. ä¸‰æ€æ¨¡å‹

- **healthy**ï¼šæ‰€æœ‰ç»„ä»¶æ­£å¸¸
- **degraded**ï¼šæ ¸å¿ƒç»„ä»¶æ­£å¸¸ï¼Œå¯é€‰ç»„ä»¶å¤±è´¥
- **unhealthy**ï¼šæ ¸å¿ƒç»„ä»¶å¤±è´¥

---

## åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„åº”ç”¨

### æ¨èé…ç½®

```python
@app.get("/ready")
async def ready():
    """ç”Ÿäº§ç¯å¢ƒå°±ç»ªæ£€æŸ¥"""
    result = await check_ai_agent_complete()

    # æ ¸å¿ƒç»„ä»¶å¤±è´¥ â†’ ä¸å¯ç”¨
    if result.status == "unhealthy":
        raise HTTPException(503, detail=result.dict())

    # é™çº§æ¨¡å¼ â†’ å¯ç”¨ä½†è­¦å‘Š
    if result.status == "degraded":
        return {
            "status": "degraded",
            "message": result.message,
            "checks": result.checks
        }

    return {
        "status": "healthy",
        "checks": result.checks
    }
```

---

## æ€»ç»“

AI Agent ç‰¹å®šå¥åº·æ£€æŸ¥çš„å…³é”®ï¼š

1. **LLM API**ï¼šä½¿ç”¨æœ€å°è¯·æ±‚ï¼Œé•¿è¶…æ—¶ï¼Œé•¿ç¼“å­˜
2. **å‘é‡æ•°æ®åº“**ï¼šæ£€æŸ¥æ‰©å±•å’Œè¡¨ï¼Œæ‰§è¡Œç®€å•æŸ¥è¯¢
3. **Embedding æ¨¡å‹**ï¼šæ£€æŸ¥åŠ è½½çŠ¶æ€å’Œç¼–ç åŠŸèƒ½
4. **ä»»åŠ¡é˜Ÿåˆ—**ï¼šæ£€æŸ¥é˜Ÿåˆ—é•¿åº¦ï¼Œé¿å…ç§¯å‹
5. **RAG ç³»ç»Ÿ**ï¼šç«¯åˆ°ç«¯æ£€æŸ¥æ‰€æœ‰ç»„ä»¶
6. **ä¸‰æ€æ¨¡å‹**ï¼šå¥åº·ã€é™çº§ã€ä¸å¥åº·

åœ¨ AI Agent åç«¯ä¸­ï¼Œåˆç†çš„å¥åº·æ£€æŸ¥å¯ä»¥ç¡®ä¿ RAG ç³»ç»Ÿçš„å¯ç”¨æ€§å’Œå¯é æ€§ã€‚
