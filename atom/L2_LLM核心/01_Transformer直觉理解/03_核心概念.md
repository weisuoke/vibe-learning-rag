# 核心概念

> 自注意力、多头注意力、位置编码 —— Transformer 的三大支柱

---

## 概述

Transformer 的核心就是三个概念：

| 概念 | 一句话解释 | 解决什么问题 |
|-----|----------|-------------|
| **自注意力** | 每个词"看"其他所有词，决定关注谁 | 理解词与词之间的关系 |
| **多头注意力** | 从多个角度同时理解 | 捕捉不同类型的关系 |
| **位置编码** | 告诉模型词的顺序 | 让模型知道"谁在前谁在后" |

---

## 核心概念1：自注意力（Self-Attention）

### 一句话定义

**自注意力让每个词都能"看到"句子中的所有其他词，并决定应该关注谁。**

### 直觉理解

想象你在开会，讨论"项目延期"的问题：

```
参会者: 产品经理、开发、测试、设计师、老板

当讨论"延期原因"时:
- 你会更关注 → 开发、测试（他们知道技术细节）
- 你会少关注 → 设计师（可能不太相关）

当讨论"客户反馈"时:
- 你会更关注 → 产品经理、老板
- 你会少关注 → 开发、测试
```

**这就是自注意力！** 根据当前的"任务"，动态决定关注谁。

### Query、Key、Value 的直觉

这三个概念是自注意力的核心，但不需要数学就能理解：

```
场景：在图书馆找书

Query（查询）: 你脑子里想找的东西
  → "我想找一本关于机器学习的入门书"

Key（键）: 每本书的标签/索引
  → "Python编程"、"机器学习基础"、"高等数学"...

Value（值）: 书的实际内容
  → 书里面写的东西
```

**工作流程：**

```
1. 你带着 Query（想找什么）去图书馆
2. 把 Query 和每本书的 Key（标签）对比
3. 找到匹配度高的书
4. 取出这些书的 Value（内容）
5. 匹配度越高的书，你看得越仔细
```

### 在 Transformer 中

```
句子: "小猫 坐在 垫子 上"

处理"小猫"这个词时:
- Query: "小猫"想知道自己应该关注谁
- Key: 每个词都有一个"标签"供匹配
- Value: 每个词的实际信息

结果:
- "小猫" 关注 "坐在" (动作相关) → 权重高
- "小猫" 关注 "垫子" (位置相关) → 权重中
- "小猫" 关注 "上" (方位词) → 权重低
```

### 可视化

```
        小猫    坐在    垫子    上
小猫    [0.4]   [0.3]   [0.2]   [0.1]
坐在    [0.3]   [0.2]   [0.3]   [0.2]
垫子    [0.2]   [0.2]   [0.3]   [0.3]
上      [0.1]   [0.2]   [0.4]   [0.3]

↑ 每一行表示这个词对其他词的注意力权重
  数字越大 = 关注越多
```

### 在 RAG 中的应用

```
RAG 场景:
用户问题: "Python 怎么读取 JSON 文件？"
检索到的上下文: "使用 json.load() 函数可以读取 JSON 文件..."

Transformer 的自注意力会:
- 让"读取"高度关注检索内容中的"json.load()"
- 让"JSON"高度关注检索内容中的"JSON 文件"
- 从而生成准确的答案
```

---

## 核心概念2：多头注意力（Multi-Head Attention）

### 一句话定义

**多头注意力是同时从多个角度理解句子，每个"头"关注不同类型的关系。**

### 直觉理解

看一部电影时，你会同时关注多个方面：

```
电影: 《盗梦空间》

头1（剧情头）: 关注故事发展、人物关系
头2（视觉头）: 关注画面、特效、构图
头3（音乐头）: 关注配乐、音效
头4（情感头）: 关注角色情绪、氛围

最终理解 = 综合所有角度的信息
```

### 为什么需要多个头？

一个句子可能有多种关系需要捕捉：

```
句子: "小明把苹果给了小红，因为她很饿"

头1（语法头）: 关注主谓宾结构
  → 小明(主) 给(谓) 苹果(宾)

头2（指代头）: 关注代词指向
  → "她" → "小红"

头3（因果头）: 关注因果关系
  → "饿" → "给苹果"

头4（实体头）: 关注人名、物品
  → 小明、小红、苹果
```

**单个注意力头很难同时捕捉所有这些关系，所以需要多个头分工合作。**

### 可视化

```
输入: "The cat sat on the mat"

头1 (语法关系):          头2 (位置关系):
The → cat (强)          cat → mat (强)
cat → sat (强)          sat → on (强)
sat → on (中)           on → mat (强)

头3 (主题关系):          头4 (其他):
cat → mat (强)          ...
The → The (强)

最终输出 = 合并所有头的理解
```

### 在 RAG 中的应用

```
RAG 场景:
问题: "2023年苹果公司的收入是多少？"

不同的头关注不同方面:
- 头1: 关注"苹果公司"（实体识别）
- 头2: 关注"2023年"（时间限定）
- 头3: 关注"收入"（查询意图）
- 头4: 关注检索内容中的数字

综合起来 → 准确定位答案
```

---

## 核心概念3：位置编码（Positional Encoding）

### 一句话定义

**位置编码告诉 Transformer 每个词在句子中的位置，因为注意力机制本身不知道顺序。**

### 为什么需要位置编码？

**关键洞察：自注意力是"无序"的！**

```
对于自注意力来说，这两个句子是一样的:

句子1: "狗 咬 人"
句子2: "人 咬 狗"

因为注意力只看"谁和谁相关"，不看"谁在前谁在后"
```

但这两个句子的意思完全不同！所以需要告诉模型词的位置。

### 直觉理解

```
场景: 排队买奶茶

没有位置信息:
  小明、小红、小刚 都在排队
  → 不知道谁先谁后

有位置信息:
  小明(1号)、小红(2号)、小刚(3号)
  → 知道小明排第一
```

位置编码就是给每个词发一个"号码牌"。

### 工作方式

```
原始输入:
"我 爱 北京"
 ↓
词向量:
[我的向量] [爱的向量] [北京的向量]
 ↓
加上位置编码:
[我的向量 + 位置1] [爱的向量 + 位置2] [北京的向量 + 位置3]
```

位置编码是一个和词向量同样大小的向量，直接加到词向量上。

### 位置编码的特点

```
1. 每个位置有唯一的编码
   位置1 ≠ 位置2 ≠ 位置3

2. 相邻位置的编码相似
   位置1 和 位置2 比较接近
   位置1 和 位置100 差别大

3. 可以表示相对位置
   模型能学会"位置3在位置1后面2个位置"
```

### 在 RAG 中的应用

**位置在 RAG 中非常重要！**

```
RAG Prompt 结构:
┌─────────────────────────────────────┐
│ 位置1-10: 系统指令                    │
│ 位置11-100: 检索到的上下文             │
│ 位置101-110: 用户问题                 │
└─────────────────────────────────────┘

研究发现:
- 开头和结尾的信息更容易被关注
- 中间的信息可能被"忽略"
- 这叫做 "Lost in the Middle" 问题
```

**实践建议：**
- 把最重要的检索结果放在开头或结尾
- 不要把关键信息埋在中间
- 控制上下文长度，避免信息过载

---

## 三个概念的协作

```
输入句子: "小猫坐在垫子上"

Step 1: 词向量 + 位置编码
  → 每个词有了"身份"(词义) 和 "位置"(顺序)

Step 2: 多头自注意力
  → 头1: 捕捉语法关系
  → 头2: 捕捉位置关系
  → 头3: 捕捉语义关系
  → ...

Step 3: 合并所有头的输出
  → 得到每个词的"上下文感知"表示

Step 4: 继续下一层...
  → 多层堆叠，理解越来越深
```

---

## 总结对比

| 概念 | 作用 | 类比 | RAG 应用 |
|-----|------|------|---------|
| 自注意力 | 决定关注谁 | 开会时关注相关的人 | 让模型关注检索到的相关内容 |
| 多头注意力 | 多角度理解 | 看电影时同时关注剧情、画面、音乐 | 同时理解实体、时间、意图等 |
| 位置编码 | 知道顺序 | 排队时的号码牌 | 影响信息放置位置的策略 |

---

## 检查清单

学完本节，你应该能回答：

- [ ] 自注意力是如何工作的？（用自己的话解释）
- [ ] Query、Key、Value 分别代表什么？
- [ ] 为什么需要多个注意力头？
- [ ] 为什么 Transformer 需要位置编码？
- [ ] 位置编码对 RAG 有什么影响？

---

**下一步：** [04_最小可用](./04_最小可用.md) - 掌握20%核心知识
