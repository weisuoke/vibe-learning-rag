# 30字核心

**RunnableConfig 是 LangChain 的运行时配置字典，通过 callbacks、tags、metadata 实现链路追踪、成本监控和动态参数调整。**

---

## 核心要素拆解

### 1. RunnableConfig（运行时配置）
- **本质**：传递给 `invoke()`/`stream()` 的配置字典
- **作用**：控制执行行为、注入回调、添加元数据
- **时机**：链创建时或调用时传入

### 2. Callbacks（回调系统）
- **本质**：事件监听器，捕获 LLM/Chain/Tool 的生命周期事件
- **作用**：日志记录、成本追踪、性能监控、错误告警
- **实现**：继承 `BaseCallbackHandler`，重写事件方法

### 3. Tags & Metadata（标签与元数据）
- **Tags**：用于过滤回调（如 `["production", "critical"]`）
- **Metadata**：可序列化的追踪信息（如 `{"user_id": "123", "session": "abc"}`）
- **用途**：LangSmith 追踪、成本归因、A/B 测试

### 4. Configurable Fields（可配置字段）
- **本质**：运行时可调整的参数（如 temperature、model_name）
- **作用**：无需重建链即可切换模型或调整参数
- **方法**：`configurable_fields()`、`configurable_alternatives()`

---

## 为什么重要？

### 在 LangChain 开发中
1. **生产监控必备**：通过回调追踪每次调用的成本、延迟、错误
2. **调试利器**：tags 和 metadata 让复杂链路的问题定位变得简单
3. **动态调优**：无需重启服务即可切换模型或调整参数
4. **成本控制**：实时追踪 token 消耗，设置预算告警

### 在 RAG 系统中
- **检索追踪**：记录每次检索的查询、结果数、相似度分数
- **生成监控**：追踪 LLM 调用的 prompt、response、token 数
- **成本归因**：按用户/会话统计成本，优化资源分配
- **质量评估**：收集输入输出数据用于离线评估

---

## 类比理解

### 前端开发类比
```javascript
// RunnableConfig 就像 Express 中间件配置
app.use(logger);        // ← 类似 callbacks
app.use(metrics);       // ← 类似 cost tracking
app.locals.env = 'prod'; // ← 类似 metadata

// 可配置字段类似环境变量
const apiUrl = process.env.API_URL || 'default';
```

### 日常生活类比
- **RunnableConfig**：外卖订单的备注栏（配送要求、联系方式、优惠券）
- **Callbacks**：订单状态推送（接单、制作中、配送中、已送达）
- **Tags**：订单标签（急单、VIP、首单优惠）
- **Metadata**：订单详情（用户ID、地址、支付方式）

---

## 最小可用示例

```python
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.runnables import RunnableConfig

# 1. 自定义回调：记录 token 消耗
class TokenCounter(BaseCallbackHandler):
    def __init__(self):
        self.total_tokens = 0

    def on_llm_end(self, response, **kwargs):
        tokens = response.llm_output.get("token_usage", {})
        self.total_tokens += tokens.get("total_tokens", 0)
        print(f"本次消耗: {tokens.get('total_tokens', 0)} tokens")

# 2. 创建配置
counter = TokenCounter()
config = RunnableConfig(
    callbacks=[counter],
    tags=["production", "cost-tracking"],
    metadata={"user_id": "user_123", "session": "abc"}
)

# 3. 使用配置调用
llm = ChatOpenAI(model="gpt-4")
response = llm.invoke("解释量子纠缠", config=config)

print(f"总消耗: {counter.total_tokens} tokens")
```

**输出示例**：
```
本次消耗: 245 tokens
总消耗: 245 tokens
```

---

## 关键要点

1. **配置传递时机**：可在链创建时或 `invoke()` 时传入，后者优先级更高
2. **回调执行顺序**：父链 → 子链，按添加顺序执行
3. **Tags 过滤机制**：回调可通过 `ignore_llm`、`ignore_chain` 等参数选择性监听
4. **Metadata 序列化**：必须是 JSON 可序列化的（避免使用对象实例）
5. **LangSmith 自动集成**：设置环境变量后自动启用追踪，无需手动配置

---

## 进阶方向

- **生产监控方案**：结合 OpenTelemetry 实现分布式追踪
- **成本优化策略**：基于历史数据动态调整 model 和 temperature
- **A/B 测试框架**：使用 configurable_alternatives 实现多模型对比
- **实时告警系统**：回调触发 webhook 通知异常情况

---

## 参考资料

- [RunnableConfig API Reference](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.config.RunnableConfig.html)
- [Custom Callbacks Guide](https://python.langchain.com/docs/how_to/custom_callbacks/)
- [LangSmith Observability Platform](https://www.langchain.com/langsmith/observability)
