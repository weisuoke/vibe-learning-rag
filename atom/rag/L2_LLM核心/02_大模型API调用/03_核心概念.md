# 大模型API调用 - 核心概念

掌握以下 3 个核心概念，就能理解大模型 API 的工作方式。

---

## 核心概念1：Chat Completions API

**对话补全 API：输入消息列表，输出模型回复。**

### 消息结构

```python
messages = [
    {"role": "system", "content": "你是一个helpful助手"},  # 系统设定
    {"role": "user", "content": "什么是RAG？"},           # 用户问题
    {"role": "assistant", "content": "RAG是..."},        # 模型回复（历史）
    {"role": "user", "content": "能举个例子吗？"}         # 新问题
]
```

### 三种角色

| 角色 | 作用 | 示例 |
|------|------|------|
| `system` | 设定模型行为、人设 | "你是专业的Python开发者" |
| `user` | 用户的输入 | "帮我写一个排序函数" |
| `assistant` | 模型的回复 | "好的，这是代码..." |

### 完整调用示例

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "你是RAG开发专家"},
        {"role": "user", "content": "什么是向量检索？"}
    ]
)

# 获取回复
answer = response.choices[0].message.content
print(answer)
```

### 在 RAG 开发中的应用

```python
# RAG 的典型 Prompt 结构
messages = [
    {
        "role": "system",
        "content": "基于以下参考资料回答问题，如果资料中没有相关信息，请说明。"
    },
    {
        "role": "user",
        "content": f"""
参考资料：
{retrieved_documents}

问题：{user_question}
"""
    }
]
```

---

## 核心概念2：流式输出 (Streaming)

**流式输出：模型边生成边返回，而不是等全部生成完再返回。**

### 为什么需要流式？

| 方式 | 用户体验 | 适用场景 |
|------|----------|----------|
| 非流式 | 等待 → 一次性显示全部 | 后台处理、批量任务 |
| 流式 | 逐字显示，像打字机 | 聊天界面、实时交互 |

**用户感知：**
- 非流式：等 5 秒 → 突然出现 500 字
- 流式：立即开始 → 逐字显示 → 5 秒后完成

### 代码实现

```python
from openai import OpenAI

client = OpenAI()

# 开启流式
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "讲个故事"}],
    stream=True  # 关键参数！
)

# 逐块处理
for chunk in stream:
    content = chunk.choices[0].delta.content
    if content:
        print(content, end="", flush=True)
```

### 流式 vs 非流式对比

```python
# 非流式：一次性返回
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    stream=False  # 默认值
)
full_text = response.choices[0].message.content

# 流式：逐块返回
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    stream=True
)
full_text = ""
for chunk in stream:
    if chunk.choices[0].delta.content:
        full_text += chunk.choices[0].delta.content
```

### 在 RAG 开发中的应用

```python
# RAG 聊天界面的流式输出
def stream_rag_response(question: str, context: str):
    """流式返回 RAG 生成的答案"""
    stream = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "基于上下文回答问题"},
            {"role": "user", "content": f"上下文：{context}\n\n问题：{question}"}
        ],
        stream=True
    )

    for chunk in stream:
        content = chunk.choices[0].delta.content
        if content:
            yield content  # 用于 Web 框架的 SSE
```

---

## 核心概念3：核心参数

**控制模型行为的关键参数：temperature、max_tokens、top_p。**

### 参数速查表

| 参数 | 作用 | 范围 | 推荐值 |
|------|------|------|--------|
| `temperature` | 控制随机性 | 0-2 | RAG: 0-0.3 |
| `max_tokens` | 最大输出长度 | 1-模型上限 | 按需设置 |
| `top_p` | 核采样概率 | 0-1 | 通常不改 |
| `stop` | 停止生成的标记 | 字符串列表 | 按需设置 |

### temperature 详解

```python
# temperature = 0：确定性输出，每次结果相同（几乎）
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "1+1=?"}],
    temperature=0  # 适合 RAG、事实性问答
)

# temperature = 1：平衡创意和准确性
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "写一首诗"}],
    temperature=1  # 适合创意写作
)

# temperature = 2：高度随机，可能不连贯
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "写一首诗"}],
    temperature=2  # 很少使用
)
```

**RAG 开发建议：** 使用 `temperature=0` 或 `0.1`，确保答案基于检索内容，减少幻觉。

### max_tokens 详解

```python
# 限制输出长度
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "详细解释量子力学"}],
    max_tokens=100  # 最多生成 100 个 token
)

# 注意：达到 max_tokens 会被截断！
# finish_reason 会是 "length" 而不是 "stop"
print(response.choices[0].finish_reason)  # "length" 表示被截断
```

**RAG 开发建议：** 根据预期答案长度设置，避免设置过小导致答案不完整。

### 完整参数示例

```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "你是RAG助手，基于上下文简洁回答"},
        {"role": "user", "content": f"上下文：{context}\n问题：{question}"}
    ],
    temperature=0.1,      # 低随机性，忠于检索内容
    max_tokens=500,       # 限制答案长度
    top_p=1,              # 默认值
    stop=["\n\n---"]      # 遇到分隔符停止
)
```

---

## 扩展概念：Anthropic Claude API

Anthropic 的 API 设计略有不同，但核心概念相同。

### 主要区别

| 特性 | OpenAI | Anthropic |
|------|--------|-----------|
| 系统提示 | `messages` 中的 `system` 角色 | 单独的 `system` 参数 |
| 输出长度 | `max_tokens` | `max_tokens`（必填） |
| 模型名 | `gpt-4o` | `claude-sonnet-4-20250514` |

### Anthropic 调用示例

```python
from anthropic import Anthropic

client = Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=1024,  # Anthropic 必须指定
    system="你是RAG开发专家",  # system 是独立参数
    messages=[
        {"role": "user", "content": "什么是向量检索？"}
    ]
)

answer = response.content[0].text
print(answer)
```

---

## 概念关系图

```
                    Chat Completions API
                           │
           ┌───────────────┼───────────────┐
           │               │               │
        messages        stream          参数控制
           │               │               │
    ┌──────┼──────┐        │        ┌──────┼──────┐
    │      │      │        │        │      │      │
  system  user  assistant  │    temperature max_tokens
    │      │      │        │        │      │
    └──────┴──────┘        │        └──────┴──────┘
           │               │               │
           └───────────────┴───────────────┘
                           │
                      RAG 生成模块
```

---

**上一节：** [02_第一性原理.md](./02_第一性原理.md)
**下一节：** [04_最小可用.md](./04_最小可用.md) - 20% 知识解决 80% 问题
