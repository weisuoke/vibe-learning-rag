# 核心概念 - 检索质量评估指标

## 概述

检索质量评估是RAG系统评估的第一步，决定了系统能否找到正确的上下文。本文档详细介绍2025-2026年生产环境中使用的核心检索评估指标。

---

## 一、Precision@K (精确率)

### 定义

Precision@K衡量检索结果Top-K中相关文档的比例。

**公式**:
```
Precision@K = 检索到的相关文档数 / K
```

### 计算示例

```python
# 示例数据
retrieved_docs = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']
relevant_docs = ['doc1', 'doc3', 'doc6', 'doc7']
k = 5

# 计算Precision@5
retrieved_k = set(retrieved_docs[:k])
relevant_set = set(relevant_docs)
hits = len(retrieved_k & relevant_set)

precision_at_k = hits / k
print(f"Precision@{k}: {precision_at_k}")  # 0.4 (2/5)
```

### Python实现

```python
def calculate_precision_at_k(retrieved_docs, relevant_docs, k):
    """
    计算Precision@K

    Args:
        retrieved_docs: 检索结果列表（按相关性排序）
        relevant_docs: 相关文档列表
        k: 评估的Top-K数量

    Returns:
        float: Precision@K分数 (0-1)
    """
    if k == 0 or len(retrieved_docs) == 0:
        return 0.0

    retrieved_k = set(retrieved_docs[:k])
    relevant_set = set(relevant_docs)

    hits = len(retrieved_k & relevant_set)
    return hits / k

# 使用示例
retrieved = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']
relevant = ['doc1', 'doc3', 'doc6']

for k in [1, 3, 5]:
    precision = calculate_precision_at_k(retrieved, relevant, k)
    print(f"Precision@{k}: {precision:.3f}")
```

### RAG应用场景

```python
# 场景: 企业知识库问答
question = "公司的年假政策是什么？"

# 检索结果
retrieved_docs = [
    'HR手册第3章',      # 相关 ✓
    '员工福利政策',     # 相关 ✓
    '公司简介',         # 无关 ✗
    '薪资制度',         # 无关 ✗
    '考勤制度'          # 无关 ✗
]

# 相关文档
relevant_docs = ['HR手册第3章', '员工福利政策', '假期管理规定']

precision_5 = calculate_precision_at_k(retrieved_docs, relevant_docs, 5)
# 0.4 - 60%的检索结果是无关的，会浪费Token和干扰生成
```

### 优化建议

| Precision@K | 评级 | 建议 |
|-------------|------|------|
| < 0.5 | 差 | 检索噪音严重，考虑ReRank或改进检索策略 |
| 0.5-0.7 | 中 | 可接受，但有优化空间 |
| 0.7-0.85 | 良好 | 检索精度较好 |
| > 0.85 | 优秀 | 检索精度优秀 |

---

## 二、Recall@K (召回率)

### 定义

Recall@K衡量所有相关文档中被检索到的比例。

**公式**:
```
Recall@K = 检索到的相关文档数 / 总相关文档数
```

### 计算示例

```python
# 示例数据
retrieved_docs = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']
relevant_docs = ['doc1', 'doc3', 'doc6', 'doc7']
k = 5

# 计算Recall@5
retrieved_k = set(retrieved_docs[:k])
relevant_set = set(relevant_docs)
hits = len(retrieved_k & relevant_set)

recall_at_k = hits / len(relevant_set) if relevant_set else 0
print(f"Recall@{k}: {recall_at_k}")  # 0.5 (2/4)
```

### Python实现

```python
def calculate_recall_at_k(retrieved_docs, relevant_docs, k):
    """
    计算Recall@K

    Args:
        retrieved_docs: 检索结果列表
        relevant_docs: 相关文档列表
        k: 评估的Top-K数量

    Returns:
        float: Recall@K分数 (0-1)
    """
    if len(relevant_docs) == 0:
        return 0.0

    retrieved_k = set(retrieved_docs[:k])
    relevant_set = set(relevant_docs)

    hits = len(retrieved_k & relevant_set)
    return hits / len(relevant_set)

# 使用示例
retrieved = ['doc1', 'doc2', 'doc3']
relevant = ['doc1', 'doc3', 'doc6', 'doc7', 'doc8']

for k in [1, 3, 5]:
    recall = calculate_recall_at_k(retrieved, relevant, k)
    print(f"Recall@{k}: {recall:.3f}")
```

### RAG应用场景

```python
# 场景: 技术文档检索
question = "如何配置Redis缓存？"

# 所有相关文档（8个）
all_relevant_docs = [
    'Redis配置指南',
    'Redis性能优化',
    'Redis集群配置',
    'Redis持久化',
    'Redis安全配置',
    'Redis监控',
    'Redis故障排查',
    'Redis最佳实践'
]

# 检索结果Top-5
retrieved_docs = [
    'Redis配置指南',    # 相关 ✓
    'Redis性能优化',    # 相关 ✓
    'MySQL配置',        # 无关 ✗
    'Nginx配置',        # 无关 ✗
    'Docker配置'        # 无关 ✗
]

recall_5 = calculate_recall_at_k(retrieved_docs, all_relevant_docs, 5)
# 0.25 - 只找到了25%的相关文档，漏掉了75%
```

### 优化建议

| Recall@K | 评级 | 建议 |
|----------|------|------|
| < 0.6 | 差 | 检索遗漏严重，考虑增加K或改进检索策略 |
| 0.6-0.75 | 中 | 可接受，但有优化空间 |
| 0.75-0.85 | 良好 | 召回率较好 |
| > 0.85 | 优秀 | 召回率优秀 |

---

## 三、MRR (Mean Reciprocal Rank)

### 定义

MRR衡量第一个相关结果的排名，关注用户体验。

**公式**:
```
MRR = 1 / 第一个相关文档的排名

对于多个查询:
MRR = (1/N) * Σ(1 / rank_i)
```

### 计算示例

```python
# 单个查询
retrieved = ['doc2', 'doc3', 'doc1', 'doc4']
relevant = ['doc1', 'doc5']

# 第一个相关文档doc1在第3位
mrr = 1 / 3
print(f"MRR: {mrr:.3f}")  # 0.333
```

### Python实现

```python
def calculate_mrr(retrieved_docs, relevant_docs):
    """
    计算MRR (Mean Reciprocal Rank)

    Args:
        retrieved_docs: 检索结果列表
        relevant_docs: 相关文档列表

    Returns:
        float: MRR分数 (0-1)
    """
    relevant_set = set(relevant_docs)

    for i, doc in enumerate(retrieved_docs, 1):
        if doc in relevant_set:
            return 1.0 / i

    return 0.0

def calculate_mrr_batch(queries_results):
    """
    计算多个查询的平均MRR

    Args:
        queries_results: [(retrieved_docs, relevant_docs), ...]

    Returns:
        float: 平均MRR分数
    """
    if not queries_results:
        return 0.0

    mrr_sum = sum(
        calculate_mrr(retrieved, relevant)
        for retrieved, relevant in queries_results
    )

    return mrr_sum / len(queries_results)

# 使用示例
queries = [
    (['doc1', 'doc2', 'doc3'], ['doc1', 'doc4']),  # MRR = 1.0
    (['doc2', 'doc3', 'doc1'], ['doc1', 'doc4']),  # MRR = 0.33
    (['doc2', 'doc3', 'doc4'], ['doc1', 'doc5']),  # MRR = 0.0
]

avg_mrr = calculate_mrr_batch(queries)
print(f"Average MRR: {avg_mrr:.3f}")
```

### RAG应用场景

```python
# 场景: 客服问答系统
questions = [
    "如何退货？",
    "退货流程是什么？",
    "怎么申请退款？"
]

# 查询1: 第1个结果就是相关的
query1_results = ['退货流程指南', '售后政策', '物流信息']
query1_relevant = ['退货流程指南']
mrr1 = calculate_mrr(query1_results, query1_relevant)  # 1.0 ✓

# 查询2: 第3个结果才是相关的
query2_results = ['售后政策', '物流信息', '退货流程指南']
query2_relevant = ['退货流程指南']
mrr2 = calculate_mrr(query2_results, query2_relevant)  # 0.33 ✗

# 查询3: 没有相关结果
query3_results = ['售后政策', '物流信息', '联系客服']
query3_relevant = ['退货流程指南']
mrr3 = calculate_mrr(query3_results, query3_relevant)  # 0.0 ✗

avg_mrr = (mrr1 + mrr2 + mrr3) / 3
print(f"Average MRR: {avg_mrr:.3f}")  # 0.44
```

### 优化建议

| MRR | 评级 | 建议 |
|-----|------|------|
| < 0.4 | 差 | 排序质量差，考虑ReRank |
| 0.4-0.6 | 中 | 可接受，但有优化空间 |
| 0.6-0.8 | 良好 | 排序质量较好 |
| > 0.8 | 优秀 | 排序质量优秀 |

---

## 四、NDCG@K (Normalized Discounted Cumulative Gain)

### 定义

NDCG@K考虑了排序位置和相关性程度，是最全面的排序质量指标。

**公式**:
```
DCG@K = Σ(rel_i / log2(i + 1))  for i in 1..K
NDCG@K = DCG@K / IDCG@K

其中:
- rel_i: 第i个文档的相关性分数
- IDCG@K: 理想排序的DCG (Ideal DCG)
```

### 计算示例

```python
import math

# 示例数据
retrieved_docs = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']
relevance_scores = {
    'doc1': 3,  # 高相关
    'doc2': 1,  # 低相关
    'doc3': 2,  # 中相关
    'doc4': 0,  # 无关
    'doc5': 3   # 高相关
}

# 计算DCG@5
dcg = 0
for i, doc in enumerate(retrieved_docs[:5], 1):
    rel = relevance_scores.get(doc, 0)
    dcg += rel / math.log2(i + 1)

print(f"DCG@5: {dcg:.3f}")
```

### Python实现

```python
import math

def calculate_dcg_at_k(retrieved_docs, relevance_scores, k):
    """
    计算DCG@K

    Args:
        retrieved_docs: 检索结果列表
        relevance_scores: {doc_id: relevance_score}
        k: 评估的Top-K数量

    Returns:
        float: DCG@K分数
    """
    dcg = 0.0
    for i, doc in enumerate(retrieved_docs[:k], 1):
        rel = relevance_scores.get(doc, 0)
        dcg += rel / math.log2(i + 1)
    return dcg

def calculate_ndcg_at_k(retrieved_docs, relevance_scores, k):
    """
    计算NDCG@K

    Args:
        retrieved_docs: 检索结果列表
        relevance_scores: {doc_id: relevance_score}
        k: 评估的Top-K数量

    Returns:
        float: NDCG@K分数 (0-1)
    """
    # 计算实际DCG
    dcg = calculate_dcg_at_k(retrieved_docs, relevance_scores, k)

    # 计算理想DCG (按相关性排序)
    ideal_docs = sorted(
        relevance_scores.keys(),
        key=lambda x: relevance_scores[x],
        reverse=True
    )
    idcg = calculate_dcg_at_k(ideal_docs, relevance_scores, k)

    if idcg == 0:
        return 0.0

    return dcg / idcg

# 使用示例
retrieved = ['doc1', 'doc2', 'doc3', 'doc4', 'doc5']
relevance = {
    'doc1': 3,
    'doc2': 1,
    'doc3': 2,
    'doc4': 0,
    'doc5': 3
}

for k in [1, 3, 5]:
    ndcg = calculate_ndcg_at_k(retrieved, relevance, k)
    print(f"NDCG@{k}: {ndcg:.3f}")
```

### 使用ranx库 (推荐)

```python
from ranx import Qrels, Run, evaluate

# 准备数据
qrels = Qrels({
    'q1': {
        'doc1': 3,
        'doc2': 1,
        'doc3': 2,
        'doc5': 3
    }
})

run = Run({
    'q1': {
        'doc1': 0.9,
        'doc2': 0.8,
        'doc3': 0.7,
        'doc4': 0.6,
        'doc5': 0.5
    }
})

# 计算NDCG@K
ndcg_scores = evaluate(qrels, run, ['ndcg@1', 'ndcg@3', 'ndcg@5'])
print(ndcg_scores)
```

### RAG应用场景

```python
# 场景: 学术论文检索
question = "深度学习在自然语言处理中的应用"

# 检索结果
retrieved_docs = [
    'paper1',  # BERT论文
    'paper2',  # GPT论文
    'paper3',  # 图像识别论文
    'paper4',  # Transformer论文
    'paper5'   # 推荐系统论文
]

# 相关性评分 (0-3)
relevance_scores = {
    'paper1': 3,  # 高度相关
    'paper2': 3,  # 高度相关
    'paper3': 0,  # 无关
    'paper4': 2,  # 中度相关
    'paper5': 1   # 低度相关
}

ndcg_5 = calculate_ndcg_at_k(retrieved_docs, relevance_scores, 5)
print(f"NDCG@5: {ndcg_5:.3f}")

# 理想排序应该是: paper1, paper2, paper4, paper5, paper3
# NDCG会惩罚paper3排在前面的情况
```

### 优化建议

| NDCG@K | 评级 | 建议 |
|--------|------|------|
| < 0.5 | 差 | 排序质量差，需要重新设计检索策略 |
| 0.5-0.7 | 中 | 可接受，但有优化空间 |
| 0.7-0.85 | 良好 | 排序质量较好 |
| > 0.85 | 优秀 | 排序质量优秀 |

---

## 五、Hit Rate (命中率)

### 定义

Hit Rate衡量至少检索到一个相关文档的查询比例。

**公式**:
```
Hit Rate@K = 命中查询数 / 总查询数

命中: Top-K中至少有一个相关文档
```

### Python实现

```python
def calculate_hit_rate_at_k(queries_results, k):
    """
    计算Hit Rate@K

    Args:
        queries_results: [(retrieved_docs, relevant_docs), ...]
        k: 评估的Top-K数量

    Returns:
        float: Hit Rate@K分数 (0-1)
    """
    if not queries_results:
        return 0.0

    hits = 0
    for retrieved, relevant in queries_results:
        retrieved_k = set(retrieved[:k])
        relevant_set = set(relevant)

        if len(retrieved_k & relevant_set) > 0:
            hits += 1

    return hits / len(queries_results)

# 使用示例
queries = [
    (['doc1', 'doc2', 'doc3'], ['doc1', 'doc4']),  # 命中 ✓
    (['doc2', 'doc3', 'doc4'], ['doc1', 'doc5']),  # 未命中 ✗
    (['doc5', 'doc2', 'doc3'], ['doc5', 'doc6']),  # 命中 ✓
]

hit_rate = calculate_hit_rate_at_k(queries, k=3)
print(f"Hit Rate@3: {hit_rate:.3f}")  # 0.667
```

### RAG应用场景

```python
# 场景: FAQ匹配系统
queries = [
    "如何重置密码？",
    "忘记密码怎么办？",
    "密码找回流程",
    "账号被锁定",
    "登录失败"
]

# 模拟检索结果
results = [
    (['密码重置指南', '账号安全'], ['密码重置指南']),      # 命中 ✓
    (['账号安全', '密码重置指南'], ['密码重置指南']),      # 命中 ✓
    (['密码重置指南', '找回密码'], ['密码重置指南']),      # 命中 ✓
    (['账号安全', '登录问题'], ['账号解锁流程']),         # 未命中 ✗
    (['账号安全', '登录问题'], ['登录故障排查']),         # 未命中 ✗
]

hit_rate_3 = calculate_hit_rate_at_k(results, k=3)
print(f"Hit Rate@3: {hit_rate_3:.1%}")  # 60%
```

---

## 六、指标对比与选择

### 指标特点对比

| 指标 | 关注点 | 优势 | 局限 | 适用场景 |
|------|--------|------|------|----------|
| **Precision@K** | 检索纯净度 | 简单直观 | 不考虑召回 | 成本敏感场景 |
| **Recall@K** | 检索完整性 | 衡量遗漏 | 不考虑精度 | 信息完整性要求高 |
| **MRR** | 首个相关结果 | 关注用户体验 | 只看第一个 | 单答案场景 |
| **NDCG@K** | 排序质量 | 最全面 | 需要相关性分数 | 多级相关性场景 |
| **Hit Rate** | 基础可用性 | 简单 | 信息量少 | 快速评估 |

### 选择建议

```python
# 场景1: FAQ匹配 (单答案)
recommended_metrics = ['MRR', 'Hit Rate@3', 'Precision@3']

# 场景2: 文档检索 (多答案)
recommended_metrics = ['Recall@5', 'NDCG@5', 'Precision@5']

# 场景3: 学术搜索 (排序重要)
recommended_metrics = ['NDCG@10', 'MRR', 'Recall@10']

# 场景4: 生产监控 (快速评估)
recommended_metrics = ['Hit Rate@5', 'MRR']
```

### 2025-2026年行业基准

```python
industry_benchmarks = {
    'FAQ系统': {
        'MRR': 0.85,
        'Hit Rate@3': 0.90,
        'Precision@3': 0.80
    },
    '文档问答': {
        'Recall@5': 0.80,
        'Precision@5': 0.75,
        'NDCG@5': 0.78
    },
    '学术检索': {
        'NDCG@10': 0.75,
        'Recall@10': 0.85,
        'MRR': 0.70
    },
    '企业知识库': {
        'Recall@5': 0.85,
        'Precision@5': 0.80,
        'NDCG@5': 0.82
    }
}
```

---

## 七、完整评估示例

```python
from ranx import Qrels, Run, evaluate

class RetrievalEvaluator:
    """检索质量评估器"""

    def __init__(self):
        self.metrics = [
            'precision@5',
            'recall@5',
            'mrr',
            'ndcg@5',
            'hit_rate@5'
        ]

    def evaluate(self, queries_results):
        """
        评估检索质量

        Args:
            queries_results: {
                'q1': {
                    'retrieved': ['doc1', 'doc2', ...],
                    'relevant': {'doc1': 3, 'doc2': 1, ...}
                },
                ...
            }

        Returns:
            dict: 评估结果
        """
        # 准备ranx格式数据
        qrels_dict = {}
        run_dict = {}

        for qid, data in queries_results.items():
            qrels_dict[qid] = data['relevant']
            run_dict[qid] = {
                doc: 1.0 / (i + 1)
                for i, doc in enumerate(data['retrieved'])
            }

        qrels = Qrels(qrels_dict)
        run = Run(run_dict)

        # 计算指标
        scores = evaluate(qrels, run, self.metrics)

        return scores

    def report(self, scores):
        """生成评估报告"""
        print("=" * 50)
        print("检索质量评估报告")
        print("=" * 50)

        for metric, score in scores.items():
            print(f"{metric:20s}: {score:.3f}")

        print("=" * 50)

# 使用示例
evaluator = RetrievalEvaluator()

queries_results = {
    'q1': {
        'retrieved': ['doc1', 'doc2', 'doc3', 'doc4', 'doc5'],
        'relevant': {'doc1': 3, 'doc3': 2, 'doc6': 1}
    },
    'q2': {
        'retrieved': ['doc2', 'doc3', 'doc1', 'doc5', 'doc4'],
        'relevant': {'doc1': 3, 'doc2': 2}
    }
}

scores = evaluator.evaluate(queries_results)
evaluator.report(scores)
```

---

## 八、总结

### 核心要点

1. **Precision@K**: 衡量检索纯净度，控制噪音
2. **Recall@K**: 衡量检索完整性，避免遗漏
3. **MRR**: 衡量首个相关结果，关注用户体验
4. **NDCG@K**: 衡量排序质量，最全面的指标
5. **Hit Rate**: 衡量基础可用性，快速评估

### 实践建议

1. **选择3-5个核心指标**: 避免指标过载
2. **根据场景选择**: 不同场景关注不同指标
3. **设置基准线**: 参考行业标准设置目标
4. **持续监控**: 生产环境实时追踪
5. **结合生成评估**: 检索只是第一步

### 2025-2026年标准

```python
production_standards = {
    'minimum': {
        'Recall@5': 0.70,
        'Precision@5': 0.60,
        'MRR': 0.50
    },
    'good': {
        'Recall@5': 0.80,
        'Precision@5': 0.75,
        'MRR': 0.70
    },
    'excellent': {
        'Recall@5': 0.85,
        'Precision@5': 0.80,
        'MRR': 0.80
    }
}
```

---

**参考资料**:
- https://www.getmaxim.ai/articles/complete-guide-to-rag-evaluation-metrics-methods-and-best-practices-for-2025
- https://www.evidentlyai.com/llm-guide/rag-evaluation
- https://github.com/AmenRa/ranx
