# å®æˆ˜ä»£ç 3ï¼šLLMè¯­ä¹‰ç¼“å­˜

## å®Œæ•´å¯è¿è¡Œç¤ºä¾‹

```python
"""
LLMè¯­ä¹‰ç¼“å­˜å®æˆ˜
æ¼”ç¤ºï¼šä½¿ç”¨Embeddingå’Œç›¸ä¼¼åº¦åŒ¹é…ï¼Œæå‡ç¼“å­˜å‘½ä¸­ç‡
"""

import redis
import hashlib
import json
import time
import numpy as np
from typing import List, Optional, Dict
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# ===== 1. åˆå§‹åŒ– =====
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
openai_client = OpenAI()

# ===== 2. è¯­ä¹‰ç¼“å­˜å®ç° =====

class SemanticLLMCache:
    """è¯­ä¹‰LLMç¼“å­˜"""

    def __init__(self, redis_client: redis.Redis, openai_client: OpenAI):
        self.redis = redis_client
        self.openai = openai_client
        self.cache_hash_key = "semantic_llm_cache"
        self.stats = {"hits": 0, "misses": 0, "api_calls": 0}

    def _get_embedding(self, text: str) -> List[float]:
        """è·å–Embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        response = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        embedding = response.data[0].embedding
        self.redis.setex(cache_key, 86400, json.dumps(embedding))
        return embedding

    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

    def add_cache(self, query: str, response: str, ttl: int = 3600):
        """æ·»åŠ è¯­ä¹‰ç¼“å­˜"""
        query_embedding = self._get_embedding(query)

        cache_data = {
            "query": query,
            "response": response,
            "embedding": query_embedding,
            "timestamp": time.time()
        }

        cache_id = hashlib.md5(query.encode()).hexdigest()
        self.redis.hset(
            self.cache_hash_key,
            cache_id,
            json.dumps(cache_data)
        )
        self.redis.expire(self.cache_hash_key, ttl)

    def lookup(
        self,
        query: str,
        threshold: float = 0.9
    ) -> Optional[Dict]:
        """æŸ¥è¯¢è¯­ä¹‰ç¼“å­˜"""
        query_embedding = self._get_embedding(query)
        cached_items = self.redis.hgetall(self.cache_hash_key)

        if not cached_items:
            return None

        best_match = None
        best_score = 0.0

        for cache_id, cache_data_json in cached_items.items():
            cache_data = json.loads(cache_data_json)
            cached_embedding = cache_data["embedding"]

            similarity = self._cosine_similarity(
                query_embedding,
                cached_embedding
            )

            if similarity > best_score:
                best_score = similarity
                best_match = {
                    "response": cache_data["response"],
                    "similarity": similarity,
                    "original_query": cache_data["query"]
                }

        if best_score >= threshold:
            self.stats["hits"] += 1
            print(f"âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦={best_score:.3f}")
            print(f"   åŸå§‹é—®é¢˜: {best_match['original_query']}")
            return best_match

        self.stats["misses"] += 1
        print(f"âŒ è¯­ä¹‰ç¼“å­˜æœªå‘½ä¸­ï¼Œæœ€é«˜ç›¸ä¼¼åº¦={best_score:.3f}")
        return None

    def get_or_generate(
        self,
        query: str,
        model: str = "gpt-4o-mini",
        threshold: float = 0.9,
        ttl: int = 3600
    ) -> str:
        """è·å–ç¼“å­˜æˆ–ç”Ÿæˆæ–°å“åº”"""
        # 1. å°è¯•è¯­ä¹‰ç¼“å­˜
        cached = self.lookup(query, threshold)
        if cached:
            return cached["response"]

        # 2. è°ƒç”¨LLM API
        print(f"ğŸ¤– è°ƒç”¨LLM API: {model}")
        start_time = time.time()

        response = self.openai.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": query}]
        )

        answer = response.choices[0].message.content
        api_time = time.time() - start_time
        self.stats["api_calls"] += 1

        print(f"â±ï¸ APIè°ƒç”¨è€—æ—¶: {api_time:.2f}ç§’")

        # 3. æ·»åŠ åˆ°è¯­ä¹‰ç¼“å­˜
        self.add_cache(query, answer, ttl)

        return answer

# ===== 3. æµ‹è¯•è¯­ä¹‰ç¼“å­˜ =====
print("=== æµ‹è¯•è¯­ä¹‰ç¼“å­˜ ===\n")

cache = SemanticLLMCache(redis_client, openai_client)

# æµ‹è¯•ç”¨ä¾‹ï¼šç›¸åŒè¯­ä¹‰çš„ä¸åŒè¡¨è¾¾
test_cases = [
    "What is Python?",
    "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ",
    "è¯·ä»‹ç»ä¸€ä¸‹Python",
    "Can you tell me about Python?",
    "What is JavaScript?",
]

for i, query in enumerate(test_cases, 1):
    print(f"\n[{i}/{len(test_cases)}] æŸ¥è¯¢: {query}")
    response = cache.get_or_generate(query, threshold=0.85)
    print(f"å“åº”: {response[:80]}...")

# ===== 4. ç»Ÿè®¡ä¿¡æ¯ =====
print("\n=== ç»Ÿè®¡ä¿¡æ¯ ===")
print(f"ç¼“å­˜å‘½ä¸­: {cache.stats['hits']}")
print(f"ç¼“å­˜æœªå‘½ä¸­: {cache.stats['misses']}")
print(f"APIè°ƒç”¨æ¬¡æ•°: {cache.stats['api_calls']}")
hit_rate = cache.stats['hits'] / (cache.stats['hits'] + cache.stats['misses'])
print(f"å‘½ä¸­ç‡: {hit_rate:.1%}")

# ===== 5. æ··åˆç¼“å­˜ç­–ç•¥ =====
print("\n=== æ··åˆç¼“å­˜ç­–ç•¥ ===")

class HybridLLMCache:
    """æ··åˆç¼“å­˜ï¼šç²¾ç¡® + è¯­ä¹‰"""

    def __init__(self, redis_client: redis.Redis, openai_client: OpenAI):
        self.redis = redis_client
        self.openai = openai_client
        self.semantic_cache = SemanticLLMCache(redis_client, openai_client)

    def _exact_cache_key(self, query: str) -> str:
        return f"llm_exact:{hashlib.md5(query.encode()).hexdigest()}"

    def get_or_generate(
        self,
        query: str,
        model: str = "gpt-4o-mini",
        semantic_threshold: float = 0.9
    ) -> str:
        """æ··åˆæŸ¥è¯¢"""
        # 1. å…ˆå°è¯•ç²¾ç¡®ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        exact_key = self._exact_cache_key(query)
        exact_cached = self.redis.get(exact_key)
        if exact_cached:
            print("âœ… ç²¾ç¡®ç¼“å­˜å‘½ä¸­")
            return exact_cached

        # 2. å†å°è¯•è¯­ä¹‰ç¼“å­˜
        semantic_result = self.semantic_cache.lookup(query, semantic_threshold)
        if semantic_result:
            # å°†è¯­ä¹‰ç¼“å­˜ç»“æœä¹Ÿå­˜å…¥ç²¾ç¡®ç¼“å­˜
            self.redis.setex(exact_key, 3600, semantic_result["response"])
            return semantic_result["response"]

        # 3. è°ƒç”¨LLM
        print(f"ğŸ¤– è°ƒç”¨LLM API")
        response = self.openai.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": query}]
        )
        answer = response.choices[0].message.content

        # 4. åŒæ—¶ç¼“å­˜åˆ°ç²¾ç¡®å’Œè¯­ä¹‰ç¼“å­˜
        self.redis.setex(exact_key, 3600, answer)
        self.semantic_cache.add_cache(query, answer)

        return answer

# æµ‹è¯•æ··åˆç¼“å­˜
hybrid_cache = HybridLLMCache(redis_client, openai_client)

print("\næµ‹è¯•æ··åˆç¼“å­˜:")
queries = [
    "What is Rust?",
    "Rustæ˜¯ä»€ä¹ˆï¼Ÿ",
    "What is Rust?",  # ç²¾ç¡®å‘½ä¸­
]

for query in queries:
    print(f"\næŸ¥è¯¢: {query}")
    response = hybrid_cache.get_or_generate(query)

# ===== 6. æ¸…ç† =====
print("\n=== æ¸…ç†æµ‹è¯•æ•°æ® ===")
redis_client.delete("semantic_llm_cache")
keys = redis_client.keys("emb:*")
if keys:
    redis_client.delete(*keys)
keys = redis_client.keys("llm_exact:*")
if keys:
    redis_client.delete(*keys)
print("âœ… æµ‹è¯•æ•°æ®å·²æ¸…ç†")
```

## è¿è¡Œè¾“å‡ºç¤ºä¾‹

```
=== æµ‹è¯•è¯­ä¹‰ç¼“å­˜ ===

[1/5] æŸ¥è¯¢: What is Python?
âŒ è¯­ä¹‰ç¼“å­˜æœªå‘½ä¸­ï¼Œæœ€é«˜ç›¸ä¼¼åº¦=0.000
ğŸ¤– è°ƒç”¨LLM API: gpt-4o-mini
â±ï¸ APIè°ƒç”¨è€—æ—¶: 1.85ç§’
å“åº”: Python is a high-level, interpreted programming language...

[2/5] æŸ¥è¯¢: Pythonæ˜¯ä»€ä¹ˆï¼Ÿ
âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦=0.952
   åŸå§‹é—®é¢˜: What is Python?
å“åº”: Python is a high-level, interpreted programming language...

[3/5] æŸ¥è¯¢: è¯·ä»‹ç»ä¸€ä¸‹Python
âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦=0.918
   åŸå§‹é—®é¢˜: What is Python?
å“åº”: Python is a high-level, interpreted programming language...

[4/5] æŸ¥è¯¢: Can you tell me about Python?
âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦=0.965
   åŸå§‹é—®é¢˜: What is Python?
å“åº”: Python is a high-level, interpreted programming language...

[5/5] æŸ¥è¯¢: What is JavaScript?
âŒ è¯­ä¹‰ç¼“å­˜æœªå‘½ä¸­ï¼Œæœ€é«˜ç›¸ä¼¼åº¦=0.782
ğŸ¤– è°ƒç”¨LLM API: gpt-4o-mini
â±ï¸ APIè°ƒç”¨è€—æ—¶: 1.92ç§’
å“åº”: JavaScript is a versatile programming language...

=== ç»Ÿè®¡ä¿¡æ¯ ===
ç¼“å­˜å‘½ä¸­: 3
ç¼“å­˜æœªå‘½ä¸­: 2
APIè°ƒç”¨æ¬¡æ•°: 2
å‘½ä¸­ç‡: 60.0%

=== æ··åˆç¼“å­˜ç­–ç•¥ ===

æµ‹è¯•æ··åˆç¼“å­˜:

æŸ¥è¯¢: What is Rust?
âŒ è¯­ä¹‰ç¼“å­˜æœªå‘½ä¸­ï¼Œæœ€é«˜ç›¸ä¼¼åº¦=0.654
ğŸ¤– è°ƒç”¨LLM API

æŸ¥è¯¢: Rustæ˜¯ä»€ä¹ˆï¼Ÿ
âœ… è¯­ä¹‰ç¼“å­˜å‘½ä¸­ï¼Œç›¸ä¼¼åº¦=0.943
   åŸå§‹é—®é¢˜: What is Rust?

æŸ¥è¯¢: What is Rust?
âœ… ç²¾ç¡®ç¼“å­˜å‘½ä¸­

=== æ¸…ç†æµ‹è¯•æ•°æ® ===
âœ… æµ‹è¯•æ•°æ®å·²æ¸…ç†
```

## å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£è¯­ä¹‰ç¼“å­˜çš„åŸç†ï¼ˆEmbedding + ç›¸ä¼¼åº¦ï¼‰
- [ ] å®ç°ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
- [ ] é€‰æ‹©åˆé€‚çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.85-0.95ï¼‰
- [ ] å®ç°æ··åˆç¼“å­˜ç­–ç•¥ï¼ˆç²¾ç¡® + è¯­ä¹‰ï¼‰
- [ ] ç†è§£è¯­ä¹‰ç¼“å­˜çš„ä¼˜åŠ¿ï¼ˆæå‡å‘½ä¸­ç‡ï¼‰å’ŒåŠ£åŠ¿ï¼ˆè®¡ç®—æˆæœ¬ï¼‰
- [ ] åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨è¯­ä¹‰ç¼“å­˜

## ä¸‹ä¸€æ­¥

å­¦ä¹ Embeddingå‘é‡ç¼“å­˜ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚
