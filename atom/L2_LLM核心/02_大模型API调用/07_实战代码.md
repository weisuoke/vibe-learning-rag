# 大模型API调用 - 实战代码

完整可运行的代码示例，复制即可使用。

---

## 环境准备

```bash
# 安装依赖
pip install openai anthropic python-dotenv

# 设置环境变量（创建 .env 文件）
echo "OPENAI_API_KEY=sk-your-key-here" >> .env
echo "ANTHROPIC_API_KEY=sk-ant-your-key-here" >> .env
```

---

## 示例1：OpenAI 基础调用

```python
"""
OpenAI API 基础调用示例
演示：基本请求、响应解析、token 统计
"""
import os
from dotenv import load_dotenv
from openai import OpenAI

# 加载环境变量
load_dotenv()

# ===== 1. 创建客户端 =====
client = OpenAI()

# ===== 2. 基础调用 =====
print("=== 基础调用 ===")

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "你是一个简洁的助手，回答控制在50字以内"},
        {"role": "user", "content": "什么是RAG？"}
    ],
    temperature=0.1,
    max_tokens=200
)

# 获取回复
answer = response.choices[0].message.content
print(f"回答：{answer}")

# Token 统计
print(f"\n输入 tokens: {response.usage.prompt_tokens}")
print(f"输出 tokens: {response.usage.completion_tokens}")
print(f"总计 tokens: {response.usage.total_tokens}")

# 结束原因
print(f"结束原因: {response.choices[0].finish_reason}")
```

**运行输出示例：**
```
=== 基础调用 ===
回答：RAG（检索增强生成）是一种结合信息检索和文本生成的技术，先从知识库检索相关内容，再让大模型基于检索结果生成答案，提高回答的准确性和时效性。

输入 tokens: 38
输出 tokens: 62
总计 tokens: 100
结束原因: stop
```

---

## 示例2：流式输出

```python
"""
流式输出示例
演示：实时显示生成内容，适合聊天界面
"""
from openai import OpenAI

client = OpenAI()

# ===== 流式调用 =====
print("=== 流式输出 ===")

stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "user", "content": "用3句话介绍Python的优点"}
    ],
    stream=True  # 开启流式
)

# 逐块输出
print("回答：", end="")
full_response = ""
for chunk in stream:
    content = chunk.choices[0].delta.content
    if content:
        print(content, end="", flush=True)
        full_response += content

print("\n")
print(f"完整回答长度：{len(full_response)} 字符")
```

**运行输出示例：**
```
=== 流式输出 ===
回答：Python语法简洁易读，适合初学者快速上手。它拥有丰富的第三方库生态，覆盖数据科学、Web开发、自动化等领域。Python的跨平台特性使代码可以在不同操作系统上无缝运行。

完整回答长度：89 字符
```

---

## 示例3：Anthropic Claude 调用

```python
"""
Anthropic Claude API 调用示例
演示：Claude 的调用方式与 OpenAI 的区别
"""
from anthropic import Anthropic

client = Anthropic()

# ===== Claude 调用 =====
print("=== Claude 调用 ===")

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=200,  # Claude 必须指定
    system="你是一个简洁的助手，回答控制在50字以内",  # system 是独立参数
    messages=[
        {"role": "user", "content": "什么是RAG？"}
    ]
)

# 获取回复（注意结构不同）
answer = response.content[0].text
print(f"回答：{answer}")

# Token 统计
print(f"\n输入 tokens: {response.usage.input_tokens}")
print(f"输出 tokens: {response.usage.output_tokens}")
```

**运行输出示例：**
```
=== Claude 调用 ===
回答：RAG是检索增强生成技术，通过先检索相关文档再让LLM基于检索内容生成答案，解决大模型知识过时和幻觉问题。

输入 tokens: 35
输出 tokens: 48
```

---

## 示例4：多轮对话

```python
"""
多轮对话示例
演示：如何维护对话历史，实现上下文连贯
"""
from openai import OpenAI

client = OpenAI()

# ===== 多轮对话 =====
print("=== 多轮对话 ===")

# 对话历史
conversation = [
    {"role": "system", "content": "你是Python编程助手"}
]

def chat(user_message: str) -> str:
    """发送消息并获取回复"""
    # 添加用户消息
    conversation.append({"role": "user", "content": user_message})

    # 调用 API
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=conversation,
        temperature=0.1,
        max_tokens=300
    )

    # 获取回复
    assistant_message = response.choices[0].message.content

    # 添加到历史（保持上下文）
    conversation.append({"role": "assistant", "content": assistant_message})

    return assistant_message

# 模拟多轮对话
print("用户：什么是列表推导式？")
print(f"助手：{chat('什么是列表推导式？')}\n")

print("用户：能举个例子吗？")
print(f"助手：{chat('能举个例子吗？')}\n")

print("用户：如何添加条件过滤？")
print(f"助手：{chat('如何添加条件过滤？')}")
```

---

## 示例5：RAG 生成模块（核心应用）

```python
"""
RAG 生成模块示例
演示：如何将检索结果注入 Prompt，生成最终答案
这是 RAG 系统的核心代码！
"""
from openai import OpenAI
from typing import List

client = OpenAI()

def generate_rag_answer(
    question: str,
    retrieved_docs: List[str],
    model: str = "gpt-4o",
    temperature: float = 0.1
) -> str:
    """
    RAG 生成模块：基于检索结果生成答案

    Args:
        question: 用户问题
        retrieved_docs: 检索到的文档列表
        model: 使用的模型
        temperature: 温度参数

    Returns:
        生成的答案
    """
    # 构建上下文
    context = "\n\n---\n\n".join(retrieved_docs)

    # 构建 Prompt
    messages = [
        {
            "role": "system",
            "content": """你是一个专业的问答助手。请基于提供的参考资料回答用户问题。

规则：
1. 只使用参考资料中的信息回答
2. 如果资料中没有相关信息，请明确说明
3. 回答要简洁准确，不要编造信息
4. 可以适当组织和总结信息"""
        },
        {
            "role": "user",
            "content": f"""参考资料：
{context}

---

问题：{question}

请基于以上参考资料回答问题。"""
        }
    ]

    # 调用 API
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=500
    )

    return response.choices[0].message.content


# ===== 测试 RAG 生成 =====
print("=== RAG 生成模块测试 ===\n")

# 模拟检索结果
mock_retrieved_docs = [
    "向量数据库是专门用于存储和检索向量的数据库系统。常见的向量数据库包括 Milvus、Pinecone、Chroma 等。",
    "Embedding 是将文本转换为稠密向量的技术。OpenAI 提供 text-embedding-3-small 和 text-embedding-3-large 两种模型。",
    "RAG（检索增强生成）的核心流程：1. 用户提问 2. 向量检索 3. 构建 Prompt 4. LLM 生成答案"
]

# 用户问题
user_question = "RAG 系统中常用哪些向量数据库？"

# 生成答案
answer = generate_rag_answer(user_question, mock_retrieved_docs)

print(f"问题：{user_question}\n")
print(f"答案：{answer}")
```

**运行输出示例：**
```
=== RAG 生成模块测试 ===

问题：RAG 系统中常用哪些向量数据库？

答案：根据参考资料，RAG 系统中常用的向量数据库包括：

1. **Milvus** - 开源向量数据库
2. **Pinecone** - 托管向量数据库服务
3. **Chroma** - 轻量级向量数据库

这些向量数据库专门用于存储和检索向量，是 RAG 系统实现语义检索的核心组件。
```

---

## 示例6：带重试的生产级调用

```python
"""
生产级 API 调用示例
演示：错误处理、重试机制、日志记录
"""
import time
from openai import OpenAI, APIError, RateLimitError, APIConnectionError
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

client = OpenAI()

@retry(
    retry=retry_if_exception_type((RateLimitError, APIConnectionError)),
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10)
)
def robust_llm_call(
    messages: list,
    model: str = "gpt-4o",
    temperature: float = 0.1,
    max_tokens: int = 500
) -> dict:
    """
    带重试机制的 LLM 调用

    Returns:
        包含 content 和 usage 的字典
    """
    start_time = time.time()

    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=30
        )

        latency = time.time() - start_time

        return {
            "content": response.choices[0].message.content,
            "finish_reason": response.choices[0].finish_reason,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            },
            "latency": latency
        }

    except RateLimitError as e:
        print(f"[警告] 触发限流，等待重试... {e}")
        raise

    except APIConnectionError as e:
        print(f"[警告] 网络错误，等待重试... {e}")
        raise

    except APIError as e:
        print(f"[错误] API 错误: {e}")
        raise


# ===== 测试 =====
print("=== 生产级调用测试 ===\n")

result = robust_llm_call(
    messages=[
        {"role": "user", "content": "用一句话介绍Python"}
    ]
)

print(f"回答：{result['content']}")
print(f"延迟：{result['latency']:.2f}s")
print(f"Token 使用：{result['usage']}")
```

---

## 代码文件下载

以上所有示例代码可以保存为单独的 Python 文件运行：

```
examples/
├── 01_basic_call.py      # 基础调用
├── 02_streaming.py       # 流式输出
├── 03_claude_call.py     # Claude 调用
├── 04_multi_turn.py      # 多轮对话
├── 05_rag_generate.py    # RAG 生成模块
└── 06_production.py      # 生产级调用
```

---

**上一节：** [06_反直觉点.md](./06_反直觉点.md)
**下一节：** [08_面试必问.md](./08_面试必问.md) - 面试高频问题
