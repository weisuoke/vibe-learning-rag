# 核心概念 - 检索优化层（3个组件）

检索优化层在基础检索之上，通过多种策略提升检索的召回率和精确度。

---

## 组件7：混合检索（Hybrid Search）

**一句话定义：** 混合检索结合Dense（语义）和Sparse（关键词）两种检索策略，通过融合算法提升召回率。

### 核心功能

1. **Dense检索（语义检索）**
   - 基于向量相似度
   - 理解语义关系
   - 适合概念性查询

2. **Sparse检索（关键词检索）**
   - 基于词频统计（BM25）
   - 精确匹配关键词
   - 适合专有名词查询

3. **结果融合**
   - RRF（Reciprocal Rank Fusion）
   - 加权平均
   - 线性组合

### 为什么需要混合检索？

**问题：** 单一检索策略有局限性

- **Dense检索的局限**：
  - 对专有名词不敏感（如"GPT-4"、"Python 3.11"）
  - 可能忽略精确匹配
  - 计算成本高

- **Sparse检索的局限**：
  - 无法理解语义（"汽车"和"车辆"被视为不同词）
  - 对同义词不敏感
  - 无法处理概念性查询

**解决方案：** 结合两者优势，互补不足

### 代码示例

```python
from typing import List, Dict
import numpy as np
from rank_bm25 import BM25Okapi
import chromadb

class HybridRetriever:
    """混合检索器"""

    def __init__(self, vectorstore, documents: List[str]):
        self.vectorstore = vectorstore
        self.documents = documents

        # 初始化BM25
        tokenized_docs = [doc.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)

    def dense_search(self, query: str, k: int = 10) -> List[Dict]:
        """Dense检索（语义）"""
        results = self.vectorstore.search(query, k=k)
        return results

    def sparse_search(self, query: str, k: int = 10) -> List[Dict]:
        """Sparse检索（BM25）"""
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)

        # 获取top-k
        top_indices = np.argsort(scores)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                "content": self.documents[idx],
                "score": scores[idx],
                "index": idx
            })

        return results

    def hybrid_search(
        self,
        query: str,
        k: int = 10,
        alpha: float = 0.5
    ) -> List[Dict]:
        """混合检索"""
        # 1. 分别检索
        dense_results = self.dense_search(query, k=k*2)
        sparse_results = self.sparse_search(query, k=k*2)

        # 2. 融合结果（RRF）
        fused = self.reciprocal_rank_fusion(
            dense_results,
            sparse_results,
            k=60  # RRF参数
        )

        # 3. 返回top-k
        return fused[:k]

    def reciprocal_rank_fusion(
        self,
        dense_results: List[Dict],
        sparse_results: List[Dict],
        k: int = 60
    ) -> List[Dict]:
        """RRF融合算法"""
        # 计算每个文档的RRF分数
        doc_scores = {}

        # Dense结果的RRF分数
        for rank, doc in enumerate(dense_results):
            doc_id = doc.get("index", doc["content"])
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {"content": doc["content"], "score": 0}
            doc_scores[doc_id]["score"] += 1 / (k + rank + 1)

        # Sparse结果的RRF分数
        for rank, doc in enumerate(sparse_results):
            doc_id = doc.get("index", doc["content"])
            if doc_id not in doc_scores:
                doc_scores[doc_id] = {"content": doc["content"], "score": 0}
            doc_scores[doc_id]["score"] += 1 / (k + rank + 1)

        # 排序
        sorted_docs = sorted(
            doc_scores.values(),
            key=lambda x: x["score"],
            reverse=True
        )

        return sorted_docs

# 使用示例
retriever = HybridRetriever(vectorstore, [chunk["content"] for chunk in chunks])

# 混合检索
query = "Python 3.11的新特性"
results = retriever.hybrid_search(query, k=5)

print(f"找到 {len(results)} 个相关文档")
for i, doc in enumerate(results):
    print(f"\n文档 {i+1}:")
    print(f"内容: {doc['content'][:100]}...")
    print(f"RRF分数: {doc['score']:.4f}")
```

### 融合策略对比

| 策略 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| RRF | 简单有效，无需调参 | 忽略原始分数 | 通用场景 |
| 加权平均 | 保留分数信息 | 需要归一化 | 分数可比时 |
| 线性组合 | 灵活可调 | 需要调参 | 特定场景优化 |

### 在RAG开发中

**效果提升：**
- 召回率提升：20-30%
- 精确度提升：10-15%
- 特别适合：专有名词 + 概念性查询

**最佳实践：**
- 默认使用RRF（简单有效）
- Dense和Sparse各检索2倍k个结果
- 最终返回top-k

---

## 组件8：重排序（Reranking）

**一句话定义：** 重排序使用更精确的模型对初步检索结果重新打分，提升精确度。

### 核心功能

1. **精确打分**
   - Cross-Encoder模型
   - 同时考虑查询和文档
   - 计算精确相关性

2. **结果优化**
   - 过滤低相关文档
   - 调整排序顺序
   - 提升Top-K质量

3. **性能平衡**
   - 只对候选集重排序
   - 避免全量计算
   - 平衡速度和精度

### 为什么需要重排序？

**问题：** 初步检索的排序不够精确

- **向量检索的局限**：
  - 只计算向量相似度
  - 无法深度理解查询和文档的关系
  - 排序可能不准确

- **BM25的局限**：
  - 只考虑词频
  - 无法理解语义
  - 对长文档有偏见

**解决方案：** 使用Cross-Encoder精确打分

### Cross-Encoder vs Bi-Encoder

| 维度 | Bi-Encoder（向量检索） | Cross-Encoder（重排序） |
|------|------------------------|-------------------------|
| **输入** | 分别编码查询和文档 | 同时编码查询和文档 |
| **速度** | 快（可预计算） | 慢（实时计算） |
| **精度** | 中 | 高 |
| **适用** | 初步检索 | 精排 |

### 代码示例

```python
from sentence_transformers import CrossEncoder
from typing import List, Dict

class Reranker:
    """重排序器"""

    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.model = CrossEncoder(model_name)

    def rerank(
        self,
        query: str,
        documents: List[Dict],
        top_k: int = 3
    ) -> List[Dict]:
        """重排序文档"""
        # 1. 准备输入对
        pairs = [(query, doc["content"]) for doc in documents]

        # 2. 批量打分
        scores = self.model.predict(pairs)

        # 3. 添加分数到文档
        for doc, score in zip(documents, scores):
            doc["rerank_score"] = float(score)

        # 4. 按分数排序
        reranked = sorted(
            documents,
            key=lambda x: x["rerank_score"],
            reverse=True
        )

        # 5. 返回top-k
        return reranked[:top_k]

    def rerank_with_threshold(
        self,
        query: str,
        documents: List[Dict],
        threshold: float = 0.5,
        top_k: int = 3
    ) -> List[Dict]:
        """带阈值过滤的重排序"""
        reranked = self.rerank(query, documents, top_k=len(documents))

        # 过滤低分文档
        filtered = [
            doc for doc in reranked
            if doc["rerank_score"] >= threshold
        ]

        return filtered[:top_k]

# 使用示例
reranker = Reranker()

# 先混合检索
hybrid_results = retriever.hybrid_search(query, k=10)

# 再重排序
reranked_results = reranker.rerank(query, hybrid_results, top_k=3)

print(f"重排序后的Top-3文档:")
for i, doc in enumerate(reranked_results):
    print(f"\n文档 {i+1}:")
    print(f"内容: {doc['content'][:100]}...")
    print(f"重排序分数: {doc['rerank_score']:.4f}")
```

### 重排序模型选择

| 模型 | 参数量 | 速度 | 精度 | 适用场景 |
|------|--------|------|------|----------|
| ms-marco-MiniLM-L-6-v2 | 22M | 快 | 中 | 通用场景 |
| ms-marco-MiniLM-L-12-v2 | 33M | 中 | 高 | 平衡选择 |
| cross-encoder/ms-marco-electra-base | 110M | 慢 | 很高 | 高精度需求 |

### 在RAG开发中

**效果提升：**
- 精确度提升：15-30%
- Top-3准确率显著提升
- 减少无关文档

**最佳实践：**
- 先检索10-20个候选
- 重排序后取Top-3-5
- 使用阈值过滤低分文档

---

## 组件9：上下文管理（Context Management）

**一句话定义：** 上下文管理负责控制传给LLM的内容量和顺序，避免超出Context Window并优化信息利用。

### 核心功能

1. **窗口控制**
   - Token计数
   - 动态截断
   - 优先级排序

2. **顺序优化**
   - Lost in the Middle问题
   - 重要信息放两端
   - 降低中间信息权重

3. **内容压缩**
   - 摘要生成
   - 冗余去除
   - 关键信息提取

### Lost in the Middle问题

**研究发现：** LLM对上下文中间部分的信息利用率低

```
┌─────────────────────────────────────┐
│  开头信息（高利用率）                │  ← LLM关注度高
├─────────────────────────────────────┤
│  中间信息（低利用率）                │  ← LLM容易忽略
├─────────────────────────────────────┤
│  结尾信息（高利用率）                │  ← LLM关注度高
└─────────────────────────────────────┘
```

**解决方案：**
1. 最相关的文档放开头和结尾
2. 次相关的文档放中间
3. 控制总长度

### 代码示例

```python
from typing import List, Dict
import tiktoken

class ContextManager:
    """上下文管理器"""

    def __init__(self, model: str = "gpt-4", max_tokens: int = 4000):
        self.encoding = tiktoken.encoding_for_model(model)
        self.max_tokens = max_tokens

    def count_tokens(self, text: str) -> int:
        """计算token数"""
        return len(self.encoding.encode(text))

    def manage_context(
        self,
        documents: List[Dict],
        query: str,
        system_prompt: str = ""
    ) -> List[Dict]:
        """管理上下文"""
        # 1. 计算可用token
        query_tokens = self.count_tokens(query)
        system_tokens = self.count_tokens(system_prompt)
        available_tokens = self.max_tokens - query_tokens - system_tokens - 500  # 预留生成空间

        # 2. 选择文档
        selected_docs = []
        total_tokens = 0

        for doc in documents:
            doc_tokens = self.count_tokens(doc["content"])

            if total_tokens + doc_tokens <= available_tokens:
                selected_docs.append(doc)
                total_tokens += doc_tokens
            else:
                break

        return selected_docs

    def optimize_order(self, documents: List[Dict]) -> List[Dict]:
        """优化文档顺序（解决Lost in the Middle）"""
        if len(documents) <= 2:
            return documents

        # 策略：最相关的放开头和结尾，次相关的放中间
        optimized = []

        # 开头：最相关
        optimized.append(documents[0])

        # 中间：次相关（倒序）
        middle_docs = documents[2:-1] if len(documents) > 3 else []
        optimized.extend(reversed(middle_docs))

        # 结尾：第二相关
        if len(documents) > 1:
            optimized.append(documents[1])

        return optimized

    def compress_context(
        self,
        documents: List[Dict],
        max_tokens: int
    ) -> str:
        """压缩上下文"""
        # 简单策略：截断每个文档
        compressed_docs = []
        tokens_per_doc = max_tokens // len(documents)

        for doc in documents:
            content = doc["content"]
            tokens = self.encoding.encode(content)

            if len(tokens) > tokens_per_doc:
                # 截断
                tokens = tokens[:tokens_per_doc]
                content = self.encoding.decode(tokens)

            compressed_docs.append(content)

        return "\n\n---\n\n".join(compressed_docs)

# 使用示例
context_manager = ContextManager(model="gpt-4", max_tokens=4000)

# 管理上下文
selected_docs = context_manager.manage_context(
    reranked_results,
    query="什么是RAG？",
    system_prompt="你是一个AI助手"
)

# 优化顺序
optimized_docs = context_manager.optimize_order(selected_docs)

# 构建上下文
context = "\n\n".join([doc["content"] for doc in optimized_docs])

print(f"选择了 {len(selected_docs)} 个文档")
print(f"总token数: {context_manager.count_tokens(context)}")
```

### 上下文管理策略

| 策略 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| Token限制 | 简单直接 | 可能截断重要信息 | 通用场景 |
| 顺序优化 | 提升信息利用率 | 需要准确排序 | 多文档场景 |
| 内容压缩 | 节省token | 可能丢失细节 | Token紧张时 |
| 动态窗口 | 灵活适应 | 实现复杂 | 复杂查询 |

### 在RAG开发中

**关键考虑：**
1. **Context Window限制**：
   - GPT-4: 8K/32K/128K
   - Claude: 100K/200K
   - 预留20-30%给生成

2. **Lost in the Middle**：
   - 最相关文档放开头
   - 第二相关放结尾
   - 次相关放中间

3. **动态调整**：
   - 简单查询：少量上下文
   - 复杂查询：更多上下文
   - 根据查询类型调整

---

## 检索优化层总结

**3个组件的协同工作：**

```
查询 → 混合检索（Dense + Sparse）→ 重排序（Cross-Encoder）→ 上下文管理 → LLM
       ↓                           ↓                          ↓
    召回率提升20-30%            精确度提升15-30%          优化信息利用率
```

**关键要点：**
1. **混合检索**：结合语义和关键词，提升召回率
2. **重排序**：精确打分，提升精确度
3. **上下文管理**：控制窗口，优化顺序

**效果对比：**

| 策略 | 召回率 | 精确度 | Top-3准确率 |
|------|--------|--------|-------------|
| 基础Dense检索 | 60% | 50% | 40% |
| + 混合检索 | 80% (+20%) | 60% (+10%) | 55% (+15%) |
| + 重排序 | 80% | 75% (+15%) | 75% (+20%) |
| + 上下文优化 | 80% | 75% | 80% (+5%) |

**最佳实践：**
1. 先实现基础检索，测量基线
2. 添加混合检索，提升召回率
3. 添加重排序，提升精确度
4. 优化上下文管理，提升信息利用率

**下一步：** 学习生成层和高级模式层
