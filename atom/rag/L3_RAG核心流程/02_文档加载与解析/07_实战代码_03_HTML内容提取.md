# 实战代码3：HTML内容提取

> 从网页和HTML文件中提取干净内容的完整示例

---

## 场景描述

演示如何从HTML网页中提取核心内容，去除噪声（导航、广告、页脚等），并构建可检索的知识库。

**适用场景：**
- 网页内容抓取
- 在线文档加载
- 技术博客知识库

---

## 依赖库

```bash
# 安装依赖
uv add beautifulsoup4 requests langchain langchain-openai chromadb python-dotenv lxml
```

---

## 完整代码

```python
"""
HTML内容提取示例
演示：从网页提取干净内容并构建知识库

依赖库：
- beautifulsoup4: HTML解析
- requests: HTTP请求
- langchain: 文档处理框架
- lxml: 高性能HTML解析器

参考来源：
- BeautifulSoup Documentation (2025): https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- LangChain WebBaseLoader (2025): https://python.langchain.com/docs/modules/data_connection/document_loaders/web_base
"""

import os
from typing import List
from dotenv import load_dotenv
from langchain.schema import Document
from bs4 import BeautifulSoup
import requests

load_dotenv()

# ===== 1. 基础HTML解析 =====
print("=== 1. 基础HTML解析 ===")

def parse_html_basic(html_content: str, source: str = "unknown") -> Document:
    """基础HTML解析：提取所有文本"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # 提取所有文本
    text = soup.get_text()

    return Document(
        page_content=text,
        metadata={"source": source, "format": "html"}
    )

# 测试基础解析
html_example = """
<html>
  <head><title>RAG技术指南</title></head>
  <body>
    <h1>RAG核心流程</h1>
    <p>RAG系统包含三个核心阶段...</p>
  </body>
</html>
"""

doc = parse_html_basic(html_example, "example.html")
print(f"提取的文本:\n{doc.page_content}\n")

# ===== 2. 智能内容提取（去除噪声） =====
print("=== 2. 智能内容提取 ===")

def parse_html_smart(html_content: str, source: str = "unknown") -> Document:
    """智能HTML解析：只提取核心内容"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. 移除噪声标签
    noise_tags = ['script', 'style', 'nav', 'footer', 'aside', 'header', 'iframe']
    for tag in soup(noise_tags):
        tag.decompose()

    # 2. 查找主要内容区域（按优先级）
    main_content = (
        soup.find('article') or
        soup.find('main') or
        soup.find('div', class_='content') or
        soup.find('div', id='content') or
        soup.find('div', class_='post') or
        soup.body
    )

    if not main_content:
        return Document(page_content="", metadata={"source": source})

    # 3. 提取文本
    text = main_content.get_text(separator='\n', strip=True)

    # 4. 清理多余空行
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    clean_text = '\n'.join(lines)

    # 5. 提取元数据
    title = soup.find('title')
    h1 = soup.find('h1')

    metadata = {
        "source": source,
        "format": "html",
        "title": title.text if title else (h1.text if h1 else ""),
        "content_length": len(clean_text)
    }

    return Document(page_content=clean_text, metadata=metadata)

# 测试智能解析
html_with_noise = """
<html>
  <head><title>RAG技术指南</title></head>
  <body>
    <nav>首页 | 关于 | 联系</nav>
    <aside>广告内容...</aside>
    <article>
      <h1>RAG核心流程</h1>
      <p>RAG系统包含三个核心阶段：</p>
      <p>1. 文档加载与索引</p>
      <p>2. 查询处理与检索</p>
      <p>3. 上下文注入与生成</p>
    </article>
    <footer>版权所有 © 2025</footer>
  </body>
</html>
"""

doc = parse_html_smart(html_with_noise, "rag_guide.html")
print(f"标题: {doc.metadata['title']}")
print(f"提取的文本:\n{doc.page_content}\n")

# ===== 3. 网页抓取 =====
print("=== 3. 网页抓取 ===")

def fetch_webpage(url: str) -> Document:
    """从URL抓取网页内容"""
    print(f"抓取网页: {url}")

    # 1. 发送HTTP请求
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        # 2. 解析HTML
        doc = parse_html_smart(response.content, url)

        # 3. 添加HTTP元数据
        doc.metadata.update({
            "url": url,
            "status_code": response.status_code,
            "content_type": response.headers.get('Content-Type', '')
        })

        print(f"✅ 抓取成功: {len(doc.page_content)} 字符")
        return doc

    except requests.RequestException as e:
        print(f"❌ 抓取失败: {e}")
        return Document(
            page_content="",
            metadata={"source": url, "error": str(e)}
        )

# 测试网页抓取
# doc = fetch_webpage("https://example.com/article")

# ===== 4. 批量网页抓取 =====
print("\n=== 4. 批量网页抓取 ===")

def fetch_webpages_batch(urls: List[str]) -> List[Document]:
    """批量抓取多个网页"""
    documents = []

    for url in urls:
        try:
            doc = fetch_webpage(url)
            if doc.page_content:
                documents.append(doc)
        except Exception as e:
            print(f"❌ {url} 失败: {e}")

    print(f"\n总共成功抓取 {len(documents)} 个网页")
    return documents

# 测试批量抓取
# urls = [
#     "https://example.com/article1",
#     "https://example.com/article2",
#     "https://example.com/article3"
# ]
# docs = fetch_webpages_batch(urls)

# ===== 5. 使用LangChain的WebBaseLoader =====
print("\n=== 5. 使用LangChain的WebBaseLoader ===")

from langchain.document_loaders import WebBaseLoader

def load_webpage_langchain(url: str) -> List[Document]:
    """使用LangChain加载网页"""
    print(f"使用LangChain加载: {url}")

    try:
        loader = WebBaseLoader(url)
        documents = loader.load()

        print(f"✅ 加载成功: {len(documents)} 个文档")
        for doc in documents:
            print(f"  内容长度: {len(doc.page_content)} 字符")
            print(f"  元数据: {doc.metadata}")

        return documents

    except Exception as e:
        print(f"❌ 加载失败: {e}")
        return []

# 测试LangChain加载
# docs = load_webpage_langchain("https://example.com/article")

# ===== 6. 按标题分段 =====
print("\n=== 6. 按标题分段 ===")

def parse_html_by_sections(html_content: str, source: str) -> List[Document]:
    """按HTML标题分段提取内容"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # 移除噪声
    for tag in soup(['script', 'style', 'nav', 'footer', 'aside']):
        tag.decompose()

    documents = []
    main_content = soup.find('article') or soup.find('main') or soup.body

    if not main_content:
        return documents

    # 按标题分段
    current_section = {"heading": "", "level": 0, "content": []}

    for element in main_content.descendants:
        if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            # 保存上一个section
            if current_section['content']:
                doc = Document(
                    page_content='\n'.join(current_section['content']),
                    metadata={
                        "source": source,
                        "format": "html",
                        "heading": current_section['heading'],
                        "heading_level": current_section['level']
                    }
                )
                documents.append(doc)

            # 开始新section
            level = int(element.name[1])
            current_section = {
                "heading": element.get_text(strip=True),
                "level": level,
                "content": []
            }
        elif element.name == 'p':
            text = element.get_text(strip=True)
            if text:
                current_section['content'].append(text)

    # 保存最后一个section
    if current_section['content']:
        doc = Document(
            page_content='\n'.join(current_section['content']),
            metadata={
                "source": source,
                "format": "html",
                "heading": current_section['heading'],
                "heading_level": current_section['level']
            }
        )
        documents.append(doc)

    return documents

# 测试分段提取
html_sections = """
<article>
  <h1>RAG技术指南</h1>
  <p>RAG系统介绍...</p>

  <h2>核心流程</h2>
  <p>RAG包含三个阶段...</p>

  <h2>实践案例</h2>
  <p>以下是实际案例...</p>
</article>
"""

docs = parse_html_by_sections(html_sections, "rag_guide.html")
print(f"分段提取了 {len(docs)} 个section:")
for doc in docs:
    print(f"  {doc.metadata['heading']} (级别: {doc.metadata['heading_level']})")

# ===== 7. RAG应用：构建网页知识库 =====
print("\n=== 7. RAG应用：构建网页知识库 ===")

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

def build_web_knowledge_base(urls: List[str], output_dir: str = "./web_kb"):
    """从网页构建知识库"""

    print(f"=== 构建网页知识库 ===")
    print(f"网页数量: {len(urls)}")
    print(f"输出目录: {output_dir}\n")

    # 1. 批量抓取网页
    print("步骤1: 批量抓取网页")
    documents = fetch_webpages_batch(urls)

    if not documents:
        print("❌ 没有成功抓取任何网页")
        return None

    # 2. 文本分块
    print("\n步骤2: 文本分块")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_documents(documents)
    print(f"✅ 分块完成: {len(chunks)} 个文本块")

    # 3. 向量化并存储
    print("\n步骤3: 向量化并存储")
    try:
        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma.from_documents(
            chunks,
            embeddings,
            persist_directory=output_dir
        )
        print(f"✅ 知识库构建成功")

        # 4. 测试检索
        print("\n步骤4: 测试检索")
        test_query = "RAG的核心流程是什么？"
        results = vectorstore.similarity_search(test_query, k=3)

        print(f"查询: {test_query}")
        print(f"找到 {len(results)} 个相关文档:")
        for i, doc in enumerate(results, 1):
            print(f"\n结果 {i}:")
            print(f"  来源: {doc.metadata['source']}")
            print(f"  标题: {doc.metadata.get('title', '未知')}")
            print(f"  内容: {doc.page_content[:150]}...")

        return vectorstore

    except Exception as e:
        print(f"❌ 向量化失败: {e}")
        return None

# 测试构建知识库
# urls = [
#     "https://docs.python.org/3/tutorial/",
#     "https://docs.langchain.com/docs/",
#     "https://platform.openai.com/docs/"
# ]
# vectorstore = build_web_knowledge_base(urls)

# ===== 8. 高级内容清洗 =====
print("\n=== 8. 高级内容清洗 ===")

import re

def clean_html_content(text: str) -> str:
    """高级文本清洗"""

    # 1. 移除多余空白
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)

    # 2. 移除特殊字符
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)

    # 3. 统一换行符
    text = text.replace('\r\n', '\n').replace('\r', '\n')

    # 4. 移除行首行尾空白
    lines = [line.strip() for line in text.split('\n')]
    text = '\n'.join(lines)

    return text.strip()

def parse_html_advanced(html_content: str, source: str) -> Document:
    """高级HTML解析：包含内容清洗"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # 移除噪声
    for tag in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
        tag.decompose()

    # 提取主要内容
    main_content = soup.find('article') or soup.find('main') or soup.body
    if not main_content:
        return Document(page_content="", metadata={"source": source})

    # 提取文本
    text = main_content.get_text(separator='\n', strip=True)

    # 高级清洗
    clean_text = clean_html_content(text)

    # 提取元数据
    title = soup.find('title')
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    meta_keywords = soup.find('meta', attrs={'name': 'keywords'})

    metadata = {
        "source": source,
        "format": "html",
        "title": title.text if title else "",
        "description": meta_desc.get('content', '') if meta_desc else "",
        "keywords": meta_keywords.get('content', '') if meta_keywords else "",
        "content_length": len(clean_text)
    }

    return Document(page_content=clean_text, metadata=metadata)

# 测试高级解析
doc = parse_html_advanced(html_with_noise, "rag_guide.html")
print(f"清洗后的文本:\n{doc.page_content}\n")

# ===== 9. 错误处理与重试 =====
print("\n=== 9. 错误处理与重试 ===")

import time

def fetch_webpage_robust(url: str, max_retries: int = 3) -> Document:
    """健壮的网页抓取（带重试）"""

    for attempt in range(max_retries):
        try:
            print(f"尝试 {attempt + 1}/{max_retries}: {url}")

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            doc = parse_html_advanced(response.content, url)
            doc.metadata["url"] = url
            doc.metadata["status_code"] = response.status_code

            print(f"✅ 成功")
            return doc

        except requests.Timeout:
            print(f"⏱️  超时")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # 指数退避
                continue
            else:
                return Document(
                    page_content="",
                    metadata={"source": url, "error": "timeout"}
                )

        except requests.RequestException as e:
            print(f"❌ 错误: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                continue
            else:
                return Document(
                    page_content="",
                    metadata={"source": url, "error": str(e)}
                )

# 测试健壮抓取
# doc = fetch_webpage_robust("https://example.com/article")

print("\n=== 示例代码执行完成 ===")
print("\n使用说明:")
print("1. 准备要抓取的URL列表")
print("2. 取消注释相应的测试代码")
print("3. 运行脚本查看效果")
print("\n功能:")
print("- 智能内容提取（去除噪声）")
print("- 批量网页抓取")
print("- 按标题分段")
print("- 构建可检索的知识库")
print("- 健壮的错误处理")
```

---

## 运行输出示例

```
=== 2. 智能内容提取 ===
标题: RAG技术指南
提取的文本:
RAG核心流程
RAG系统包含三个核心阶段：
1. 文档加载与索引
2. 查询处理与检索
3. 上下文注入与生成

=== 6. 按标题分段 ===
分段提取了 3 个section:
  RAG技术指南 (级别: 1)
  核心流程 (级别: 2)
  实践案例 (级别: 2)

=== 7. RAG应用：构建网页知识库 ===
步骤1: 批量抓取网页
抓取网页: https://example.com/article1
✅ 抓取成功: 1250 字符

总共成功抓取 3 个网页

步骤2: 文本分块
✅ 分块完成: 15 个文本块

步骤3: 向量化并存储
✅ 知识库构建成功
```

---

## 关键要点

### 1. 噪声标签清单

```python
# 需要移除的噪声标签
NOISE_TAGS = [
    'script',   # JavaScript代码
    'style',    # CSS样式
    'nav',      # 导航栏
    'footer',   # 页脚
    'aside',    # 侧边栏
    'header',   # 页头
    'iframe',   # 内嵌框架
    'noscript'  # 无脚本内容
]
```

### 2. 主要内容区域优先级

```python
# 按优先级查找主要内容
main_content = (
    soup.find('article') or      # 最优先
    soup.find('main') or          # 次优先
    soup.find('div', class_='content') or
    soup.find('div', id='content') or
    soup.body                     # 最后选择
)
```

### 3. HTTP请求最佳实践

```python
headers = {
    'User-Agent': 'Mozilla/5.0...',  # 模拟浏览器
    'Accept': 'text/html',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
}

response = requests.get(
    url,
    headers=headers,
    timeout=10,        # 设置超时
    allow_redirects=True  # 允许重定向
)
```

---

## 常见问题

### Q1: 如何处理JavaScript渲染的网页？

BeautifulSoup无法处理JavaScript渲染的内容，需要使用：
- Selenium
- Playwright
- Puppeteer

### Q2: 如何处理反爬虫机制？

```python
# 1. 添加延迟
time.sleep(random.uniform(1, 3))

# 2. 使用代理
proxies = {'http': 'http://proxy:port'}
response = requests.get(url, proxies=proxies)

# 3. 轮换User-Agent
user_agents = [...]
headers = {'User-Agent': random.choice(user_agents)}
```

### Q3: 如何提取特定CSS选择器的内容？

```python
# 使用CSS选择器
content = soup.select('.article-content')
for elem in content:
    print(elem.get_text())
```

---

## 扩展阅读

- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) (2025)
- [requests Documentation](https://requests.readthedocs.io/) (2025)
- [LangChain WebBaseLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/web_base) (2025)

---

**版本：** v1.0
**最后更新：** 2026-02-15
**下一步：** 阅读 [07_实战代码_04_GitHub仓库加载.md](./07_实战代码_04_GitHub仓库加载.md)
