# 实战代码

完整可运行的 RAG Prompt 实现示例。

---

## 完整示例：RAG 问答系统

```python
"""
RAG Prompt Engineering 实战示例
演示：构建一个完整的 RAG 问答系统的 Prompt 部分
"""

from openai import OpenAI
from typing import Optional

# ===== 1. 配置 =====

client = OpenAI()  # 需要设置 OPENAI_API_KEY 环境变量

# ===== 2. 系统提示词模板 =====

SYSTEM_PROMPTS = {
    # 通用问答
    "general": """你是一个知识库问答助手。

## 核心规则
1. 只基于提供的参考文档回答问题
2. 如果文档中没有相关信息，明确告知用户
3. 不要编造或推测文档中没有的内容

## 回答风格
- 简洁准确，直接回答问题
- 必要时引用文档来源
- 使用友好专业的语气""",

    # 客服场景
    "customer_service": """你是一个专业的客服助手。

## 核心规则
1. 基于产品文档回答用户问题
2. 无法回答时，建议用户联系人工客服
3. 不透露内部信息，不做超出文档的承诺

## 回答风格
- 友好、耐心、专业
- 先给答案，再给解释
- 复杂问题分步骤说明""",

    # 技术文档
    "technical": """你是一个技术文档助手。

## 核心规则
1. 基于 API 文档和技术规范回答问题
2. 提供代码示例时确保语法正确
3. 不确定的内容要明确标注

## 回答风格
- 技术准确，术语规范
- 适当提供代码示例
- 指出潜在的注意事项"""
}

# ===== 3. 上下文注入函数 =====

def format_documents(
    documents: list[dict],
    include_metadata: bool = True
) -> str:
    """
    将检索到的文档格式化为上下文字符串

    Args:
        documents: 文档列表，每个文档包含 content 和可选的 source, page 等
        include_metadata: 是否包含元数据

    Returns:
        格式化后的上下文字符串
    """
    parts = []

    for i, doc in enumerate(documents, 1):
        if include_metadata:
            source = doc.get('source', '未知来源')
            page = doc.get('page', '')
            page_info = f" (第{page}页)" if page else ""

            parts.append(f"""【文档 {i}】
来源：{source}{page_info}
内容：
{doc['content']}""")
        else:
            parts.append(f"【文档 {i}】\n{doc['content']}")

    return "\n\n---\n\n".join(parts)


# ===== 4. 用户问题模板 =====

QUERY_TEMPLATES = {
    "default": """## 用户问题
{query}

请基于以上参考文档回答这个问题。""",

    "concise": """## 用户问题
{query}

请用 1-2 句话简洁回答。""",

    "detailed": """## 用户问题
{query}

请详细回答，并引用具体的文档来源。""",

    "step_by_step": """## 用户问题
{query}

请分步骤回答，每个步骤单独一行。"""
}


def format_query(query: str, template: str = "default") -> str:
    """格式化用户问题"""
    tmpl = QUERY_TEMPLATES.get(template, QUERY_TEMPLATES["default"])
    return tmpl.format(query=query)


# ===== 5. 构建完整的 RAG Prompt =====

def build_rag_messages(
    documents: list[dict],
    query: str,
    scenario: str = "general",
    query_template: str = "default",
    include_metadata: bool = True
) -> list[dict]:
    """
    构建完整的 RAG 消息列表

    Args:
        documents: 检索到的文档列表
        query: 用户问题
        scenario: 场景类型 (general/customer_service/technical)
        query_template: 问题模板类型
        include_metadata: 是否包含文档元数据

    Returns:
        OpenAI API 格式的消息列表
    """
    # 获取系统提示词
    system_prompt = SYSTEM_PROMPTS.get(scenario, SYSTEM_PROMPTS["general"])

    # 格式化文档
    context = format_documents(documents, include_metadata)

    # 格式化问题
    formatted_query = format_query(query, query_template)

    # 构建用户消息
    user_content = f"""## 参考文档

{context}

{formatted_query}"""

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]


# ===== 6. 调用 LLM 生成回答 =====

def generate_answer(
    messages: list[dict],
    model: str = "gpt-4o-mini",
    temperature: float = 0.3,
    max_tokens: int = 1000
) -> str:
    """
    调用 LLM 生成回答

    Args:
        messages: 消息列表
        model: 模型名称
        temperature: 温度参数
        max_tokens: 最大 token 数

    Returns:
        LLM 生成的回答
    """
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens
    )

    return response.choices[0].message.content


# ===== 7. 完整的 RAG 问答函数 =====

def rag_qa(
    documents: list[dict],
    query: str,
    scenario: str = "general",
    query_template: str = "default"
) -> str:
    """
    完整的 RAG 问答流程

    Args:
        documents: 检索到的文档
        query: 用户问题
        scenario: 场景类型
        query_template: 问题模板

    Returns:
        生成的回答
    """
    # 构建消息
    messages = build_rag_messages(
        documents=documents,
        query=query,
        scenario=scenario,
        query_template=query_template
    )

    # 生成回答
    answer = generate_answer(messages)

    return answer


# ===== 8. 使用示例 =====

if __name__ == "__main__":
    # 模拟检索到的文档
    retrieved_docs = [
        {
            "content": "退款政策：用户在购买后7天内可申请无理由退款。超过7天但在30天内，如产品存在质量问题，可申请退款。",
            "source": "退款政策.pdf",
            "page": 1
        },
        {
            "content": "退款流程：1. 登录账户 2. 进入"我的订单" 3. 选择要退款的订单 4. 点击"申请退款" 5. 填写退款原因 6. 等待审核（1-3个工作日）",
            "source": "操作指南.pdf",
            "page": 5
        },
        {
            "content": "退款到账时间：审核通过后，退款将在3-5个工作日内原路返回。信用卡退款可能需要额外1-2个工作日。",
            "source": "常见问题.pdf",
            "page": 12
        }
    ]

    # 用户问题
    user_query = "我想退款，应该怎么操作？大概多久能到账？"

    print("=" * 60)
    print("【RAG 问答示例】")
    print("=" * 60)

    # 方式1：使用通用场景
    print("\n--- 通用场景 ---")
    messages = build_rag_messages(retrieved_docs, user_query, scenario="general")
    print("系统提示词长度:", len(messages[0]["content"]), "字符")
    print("用户消息长度:", len(messages[1]["content"]), "字符")

    # 方式2：使用客服场景 + 分步骤模板
    print("\n--- 客服场景 + 分步骤回答 ---")
    messages = build_rag_messages(
        retrieved_docs,
        user_query,
        scenario="customer_service",
        query_template="step_by_step"
    )

    # 打印构建的 Prompt（用于调试）
    print("\n【构建的 Prompt 预览】")
    print("-" * 40)
    print("[System]")
    print(messages[0]["content"][:200] + "...")
    print("-" * 40)
    print("[User]")
    print(messages[1]["content"][:500] + "...")
    print("-" * 40)

    # 如果有 API Key，可以实际调用
    # answer = rag_qa(retrieved_docs, user_query, scenario="customer_service")
    # print("\n【LLM 回答】")
    # print(answer)

    print("\n" + "=" * 60)
    print("提示：设置 OPENAI_API_KEY 环境变量后可实际调用 LLM")
    print("=" * 60)
```

---

## 运行输出示例

```
============================================================
【RAG 问答示例】
============================================================

--- 通用场景 ---
系统提示词长度: 186 字符
用户消息长度: 612 字符

--- 客服场景 + 分步骤回答 ---

【构建的 Prompt 预览】
----------------------------------------
[System]
你是一个专业的客服助手。

## 核心规则
1. 基于产品文档回答用户问题
2. 无法回答时，建议用户联系人工客服
3. 不透露内部信息，不做超出文档的承诺

## 回答风格
- 友好、耐心、专业
- 先给答案，再给解释
- 复杂问题分步骤说明...
----------------------------------------
[User]
## 参考文档

【文档 1】
来源：退款政策.pdf (第1页)
内容：
退款政策：用户在购买后7天内可申请无理由退款。超过7天但在30天内，如产品存在质量问题，可申请退款。

---

【文档 2】
来源：操作指南.pdf (第5页)
内容：
退款流程：1. 登录账户 2. 进入"我的订单" 3. 选择要退款的订单 4. 点击"申请退款" 5. 填写退款原因 6. 等待审核（1-3个工作日）...
----------------------------------------

============================================================
提示：设置 OPENAI_API_KEY 环境变量后可实际调用 LLM
============================================================
```

---

## 实际调用后的回答示例

```
【LLM 回答】

您好！关于退款操作和到账时间，我来为您分步说明：

**退款操作步骤：**
1. 登录您的账户
2. 进入"我的订单"页面
3. 选择需要退款的订单
4. 点击"申请退款"按钮
5. 填写退款原因
6. 提交后等待审核（1-3个工作日）

**到账时间：**
- 审核通过后，退款将在 3-5 个工作日内原路返回
- 如果您使用信用卡支付，可能需要额外 1-2 个工作日

**温馨提示：**
根据退款政策，购买后 7 天内可申请无理由退款。

如有其他问题，欢迎随时咨询！
```

---

## 代码要点总结

| 模块 | 作用 | 关键点 |
|------|------|--------|
| `SYSTEM_PROMPTS` | 不同场景的系统提示词 | 角色 + 规则 + 风格 |
| `format_documents()` | 格式化检索结果 | 编号 + 元数据 + 分隔 |
| `QUERY_TEMPLATES` | 用户问题模板 | 控制输出格式 |
| `build_rag_messages()` | 组装完整 Prompt | 系统 + 上下文 + 问题 |
| `generate_answer()` | 调用 LLM | temperature 建议 0.3 |

---

**下一步：** [08_面试必问](./08_面试必问.md) - 如何在面试中展示 Prompt Engineering 能力
