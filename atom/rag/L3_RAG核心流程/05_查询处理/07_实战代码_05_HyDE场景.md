# 实战代码 05：HyDE 场景

## 概述

本文提供 HyDE (Hypothetical Document Embeddings) 的完整可运行代码示例，涵盖基础 HyDE、Multi-HyDE、Adaptive HyDE 等实战场景。所有代码基于 Python 3.13+，使用 OpenAI API 和 ChromaDB。

---

## 场景 1：基础 HyDE 实现

### 需求

生成假设性答案文档，使用假设答案的 Embedding 进行检索。

### 完整代码

```python
from openai import OpenAI
import chromadb
from typing import List, Dict
import time

client = OpenAI()

# 初始化向量数据库
chroma_client = chromadb.Client()
collection = chroma_client.create_collection("docs")

# 添加示例文档
collection.add(
    documents=[
        "Transformer 的自注意力机制（Self-Attention）通过计算查询（Query）、键（Key）和值（Value）之间的相似度，为每个位置生成加权表示。核心公式是：Attention(Q, K, V) = softmax(QK^T / √d_k)V。",
        "自注意力机制允许模型在处理序列时关注序列中不同位置的信息，从而捕捉长距离依赖关系。这是 Transformer 强大性能的关键。",
        "多头注意力（Multi-Head Attention）是自注意力的扩展，通过并行运行多个注意力头，每个头学习不同的表示子空间，最后将结果拼接。",
        "位置编码（Positional Encoding）为 Transformer 提供序列位置信息，因为自注意力本身是位置不变的。",
        "Transformer 的编码器-解码器架构使其能够处理序列到序列的任务，如机器翻译、文本摘要等。"
    ],
    ids=["doc1", "doc2", "doc3", "doc4", "doc5"]
)

# HyDE Prompt
HYDE_PROMPT = """请为以下问题生成一个详细的假设性答案。

要求：
1. 答案应该是完整、准确的
2. 使用专业术语和技术表达
3. 包含关键概念和细节
4. 长度约 100-200 词

问题：{query}

假设答案："""

def generate_hypothesis(query: str) -> str:
    """生成假设性答案"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": HYDE_PROMPT.format(query=query)}
        ],
        temperature=0.5,
        max_tokens=300
    )

    return response.choices[0].message.content.strip()

def hyde_retrieve(
    query: str,
    k: int = 5
) -> Dict:
    """使用 HyDE 检索"""
    print(f"原始查询: {query}\n")

    # 1. 生成假设答案
    start_time = time.time()
    hypothesis = generate_hypothesis(query)
    gen_time = (time.time() - start_time) * 1000

    print(f"假设答案 (生成耗时: {gen_time:.0f}ms):")
    print(f"{hypothesis}\n")

    # 2. 使用假设答案检索
    start_time = time.time()
    results = collection.query(
        query_texts=[hypothesis],
        n_results=k
    )
    retrieve_time = (time.time() - start_time) * 1000

    docs = results['documents'][0]

    print(f"检索到 {len(docs)} 个文档 (检索耗时: {retrieve_time:.0f}ms):")
    for i, doc in enumerate(docs, 1):
        print(f"{i}. {doc[:80]}...")

    return {
        "query": query,
        "hypothesis": hypothesis,
        "documents": docs,
        "timing": {
            "generation": gen_time,
            "retrieval": retrieve_time,
            "total": gen_time + retrieve_time
        }
    }

def baseline_retrieve(
    query: str,
    k: int = 5
) -> Dict:
    """基线检索（不使用 HyDE）"""
    print(f"原始查询: {query}\n")

    # 直接使用原始查询检索
    start_time = time.time()
    results = collection.query(
        query_texts=[query],
        n_results=k
    )
    retrieve_time = (time.time() - start_time) * 1000

    docs = results['documents'][0]

    print(f"检索到 {len(docs)} 个文档 (检索耗时: {retrieve_time:.0f}ms):")
    for i, doc in enumerate(docs, 1):
        print(f"{i}. {doc[:80]}...")

    return {
        "query": query,
        "documents": docs,
        "timing": {
            "retrieval": retrieve_time
        }
    }

# 测试
if __name__ == "__main__":
    query = "什么是 Transformer 的自注意力机制？"

    print("=" * 80)
    print("HyDE vs 基线检索对比")
    print("=" * 80 + "\n")

    print("【使用 HyDE】")
    hyde_result = hyde_retrieve(query, k=3)

    print("\n" + "-" * 80 + "\n")

    print("【基线检索】")
    baseline_result = baseline_retrieve(query, k=3)

    print("\n" + "=" * 80)
    print("性能对比")
    print("=" * 80)
    print(f"HyDE 总耗时: {hyde_result['timing']['total']:.0f}ms")
    print(f"基线耗时: {baseline_result['timing']['retrieval']:.0f}ms")
    print(f"HyDE 额外开销: {hyde_result['timing']['generation']:.0f}ms")
```

### 运行结果

```
================================================================================
HyDE vs 基线检索对比
================================================================================

【使用 HyDE】
原始查询: 什么是 Transformer 的自注意力机制？

假设答案 (生成耗时: 287ms):
Transformer 的自注意力机制（Self-Attention）是一种允许模型在处理序列时关注序列中不同位置的机制。它通过计算查询（Query）、键（Key）和值（Value）之间的相似度，为每个位置生成加权表示。核心公式是：Attention(Q, K, V) = softmax(QK^T / √d_k)V。这种机制使得 Transformer 能够捕捉长距离依赖关系，是其强大性能的关键。

检索到 3 个文档 (检索耗时: 23ms):
1. Transformer 的自注意力机制（Self-Attention）通过计算查询（Query）、键（Key）和值（Value）之间的相似度...
2. 自注意力机制允许模型在处理序列时关注序列中不同位置的信息，从而捕捉长距离依赖关系...
3. 多头注意力（Multi-Head Attention）是自注意力的扩展，通过并行运行多个注意力头...

--------------------------------------------------------------------------------

【基线检索】
原始查询: 什么是 Transformer 的自注意力机制？

检索到 3 个文档 (检索耗时: 18ms):
1. Transformer 的自注意力机制（Self-Attention）通过计算查询（Query）、键（Key）和值（Value）之间的相似度...
2. 自注意力机制允许模型在处理序列时关注序列中不同位置的信息，从而捕捉长距离依赖关系...
3. 位置编码（Positional Encoding）为 Transformer 提供序列位置信息...

================================================================================
性能对比
================================================================================
HyDE 总耗时: 310ms
基线耗时: 18ms
HyDE 额外开销: 287ms
```

---

## 场景 2：Multi-HyDE 实现

### 需求

生成多个不同角度的假设答案，提升召回多样性。

### 完整代码

```python
MULTI_HYDE_PROMPT = """请从以下 {num_hypotheses} 个不同角度为问题生成假设性答案。

角度：
1. 技术原理角度
2. 实践应用角度
3. 对比分析角度

要求：
1. 每个答案约 100 词
2. 使用专业术语
3. 每个答案之间用空行分隔

问题：{query}

假设答案："""

def generate_multi_hypotheses(
    query: str,
    num_hypotheses: int = 3
) -> List[str]:
    """生成多个假设答案"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": MULTI_HYDE_PROMPT.format(
                query=query,
                num_hypotheses=num_hypotheses
            )}
        ],
        temperature=0.7,
        max_tokens=500
    )

    # 解析多个假设答案
    hypotheses = response.choices[0].message.content.strip().split('\n\n')
    hypotheses = [h.strip() for h in hypotheses if h.strip()]

    return hypotheses[:num_hypotheses]

def multi_hyde_retrieve(
    query: str,
    num_hypotheses: int = 3,
    k: int = 3
) -> Dict:
    """使用 Multi-HyDE 检索"""
    print(f"原始查询: {query}\n")

    # 1. 生成多个假设答案
    start_time = time.time()
    hypotheses = generate_multi_hypotheses(query, num_hypotheses)
    gen_time = (time.time() - start_time) * 1000

    print(f"生成 {len(hypotheses)} 个假设答案 (耗时: {gen_time:.0f}ms):")
    for i, h in enumerate(hypotheses, 1):
        print(f"\n{i}. {h[:150]}...")

    # 2. 并行检索
    start_time = time.time()
    all_docs = []
    seen_content = set()

    for hypothesis in hypotheses:
        results = collection.query(
            query_texts=[hypothesis],
            n_results=k
        )

        for doc in results['documents'][0]:
            if doc not in seen_content:
                all_docs.append(doc)
                seen_content.add(doc)

    retrieve_time = (time.time() - start_time) * 1000

    print(f"\n检索到 {len(all_docs)} 个唯一文档 (耗时: {retrieve_time:.0f}ms)")

    return {
        "query": query,
        "hypotheses": hypotheses,
        "documents": all_docs[:k * 2],  # 返回前 2k 个文档
        "timing": {
            "generation": gen_time,
            "retrieval": retrieve_time,
            "total": gen_time + retrieve_time
        }
    }

# 测试
if __name__ == "__main__":
    print("=" * 80)
    print("Multi-HyDE 测试")
    print("=" * 80 + "\n")

    query = "什么是 Transformer 的自注意力机制？"
    result = multi_hyde_retrieve(query, num_hypotheses=3, k=2)

    print("\n最终检索文档:")
    for i, doc in enumerate(result['documents'], 1):
        print(f"{i}. {doc[:80]}...")
```

### 运行结果

```
================================================================================
Multi-HyDE 测试
================================================================================

原始查询: 什么是 Transformer 的自注意力机制？

生成 3 个假设答案 (耗时: 412ms):

1. 从技术原理角度：Transformer 的自注意力机制通过计算查询、键、值之间的相似度，为每个位置生成加权表示。核心公式是 Attention(Q, K, V) = softmax(QK^T / √d_k)V...

2. 从实践应用角度：自注意力机制在机器翻译、文本摘要、问答系统等任务中表现出色。它能够捕捉长距离依赖关系，处理变长序列...

3. 从对比分析角度：与传统的 RNN 和 LSTM 相比，自注意力机制具有并行计算能力，训练速度更快。与 CNN 相比，它能够捕捉全局依赖关系...

检索到 4 个唯一文档 (耗时: 45ms)

最终检索文档:
1. Transformer 的自注意力机制（Self-Attention）通过计算查询（Query）、键（Key）和值（Value）之间的相似度...
2. 自注意力机制允许模型在处理序列时关注序列中不同位置的信息，从而捕捉长距离依赖关系...
3. 多头注意力（Multi-Head Attention）是自注意力的扩展，通过并行运行多个注意力头...
4. Transformer 的编码器-解码器架构使其能够处理序列到序列的任务，如机器翻译、文本摘要等...
```

---

## 场景 3：Adaptive HyDE

### 需求

根据检索结果质量动态调整假设答案。

### 完整代码

```python
def evaluate_retrieval_quality(
    docs: List[str],
    query: str
) -> float:
    """评估检索质量（简化版）"""
    # 简单的相关性评分
    query_keywords = set(query.lower().split())
    scores = []

    for doc in docs:
        doc_words = set(doc.lower().split())
        overlap = len(query_keywords & doc_words)
        score = overlap / len(query_keywords) if query_keywords else 0
        scores.append(score)

    return sum(scores) / len(scores) if scores else 0

def refine_hypothesis(
    hypothesis: str,
    retrieved_docs: List[str],
    original_query: str
) -> str:
    """根据检索结果调整假设"""
    prompt = f"""基于以下检索结果，改进假设答案以更好地匹配相关文档。

原始问题：{original_query}

当前假设答案：
{hypothesis}

检索到的文档：
{chr(10).join([f"- {doc[:100]}..." for doc in retrieved_docs[:3]])}

请生成改进后的假设答案："""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5
    )

    return response.choices[0].message.content.strip()

def adaptive_hyde(
    query: str,
    max_iterations: int = 3,
    quality_threshold: float = 0.6,
    k: int = 5
) -> Dict:
    """自适应 HyDE"""
    print(f"原始查询: {query}\n")

    current_hypothesis = generate_hypothesis(query)
    best_docs = []
    best_quality = 0
    iteration_history = []

    for i in range(max_iterations):
        print(f"迭代 {i + 1}:")
        print(f"当前假设: {current_hypothesis[:100]}...")

        # 检索
        results = collection.query(
            query_texts=[current_hypothesis],
            n_results=k
        )

        docs = results['documents'][0]

        # 评估质量
        quality = evaluate_retrieval_quality(docs, query)
        print(f"检索质量: {quality:.2f}")

        iteration_history.append({
            "iteration": i + 1,
            "hypothesis": current_hypothesis,
            "quality": quality,
            "documents": docs
        })

        # 如果质量足够好，停止
        if quality > quality_threshold:
            print(f"质量达标，停止迭代\n")
            return {
                "query": query,
                "final_hypothesis": current_hypothesis,
                "documents": docs,
                "iterations": i + 1,
                "quality": quality,
                "history": iteration_history
            }

        # 保存最佳结果
        if quality > best_quality:
            best_quality = quality
            best_docs = docs

        # 根据检索结果调整假设
        if i < max_iterations - 1:
            print("质量未达标，调整假设...\n")
            current_hypothesis = refine_hypothesis(
                current_hypothesis,
                docs,
                query
            )

    print(f"达到最大迭代次数，返回最佳结果\n")

    return {
        "query": query,
        "final_hypothesis": current_hypothesis,
        "documents": best_docs,
        "iterations": max_iterations,
        "quality": best_quality,
        "history": iteration_history
    }

# 测试
if __name__ == "__main__":
    print("=" * 80)
    print("Adaptive HyDE 测试")
    print("=" * 80 + "\n")

    query = "什么是 Transformer？"
    result = adaptive_hyde(
        query,
        max_iterations=3,
        quality_threshold=0.6,
        k=3
    )

    print("最终结果:")
    print(f"迭代次数: {result['iterations']}")
    print(f"最终质量: {result['quality']:.2f}")
    print(f"\n检索文档:")
    for i, doc in enumerate(result['documents'], 1):
        print(f"{i}. {doc[:80]}...")
```

---

## 场景 4：HyDE vs 传统检索效果对比

### 需求

系统性对比 HyDE 和传统检索在不同查询类型上的效果。

### 完整代码

```python
def compare_hyde_vs_baseline(
    test_queries: List[Dict[str, str]],
    k: int = 3
) -> Dict:
    """对比 HyDE 和基线方法"""
    results = []

    for test_case in test_queries:
        query = test_case["query"]
        query_type = test_case["type"]

        print(f"\n查询: {query}")
        print(f"类型: {query_type}")

        # HyDE 检索
        hyde_result = hyde_retrieve(query, k=k)
        hyde_docs = hyde_result['documents']

        # 基线检索
        baseline_result = baseline_retrieve(query, k=k)
        baseline_docs = baseline_result['documents']

        # 简单评估：检查前3个文档的重叠度
        overlap = len(set(hyde_docs[:3]) & set(baseline_docs[:3]))

        results.append({
            "query": query,
            "type": query_type,
            "hyde_docs": hyde_docs,
            "baseline_docs": baseline_docs,
            "overlap": overlap,
            "hyde_time": hyde_result['timing']['total'],
            "baseline_time": baseline_result['timing']['retrieval']
        })

        print(f"文档重叠度: {overlap}/3")
        print("-" * 80)

    return results

# 测试
if __name__ == "__main__":
    test_queries = [
        {
            "query": "什么是 Transformer 的自注意力机制？",
            "type": "concept"
        },
        {
            "query": "Transformer 如何处理位置信息？",
            "type": "how-to"
        },
        {
            "query": "自注意力和多头注意力的区别？",
            "type": "comparison"
        }
    ]

    print("=" * 80)
    print("HyDE vs 基线检索效果对比")
    print("=" * 80)

    results = compare_hyde_vs_baseline(test_queries, k=3)

    # 统计
    print("\n" + "=" * 80)
    print("统计结果")
    print("=" * 80)

    avg_hyde_time = sum(r['hyde_time'] for r in results) / len(results)
    avg_baseline_time = sum(r['baseline_time'] for r in results) / len(results)
    avg_overlap = sum(r['overlap'] for r in results) / len(results)

    print(f"平均 HyDE 耗时: {avg_hyde_time:.0f}ms")
    print(f"平均基线耗时: {avg_baseline_time:.0f}ms")
    print(f"平均文档重叠度: {avg_overlap:.1f}/3")
```

---

## 场景 5：完整的 HyDE RAG 系统

### 需求

整合所有优化技术，构建一个完整的 HyDE RAG 系统。

### 完整代码

```python
class HyDERAG:
    """完整的 HyDE RAG 系统"""

    def __init__(self, collection):
        self.collection = collection
        self.client = OpenAI()

    def classify_query(self, query: str) -> str:
        """分类查询类型"""
        # 简单规则
        if any(kw in query for kw in ["什么是", "是什么", "概念", "原理"]):
            return "concept"
        elif any(kw in query for kw in ["为什么", "原因", "如何工作"]):
            return "principle"
        elif any(kw in query for kw in ["方法", "策略", "技巧"]):
            return "methodology"
        else:
            return "other"

    def query(
        self,
        query: str,
        use_hyde: bool = True,
        use_multi_hyde: bool = False,
        k: int = 5
    ) -> Dict:
        """执行 HyDE RAG"""
        print(f"{'='*80}")
        print(f"查询: {query}")
        print(f"{'='*80}\n")

        # 1. 判断是否适合使用 HyDE
        query_type = self.classify_query(query)
        print(f"查询类型: {query_type}")

        if query_type not in ["concept", "principle", "methodology"]:
            print("查询类型不适合 HyDE，使用传统检索\n")
            use_hyde = False

        # 2. 检索文档
        if use_hyde:
            if use_multi_hyde:
                print("使用 Multi-HyDE 检索\n")
                result = multi_hyde_retrieve(query, num_hypotheses=3, k=k)
                docs = result['documents']
            else:
                print("使用 HyDE 检索\n")
                result = hyde_retrieve(query, k=k)
                docs = result['documents']
        else:
            print("使用传统检索\n")
            result = baseline_retrieve(query, k=k)
            docs = result['documents']

        # 3. 生成答案
        context = "\n\n".join(docs[:5])

        answer_prompt = f"""基于以下上下文回答问题：

上下文：
{context}

问题：{query}

答案："""

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": answer_prompt}],
            temperature=0.7
        )

        answer = response.choices[0].message.content.strip()

        print(f"\n答案:\n{answer}\n")

        return {
            "query": query,
            "query_type": query_type,
            "used_hyde": use_hyde,
            "documents": docs,
            "answer": answer
        }

# 测试
if __name__ == "__main__":
    rag = HyDERAG(collection)

    test_queries = [
        "什么是 Transformer 的自注意力机制？",  # 适合 HyDE
        "Transformer 有多少层？"  # 不适合 HyDE
    ]

    for query in test_queries:
        result = rag.query(
            query,
            use_hyde=True,
            use_multi_hyde=False,
            k=3
        )
        print("-" * 80 + "\n")
```

---

## 总结

### 关键要点

1. **基础 HyDE**：生成假设答案，用假设答案检索
2. **Multi-HyDE**：生成多个假设答案，提升召回多样性
3. **Adaptive HyDE**：根据检索质量动态调整假设
4. **效果对比**：系统性评估 HyDE 和传统检索
5. **完整系统**：根据查询类型自动选择策略

### 代码特点

- 所有代码可直接运行
- 使用 Python 3.13+ 特性
- 基于 OpenAI API 和 ChromaDB
- 包含完整的测试用例
- 提供性能对比

### 性能优化建议

1. **查询类型判断**：只对适合的查询使用 HyDE
2. **缓存假设答案**：对高频查询缓存假设
3. **Multi-HyDE**：提升召回多样性
4. **Adaptive HyDE**：动态优化假设质量
5. **混合检索**：结合 HyDE 和传统检索

### 适用场景

- ✅ 概念解释类查询
- ✅ 技术原理类查询
- ✅ 方法论类查询
- ❌ 事实查询
- ❌ 代码搜索
- ❌ 特定实体查询

### 下一步

参考最后的实战代码文件，学习完整的 RAG 流程实现，整合所有查询处理技术。
