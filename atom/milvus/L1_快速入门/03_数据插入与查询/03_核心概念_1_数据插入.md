# 核心概念 1: 数据插入

## 什么是数据插入？

**数据插入（Data Insertion）** 是将向量数据和元数据写入 Milvus Collection 的过程，是向量数据库操作的起点。

### 核心定义

```python
# 数据插入的本质
collection.insert([
    [id1, id2, id3, ...],           # 主键 ID
    [vector1, vector2, vector3, ...], # 向量数据
    [meta1, meta2, meta3, ...]       # 元数据
])
```

**类比理解**:
- **前端类比**: 就像 `POST /api/users` 批量创建用户记录
- **日常类比**: 就像往图书馆添加新书，每本书有编号、内容摘要（向量）、作者信息（元数据）

### 为什么需要数据插入？

在 RAG 系统中，数据插入是构建知识库的第一步：

```
文档 → 分块 → 向量化 → 插入 Milvus → 可检索
```

**实际场景**:
- 文档问答系统：插入文档的向量表示
- 智能客服：插入历史对话的向量
- 推荐系统：插入商品描述的向量
- 知识库检索：插入知识条目的向量

---

## 数据插入的组成部分

### 1. 主键 ID（Primary Key）

主键是每条记录的唯一标识符。

#### 两种方式

**方式 1: 自动生成 ID（推荐）**

```python
from pymilvus import Collection, FieldSchema, CollectionSchema, DataType

# 定义 Schema（auto_id=True）
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=500)
]
schema = CollectionSchema(fields, description="Auto ID example")
collection = Collection("auto_id_collection", schema)

# 插入数据（不需要提供 ID）
data = [
    [[0.1, 0.2, ...], [0.3, 0.4, ...]],  # 向量
    ["text1", "text2"]                    # 元数据
]
result = collection.insert(data)
print(f"插入成功，自动生成的 ID: {result.primary_keys}")
```

**方式 2: 手动指定 ID**

```python
# 定义 Schema（auto_id=False）
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=500)
]
schema = CollectionSchema(fields, description="Manual ID example")
collection = Collection("manual_id_collection", schema)

# 插入数据（必须提供 ID）
data = [
    [1, 2, 3],                           # 手动指定的 ID
    [[0.1, 0.2, ...], [0.3, 0.4, ...], [0.5, 0.6, ...]],  # 向量
    ["text1", "text2", "text3"]          # 元数据
]
result = collection.insert(data)
```

**选择建议**:
- **auto_id=True**: 适合大多数场景，简单方便
- **auto_id=False**: 适合需要与外部系统 ID 对应的场景（如数据库主键）

---

### 2. 向量字段（Vector Field）

向量字段存储高维向量数据，是相似度检索的核心。

#### 维度匹配

**关键要求**: 插入的向量维度必须与 Schema 定义的维度完全一致。

```python
# Schema 定义：128 维
FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)

# ✅ 正确：128 维向量
vector = [0.1] * 128
collection.insert([[vector]])

# ❌ 错误：维度不匹配
vector = [0.1] * 256  # 256 维
collection.insert([[vector]])  # 报错！
```

#### 向量数据类型

Milvus 支持两种向量类型：

```python
# 1. FLOAT_VECTOR（浮点向量，最常用）
FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)

# 2. BINARY_VECTOR（二值向量，节省空间）
FieldSchema(name="binary_embedding", dtype=DataType.BINARY_VECTOR, dim=128)
```

**RAG 场景推荐**: 使用 `FLOAT_VECTOR`，因为大多数 Embedding 模型输出浮点向量。

---

### 3. 标量字段（Scalar Field）

标量字段存储元数据，用于过滤和返回结果。

#### 支持的数据类型

```python
from pymilvus import DataType

# 整数类型
FieldSchema(name="age", dtype=DataType.INT64)

# 浮点类型
FieldSchema(name="score", dtype=DataType.FLOAT)

# 字符串类型（需要指定最大长度）
FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=1000)

# 布尔类型
FieldSchema(name="is_active", dtype=DataType.BOOL)

# JSON 类型（Milvus 2.2+）
FieldSchema(name="metadata", dtype=DataType.JSON)
```

#### RAG 场景常用字段

```python
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),

    # RAG 常用元数据
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2000),      # 原始文本
    FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),     # 文档 ID
    FieldSchema(name="chunk_id", dtype=DataType.INT64),                     # 分块 ID
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=50),    # 分类
    FieldSchema(name="timestamp", dtype=DataType.INT64),                    # 时间戳
]
```

---

## 插入方式对比

### 单条插入 vs 批量插入

#### 单条插入（不推荐）

```python
import time

# 单条插入 1000 条数据
start = time.time()
for i in range(1000):
    data = [
        [[0.1] * 128],  # 单个向量
        [f"text_{i}"]   # 单个文本
    ]
    collection.insert(data)
end = time.time()
print(f"单条插入耗时: {end - start:.2f}秒")  # 约 10-20 秒
```

#### 批量插入（推荐）

```python
# 批量插入 1000 条数据
start = time.time()
vectors = [[0.1] * 128 for _ in range(1000)]
texts = [f"text_{i}" for i in range(1000)]
data = [vectors, texts]
collection.insert(data)
end = time.time()
print(f"批量插入耗时: {end - start:.2f}秒")  # 约 0.1-0.5 秒
```

### 性能对比

| 插入方式 | 1000 条数据耗时 | 10000 条数据耗时 | 性能比 |
|---------|----------------|-----------------|--------|
| 单条插入 | 10-20 秒       | 100-200 秒      | 1x     |
| 批量插入 | 0.1-0.5 秒     | 1-5 秒          | 20-100x |

**结论**: 批量插入比单条插入快 **20-100 倍**！

### 批量大小建议

```python
# 推荐的批量大小
BATCH_SIZE = 1000  # 一般场景
BATCH_SIZE = 5000  # 高性能场景
BATCH_SIZE = 100   # 低内存场景

# 分批插入大量数据
def batch_insert(collection, all_vectors, all_texts, batch_size=1000):
    total = len(all_vectors)
    for i in range(0, total, batch_size):
        batch_vectors = all_vectors[i:i+batch_size]
        batch_texts = all_texts[i:i+batch_size]
        data = [batch_vectors, batch_texts]
        collection.insert(data)
        print(f"已插入 {min(i+batch_size, total)}/{total} 条数据")
```

---

## 完整代码示例

### 场景：插入文档向量到 Milvus

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType
import numpy as np

# 1. 连接 Milvus
connections.connect(host="localhost", port="19530")

# 2. 定义 Schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2000),
    FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),
    FieldSchema(name="chunk_id", dtype=DataType.INT64),
]
schema = CollectionSchema(fields, description="Document embeddings")

# 3. 创建 Collection
collection_name = "document_vectors"
collection = Collection(collection_name, schema)

# 4. 准备数据（模拟）
num_entities = 1000
embeddings = np.random.rand(num_entities, 384).tolist()  # 384 维向量
texts = [f"这是第 {i} 个文档块的内容..." for i in range(num_entities)]
doc_ids = [f"doc_{i//10}" for i in range(num_entities)]  # 每个文档 10 个块
chunk_ids = [i % 10 for i in range(num_entities)]

# 5. 批量插入
data = [
    embeddings,  # 向量字段
    texts,       # 文本字段
    doc_ids,     # 文档 ID
    chunk_ids    # 分块 ID
]

print(f"开始插入 {num_entities} 条数据...")
result = collection.insert(data)
print(f"插入成功！自动生成的 ID 数量: {len(result.primary_keys)}")

# 6. 刷新数据（确保数据持久化）
collection.flush()
print("数据已刷新到磁盘")

# 7. 查看 Collection 统计
print(f"Collection 中的实体数量: {collection.num_entities}")
```

**输出示例**:
```
开始插入 1000 条数据...
插入成功！自动生成的 ID 数量: 1000
数据已刷新到磁盘
Collection 中的实体数量: 1000
```

---

## 数据插入的关键要点

### 1. 数据顺序必须与 Schema 一致

```python
# Schema 定义顺序
fields = [
    FieldSchema(name="id", ...),
    FieldSchema(name="embedding", ...),
    FieldSchema(name="text", ...),
    FieldSchema(name="category", ...),
]

# ✅ 正确：数据顺序与 Schema 一致
data = [
    [1, 2, 3],                    # id
    [vec1, vec2, vec3],           # embedding
    ["text1", "text2", "text3"],  # text
    ["cat1", "cat2", "cat3"]      # category
]

# ❌ 错误：数据顺序不一致
data = [
    ["text1", "text2", "text3"],  # 错误：应该是 id
    [vec1, vec2, vec3],
    [1, 2, 3],
    ["cat1", "cat2", "cat3"]
]
```

### 2. 插入后需要等待 flush

```python
# 插入数据
collection.insert(data)

# ⚠️ 此时数据可能还在内存缓冲区，未持久化

# 刷新数据到磁盘
collection.flush()

# ✅ 现在数据已持久化，可以安全检索
```

### 3. auto_id=True 时不要提供 ID

```python
# Schema 定义 auto_id=True
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128),
]

# ✅ 正确：不提供 ID
data = [
    [[0.1] * 128, [0.2] * 128],  # 只提供向量
]

# ❌ 错误：提供了 ID
data = [
    [1, 2],                      # 不应该提供 ID
    [[0.1] * 128, [0.2] * 128],
]
```

### 4. 向量维度必须匹配

```python
# Schema 定义 dim=384
FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)

# ✅ 正确：384 维
vector = [0.1] * 384

# ❌ 错误：维度不匹配
vector = [0.1] * 768  # 报错！
```

---

## 在 RAG 中的应用

### RAG 数据插入流程

```python
from sentence_transformers import SentenceTransformer
from pymilvus import connections, Collection

# 1. 初始化 Embedding 模型
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. 连接 Milvus
connections.connect(host="localhost", port="19530")
collection = Collection("rag_documents")

# 3. 准备文档数据
documents = [
    "Milvus 是一个开源的向量数据库。",
    "RAG 结合了检索和生成两种技术。",
    "向量检索可以实现语义搜索。",
]

# 4. 向量化文档
embeddings = model.encode(documents).tolist()

# 5. 批量插入 Milvus
data = [
    embeddings,  # 向量
    documents    # 原始文本
]
collection.insert(data)
collection.flush()

print(f"成功插入 {len(documents)} 个文档向量")
```

### RAG 场景的最佳实践

1. **分批插入大量文档**
   ```python
   BATCH_SIZE = 1000
   for i in range(0, len(all_documents), BATCH_SIZE):
       batch = all_documents[i:i+BATCH_SIZE]
       embeddings = model.encode(batch).tolist()
       collection.insert([embeddings, batch])
   ```

2. **保存元数据用于溯源**
   ```python
   data = [
       embeddings,
       texts,
       doc_ids,      # 文档 ID
       chunk_ids,    # 分块 ID
       timestamps    # 时间戳
   ]
   ```

3. **定期 flush 确保数据持久化**
   ```python
   collection.insert(data)
   collection.flush()  # 每次插入后刷新
   ```

---

## 常见错误与解决方案

### 错误 1: 维度不匹配

```python
# 错误信息
# "dimension mismatch: expected 384, got 768"

# 解决方案：检查 Embedding 模型的输出维度
model = SentenceTransformer('all-MiniLM-L6-v2')  # 384 维
print(f"模型输出维度: {model.get_sentence_embedding_dimension()}")

# 确保 Schema 定义的维度与模型一致
FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)
```

### 错误 2: 数据顺序错误

```python
# 错误信息
# "field type mismatch"

# 解决方案：确保数据顺序与 Schema 一致
# 使用字典明确指定字段（Milvus 2.3+）
data = {
    "embedding": embeddings,
    "text": texts,
    "doc_id": doc_ids
}
collection.insert(data)
```

### 错误 3: 字符串超长

```python
# 错误信息
# "string length exceeds max length"

# 解决方案：截断或增加 max_length
FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=5000)

# 或者在插入前截断
texts = [text[:2000] for text in texts]
```

---

## 总结

### 核心要点

1. **批量插入优先**: 比单条插入快 20-100 倍
2. **数据顺序一致**: 必须与 Schema 定义顺序一致
3. **维度必须匹配**: 向量维度必须与 Schema 定义一致
4. **及时 flush**: 插入后调用 `flush()` 确保数据持久化
5. **auto_id 推荐**: 大多数场景使用 `auto_id=True` 更简单

### RAG 应用建议

- 使用批量插入提高性能
- 保存完整元数据用于溯源
- 定期 flush 确保数据安全
- 选择合适的 Embedding 模型维度

### 下一步

学习 **核心概念 2: 索引创建**，了解如何为插入的数据创建高效索引。
