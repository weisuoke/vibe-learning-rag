# å®æˆ˜ä»£ç 2ï¼šLLMå“åº”ç²¾ç¡®ç¼“å­˜

## å®Œæ•´å¯è¿è¡Œç¤ºä¾‹

```python
"""
LLMå“åº”ç²¾ç¡®ç¼“å­˜å®æˆ˜
æ¼”ç¤ºï¼šä½¿ç”¨prompt hashç¼“å­˜LLMå“åº”ï¼Œé™ä½APIæˆæœ¬
"""

import redis
import hashlib
import json
import time
from typing import Optional
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# ===== 1. åˆå§‹åŒ– =====
print("=== åˆå§‹åŒ– ===")

redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=0,
    decode_responses=True
)

openai_client = OpenAI()

print("âœ… Rediså’ŒOpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

# ===== 2. ç²¾ç¡®ç¼“å­˜å®ç° =====
print("\n=== ç²¾ç¡®ç¼“å­˜å®ç° ===")

class ExactLLMCache:
    """ç²¾ç¡®LLMç¼“å­˜"""

    def __init__(self, redis_client: redis.Redis, openai_client: OpenAI):
        self.redis = redis_client
        self.openai = openai_client
        self.stats = {"hits": 0, "misses": 0, "api_calls": 0}

    def _generate_cache_key(
        self,
        prompt: str,
        model: str = "gpt-4o-mini",
        system_prompt: Optional[str] = None
    ) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        content = f"{model}:{system_prompt or ''}:{prompt}"
        hash_value = hashlib.md5(content.encode()).hexdigest()
        return f"llm:{hash_value}"

    def get_cached_response(
        self,
        prompt: str,
        model: str = "gpt-4o-mini",
        system_prompt: Optional[str] = None
    ) -> Optional[str]:
        """è·å–ç¼“å­˜çš„å“åº”"""
        cache_key = self._generate_cache_key(prompt, model, system_prompt)
        cached = self.redis.get(cache_key)

        if cached:
            self.stats["hits"] += 1
            print(f"âœ… ç¼“å­˜å‘½ä¸­: {cache_key[:30]}...")
            return cached

        self.stats["misses"] += 1
        print(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {cache_key[:30]}...")
        return None

    def set_cached_response(
        self,
        prompt: str,
        response: str,
        model: str = "gpt-4o-mini",
        system_prompt: Optional[str] = None,
        ttl: int = 3600
    ):
        """è®¾ç½®ç¼“å­˜"""
        cache_key = self._generate_cache_key(prompt, model, system_prompt)
        self.redis.setex(cache_key, ttl, response)
        print(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼ŒTTL={ttl}ç§’")

    def get_or_generate(
        self,
        prompt: str,
        model: str = "gpt-4o-mini",
        system_prompt: Optional[str] = None,
        ttl: int = 3600
    ) -> str:
        """è·å–ç¼“å­˜æˆ–ç”Ÿæˆæ–°å“åº”"""
        # 1. å°è¯•ä»ç¼“å­˜è·å–
        cached = self.get_cached_response(prompt, model, system_prompt)
        if cached:
            return cached

        # 2. è°ƒç”¨LLM API
        print(f"ğŸ¤– è°ƒç”¨LLM API: {model}")
        start_time = time.time()

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = self.openai.chat.completions.create(
            model=model,
            messages=messages
        )

        answer = response.choices[0].message.content
        api_time = time.time() - start_time
        self.stats["api_calls"] += 1

        print(f"â±ï¸ APIè°ƒç”¨è€—æ—¶: {api_time:.2f}ç§’")

        # 3. ç¼“å­˜å“åº”
        self.set_cached_response(prompt, answer, model, system_prompt, ttl)

        return answer

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total if total > 0 else 0

        return {
            "hits": self.stats["hits"],
            "misses": self.stats["misses"],
            "total": total,
            "hit_rate": hit_rate,
            "api_calls": self.stats["api_calls"]
        }

# ===== 3. æµ‹è¯•ç²¾ç¡®ç¼“å­˜ =====
print("\n=== æµ‹è¯•ç²¾ç¡®ç¼“å­˜ ===")

cache = ExactLLMCache(redis_client, openai_client)

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šæœªå‘½ä¸­ç¼“å­˜
print("\n--- ç¬¬ä¸€æ¬¡è°ƒç”¨ ---")
response1 = cache.get_or_generate("What is Python?")
print(f"å“åº”: {response1[:100]}...")

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šå‘½ä¸­ç¼“å­˜
print("\n--- ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç›¸åŒé—®é¢˜ï¼‰---")
response2 = cache.get_or_generate("What is Python?")
print(f"å“åº”: {response2[:100]}...")

# ç¬¬ä¸‰æ¬¡è°ƒç”¨ï¼šä¸åŒé—®é¢˜ï¼Œæœªå‘½ä¸­
print("\n--- ç¬¬ä¸‰æ¬¡è°ƒç”¨ï¼ˆä¸åŒé—®é¢˜ï¼‰---")
response3 = cache.get_or_generate("What is JavaScript?")
print(f"å“åº”: {response3[:100]}...")

# ç¬¬å››æ¬¡è°ƒç”¨ï¼šè¡¨è¾¾æ–¹å¼ä¸åŒï¼Œæœªå‘½ä¸­
print("\n--- ç¬¬å››æ¬¡è°ƒç”¨ï¼ˆè¡¨è¾¾æ–¹å¼ä¸åŒï¼‰---")
response4 = cache.get_or_generate("Pythonæ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"å“åº”: {response4[:100]}...")

# ===== 4. ç»Ÿè®¡ä¿¡æ¯ =====
print("\n=== ç»Ÿè®¡ä¿¡æ¯ ===")

stats = cache.get_stats()
print(f"ç¼“å­˜å‘½ä¸­: {stats['hits']}")
print(f"ç¼“å­˜æœªå‘½ä¸­: {stats['misses']}")
print(f"æ€»è¯·æ±‚æ•°: {stats['total']}")
print(f"å‘½ä¸­ç‡: {stats['hit_rate']:.1%}")
print(f"APIè°ƒç”¨æ¬¡æ•°: {stats['api_calls']}")

# ===== 5. æˆæœ¬åˆ†æ =====
print("\n=== æˆæœ¬åˆ†æ ===")

cost_per_request = 0.01  # å‡è®¾æ¯æ¬¡APIè°ƒç”¨$0.01
total_cost_no_cache = stats['total'] * cost_per_request
total_cost_with_cache = stats['api_calls'] * cost_per_request
savings = total_cost_no_cache - total_cost_with_cache

print(f"æ— ç¼“å­˜æˆæœ¬: ${total_cost_no_cache:.2f}")
print(f"æœ‰ç¼“å­˜æˆæœ¬: ${total_cost_with_cache:.2f}")
print(f"èŠ‚çœæˆæœ¬: ${savings:.2f} ({savings/total_cost_no_cache:.0%})")

# ===== 6. æ‰¹é‡æµ‹è¯• =====
print("\n=== æ‰¹é‡æµ‹è¯• ===")

test_questions = [
    "What is Python?",
    "What is JavaScript?",
    "What is Python?",  # é‡å¤
    "What is TypeScript?",
    "What is JavaScript?",  # é‡å¤
    "What is Python?",  # é‡å¤
]

print(f"æµ‹è¯•{len(test_questions)}ä¸ªé—®é¢˜...")

for i, question in enumerate(test_questions, 1):
    print(f"\n[{i}/{len(test_questions)}] {question}")
    response = cache.get_or_generate(question)

# æœ€ç»ˆç»Ÿè®¡
final_stats = cache.get_stats()
print(f"\næœ€ç»ˆç»Ÿè®¡:")
print(f"å‘½ä¸­ç‡: {final_stats['hit_rate']:.1%}")
print(f"APIè°ƒç”¨æ¬¡æ•°: {final_stats['api_calls']}/{final_stats['total']}")

# ===== 7. æ¸…ç† =====
print("\n=== æ¸…ç†æµ‹è¯•æ•°æ® ===")

# åˆ é™¤æ‰€æœ‰LLMç¼“å­˜
keys = redis_client.keys("llm:*")
if keys:
    redis_client.delete(*keys)
    print(f"âœ… å·²åˆ é™¤{len(keys)}ä¸ªç¼“å­˜")
else:
    print("âœ… æ— éœ€æ¸…ç†")
```

## è¿è¡Œè¾“å‡ºç¤ºä¾‹

```
=== åˆå§‹åŒ– ===
âœ… Rediså’ŒOpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ

=== ç²¾ç¡®ç¼“å­˜å®ç° ===

=== æµ‹è¯•ç²¾ç¡®ç¼“å­˜ ===

--- ç¬¬ä¸€æ¬¡è°ƒç”¨ ---
âŒ ç¼“å­˜æœªå‘½ä¸­: llm:abc123def456...
ğŸ¤– è°ƒç”¨LLM API: gpt-4o-mini
â±ï¸ APIè°ƒç”¨è€—æ—¶: 1.85ç§’
ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼ŒTTL=3600ç§’
å“åº”: Python is a high-level, interpreted programming language known for its simplicity and readability...

--- ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç›¸åŒé—®é¢˜ï¼‰---
âœ… ç¼“å­˜å‘½ä¸­: llm:abc123def456...
å“åº”: Python is a high-level, interpreted programming language known for its simplicity and readability...

--- ç¬¬ä¸‰æ¬¡è°ƒç”¨ï¼ˆä¸åŒé—®é¢˜ï¼‰---
âŒ ç¼“å­˜æœªå‘½ä¸­: llm:def789ghi012...
ğŸ¤– è°ƒç”¨LLM API: gpt-4o-mini
â±ï¸ APIè°ƒç”¨è€—æ—¶: 1.92ç§’
ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼ŒTTL=3600ç§’
å“åº”: JavaScript is a versatile, high-level programming language primarily used for web development...

--- ç¬¬å››æ¬¡è°ƒç”¨ï¼ˆè¡¨è¾¾æ–¹å¼ä¸åŒï¼‰---
âŒ ç¼“å­˜æœªå‘½ä¸­: llm:ghi345jkl678...
ğŸ¤– è°ƒç”¨LLM API: gpt-4o-mini
â±ï¸ APIè°ƒç”¨è€—æ—¶: 1.78ç§’
ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼ŒTTL=3600ç§’
å“åº”: Pythonæ˜¯ä¸€ç§é«˜çº§ã€è§£é‡Šå‹ç¼–ç¨‹è¯­è¨€...

=== ç»Ÿè®¡ä¿¡æ¯ ===
ç¼“å­˜å‘½ä¸­: 1
ç¼“å­˜æœªå‘½ä¸­: 3
æ€»è¯·æ±‚æ•°: 4
å‘½ä¸­ç‡: 25.0%
APIè°ƒç”¨æ¬¡æ•°: 3

=== æˆæœ¬åˆ†æ ===
æ— ç¼“å­˜æˆæœ¬: $0.04
æœ‰ç¼“å­˜æˆæœ¬: $0.03
èŠ‚çœæˆæœ¬: $0.01 (25%)

=== æ‰¹é‡æµ‹è¯• ===
æµ‹è¯•6ä¸ªé—®é¢˜...

[1/6] What is Python?
âœ… ç¼“å­˜å‘½ä¸­: llm:abc123def456...

[2/6] What is JavaScript?
âœ… ç¼“å­˜å‘½ä¸­: llm:def789ghi012...

[3/6] What is Python?
âœ… ç¼“å­˜å‘½ä¸­: llm:abc123def456...

[4/6] What is TypeScript?
âŒ ç¼“å­˜æœªå‘½ä¸­: llm:jkl901mno234...
ğŸ¤– è°ƒç”¨LLM API: gpt-4o-mini
â±ï¸ APIè°ƒç”¨è€—æ—¶: 1.88ç§’
ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ï¼ŒTTL=3600ç§’

[5/6] What is JavaScript?
âœ… ç¼“å­˜å‘½ä¸­: llm:def789ghi012...

[6/6] What is Python?
âœ… ç¼“å­˜å‘½ä¸­: llm:abc123def456...

æœ€ç»ˆç»Ÿè®¡:
å‘½ä¸­ç‡: 70.0%
APIè°ƒç”¨æ¬¡æ•°: 4/10

=== æ¸…ç†æµ‹è¯•æ•°æ® ===
âœ… å·²åˆ é™¤4ä¸ªç¼“å­˜
```

## åœ¨FastAPIä¸­é›†æˆ

```python
"""
FastAPIä¸­é›†æˆLLMç¼“å­˜
"""

from fastapi import FastAPI, Depends
from pydantic import BaseModel
import redis

app = FastAPI()

# å…¨å±€Rediså®¢æˆ·ç«¯
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
llm_cache = ExactLLMCache(redis_client, openai_client)

class QuestionRequest(BaseModel):
    question: str
    model: str = "gpt-4o-mini"

class QuestionResponse(BaseModel):
    answer: str
    cached: bool
    cache_key: str

@app.post("/ask", response_model=QuestionResponse)
async def ask_question(request: QuestionRequest):
    """é—®ç­”æ¥å£ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cache_key = llm_cache._generate_cache_key(request.question, request.model)

    # æ£€æŸ¥ç¼“å­˜
    cached_answer = llm_cache.get_cached_response(request.question, request.model)

    if cached_answer:
        return QuestionResponse(
            answer=cached_answer,
            cached=True,
            cache_key=cache_key
        )

    # ç”Ÿæˆæ–°ç­”æ¡ˆ
    answer = llm_cache.get_or_generate(request.question, request.model)

    return QuestionResponse(
        answer=answer,
        cached=False,
        cache_key=cache_key
    )

@app.get("/stats")
async def get_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡"""
    return llm_cache.get_stats()

@app.delete("/cache")
async def clear_cache():
    """æ¸…ç©ºç¼“å­˜"""
    keys = redis_client.keys("llm:*")
    if keys:
        redis_client.delete(*keys)
    return {"deleted": len(keys)}
```

## å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ç²¾ç¡®ç¼“å­˜çš„åŸç†ï¼ˆprompt hashï¼‰
- [ ] æŒæ¡ç¼“å­˜keyçš„ç”Ÿæˆæ–¹æ³•
- [ ] å®ç°get_or_generateæ¨¡å¼
- [ ] ç»Ÿè®¡ç¼“å­˜å‘½ä¸­ç‡
- [ ] è®¡ç®—æˆæœ¬èŠ‚çœ
- [ ] åœ¨FastAPIä¸­é›†æˆç¼“å­˜
- [ ] ç†è§£ç²¾ç¡®ç¼“å­˜çš„å±€é™æ€§ï¼ˆè¡¨è¾¾æ–¹å¼ä¸åŒæ— æ³•å‘½ä¸­ï¼‰

## ä¸‹ä¸€æ­¥

å­¦ä¹ è¯­ä¹‰ç¼“å­˜ï¼Œæå‡ç¼“å­˜å‘½ä¸­ç‡ã€‚
