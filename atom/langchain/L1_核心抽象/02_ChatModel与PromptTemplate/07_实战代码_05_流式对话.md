# 实战代码 05: 流式对话

> **本文目标**: 掌握流式输出的实现方法和应用场景

---

## 概述

流式输出能显著降低用户感知延迟，提升聊天体验。本文提供完整的流式对话实现示例。

---

## 1. 基础流式输出

### 1.1 简单流式调用

```python
"""
示例1: 基础流式输出
演示：使用 stream() 方法实现流式输出
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

load_dotenv()

# ===== 1. 创建模型 =====
model = ChatOpenAI(model="gpt-4o-mini")

# ===== 2. 流式调用 =====
print("=== 流式输出 ===")
print("AI: ", end="", flush=True)

for chunk in model.stream([HumanMessage(content="讲一个关于Python的笑话")]):
    print(chunk.content, end="", flush=True)

print("\n")  # 换行
```

### 1.2 与模板组合

```python
"""
示例2: 流式输出与模板组合
演示：在链中使用流式输出
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

# ===== 1. 创建链 =====
template = ChatPromptTemplate.from_messages([
    ("system", "你是{role}"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

# ===== 2. 流式调用链 =====
print("=== 链式流式输出 ===")
print("AI: ", end="", flush=True)

for chunk in chain.stream({
    "role": "故事讲述者",
    "question": "讲一个关于程序员的故事"
}):
    print(chunk.content, end="", flush=True)

print("\n")
```

---

## 2. 流式数据处理

### 2.1 收集流式数据

```python
"""
示例3: 收集流式数据
演示：边输出边收集完整响应
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

def stream_and_collect(chain, input_data):
    """流式输出并收集完整响应"""
    print("AI: ", end="", flush=True)

    full_response = ""
    for chunk in chain.stream(input_data):
        content = chunk.content
        print(content, end="", flush=True)
        full_response += content

    print("\n")
    return full_response

# ===== 使用 =====
template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

response = stream_and_collect(chain, {"question": "Python的特点是什么？"})

print(f"\n完整响应长度: {len(response)} 字符")
```

### 2.2 实时处理流式数据

```python
"""
示例4: 实时处理流式数据
演示：边接收边处理内容
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

def stream_with_processing(chain, input_data, forbidden_words=None):
    """流式输出并实时检测"""
    if forbidden_words is None:
        forbidden_words = ["不当词汇", "敏感内容"]

    print("AI: ", end="", flush=True)

    full_response = ""
    for chunk in chain.stream(input_data):
        content = chunk.content
        full_response += content

        # 实时检测
        for word in forbidden_words:
            if word in full_response:
                print(f"\n\n[检测到敏感内容: {word}，停止输出]")
                return None

        print(content, end="", flush=True)

    print("\n")
    return full_response

# ===== 使用 =====
template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

print("=== 带内容检测的流式输出 ===")
response = stream_with_processing(
    chain,
    {"question": "介绍一下Python"},
    forbidden_words=["Java"]  # 示例：检测特定词汇
)

if response:
    print(f"输出完成，共 {len(response)} 字符")
```

---

## 3. 流式聊天应用

### 3.1 命令行流式聊天

```python
"""
示例5: 命令行流式聊天机器人
演示：完整的流式聊天应用
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage
from typing import List

load_dotenv()

class StreamingChatBot:
    """流式聊天机器人"""

    def __init__(self, system_message: str = "你是友好的助手"):
        self.model = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
        self.template = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}")
        ])
        self.chain = self.template | self.model
        self.history: List = []
        self.max_history = 10

    def chat(self, question: str) -> str:
        """流式聊天"""
        # 限制历史长度
        history = self.history[-self.max_history:]

        # 流式输出
        print("AI: ", end="", flush=True)

        full_response = ""
        for chunk in self.chain.stream({
            "history": history,
            "question": question
        }):
            content = chunk.content
            print(content, end="", flush=True)
            full_response += content

        print("\n")

        # 更新历史
        self.history.append(HumanMessage(content=question))
        self.history.append(chunk)  # 使用最后一个chunk（完整的AIMessage）

        return full_response

    def run(self):
        """运行聊天机器人"""
        print("=== 流式聊天机器人 ===")
        print("输入 'quit' 退出\n")

        while True:
            try:
                user_input = input("你: ").strip()

                if user_input.lower() == 'quit':
                    print("再见！")
                    break

                if not user_input:
                    continue

                self.chat(user_input)

            except KeyboardInterrupt:
                print("\n再见！")
                break
            except Exception as e:
                print(f"\n❌ 错误: {e}\n")

# ===== 运行 =====
if __name__ == "__main__":
    bot = StreamingChatBot(system_message="你是专业的Python助手")
    bot.run()
```

### 3.2 带进度指示的流式输出

```python
"""
示例6: 带进度指示的流式输出
演示：显示输出进度
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import sys

load_dotenv()

def stream_with_progress(chain, input_data):
    """带进度指示的流式输出"""
    print("AI: ", end="", flush=True)

    char_count = 0
    word_count = 0
    full_response = ""

    for chunk in chain.stream(input_data):
        content = chunk.content
        print(content, end="", flush=True)

        full_response += content
        char_count += len(content)
        word_count = len(full_response.split())

        # 每50个字符显示一次进度（在新行）
        if char_count % 50 == 0 and char_count > 0:
            sys.stdout.write(f" [{char_count}字]")
            sys.stdout.flush()

    print(f"\n\n[完成: {char_count}字符, {word_count}词]")
    return full_response

# ===== 使用 =====
template = ChatPromptTemplate.from_messages([
    ("system", "你是助手"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
chain = template | model

print("=== 带进度的流式输出 ===")
response = stream_with_progress(chain, {
    "question": "详细介绍Python的历史和发展"
})
```

---

## 4. 异步流式输出

### 4.1 基础异步流式

```python
"""
示例7: 异步流式输出
演示：使用 astream() 实现异步流式输出
"""
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

async def async_stream_chat(question: str):
    """异步流式聊天"""
    template = ChatPromptTemplate.from_messages([
        ("system", "你是助手"),
        ("human", "{question}")
    ])

    model = ChatOpenAI(model="gpt-4o-mini")
    chain = template | model

    print("AI: ", end="", flush=True)

    full_response = ""
    async for chunk in chain.astream({"question": question}):
        content = chunk.content
        print(content, end="", flush=True)
        full_response += content

    print("\n")
    return full_response

# ===== 使用 =====
async def main():
    print("=== 异步流式输出 ===")
    response = await async_stream_chat("Python有什么特点？")
    print(f"完成，共 {len(response)} 字符")

if __name__ == "__main__":
    asyncio.run(main())
```

### 4.2 并发流式输出

```python
"""
示例8: 并发流式输出
演示：同时处理多个流式请求
"""
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

load_dotenv()

async def async_stream_with_id(question_id: int, question: str):
    """带ID的异步流式输出"""
    template = ChatPromptTemplate.from_messages([
        ("system", "你是助手，回答要简洁"),
        ("human", "{question}")
    ])

    model = ChatOpenAI(model="gpt-4o-mini")
    chain = template | model

    print(f"\n[问题{question_id}] {question}")
    print(f"[回答{question_id}] ", end="", flush=True)

    full_response = ""
    async for chunk in chain.astream({"question": question}):
        content = chunk.content
        print(content, end="", flush=True)
        full_response += content

    print()
    return question_id, full_response

async def concurrent_streaming():
    """并发处理多个流式请求"""
    questions = [
        "什么是Python？",
        "什么是JavaScript？",
        "什么是Rust？"
    ]

    # 并发执行
    tasks = [
        async_stream_with_id(i+1, q)
        for i, q in enumerate(questions)
    ]

    results = await asyncio.gather(*tasks)

    print("\n=== 完成统计 ===")
    for qid, response in results:
        print(f"问题{qid}: {len(response)} 字符")

# ===== 使用 =====
if __name__ == "__main__":
    print("=== 并发流式输出 ===")
    asyncio.run(concurrent_streaming())
```

---

## 5. Web 应用集成

### 5.1 FastAPI + SSE

```python
"""
示例9: FastAPI + Server-Sent Events
演示：Web应用中的流式输出
"""
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel

load_dotenv()

app = FastAPI()

class Question(BaseModel):
    question: str

async def generate_stream(question: str):
    """生成流式响应"""
    template = ChatPromptTemplate.from_messages([
        ("system", "你是助手"),
        ("human", "{question}")
    ])

    model = ChatOpenAI(model="gpt-4o-mini")
    chain = template | model

    async for chunk in chain.astream({"question": question}):
        # SSE 格式
        yield f"data: {chunk.content}\n\n"

    yield "data: [DONE]\n\n"

@app.post("/stream")
async def stream_endpoint(q: Question):
    """流式输出端点"""
    return StreamingResponse(
        generate_stream(q.question),
        media_type="text/event-stream"
    )

# ===== 使用 =====
# 运行: uvicorn script_name:app --reload
# 测试: curl -X POST http://localhost:8000/stream \
#       -H "Content-Type: application/json" \
#       -d '{"question":"你好"}'
```

### 5.2 WebSocket 流式聊天

```python
"""
示例10: WebSocket 流式聊天
演示：实时双向通信
"""
from fastapi import FastAPI, WebSocket
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

load_dotenv()

app = FastAPI()

class WebSocketChat:
    """WebSocket 聊天管理器"""

    def __init__(self):
        self.model = ChatOpenAI(model="gpt-4o-mini")
        self.template = ChatPromptTemplate.from_messages([
            ("system", "你是助手"),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}")
        ])
        self.chain = self.template | self.model
        self.history = []

    async def chat(self, question: str, websocket: WebSocket):
        """流式聊天"""
        full_response = ""

        async for chunk in self.chain.astream({
            "history": self.history[-10:],  # 限制历史
            "question": question
        }):
            content = chunk.content
            await websocket.send_text(content)
            full_response += content

        # 更新历史
        self.history.append(HumanMessage(content=question))
        self.history.append(AIMessage(content=full_response))

        # 发送结束标记
        await websocket.send_text("[DONE]")

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket 端点"""
    await websocket.accept()
    chat = WebSocketChat()

    try:
        while True:
            # 接收用户消息
            question = await websocket.receive_text()

            if question.lower() == 'quit':
                break

            # 流式响应
            await chat.chat(question, websocket)

    except Exception as e:
        print(f"错误: {e}")
    finally:
        await websocket.close()

# ===== 使用 =====
# 运行: uvicorn script_name:app --reload
# 前端连接: const ws = new WebSocket('ws://localhost:8000/ws');
```

---

## 6. 性能对比

### 6.1 invoke vs stream 对比

```python
"""
示例11: 性能对比
演示：invoke 和 stream 的性能差异
"""
import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

load_dotenv()

model = ChatOpenAI(model="gpt-4o-mini")
question = "详细介绍Python的历史和发展"

print("=== 性能对比 ===\n")

# ===== 测试 invoke =====
print("--- invoke 方式 ---")
start = time.time()
response = model.invoke([HumanMessage(content=question)])
end = time.time()

print(f"首字时间: {end - start:.2f}秒")
print(f"总时间: {end - start:.2f}秒")
print(f"响应长度: {len(response.content)} 字符\n")

# ===== 测试 stream =====
print("--- stream 方式 ---")
start = time.time()
first_chunk_time = None
full_response = ""

for i, chunk in enumerate(model.stream([HumanMessage(content=question)])):
    if i == 0:
        first_chunk_time = time.time() - start
    full_response += chunk.content

end = time.time()

print(f"首字时间: {first_chunk_time:.2f}秒")
print(f"总时间: {end - start:.2f}秒")
print(f"响应长度: {len(full_response)} 字符")

print(f"\n感知延迟降低: {((end - start) - first_chunk_time) / (end - start) * 100:.1f}%")
```

---

## 7. 最佳实践

### 7.1 推荐做法

```python
"""
最佳实践示例
"""

# ✅ 1. 使用 flush 确保实时输出
for chunk in chain.stream(input):
    print(chunk.content, end="", flush=True)

# ✅ 2. 收集完整响应
full_response = ""
for chunk in chain.stream(input):
    print(chunk.content, end="", flush=True)
    full_response += chunk.content

# ✅ 3. 实时处理和检测
for chunk in chain.stream(input):
    content = chunk.content
    if should_stop(content):
        break
    print(content, end="", flush=True)

# ✅ 4. 异步场景使用 astream
async for chunk in chain.astream(input):
    await websocket.send_text(chunk.content)

# ✅ 5. 错误处理
try:
    for chunk in chain.stream(input):
        print(chunk.content, end="", flush=True)
except Exception as e:
    print(f"\n错误: {e}")
```

### 7.2 避免的做法

```python
# ❌ 1. 不使用 flush
for chunk in chain.stream(input):
    print(chunk.content, end="")  # 可能不会实时显示

# ❌ 2. 在流式中使用 invoke
response = chain.invoke(input)  # 失去流式优势

# ❌ 3. 不收集完整响应
for chunk in chain.stream(input):
    print(chunk.content, end="", flush=True)
# 没有保存完整响应，无法后续处理

# ❌ 4. 同步场景使用 astream
for chunk in chain.astream(input):  # 语法错误
    pass

# ❌ 5. 忽略错误
for chunk in chain.stream(input):
    print(chunk.content, end="", flush=True)
# 网络错误会导致程序崩溃
```

---

## 8. 完整应用示例

### 8.1 生产级流式聊天

```python
"""
示例12: 生产级流式聊天应用
演示：完整的流式聊天解决方案
"""
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage
from typing import List, Optional, Callable
import time

load_dotenv()

class ProductionStreamingChat:
    """生产级流式聊天"""

    def __init__(
        self,
        system_message: str = "你是助手",
        max_history: int = 10,
        on_chunk: Optional[Callable] = None,
        on_complete: Optional[Callable] = None
    ):
        self.model = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
        self.template = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}")
        ])
        self.chain = self.template | self.model
        self.history: List = []
        self.max_history = max_history
        self.on_chunk = on_chunk
        self.on_complete = on_complete

    def chat(self, question: str) -> dict:
        """
        流式聊天

        Returns:
            dict: {
                "response": str,
                "chunks": int,
                "duration": float
            }
        """
        start_time = time.time()
        history = self.history[-self.max_history:]

        print("AI: ", end="", flush=True)

        full_response = ""
        chunk_count = 0

        try:
            for chunk in self.chain.stream({
                "history": history,
                "question": question
            }):
                content = chunk.content
                print(content, end="", flush=True)

                full_response += content
                chunk_count += 1

                # 回调
                if self.on_chunk:
                    self.on_chunk(content, chunk_count)

            print("\n")

            # 更新历史
            self.history.append(HumanMessage(content=question))
            self.history.append(chunk)

            duration = time.time() - start_time

            result = {
                "response": full_response,
                "chunks": chunk_count,
                "duration": duration
            }

            # 完成回调
            if self.on_complete:
                self.on_complete(result)

            return result

        except Exception as e:
            print(f"\n❌ 错误: {e}\n")
            return {
                "response": "",
                "chunks": 0,
                "duration": time.time() - start_time,
                "error": str(e)
            }

# ===== 使用示例 =====
def on_chunk_callback(content: str, count: int):
    """每个chunk的回调"""
    if count % 10 == 0:
        print(f" [{count}]", end="", flush=True)

def on_complete_callback(result: dict):
    """完成时的回调"""
    print(f"[完成: {result['chunks']}块, {result['duration']:.2f}秒]")

if __name__ == "__main__":
    chat = ProductionStreamingChat(
        system_message="你是专业的Python助手",
        on_chunk=on_chunk_callback,
        on_complete=on_complete_callback
    )

    print("=== 生产级流式聊天 ===\n")

    questions = [
        "介绍Python",
        "它有什么特点？",
        "适合什么场景？"
    ]

    for q in questions:
        print(f"用户: {q}")
        result = chat.chat(q)
        print()
```

---

## 检查清单

完成本节实战后，你应该能够：

- [ ] 使用 stream() 实现基础流式输出
- [ ] 在链中使用流式输出
- [ ] 收集和处理流式数据
- [ ] 实现命令行流式聊天
- [ ] 使用 astream() 实现异步流式
- [ ] 实现并发流式输出
- [ ] 集成 FastAPI + SSE
- [ ] 实现 WebSocket 流式聊天
- [ ] 理解 invoke vs stream 的性能差异
- [ ] 构建生产级流式应用

---

**下一步**: 阅读 `07_实战代码_06_批处理场景.md` 学习批量处理的优化技巧
