# 核心概念3：连接池监控与调优

## 为什么需要监控连接池？

**连接池是数据库访问的关键路径，监控连接池状态可以及时发现性能瓶颈和资源泄漏问题。**

### 常见问题场景

```python
# 场景1：连接池耗尽
# 现象：请求响应时间突然变长，大量请求超时
# 原因：并发请求数超过连接池容量

# 场景2：连接泄漏
# 现象：连接池中的可用连接越来越少
# 原因：某些请求没有正确关闭 Session

# 场景3：慢查询
# 现象：数据库 CPU 100%，连接池使用率高
# 原因：某些查询执行时间过长，占用连接

# 场景4：连接失效
# 现象：偶尔出现 "MySQL server has gone away" 错误
# 原因：连接超时被数据库主动断开
```

---

## 连接池监控指标

### 1. 核心指标

| 指标 | 说明 | 正常范围 | 告警阈值 |
|------|------|---------|---------|
| **pool_size** | 连接池大小（配置值） | 10-50 | - |
| **checked_in** | 空闲连接数 | > 20% | < 10% |
| **checked_out** | 正在使用的连接数 | < 80% | > 90% |
| **overflow** | 溢出连接数 | 0-50% | > 80% |
| **pool_usage** | 连接池使用率 | < 80% | > 90% |
| **wait_time** | 获取连接等待时间 | < 10ms | > 100ms |
| **connection_errors** | 连接错误次数 | 0 | > 10/min |

### 2. 计算公式

```python
# 连接池使用率
pool_usage = (checked_out + overflow) / (pool_size + max_overflow)

# 空闲连接率
idle_rate = checked_in / pool_size

# 溢出率
overflow_rate = overflow / max_overflow
```

---

## 实现连接池监控

### 1. 基础监控：查看连接池状态

```python
# app/core/database.py
from sqlalchemy import create_engine

engine = create_engine(
    DATABASE_URL,
    pool_size=10,
    max_overflow=20,
)

def get_pool_status():
    """
    获取连接池状态
    """
    pool = engine.pool

    return {
        "pool_size": pool.size(),           # 连接池大小
        "checked_in": pool.checkedin(),     # 空闲连接数
        "checked_out": pool.checkedout(),   # 正在使用的连接数
        "overflow": pool.overflow(),        # 溢出连接数
        "status": pool.status(),            # 状态字符串
    }

# 使用示例
status = get_pool_status()
print(status)
# 输出：
# {
#   "pool_size": 10,
#   "checked_in": 5,
#   "checked_out": 3,
#   "overflow": 2,
#   "status": "Pool size: 10  Connections in pool: 5  Current Overflow: 2  Current Checked out connections: 3"
# }
```

**前端类比：**
```javascript
// 类似监控 HTTP 连接池状态
const agent = new http.Agent({ maxSockets: 10 });

console.log({
  maxSockets: agent.maxSockets,
  sockets: Object.keys(agent.sockets).length,
  requests: Object.keys(agent.requests).length,
});
```

---

### 2. 健康检查端点

```python
# app/api/health.py
from fastapi import APIRouter, HTTPException
from app.core.database import engine

router = APIRouter()

@router.get("/health/db")
async def check_db_health():
    """
    数据库健康检查
    """
    try:
        # 1. 检查连接池状态
        pool = engine.pool
        pool_size = pool.size()
        checked_in = pool.checkedin()
        checked_out = pool.checkedout()
        overflow = pool.overflow()

        # 2. 计算使用率
        pool_usage = (checked_out + overflow) / (pool_size + engine.pool._max_overflow)

        # 3. 测试数据库连接
        with engine.connect() as conn:
            conn.execute("SELECT 1")

        # 4. 判断健康状态
        if pool_usage > 0.9:
            status = "warning"
            message = "连接池使用率过高"
        elif checked_in == 0:
            status = "warning"
            message = "无空闲连接"
        else:
            status = "healthy"
            message = "数据库连接正常"

        return {
            "status": status,
            "message": message,
            "pool": {
                "size": pool_size,
                "checked_in": checked_in,
                "checked_out": checked_out,
                "overflow": overflow,
                "usage": f"{pool_usage * 100:.1f}%",
            }
        }

    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail=f"数据库连接失败: {str(e)}"
        )
```

**使用示例：**
```bash
# 健康检查
curl http://localhost:8000/health/db

# 输出：
# {
#   "status": "healthy",
#   "message": "数据库连接正常",
#   "pool": {
#     "size": 10,
#     "checked_in": 5,
#     "checked_out": 3,
#     "overflow": 2,
#     "usage": "50.0%"
#   }
# }
```

---

### 3. 连接池事件监听

```python
# app/core/database.py
from sqlalchemy import event
import time
import logging

logger = logging.getLogger(__name__)

# 监听连接创建
@event.listens_for(engine, "connect")
def receive_connect(dbapi_conn, connection_record):
    """
    连接创建时触发
    """
    connection_record.info['connect_time'] = time.time()
    logger.info(f"新连接创建: {id(dbapi_conn)}")

# 监听连接获取
@event.listens_for(engine, "checkout")
def receive_checkout(dbapi_conn, connection_record, connection_proxy):
    """
    连接被获取时触发
    """
    connection_record.info['checkout_time'] = time.time()

    # 计算等待时间
    wait_time = time.time() - connection_record.info.get('checkin_time', time.time())

    # 记录慢获取（超过 100ms）
    if wait_time > 0.1:
        logger.warning(f"连接获取慢: {wait_time:.2f}s")

# 监听连接归还
@event.listens_for(engine, "checkin")
def receive_checkin(dbapi_conn, connection_record):
    """
    连接被归还时触发
    """
    connection_record.info['checkin_time'] = time.time()

    # 计算连接使用时间
    checkout_time = connection_record.info.get('checkout_time', time.time())
    usage_time = time.time() - checkout_time

    # 记录长时间占用（超过 10 秒）
    if usage_time > 10:
        logger.warning(f"连接长时间占用: {usage_time:.2f}s")
```

---

### 4. 慢查询监控

```python
# app/core/database.py
from sqlalchemy import event
import time
import logging

logger = logging.getLogger(__name__)

# 监听查询开始
@event.listens_for(engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    """
    查询执行前触发
    """
    conn.info.setdefault('query_start_time', []).append(time.time())
    conn.info.setdefault('query_statement', []).append(statement)

# 监听查询结束
@event.listens_for(engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    """
    查询执行后触发
    """
    # 计算查询时间
    total_time = time.time() - conn.info['query_start_time'].pop()
    query_statement = conn.info['query_statement'].pop()

    # 记录慢查询（超过 100ms）
    if total_time > 0.1:
        logger.warning(
            f"慢查询 ({total_time:.2f}s): {query_statement[:200]}"
        )

    # 记录所有查询（调试模式）
    if logger.level == logging.DEBUG:
        logger.debug(
            f"查询 ({total_time:.3f}s): {query_statement[:100]}"
        )
```

---

### 5. Prometheus 监控集成

```python
# app/core/metrics.py
from prometheus_client import Counter, Gauge, Histogram
from sqlalchemy import event
from app.core.database import engine

# 定义指标
pool_size_gauge = Gauge('db_pool_size', '连接池大小')
pool_checked_in_gauge = Gauge('db_pool_checked_in', '空闲连接数')
pool_checked_out_gauge = Gauge('db_pool_checked_out', '正在使用的连接数')
pool_overflow_gauge = Gauge('db_pool_overflow', '溢出连接数')

connection_checkout_counter = Counter('db_connection_checkout_total', '连接获取次数')
connection_checkin_counter = Counter('db_connection_checkin_total', '连接归还次数')
connection_error_counter = Counter('db_connection_error_total', '连接错误次数')

query_duration_histogram = Histogram('db_query_duration_seconds', '查询耗时')

# 更新连接池指标
def update_pool_metrics():
    """
    更新连接池指标
    """
    pool = engine.pool
    pool_size_gauge.set(pool.size())
    pool_checked_in_gauge.set(pool.checkedin())
    pool_checked_out_gauge.set(pool.checkedout())
    pool_overflow_gauge.set(pool.overflow())

# 监听连接事件
@event.listens_for(engine, "checkout")
def receive_checkout(dbapi_conn, connection_record, connection_proxy):
    connection_checkout_counter.inc()
    update_pool_metrics()

@event.listens_for(engine, "checkin")
def receive_checkin(dbapi_conn, connection_record):
    connection_checkin_counter.inc()
    update_pool_metrics()

# 监听查询事件
@event.listens_for(engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total_time = time.time() - conn.info['query_start_time'].pop()
    query_duration_histogram.observe(total_time)
```

**暴露 Prometheus 指标：**

```python
# app/api/metrics.py
from fastapi import APIRouter
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from fastapi.responses import Response

router = APIRouter()

@router.get("/metrics")
async def metrics():
    """
    Prometheus 指标端点
    """
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )
```

---

## 连接池调优策略

### 1. 根据并发量调整 pool_size

**公式（经验值）：**
```python
# pool_size = 并发请求数 × 单个请求的数据库操作时间 / 请求总时间

# 示例：
# - 并发请求数：100
# - 单个请求的数据库操作时间：20ms
# - 请求总时间：100ms

pool_size = 100 × 0.02 / 0.1 = 20
```

**实际调优步骤：**

```python
# 1. 监控当前连接池使用率
pool_usage = (checked_out + overflow) / (pool_size + max_overflow)

# 2. 根据使用率调整
if pool_usage > 0.8:
    # 使用率过高，增大连接池
    new_pool_size = pool_size * 1.5
elif pool_usage < 0.3:
    # 使用率过低，减小连接池
    new_pool_size = pool_size * 0.7

# 3. 重启应用，应用新配置
```

**前端类比：**
```javascript
// 类似调整 HTTP 连接池大小
const agent = new http.Agent({
  maxSockets: 20,  // 根据并发量调整
});
```

---

### 2. 优化 pool_recycle 时间

**目标：** 在连接失效前主动回收

```python
# 查询数据库的连接超时时间
# MySQL:
SHOW VARIABLES LIKE 'wait_timeout';
# 输出：28800（8小时）

# PostgreSQL:
SHOW idle_in_transaction_session_timeout;
# 输出：0（无限制）

# 配置 pool_recycle
# 建议：小于数据库超时时间的 80%
pool_recycle = 28800 * 0.8 = 23040  # 约 6.4 小时

# 实际配置
engine = create_engine(
    DATABASE_URL,
    pool_recycle=3600,  # 1小时（更保守）
)
```

**日常生活类比：**
- **pool_recycle = 牛奶保质期**
- 牛奶开封后 7 天过期，建议 5 天内喝完（80%）
- 防止喝到变质的牛奶

---

### 3. 启用 pool_pre_ping

**目标：** 使用前检测连接是否有效

```python
# ✅ 推荐：生产环境必须开启
engine = create_engine(
    DATABASE_URL,
    pool_pre_ping=True,  # 使用前先 ping
)

# 性能开销分析
# - SELECT 1 耗时：<1ms
# - 如果连接刚被使用过（<1秒），跳过 ping
# - 实际开销：几乎可以忽略

# 收益分析
# - 避免使用失效连接（节省 65-160ms 重建连接）
# - 提升用户体验（无感知重连）
```

---

### 4. 调整 pool_timeout

**目标：** 平衡等待时间和用户体验

```python
# Web API：30秒（避免请求长时间挂起）
engine = create_engine(
    DATABASE_URL,
    pool_timeout=30,
)

# 批处理：300秒（可以等待更久）
engine = create_engine(
    DATABASE_URL,
    pool_timeout=300,
)

# 实时系统：5秒（快速失败）
engine = create_engine(
    DATABASE_URL,
    pool_timeout=5,
)
```

**前端类比：**
```javascript
// 类似 fetch 请求超时
const controller = new AbortController();
setTimeout(() => controller.abort(), 30000);

fetch(url, { signal: controller.signal });
```

---

### 5. 连接池预热

**目标：** 应用启动时预创建连接，避免首次请求慢

```python
# app/main.py
from fastapi import FastAPI
from app.core.database import engine

app = FastAPI()

@app.on_event("startup")
async def startup_event():
    """
    应用启动时预热连接池
    """
    # 预创建连接
    with engine.connect() as conn:
        conn.execute("SELECT 1")

    print("连接池预热完成")

    # 可选：预创建多个连接
    connections = []
    for _ in range(engine.pool.size()):
        conn = engine.connect()
        connections.append(conn)

    # 归还连接
    for conn in connections:
        conn.close()

    print(f"预创建 {len(connections)} 个连接")
```

---

## 常见性能问题诊断

### 问题1：连接池耗尽

**现象：**
```python
TimeoutError: QueuePool limit of size 10 overflow 20 reached,
connection timed out, timeout 30
```

**诊断步骤：**

```python
# 1. 查看连接池状态
status = get_pool_status()
print(status)
# 输出：
# {
#   "pool_size": 10,
#   "checked_in": 0,      # 无空闲连接
#   "checked_out": 10,    # 所有连接都在使用
#   "overflow": 20,       # 溢出连接已满
# }

# 2. 查看慢查询日志
# 是否有查询执行时间过长？

# 3. 查看连接泄漏
# 是否有连接没有正确关闭？

# 4. 查看并发量
# 是否并发请求数超过连接池容量？
```

**解决方案：**

```python
# 方案1：增大连接池
engine = create_engine(
    DATABASE_URL,
    pool_size=20,       # 增大常驻连接
    max_overflow=40,    # 增大临时连接
)

# 方案2：优化慢查询
# - 添加索引
# - 减少 JOIN
# - 只查询需要的字段

# 方案3：使用缓存
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_user_cached(user_id: int):
    # 缓存热点数据
    pass

# 方案4：异步处理
# 将耗时操作移到后台任务
from fastapi import BackgroundTasks

@app.post("/users")
async def create_user(user_data: UserCreate, background_tasks: BackgroundTasks):
    # 同步创建用户
    user = create_user_sync(user_data)

    # 异步发送邮件（不占用连接）
    background_tasks.add_task(send_welcome_email, user.email)

    return user
```

---

### 问题2：连接泄漏

**现象：**
```python
# 连接池中的空闲连接越来越少
# 最终导致连接池耗尽
```

**诊断步骤：**

```python
# 1. 监控空闲连接数
import time

while True:
    status = get_pool_status()
    print(f"空闲连接数: {status['checked_in']}")
    time.sleep(10)

# 输出：
# 空闲连接数: 5
# 空闲连接数: 4
# 空闲连接数: 3
# 空闲连接数: 2  # 持续下降 ❌
# 空闲连接数: 1
# 空闲连接数: 0

# 2. 检查代码中是否有未关闭的 Session
# 搜索代码中的 SessionLocal() 调用
# 确保每个调用都有对应的 db.close()
```

**解决方案：**

```python
# ❌ 错误：没有使用 try-finally
def get_db_wrong():
    db = SessionLocal()
    yield db
    db.close()  # 如果发生异常，这行不会执行

# ✅ 正确：使用 try-finally
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()  # 无论是否异常，都会执行

# ✅ 更好：使用上下文管理器
from contextlib import contextmanager

@contextmanager
def get_db_context():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# 使用
with get_db_context() as db:
    user = db.query(User).first()
```

---

### 问题3：慢查询导致连接占用时间长

**现象：**
```python
# 连接池使用率高
# 数据库 CPU 100%
# 请求响应时间长
```

**诊断步骤：**

```python
# 1. 查看慢查询日志
# 找出执行时间超过 100ms 的查询

# 2. 分析查询计划
# PostgreSQL:
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';

# MySQL:
EXPLAIN SELECT * FROM users WHERE email = 'test@example.com';

# 3. 检查索引
# 是否缺少索引？
# 是否使用了索引？
```

**解决方案：**

```python
# 方案1：添加索引
from sqlalchemy import Index

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True)
    email = Column(String, unique=True)

    # 添加索引
    __table_args__ = (
        Index('idx_user_email', 'email'),
    )

# 方案2：优化查询
# ❌ 错误：查询所有字段
users = db.query(User).all()

# ✅ 正确：只查询需要的字段
users = db.query(User.id, User.name).all()

# 方案3：使用缓存
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_user_by_email(email: str):
    db = SessionLocal()
    user = db.query(User).filter(User.email == email).first()
    db.close()
    return user

# 方案4：分页查询
# ❌ 错误：一次查询所有数据
users = db.query(User).all()  # 可能有 100 万条数据

# ✅ 正确：分页查询
users = db.query(User).limit(100).offset(0).all()
```

---

## 生产环境最佳实践

### 1. 完整的监控配置

```python
# app/core/database.py
import os
import time
import logging
from sqlalchemy import create_engine, event
from sqlalchemy.orm import sessionmaker

logger = logging.getLogger(__name__)

# 1. 创建引擎
DATABASE_URL = os.getenv("DATABASE_URL")
POOL_SIZE = int(os.getenv("POOL_SIZE", "20"))
MAX_OVERFLOW = int(os.getenv("MAX_OVERFLOW", "30"))

engine = create_engine(
    DATABASE_URL,
    pool_size=POOL_SIZE,
    max_overflow=MAX_OVERFLOW,
    pool_timeout=30,
    pool_recycle=3600,
    pool_pre_ping=True,
    echo=False,
)

# 2. 监听连接事件
@event.listens_for(engine, "connect")
def receive_connect(dbapi_conn, connection_record):
    connection_record.info['connect_time'] = time.time()
    logger.info(f"新连接创建: {id(dbapi_conn)}")

@event.listens_for(engine, "checkout")
def receive_checkout(dbapi_conn, connection_record, connection_proxy):
    connection_record.info['checkout_time'] = time.time()

    # 记录慢获取
    wait_time = time.time() - connection_record.info.get('checkin_time', time.time())
    if wait_time > 0.1:
        logger.warning(f"连接获取慢: {wait_time:.2f}s")

@event.listens_for(engine, "checkin")
def receive_checkin(dbapi_conn, connection_record):
    connection_record.info['checkin_time'] = time.time()

    # 记录长时间占用
    checkout_time = connection_record.info.get('checkout_time', time.time())
    usage_time = time.time() - checkout_time
    if usage_time > 10:
        logger.warning(f"连接长时间占用: {usage_time:.2f}s")

# 3. 监听查询事件
@event.listens_for(engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total_time = time.time() - conn.info['query_start_time'].pop()

    # 记录慢查询
    if total_time > 0.1:
        logger.warning(f"慢查询 ({total_time:.2f}s): {statement[:200]}")

# 4. 创建 SessionLocal
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# 5. 依赖注入函数
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

---

### 2. 环境变量配置

```bash
# .env.production
DATABASE_URL=postgresql://user:password@prod-db:5432/prod_db
POOL_SIZE=20
MAX_OVERFLOW=30
POOL_TIMEOUT=30
POOL_RECYCLE=3600
```

---

### 3. 定期健康检查

```python
# app/tasks/health_check.py
import asyncio
from app.core.database import engine, get_pool_status
import logging

logger = logging.getLogger(__name__)

async def periodic_health_check():
    """
    定期健康检查（每 60 秒）
    """
    while True:
        try:
            # 1. 检查连接池状态
            status = get_pool_status()
            pool_usage = (status['checked_out'] + status['overflow']) / (
                status['pool_size'] + engine.pool._max_overflow
            )

            # 2. 记录状态
            logger.info(f"连接池状态: {status}")

            # 3. 告警
            if pool_usage > 0.9:
                logger.error(f"连接池使用率过高: {pool_usage * 100:.1f}%")

            if status['checked_in'] == 0:
                logger.error("无空闲连接")

        except Exception as e:
            logger.error(f"健康检查失败: {e}")

        await asyncio.sleep(60)

# 在应用启动时启动健康检查
# app/main.py
@app.on_event("startup")
async def startup_event():
    asyncio.create_task(periodic_health_check())
```

---

## 总结

### 连接池监控与调优的核心要点

| 要点 | 说明 | 推荐配置 |
|------|------|---------|
| **监控指标** | pool_usage、wait_time、慢查询 | Prometheus + Grafana |
| **健康检查** | 定期检查连接池状态 | 每 60 秒 |
| **pool_size** | 根据并发量调整 | 并发数 × 0.5 |
| **pool_recycle** | 小于数据库超时时间 | 3600 秒（1小时） |
| **pool_pre_ping** | 生产环境必须开启 | True |
| **慢查询优化** | 添加索引、优化 SQL | < 100ms |

### 调优流程

```
1. 监控连接池状态
   ↓
2. 发现性能瓶颈
   ↓
3. 分析根本原因
   ↓
4. 应用优化方案
   ↓
5. 验证优化效果
   ↓
6. 持续监控
```

**记住：** 连接池监控和调优是持续的过程，需要根据实际负载动态调整配置。
