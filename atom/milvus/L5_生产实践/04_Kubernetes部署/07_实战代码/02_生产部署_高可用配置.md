# å®æˆ˜ä»£ç  - åœºæ™¯2ï¼šç”Ÿäº§éƒ¨ç½²ï¼ˆé«˜å¯ç”¨é…ç½®ï¼‰

## åœºæ™¯æè¿°

**ç›®æ ‡ï¼š**åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é«˜å¯ç”¨Milvusé›†ç¾¤ï¼Œæ”¯æŒå¤§è§„æ¨¡å‘é‡æ£€ç´¢

**ç‰¹ç‚¹ï¼š**
- é›†ç¾¤æ¨¡å¼ï¼Œå¤šå‰¯æœ¬éƒ¨ç½²
- å¯ç”¨æŒä¹…åŒ–å­˜å‚¨
- ä½¿ç”¨å¤–éƒ¨ä¾èµ–ï¼ˆetcdã€S3ã€Pulsarï¼‰
- é…ç½®Podåäº²å’Œæ€§
- é…ç½®èµ„æºé™åˆ¶å’Œå¥åº·æ£€æŸ¥
- é…ç½®ç›‘æ§å’Œå‘Šè­¦

**é€‚ç”¨åœºæ™¯ï¼š**
- ç”Ÿäº§ç¯å¢ƒ
- å¤§è§„æ¨¡RAGç³»ç»Ÿ
- é«˜å¯ç”¨è¦æ±‚ï¼ˆ99.9%+ï¼‰

---

## å®Œæ•´éƒ¨ç½²è„šæœ¬

### 1. ç¯å¢ƒå‡†å¤‡

```bash
#!/bin/bash
# deploy-prod.sh - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬

set -e

echo "=== Milvusç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬ ==="

# æ£€æŸ¥ç¯å¢ƒ
if ! command -v kubectl &> /dev/null; then
    echo "âŒ kubectlæœªå®‰è£…"
    exit 1
fi

if ! command -v helm &> /dev/null; then
    echo "âŒ Helmæœªå®‰è£…"
    exit 1
fi

# æ£€æŸ¥é›†ç¾¤è¿æ¥
if ! kubectl cluster-info &> /dev/null; then
    echo "âŒ æ— æ³•è¿æ¥åˆ°Kubernetesé›†ç¾¤"
    exit 1
fi

echo "âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡"

# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace milvus-prod --dry-run=client -o yaml | kubectl apply -f -

# æ·»åŠ Helmä»“åº“
helm repo add milvus https://zilliztech.github.io/milvus-helm/
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

echo "âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ"
```

### 2. éƒ¨ç½²å¤–éƒ¨ä¾èµ–

```bash
#!/bin/bash
# deploy-dependencies.sh - éƒ¨ç½²å¤–éƒ¨ä¾èµ–

echo "=== éƒ¨ç½²å¤–éƒ¨ä¾èµ– ==="

# éƒ¨ç½²etcdé›†ç¾¤
echo "ğŸ“¦ éƒ¨ç½²etcdé›†ç¾¤..."
helm install etcd bitnami/etcd \
  --set replicaCount=3 \
  --set auth.rbac.create=false \
  --set persistence.enabled=true \
  --set persistence.storageClass=fast-ssd \
  --set persistence.size=20Gi \
  --set resources.requests.cpu=1 \
  --set resources.requests.memory=2Gi \
  --set resources.limits.cpu=2 \
  --set resources.limits.memory=4Gi \
  -n milvus-prod \
  --wait

# éƒ¨ç½²MinIOé›†ç¾¤
echo "ğŸ“¦ éƒ¨ç½²MinIOé›†ç¾¤..."
helm install minio bitnami/minio \
  --set mode=distributed \
  --set statefulset.replicaCount=4 \
  --set persistence.enabled=true \
  --set persistence.storageClass=standard \
  --set persistence.size=500Gi \
  --set resources.requests.cpu=1 \
  --set resources.requests.memory=4Gi \
  --set resources.limits.cpu=2 \
  --set resources.limits.memory=8Gi \
  --set auth.rootUser=minioadmin \
  --set auth.rootPassword=minioadmin123 \
  -n milvus-prod \
  --wait

# åˆ›å»ºMinIO bucket
echo "ğŸ“¦ åˆ›å»ºMinIO bucket..."
kubectl run minio-client --rm -i --tty \
  --image=minio/mc \
  --restart=Never \
  -n milvus-prod \
  -- /bin/sh -c "
    mc alias set myminio http://minio.milvus-prod.svc.cluster.local:9000 minioadmin minioadmin123
    mc mb myminio/milvus-bucket
    echo 'Bucket created successfully'
  "

# éƒ¨ç½²Pulsaré›†ç¾¤
echo "ğŸ“¦ éƒ¨ç½²Pulsaré›†ç¾¤..."
helm install pulsar apache/pulsar \
  --set components.broker=true \
  --set components.bookkeeper=true \
  --set components.zookeeper=true \
  --set broker.replicaCount=3 \
  --set bookkeeper.replicaCount=3 \
  --set zookeeper.replicaCount=3 \
  --set persistence.enabled=true \
  --set persistence.storageClass=fast-ssd \
  -n milvus-prod \
  --wait

echo "âœ… å¤–éƒ¨ä¾èµ–éƒ¨ç½²å®Œæˆ"
```

### 3. ç”Ÿäº§é…ç½®æ–‡ä»¶

```yaml
# prod-values.yaml - ç”Ÿäº§ç¯å¢ƒé…ç½®

# é›†ç¾¤æ¨¡å¼
cluster:
  enabled: true

# é•œåƒé…ç½®
image:
  all:
    repository: milvusdb/milvus
    tag: v2.3.0
    pullPolicy: IfNotPresent

# Proxyé…ç½®
proxy:
  replicas: 3  # 3ä¸ªå‰¯æœ¬ï¼Œæ”¯æŒæ•…éšœè½¬ç§»
  resources:
    requests:
      cpu: 2
      memory: 4Gi
    limits:
      cpu: 4
      memory: 8Gi

  # Podåäº²å’Œæ€§ï¼ˆç¡®ä¿åˆ†å¸ƒåœ¨ä¸åŒèŠ‚ç‚¹ï¼‰
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/component
            operator: In
            values:
            - proxy
        topologyKey: kubernetes.io/hostname

  # å¥åº·æ£€æŸ¥
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  readinessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

# QueryNodeé…ç½®
queryNode:
  replicas: 5  # 5ä¸ªå‰¯æœ¬ï¼Œæ ¹æ®è´Ÿè½½è°ƒæ•´
  resources:
    requests:
      cpu: 4
      memory: 16Gi
    limits:
      cpu: 8
      memory: 32Gi

  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/component
            operator: In
            values:
            - querynode
        topologyKey: kubernetes.io/hostname

  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10

  readinessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 5

# DataNodeé…ç½®
dataNode:
  replicas: 3
  resources:
    requests:
      cpu: 2
      memory: 8Gi
    limits:
      cpu: 4
      memory: 16Gi

  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/component
            operator: In
            values:
            - datanode
        topologyKey: kubernetes.io/hostname

# IndexNodeé…ç½®
indexNode:
  replicas: 2
  resources:
    requests:
      cpu: 4
      memory: 8Gi
    limits:
      cpu: 8
      memory: 16Gi

# Coordinatoré…ç½®
rootCoord:
  replicas: 1
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi

dataCoord:
  replicas: 1
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi

queryCoord:
  replicas: 1
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi

indexCoord:
  replicas: 1
  resources:
    requests:
      cpu: 1
      memory: 2Gi
    limits:
      cpu: 2
      memory: 4Gi

# å¤–éƒ¨etcdé…ç½®
externalEtcd:
  enabled: true
  endpoints:
    - etcd-0.etcd-headless.milvus-prod.svc.cluster.local:2379
    - etcd-1.etcd-headless.milvus-prod.svc.cluster.local:2379
    - etcd-2.etcd-headless.milvus-prod.svc.cluster.local:2379

# å¤–éƒ¨S3é…ç½®ï¼ˆMinIOï¼‰
externalS3:
  enabled: true
  host: minio.milvus-prod.svc.cluster.local
  port: 9000
  useSSL: false
  bucketName: milvus-bucket
  accessKey: minioadmin
  secretKey: minioadmin123
  useIAM: false
  cloudProvider: aws

# å¤–éƒ¨Pulsaré…ç½®
externalPulsar:
  enabled: true
  host: pulsar-broker.milvus-prod.svc.cluster.local
  port: 6650

# Serviceé…ç½®
service:
  type: LoadBalancer
  port: 19530
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"

# ç›‘æ§é…ç½®
metrics:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s

# æ—¥å¿—é…ç½®
log:
  level: info
  format: json

# Milvusé…ç½®
config:
  common:
    retentionDuration: "432000"  # 5å¤©
  dataCoord:
    segment:
      maxSize: "1024"  # 1GB
    enableCompaction: true
  queryNode:
    gracefulTime: "5000"
    gracefulStopTimeout: "30"
  proxy:
    maxTaskNum: "1024"
```

### 4. éƒ¨ç½²Milvus

```bash
#!/bin/bash
# ç»§ç»­ deploy-prod.sh

echo "ğŸš€ å¼€å§‹éƒ¨ç½²Milvus..."

# éƒ¨ç½²Milvus
helm install milvus-prod milvus/milvus \
  -f prod-values.yaml \
  -n milvus-prod \
  --wait \
  --timeout 15m

echo "âœ… Milvuséƒ¨ç½²å®Œæˆ"

# ç­‰å¾…æ‰€æœ‰Podå°±ç»ª
echo "â³ ç­‰å¾…æ‰€æœ‰Podå°±ç»ª..."
kubectl wait --for=condition=ready pod \
  -l app.kubernetes.io/instance=milvus-prod \
  -n milvus-prod \
  --timeout=600s

echo "âœ… æ‰€æœ‰Podå·²å°±ç»ª"

# æ˜¾ç¤ºéƒ¨ç½²ä¿¡æ¯
echo ""
echo "=== éƒ¨ç½²ä¿¡æ¯ ==="
kubectl get pods -n milvus-prod -o wide
echo ""
kubectl get svc -n milvus-prod

# è·å–LoadBalanceråœ°å€
EXTERNAL_IP=$(kubectl get svc milvus-prod -n milvus-prod -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
if [ -z "$EXTERNAL_IP" ]; then
    EXTERNAL_IP=$(kubectl get svc milvus-prod -n milvus-prod -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
fi

echo ""
echo "=== è®¿é—®ä¿¡æ¯ ==="
echo "Milvusåœ°å€: ${EXTERNAL_IP}:19530"
echo ""
echo "ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿æ¥ï¼š"
echo "  from pymilvus import connections"
echo "  connections.connect(host='${EXTERNAL_IP}', port='19530')"
```

---

## é«˜å¯ç”¨éªŒè¯

### éªŒè¯è„šæœ¬

```python
"""
verify_ha_deployment.py - éªŒè¯é«˜å¯ç”¨éƒ¨ç½²

åŠŸèƒ½ï¼š
1. è¿æ¥åˆ°ç”Ÿäº§é›†ç¾¤
2. åˆ›å»ºæµ‹è¯•Collection
3. æ’å…¥å¤§é‡æ•°æ®
4. æ‰§è¡Œå¹¶å‘æ£€ç´¢æµ‹è¯•
5. æ¨¡æ‹Ÿæ•…éšœåœºæ™¯
6. éªŒè¯æ•…éšœæ¢å¤
"""

from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility
import numpy as np
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===== é…ç½® =====
HOST = "your-loadbalancer-ip"  # æ›¿æ¢ä¸ºå®é™…çš„LoadBalancer IP
PORT = "19530"
COLLECTION_NAME = "ha_test_collection"
DIM = 128
NUM_ENTITIES = 100000  # 10ä¸‡æ¡æ•°æ®
NUM_QUERIES = 1000     # 1000æ¬¡æŸ¥è¯¢
CONCURRENT_QUERIES = 10  # 10ä¸ªå¹¶å‘æŸ¥è¯¢

# ===== 1. è¿æ¥é›†ç¾¤ =====
print("=== æ­¥éª¤1ï¼šè¿æ¥åˆ°ç”Ÿäº§é›†ç¾¤ ===")

try:
    connections.connect(
        alias="default",
        host=HOST,
        port=PORT,
        timeout=10
    )
    print(f"âœ… æˆåŠŸè¿æ¥åˆ°Milvus: {HOST}:{PORT}")
except Exception as e:
    print(f"âŒ è¿æ¥å¤±è´¥: {e}")
    exit(1)

# ===== 2. åˆ›å»ºCollection =====
print("\n=== æ­¥éª¤2ï¼šåˆ›å»ºæµ‹è¯•Collection ===")

if utility.has_collection(COLLECTION_NAME):
    utility.drop_collection(COLLECTION_NAME)
    print(f"ğŸ—‘ï¸  åˆ é™¤å·²å­˜åœ¨çš„Collection")

fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=DIM),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=512)
]
schema = CollectionSchema(fields=fields, description="é«˜å¯ç”¨æµ‹è¯•Collection")
collection = Collection(name=COLLECTION_NAME, schema=schema)
print(f"âœ… åˆ›å»ºCollection: {COLLECTION_NAME}")

# ===== 3. åˆ›å»ºç´¢å¼• =====
print("\n=== æ­¥éª¤3ï¼šåˆ›å»ºç´¢å¼• ===")

index_params = {
    "index_type": "IVF_FLAT",
    "metric_type": "L2",
    "params": {"nlist": 1024}
}
collection.create_index(field_name="embedding", index_params=index_params)
print("âœ… åˆ›å»ºç´¢å¼•å®Œæˆ")

# ===== 4. æ’å…¥å¤§é‡æ•°æ® =====
print(f"\n=== æ­¥éª¤4ï¼šæ’å…¥{NUM_ENTITIES}æ¡æ•°æ® ===")

BATCH_SIZE = 10000
start_time = time.time()

for i in range(0, NUM_ENTITIES, BATCH_SIZE):
    batch_size = min(BATCH_SIZE, NUM_ENTITIES - i)
    ids = list(range(i, i + batch_size))
    embeddings = np.random.random((batch_size, DIM)).tolist()
    texts = [f"æµ‹è¯•æ–‡æœ¬_{j}" for j in range(i, i + batch_size)]

    entities = [ids, embeddings, texts]
    collection.insert(entities)

    if (i + batch_size) % 10000 == 0:
        print(f"  å·²æ’å…¥ {i + batch_size}/{NUM_ENTITIES} æ¡æ•°æ®")

collection.flush()
insert_time = time.time() - start_time
print(f"âœ… æ’å…¥å®Œæˆï¼Œè€—æ—¶: {insert_time:.2f}ç§’")
print(f"   ååé‡: {NUM_ENTITIES/insert_time:.0f} æ¡/ç§’")

# ===== 5. åŠ è½½Collection =====
print("\n=== æ­¥éª¤5ï¼šåŠ è½½Collection ===")

collection.load()
print("âœ… CollectionåŠ è½½å®Œæˆ")
time.sleep(5)

# ===== 6. å¹¶å‘æ£€ç´¢æµ‹è¯• =====
print(f"\n=== æ­¥éª¤6ï¼šå¹¶å‘æ£€ç´¢æµ‹è¯•ï¼ˆ{NUM_QUERIES}æ¬¡æŸ¥è¯¢ï¼Œ{CONCURRENT_QUERIES}å¹¶å‘ï¼‰ ===")

search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
query_vectors = [np.random.random(DIM).tolist() for _ in range(NUM_QUERIES)]

def search_once(query_vector):
    """æ‰§è¡Œä¸€æ¬¡æœç´¢"""
    try:
        start = time.time()
        results = collection.search(
            data=[query_vector],
            anns_field="embedding",
            param=search_params,
            limit=10
        )
        latency = (time.time() - start) * 1000
        return {"success": True, "latency": latency}
    except Exception as e:
        return {"success": False, "error": str(e)}

# æ‰§è¡Œå¹¶å‘æœç´¢
start_time = time.time()
latencies = []
errors = 0

with ThreadPoolExecutor(max_workers=CONCURRENT_QUERIES) as executor:
    futures = [executor.submit(search_once, qv) for qv in query_vectors]

    for i, future in enumerate(as_completed(futures)):
        result = future.result()
        if result["success"]:
            latencies.append(result["latency"])
        else:
            errors += 1

        if (i + 1) % 100 == 0:
            print(f"  å·²å®Œæˆ {i + 1}/{NUM_QUERIES} æ¬¡æŸ¥è¯¢")

total_time = time.time() - start_time

# ç»Ÿè®¡ç»“æœ
latencies.sort()
p50 = latencies[int(len(latencies) * 0.5)]
p95 = latencies[int(len(latencies) * 0.95)]
p99 = latencies[int(len(latencies) * 0.99)]
avg_latency = sum(latencies) / len(latencies)
qps = NUM_QUERIES / total_time

print(f"\nâœ… å¹¶å‘æ£€ç´¢æµ‹è¯•å®Œæˆ")
print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
print(f"   QPS: {qps:.0f}")
print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
print(f"   P50å»¶è¿Ÿ: {p50:.2f}ms")
print(f"   P95å»¶è¿Ÿ: {p95:.2f}ms")
print(f"   P99å»¶è¿Ÿ: {p99:.2f}ms")
print(f"   é”™è¯¯æ•°: {errors}")

# ===== 7. æ•…éšœæ¢å¤æµ‹è¯• =====
print("\n=== æ­¥éª¤7ï¼šæ•…éšœæ¢å¤æµ‹è¯• ===")
print("è¯·åœ¨å¦ä¸€ä¸ªç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¨¡æ‹Ÿæ•…éšœï¼š")
print("  kubectl delete pod -l app.kubernetes.io/component=querynode -n milvus-prod --force --grace-period=0")
print("\nç­‰å¾…30ç§’åç»§ç»­æµ‹è¯•...")
time.sleep(30)

# æ•…éšœåç»§ç»­æŸ¥è¯¢
print("æ‰§è¡Œæ•…éšœåæŸ¥è¯¢æµ‹è¯•...")
post_failure_latencies = []
post_failure_errors = 0

for i in range(100):
    result = search_once(query_vectors[i])
    if result["success"]:
        post_failure_latencies.append(result["latency"])
    else:
        post_failure_errors += 1

if post_failure_latencies:
    avg_post_failure = sum(post_failure_latencies) / len(post_failure_latencies)
    print(f"âœ… æ•…éšœåæŸ¥è¯¢æˆåŠŸ")
    print(f"   å¹³å‡å»¶è¿Ÿ: {avg_post_failure:.2f}ms")
    print(f"   é”™è¯¯æ•°: {post_failure_errors}/100")
else:
    print(f"âŒ æ•…éšœåæŸ¥è¯¢å¤±è´¥")

# ===== 8. æ¸…ç† =====
print("\n=== æ­¥éª¤8ï¼šæ¸…ç†æµ‹è¯•æ•°æ® ===")

collection.release()
utility.drop_collection(COLLECTION_NAME)
connections.disconnect("default")

print("âœ… æ¸…ç†å®Œæˆ")
print("\n=== é«˜å¯ç”¨éªŒè¯å®Œæˆ ===")
```

---

## ç›‘æ§é…ç½®

### Prometheusé…ç½®

```yaml
# prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: milvus-prod
data:
  prometheus.yml: |
    global:
      scrape_interval: 30s
      evaluation_interval: 30s

    scrape_configs:
    - job_name: 'milvus'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - milvus-prod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
        action: keep
        regex: milvus-prod
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
```

### Grafana Dashboard

```bash
# å¯¼å…¥Milvuså®˜æ–¹Dashboard
kubectl apply -f https://raw.githubusercontent.com/milvus-io/milvus/master/deployments/monitor/grafana/dashboards/milvus-dashboard.json
```

---

## å¤‡ä»½ç­–ç•¥

### è‡ªåŠ¨å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup-milvus.sh - è‡ªåŠ¨å¤‡ä»½è„šæœ¬

BACKUP_DIR="/backups/milvus"
DATE=$(date +%Y%m%d_%H%M%S)
NAMESPACE="milvus-prod"

echo "=== Milvuså¤‡ä»½è„šæœ¬ ==="
echo "å¤‡ä»½æ—¶é—´: $DATE"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR/$DATE

# å¤‡ä»½etcdæ•°æ®
echo "ğŸ“¦ å¤‡ä»½etcdæ•°æ®..."
kubectl exec -n $NAMESPACE etcd-0 -- etcdctl snapshot save /tmp/etcd-backup.db
kubectl cp $NAMESPACE/etcd-0:/tmp/etcd-backup.db $BACKUP_DIR/$DATE/etcd-backup.db

# å¤‡ä»½MinIOæ•°æ®ï¼ˆä½¿ç”¨mcå·¥å…·ï¼‰
echo "ğŸ“¦ å¤‡ä»½MinIOæ•°æ®..."
kubectl run minio-backup --rm -i --tty \
  --image=minio/mc \
  --restart=Never \
  -n $NAMESPACE \
  -- /bin/sh -c "
    mc alias set myminio http://minio.$NAMESPACE.svc.cluster.local:9000 minioadmin minioadmin123
    mc mirror myminio/milvus-bucket /backup
  "

# å‹ç¼©å¤‡ä»½
echo "ğŸ“¦ å‹ç¼©å¤‡ä»½..."
tar -czf $BACKUP_DIR/milvus-backup-$DATE.tar.gz -C $BACKUP_DIR $DATE

# åˆ é™¤ä¸´æ—¶ç›®å½•
rm -rf $BACKUP_DIR/$DATE

# ä¿ç•™æœ€è¿‘7å¤©çš„å¤‡ä»½
find $BACKUP_DIR -name "milvus-backup-*.tar.gz" -mtime +7 -delete

echo "âœ… å¤‡ä»½å®Œæˆ: $BACKUP_DIR/milvus-backup-$DATE.tar.gz"
```

### é…ç½®å®šæ—¶å¤‡ä»½

```yaml
# backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: milvus-backup
  namespace: milvus-prod
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: bitnami/kubectl:latest
            command:
            - /bin/bash
            - -c
            - |
              # å¤‡ä»½è„šæœ¬å†…å®¹
              ...
          restartPolicy: OnFailure
```

---

## æ€»ç»“

### ç”Ÿäº§éƒ¨ç½²ç‰¹ç‚¹

| ç‰¹æ€§ | é…ç½® | ä»·å€¼ |
|------|------|------|
| **é«˜å¯ç”¨** | å¤šå‰¯æœ¬ + åäº²å’Œæ€§ | 99.9%å¯ç”¨æ€§ |
| **æŒä¹…åŒ–** | å¤–éƒ¨å­˜å‚¨ | æ•°æ®å®‰å…¨ |
| **ç›‘æ§** | Prometheus + Grafana | å¯è§‚æµ‹æ€§ |
| **å¤‡ä»½** | å®šæ—¶å¤‡ä»½ | ç¾éš¾æ¢å¤ |
| **æ‰©å±•æ€§** | LoadBalancer | æ”¯æŒå¤§è§„æ¨¡è®¿é—® |

### æ€§èƒ½æŒ‡æ ‡

- **QPS**: 5000-10000ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰
- **P99å»¶è¿Ÿ**: <100ms
- **å¯ç”¨æ€§**: 99.9%+
- **æ•°æ®æŒä¹…æ€§**: 99.999999999%ï¼ˆä½¿ç”¨äº‘å­˜å‚¨ï¼‰

### ä¸‹ä¸€æ­¥

å®Œæˆç”Ÿäº§éƒ¨ç½²åï¼Œç»§ç»­å­¦ä¹ ï¼š
- **åœºæ™¯3ï¼šOperatoréƒ¨ç½²** - è‡ªåŠ¨åŒ–è¿ç»´
- **åœºæ™¯4ï¼šè‡ªåŠ¨æ‰©ç¼©å®¹** - å¼¹æ€§ä¼¸ç¼©
- **åœºæ™¯5ï¼šç°åº¦å‘å¸ƒ** - é›¶åœæœºå‡çº§
