# 核心概念3：生产环境监控

## 概述

生产环境需要全链路可观测性，包括：
- **Tracing**：追踪请求的完整路径
- **Metrics**：收集性能指标
- **Logging**：记录关键事件

**两大方案**：
1. **LangSmith**：LangChain官方平台，零代码集成
2. **OpenTelemetry**：开源标准，灵活强大

---

## 生产监控三大支柱

### 1. Tracing（追踪）

**目的**：追踪请求的完整执行路径

**关键信息**：
- 请求ID
- 执行时间线
- 每个步骤的输入输出
- 父子关系

**类比**：就像快递追踪，知道包裹经过了哪些站点。

---

### 2. Metrics（指标）

**目的**：收集量化的性能数据

**关键指标**：
- 延迟（Latency）
- 吞吐量（Throughput）
- 错误率（Error Rate）
- Token使用（Token Usage）
- 成本（Cost）

**类比**：就像汽车仪表盘，显示速度、油耗、温度等。

---

### 3. Logging（日志）

**目的**：记录关键事件和错误

**关键信息**：
- 时间戳
- 事件类型
- 错误信息
- 上下文数据

**类比**：就像飞机黑匣子，记录所有重要事件。

---

## LangSmith 平台

### 什么是LangSmith？

LangSmith是LangChain官方的可观测性平台，提供：
- 自动追踪所有链执行
- 可视化仪表板
- 性能分析
- 成本追踪
- 调试工具

**官网**：https://smith.langchain.com/

---

### 快速开始

#### 步骤1：注册并获取API密钥

1. 访问 https://smith.langchain.com/
2. 注册账号
3. 创建API密钥

---

#### 步骤2：配置环境变量

```bash
# .env 文件
OPENAI_API_KEY=your_openai_key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_key
LANGCHAIN_PROJECT=my_project  # 可选，项目名称
```

---

#### 步骤3：运行代码（无需修改）

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

load_dotenv()

# 正常使用，自动追踪
prompt = ChatPromptTemplate.from_template("回答问题: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model | StrOutputParser()

result = chain.invoke({"question": "什么是LCEL?"})
print(result)
```

**就这么简单！** 所有执行都会自动上传到LangSmith。

---

#### 步骤4：查看追踪

访问 https://smith.langchain.com/ 查看：
- 完整的执行流程图
- 每个步骤的输入输出
- 延迟和Token使用
- 成本统计

---

### LangSmith 功能详解

#### 1. 追踪和调试

**查看完整执行流程**：
- 链的结构
- 每个组件的执行顺序
- 输入输出数据
- 错误堆栈

**示例**：
```
RunnableSequence
├─ ChatPromptTemplate (2ms)
│  Input: {"question": "什么是LCEL?"}
│  Output: [HumanMessage(content="回答问题: 什么是LCEL?")]
├─ ChatOpenAI (1234ms)
│  Input: [HumanMessage(...)]
│  Output: AIMessage(content="LCEL是...")
│  Tokens: {prompt: 15, completion: 120, total: 135}
└─ StrOutputParser (1ms)
   Input: AIMessage(...)
   Output: "LCEL是..."
```

---

#### 2. 性能分析

**关键指标**：
- **总延迟**：整个链的执行时间
- **组件延迟**：每个组件的耗时
- **Token使用**：prompt tokens + completion tokens
- **成本**：基于Token使用计算

**示例**：
```
Total Latency: 1.237s
├─ Prompt: 0.002s (0.2%)
├─ LLM: 1.234s (99.7%)
└─ Parser: 0.001s (0.1%)

Tokens: 135 (prompt: 15, completion: 120)
Cost: $0.00027
```

---

#### 3. 成本追踪

LangSmith自动计算每次调用的成本：

| 模型 | Prompt价格 | Completion价格 |
|------|------------|----------------|
| gpt-4o | $2.50/1M | $10.00/1M |
| gpt-4o-mini | $0.15/1M | $0.60/1M |
| gpt-3.5-turbo | $0.50/1M | $1.50/1M |

**成本报告**：
- 每天/每周/每月的总成本
- 按项目分组
- 按模型分组
- 成本趋势图

---

#### 4. 错误监控

自动捕获和分类错误：
- Rate limit errors
- Timeout errors
- Authentication errors
- Model errors

**告警**：可以设置告警规则，错误率超过阈值时通知。

---

### LangSmith 高级功能

#### 1. 数据集和评估

创建测试数据集，评估链的性能：

```python
from langsmith import Client

client = Client()

# 创建数据集
dataset = client.create_dataset("my_dataset")

# 添加示例
client.create_example(
    dataset_id=dataset.id,
    inputs={"question": "什么是LCEL?"},
    outputs={"answer": "LCEL是..."}
)

# 运行评估
from langchain.smith import RunEvalConfig

eval_config = RunEvalConfig(
    evaluators=["qa"],  # 使用QA评估器
)

results = client.run_on_dataset(
    dataset_name="my_dataset",
    llm_or_chain_factory=lambda: chain,
    evaluation=eval_config
)
```

---

#### 2. 标签和元数据

为追踪添加标签和元数据：

```python
result = chain.invoke(
    {"question": "什么是LCEL?"},
    config={
        "tags": ["production", "user_123"],
        "metadata": {
            "user_id": "123",
            "session_id": "abc",
            "version": "v1.0"
        }
    }
)
```

在LangSmith中可以按标签过滤和搜索。

---

#### 3. 采样

只追踪部分请求，减少成本：

```python
import random
import os

# 10%采样
if random.random() < 0.1:
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
else:
    os.environ["LANGCHAIN_TRACING_V2"] = "false"

result = chain.invoke({"question": "什么是LCEL?"})
```

---

## OpenTelemetry 集成

### 什么是OpenTelemetry？

OpenTelemetry（简称OTel）是云原生计算基金会（CNCF）的开源可观测性标准，支持：
- 分布式追踪
- 指标收集
- 日志记录
- 导出到多种后端（Prometheus, Jaeger, Zipkin, Uptrace等）

**官网**：https://opentelemetry.io/

---

### 为什么选择OpenTelemetry？

| 特性 | LangSmith | OpenTelemetry |
|------|-----------|---------------|
| **集成难度** | 极简（环境变量） | 中等（需要配置） |
| **成本** | 付费（有免费额度） | 免费（开源） |
| **灵活性** | 固定功能 | 高度可定制 |
| **数据控制** | 数据在LangSmith | 数据在自己的系统 |
| **生态** | LangChain专用 | 通用标准 |

**选择建议**：
- 快速上手 → LangSmith
- 数据隐私 → OpenTelemetry
- 已有监控系统 → OpenTelemetry
- 多语言/多框架 → OpenTelemetry

---

### OpenTelemetry 快速开始

#### 步骤1：安装依赖

```bash
uv add opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp
```

---

#### 步骤2：配置TracerProvider

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource

# 配置资源（服务名称）
resource = Resource.create({
    "service.name": "my-langchain-app",
    "service.version": "1.0.0"
})

# 配置TracerProvider
trace.set_tracer_provider(TracerProvider(resource=resource))

# 配置导出器（导出到Uptrace/Jaeger/Zipkin等）
exporter = OTLPSpanExporter(
    endpoint="http://localhost:4318/v1/traces"  # 根据实际情况修改
)

# 添加处理器
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(exporter)
)

# 获取tracer
tracer = trace.get_tracer(__name__)
```

---

#### 步骤3：在链中使用

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()

prompt = ChatPromptTemplate.from_template("回答问题: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model

# 使用span追踪
with tracer.start_as_current_span("my_chain") as span:
    # 设置span属性
    span.set_attribute("question", "什么是LCEL?")
    span.set_attribute("model", "gpt-4o-mini")

    # 执行链
    result = chain.invoke({"question": "什么是LCEL?"})

    # 记录结果
    span.set_attribute("response_length", len(result.content))
    span.set_attribute("status", "success")

print(result.content)
```

---

### 自定义Span和属性

#### 创建子Span

```python
with tracer.start_as_current_span("parent_span") as parent:
    parent.set_attribute("operation", "rag_query")

    # 子span：检索
    with tracer.start_as_current_span("retrieval") as retrieval_span:
        retrieval_span.set_attribute("query", "什么是LCEL?")
        docs = retriever.invoke("什么是LCEL?")
        retrieval_span.set_attribute("num_docs", len(docs))

    # 子span：生成
    with tracer.start_as_current_span("generation") as gen_span:
        gen_span.set_attribute("model", "gpt-4o-mini")
        result = chain.invoke({"question": "什么是LCEL?"})
        gen_span.set_attribute("tokens", 135)
```

---

#### 记录事件

```python
with tracer.start_as_current_span("my_chain") as span:
    # 记录事件
    span.add_event("chain_started", {
        "timestamp": time.time(),
        "input": "什么是LCEL?"
    })

    result = chain.invoke({"question": "什么是LCEL?"})

    span.add_event("chain_completed", {
        "timestamp": time.time(),
        "output_length": len(result.content)
    })
```

---

#### 记录错误

```python
with tracer.start_as_current_span("my_chain") as span:
    try:
        result = chain.invoke({"question": "什么是LCEL?"})
    except Exception as e:
        # 记录错误
        span.set_status(trace.Status(trace.StatusCode.ERROR))
        span.record_exception(e)
        raise
```

---

### Metrics 收集

#### 配置Metrics

```python
from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter

# 配置导出器
exporter = OTLPMetricExporter(
    endpoint="http://localhost:4318/v1/metrics"
)

# 配置MeterProvider
reader = PeriodicExportingMetricReader(exporter, export_interval_millis=5000)
metrics.set_meter_provider(MeterProvider(metric_readers=[reader]))

# 获取meter
meter = metrics.get_meter(__name__)
```

---

#### 创建指标

```python
# 计数器：统计请求数
request_counter = meter.create_counter(
    name="chain_requests",
    description="Number of chain requests",
    unit="1"
)

# 直方图：统计延迟分布
latency_histogram = meter.create_histogram(
    name="chain_latency",
    description="Chain execution latency",
    unit="ms"
)

# 使用
import time

start = time.time()
result = chain.invoke({"question": "什么是LCEL?"})
duration = (time.time() - start) * 1000  # 转换为毫秒

request_counter.add(1, {"model": "gpt-4o-mini", "status": "success"})
latency_histogram.record(duration, {"model": "gpt-4o-mini"})
```

---

### 导出到监控系统

#### 1. Uptrace（推荐）

Uptrace是一个开源的APM工具，专门支持OpenTelemetry。

**配置**：
```python
exporter = OTLPSpanExporter(
    endpoint="https://api.uptrace.dev:4318/v1/traces",
    headers={"uptrace-dsn": "your_dsn"}
)
```

**参考**：https://uptrace.dev/blog/langchain-observability

---

#### 2. Jaeger

Jaeger是一个开源的分布式追踪系统。

**配置**：
```python
from opentelemetry.exporter.jaeger.thrift import JaegerExporter

exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831
)
```

---

#### 3. Prometheus + Grafana

Prometheus收集指标，Grafana可视化。

**配置**：
```python
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from prometheus_client import start_http_server

# 启动Prometheus HTTP服务器
start_http_server(port=8000)

# 配置Prometheus导出器
reader = PrometheusMetricReader()
metrics.set_meter_provider(MeterProvider(metric_readers=[reader]))
```

访问 http://localhost:8000/metrics 查看指标。

---

## 关键指标

### 1. 延迟（Latency）

**定义**：请求从开始到结束的时间

**关键指标**：
- P50（中位数）
- P95（95分位数）
- P99（99分位数）
- 最大值

**示例**：
```python
latencies = [1.2, 1.5, 1.3, 2.1, 1.4, 1.6, 1.8, 1.7, 1.5, 3.2]
p50 = sorted(latencies)[len(latencies) // 2]  # 1.55s
p95 = sorted(latencies)[int(len(latencies) * 0.95)]  # 3.2s
```

---

### 2. 吞吐量（Throughput）

**定义**：单位时间内处理的请求数

**单位**：requests/second (RPS)

**示例**：
```python
total_requests = 1000
duration = 60  # 秒
throughput = total_requests / duration  # 16.67 RPS
```

---

### 3. 错误率（Error Rate）

**定义**：失败请求占总请求的比例

**计算**：
```python
total_requests = 1000
failed_requests = 10
error_rate = failed_requests / total_requests  # 1%
```

---

### 4. Token使用（Token Usage）

**定义**：LLM调用消耗的Token数量

**关键指标**：
- Prompt tokens
- Completion tokens
- Total tokens

**示例**：
```python
tokens = {
    "prompt_tokens": 15,
    "completion_tokens": 120,
    "total_tokens": 135
}
```

---

### 5. 成本（Cost）

**定义**：基于Token使用计算的成本

**计算**：
```python
# gpt-4o-mini价格
prompt_price = 0.15 / 1_000_000  # $0.15/1M tokens
completion_price = 0.60 / 1_000_000  # $0.60/1M tokens

cost = (
    tokens["prompt_tokens"] * prompt_price +
    tokens["completion_tokens"] * completion_price
)
# 15 * 0.00000015 + 120 * 0.0000006 = 0.000074 = $0.000074
```

---

## 完整示例：OpenTelemetry + LangChain

```python
from opentelemetry import trace, metrics
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import time

load_dotenv()

# 配置OpenTelemetry
resource = Resource.create({"service.name": "langchain-app"})
trace.set_tracer_provider(TracerProvider(resource=resource))
exporter = OTLPSpanExporter(endpoint="http://localhost:4318/v1/traces")
trace.get_tracer_provider().add_span_processor(BatchSpanProcessor(exporter))

tracer = trace.get_tracer(__name__)

# 配置Metrics
metrics.set_meter_provider(MeterProvider())
meter = metrics.get_meter(__name__)
request_counter = meter.create_counter("requests")
latency_histogram = meter.create_histogram("latency")

# 构建链
prompt = ChatPromptTemplate.from_template("回答问题: {question}")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model

# 使用追踪
with tracer.start_as_current_span("langchain_request") as span:
    span.set_attribute("question", "什么是LCEL?")

    start = time.time()
    try:
        result = chain.invoke({"question": "什么是LCEL?"})
        duration = (time.time() - start) * 1000

        span.set_attribute("status", "success")
        span.set_attribute("response_length", len(result.content))

        request_counter.add(1, {"status": "success"})
        latency_histogram.record(duration)

        print(result.content)
    except Exception as e:
        span.set_status(trace.Status(trace.StatusCode.ERROR))
        span.record_exception(e)
        request_counter.add(1, {"status": "error"})
        raise
```

---

## 最佳实践

### 1. 选择合适的监控方案

| 场景 | 推荐方案 |
|------|----------|
| 快速上手 | LangSmith |
| 数据隐私 | OpenTelemetry |
| 已有监控系统 | OpenTelemetry |
| 小团队 | LangSmith |
| 大团队 | OpenTelemetry |

---

### 2. 使用采样减少成本

```python
# LangSmith采样
if random.random() < 0.1:  # 10%
    os.environ["LANGCHAIN_TRACING_V2"] = "true"

# OpenTelemetry采样
from opentelemetry.sdk.trace.sampling import TraceIdRatioBased
sampler = TraceIdRatioBased(0.1)  # 10%采样
```

---

### 3. 设置告警规则

**关键告警**：
- 错误率 > 1%
- P95延迟 > 5s
- 成本 > 预算

---

### 4. 定期审查指标

**每周审查**：
- 延迟趋势
- 错误类型分布
- 成本变化

---

## 参考资源

### 官方文档
1. **LangSmith**: https://docs.langchain.com/langsmith
2. **OpenTelemetry Python**: https://opentelemetry.io/docs/languages/python/
3. **Uptrace LangChain Guide** (2025): https://uptrace.dev/blog/langchain-observability

### 相关文章
1. **LangSmith Advanced Tracing** (2025): https://sparkco.ai/blog/advanced-langsmith-tracing-techniques-in-2025
2. **Top 7 LLM Observability Tools** (2026): https://www.confident-ai.com/knowledge-base/top-7-llm-observability-tools

---

## 总结

**核心要点**：

1. **LangSmith**：零代码集成，适合快速上手
2. **OpenTelemetry**：开源标准，灵活强大
3. **关键指标**：延迟、吞吐量、错误率、Token、成本
4. **采样策略**：减少成本，保持可观测性
5. **告警规则**：及时发现问题

**记住**：生产环境监控不是可选项，而是必需品。选择合适的方案，持续优化。

---

**版本信息**
- LangChain: v0.3+ (2025-2026)
- Python: 3.13+
- 最后更新: 2026-02-20
