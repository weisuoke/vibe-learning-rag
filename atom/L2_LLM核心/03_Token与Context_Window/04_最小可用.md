# Token与Context Window - 最小可用

掌握以下 20% 的核心知识，就能解决 80% 的 Token 相关问题。

---

## 4.1 计算 Token 数量

**这是最基础的能力：知道你的文本有多少 Token。**

```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-4o") -> int:
    """计算文本的 Token 数量"""
    encoder = tiktoken.encoding_for_model(model)
    return len(encoder.encode(text))

# 使用示例
text = "你好，我想了解 RAG 开发的基础知识。"
tokens = count_tokens(text)
print(f"Token 数量: {tokens}")  # 约 15-20 Token
```

**经验估算（不用代码时）：**
- 中文：字数 × 1.5 ≈ Token 数
- 英文：单词数 × 1.3 ≈ Token 数

---

## 4.2 知道模型的 Context Window

**记住常用模型的限制，避免超限报错。**

| 模型 | Context Window | 实际可用（留输出空间） |
|------|----------------|----------------------|
| gpt-4o | 128K | ~120K |
| gpt-4o-mini | 128K | ~120K |
| gpt-3.5-turbo | 16K | ~14K |
| claude-3.5-sonnet | 200K | ~190K |

**超限时的错误信息：**

```
Error: This model's maximum context length is 128000 tokens.
However, your messages resulted in 150000 tokens.
```

---

## 4.3 预留输出空间

**Context Window = 输入 + 输出，必须给输出留空间！**

```python
def get_available_input_tokens(
    model: str = "gpt-4o",
    max_output: int = 4000
) -> int:
    """计算可用于输入的 Token 数量"""

    limits = {
        "gpt-4o": 128000,
        "gpt-4o-mini": 128000,
        "gpt-3.5-turbo": 16000,
    }

    context_window = limits.get(model, 8000)
    return context_window - max_output

# 示例
available = get_available_input_tokens("gpt-4o", max_output=4000)
print(f"可用输入空间: {available} Token")  # 124000
```

---

## 4.4 检索内容截断

**当检索内容超限时，必须截断。**

```python
def truncate_docs_to_limit(
    docs: list[str],
    max_tokens: int,
    model: str = "gpt-4o"
) -> str:
    """将文档列表截断到 Token 限制内"""

    result = []
    current = 0

    for doc in docs:
        doc_tokens = count_tokens(doc, model)
        if current + doc_tokens <= max_tokens:
            result.append(doc)
            current += doc_tokens
        else:
            break  # 超限，停止添加

    return "\n\n".join(result)

# 使用示例
docs = ["文档1内容...", "文档2内容...", "文档3内容..."]
context = truncate_docs_to_limit(docs, max_tokens=10000)
```

---

## 4.5 完整的 RAG Token 管理

**把上面的知识组合起来，就是完整的 Token 管理。**

```python
def build_rag_messages(
    user_query: str,
    retrieved_docs: list[str],
    system_prompt: str = "基于以下资料回答问题。",
    model: str = "gpt-4o",
    max_output: int = 4000
) -> list[dict]:
    """构建 RAG 消息，自动处理 Token 限制"""

    # 1. 计算可用空间
    available = get_available_input_tokens(model, max_output)

    # 2. 减去固定部分
    fixed_tokens = count_tokens(system_prompt) + count_tokens(user_query)
    doc_budget = available - fixed_tokens - 500  # 留 500 安全边际

    # 3. 截断检索内容
    context = truncate_docs_to_limit(retrieved_docs, doc_budget, model)

    # 4. 构建消息
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"参考资料：\n{context}\n\n问题：{user_query}"}
    ]
```

---

## 这些知识足以：

- ✅ 计算任意文本的 Token 数量
- ✅ 避免 Context Window 超限报错
- ✅ 合理分配 RAG 的 Token 预算
- ✅ 自动截断过长的检索内容
- ✅ 为后续学习 Chunking 策略打基础

---

**上一节：** [03_核心概念.md](./03_核心概念.md)
**下一节：** [05_双重类比.md](./05_双重类比.md)
