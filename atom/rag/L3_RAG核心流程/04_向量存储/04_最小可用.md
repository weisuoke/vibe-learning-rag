# 最小可用

## 概述

向量存储涉及索引算法、距离度量、数据库选型等多个维度，但初学者不需要一次性掌握所有内容。本文定义"最小可用知识集"，帮助你快速上手RAG开发。

---

## 最小可用知识集（5个核心点）

### 1. 理解向量存储的本质

**核心认知**：
- 向量存储 = 数值数组 + 相似度计算 + 索引加速
- 目的：将文本转换为向量，通过数学计算找到语义相似的内容
- 在RAG中的作用：检索与用户问题相关的文档片段

**最小理解**：
```python
# 向量存储的本质
text = "什么是RAG？"
vector = embedding_model.encode(text)  # [0.1, 0.3, -0.2, ...]
# 存储：vector + metadata
# 检索：计算query_vector与所有stored_vectors的相似度
```

**为什么重要**：
- 理解本质才能做出正确的技术选型
- 避免盲目追求复杂方案

---

### 2. 会用ChromaDB构建基础RAG

**核心技能**：
- 安装ChromaDB
- 创建collection
- 插入文档和向量
- 执行语义检索

**最小可用代码**：
```python
import chromadb
from sentence_transformers import SentenceTransformer

# 1. 初始化
client = chromadb.Client()
collection = client.create_collection("my_docs")
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. 插入文档
documents = [
    "RAG是检索增强生成技术",
    "向量存储用于语义检索",
    "ChromaDB是轻量级向量数据库"
]
embeddings = model.encode(documents).tolist()
collection.add(
    documents=documents,
    embeddings=embeddings,
    ids=[f"doc_{i}" for i in range(len(documents))]
)

# 3. 检索
query = "什么是RAG？"
query_embedding = model.encode(query).tolist()
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=2
)
print(results['documents'])
```

**为什么重要**：
- ChromaDB是2026年最适合初学者的向量数据库
- 零配置，5分钟上手
- 覆盖80%的RAG开发场景

---

### 3. 理解Cosine vs Euclidean距离

**核心认知**：
- **Cosine Similarity**：比较方向，忽略长度（最常用）
- **Euclidean Distance**：比较空间距离，考虑长度
- **选择标准**：匹配embedding模型的训练度量

**最小理解**：
```python
import numpy as np

v1 = np.array([1, 2, 3])
v2 = np.array([2, 4, 6])  # v2是v1的2倍

# Cosine相似度：只看方向
cosine = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
print(f"Cosine: {cosine}")  # 1.0（方向完全相同）

# Euclidean距离：看空间距离
euclidean = np.linalg.norm(v1 - v2)
print(f"Euclidean: {euclidean}")  # 3.74（距离较远）
```

**实用规则**：
- 大多数embedding模型（如sentence-transformers）使用Cosine训练
- 默认选择Cosine Similarity
- 归一化向量后，Cosine和Dot Product等价（Dot Product更快）

**为什么重要**：
- 错误的距离度量会导致检索结果不准确
- 这是最常见的配置错误之一

---

### 4. 知道HNSW的基本配置

**核心参数**：
- **M**：每个节点的连接数（推荐：16-32）
- **ef_construction**：构建时的候选数（推荐：100-200）
- **ef_search**：查询时的候选数（推荐：50-100）

**最小配置**：
```python
import chromadb

# ChromaDB默认使用HNSW
client = chromadb.Client()
collection = client.create_collection(
    name="my_docs",
    metadata={
        "hnsw:space": "cosine",        # 距离度量
        "hnsw:construction_ef": 100,   # 构建质量
        "hnsw:M": 16                   # 连接数
    }
)
```

**实用规则**：
- 小规模（<10万向量）：使用默认配置
- 中规模（10万-100万）：M=32, ef_construction=200
- 追求速度：降低ef_search
- 追求精度：提高ef_search

**为什么重要**：
- HNSW是2026年主流索引算法
- 合理配置可以平衡速度和精度
- 默认配置已经足够好，不要过早优化

---

### 5. 能部署简单的向量检索服务

**核心技能**：
- 使用FastAPI封装检索接口
- 支持文档插入和查询
- 基本的错误处理

**最小可用服务**：
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import chromadb
from sentence_transformers import SentenceTransformer

app = FastAPI()
client = chromadb.Client()
collection = client.get_or_create_collection("docs")
model = SentenceTransformer('all-MiniLM-L6-v2')

class Document(BaseModel):
    text: str
    metadata: dict = {}

class Query(BaseModel):
    text: str
    top_k: int = 5

@app.post("/add")
def add_document(doc: Document):
    embedding = model.encode(doc.text).tolist()
    doc_id = f"doc_{collection.count()}"
    collection.add(
        documents=[doc.text],
        embeddings=[embedding],
        metadatas=[doc.metadata],
        ids=[doc_id]
    )
    return {"id": doc_id, "status": "added"}

@app.post("/search")
def search(query: Query):
    query_embedding = model.encode(query.text).tolist()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=query.top_k
    )
    return {
        "documents": results['documents'][0],
        "distances": results['distances'][0]
    }

# 运行：uvicorn main:app --reload
```

**为什么重要**：
- 将向量检索封装为API是生产部署的第一步
- FastAPI是2026年Python后端的标准选择
- 这个模板可以直接用于原型验证

---

## 学习路径

### 第一周：基础理解
- ✅ 理解向量存储的本质（1天）
- ✅ 安装ChromaDB并运行第一个示例（1天）
- ✅ 理解Cosine vs Euclidean（1天）
- ✅ 实验不同的embedding模型（2天）
- ✅ 部署简单的检索服务（2天）

### 第二周：深入实践
- 学习HNSW索引原理
- 实验不同的索引参数
- 对比ChromaDB vs FAISS性能
- 集成到实际RAG项目

### 第三周：生产准备
- 学习持久化策略
- 了解Milvus分布式架构
- 实现监控和日志
- 性能优化和成本控制

---

## 最小可用检查清单

在进入进阶学习前，确保你能做到：

### 理论理解
- [ ] 能用一句话解释向量存储的作用
- [ ] 知道Cosine和Euclidean的区别
- [ ] 理解HNSW的基本思想（分层导航）
- [ ] 知道为什么需要索引（暴力搜索太慢）

### 实践能力
- [ ] 能用ChromaDB构建一个简单的RAG系统
- [ ] 能插入文档并执行语义检索
- [ ] 能配置HNSW的基本参数
- [ ] 能部署一个FastAPI检索服务

### 问题解决
- [ ] 遇到检索结果不准确，知道检查距离度量
- [ ] 遇到检索速度慢，知道调整ef_search
- [ ] 遇到内存不足，知道考虑IVF或PQ压缩
- [ ] 遇到数据丢失，知道配置持久化

---

## 常见误区

### 误区1：一开始就学习所有索引算法
**正确做法**：先用ChromaDB的默认HNSW配置，等遇到性能瓶颈再优化。

### 误区2：过早优化参数
**正确做法**：默认配置已经很好，先关注数据质量和chunk策略。

### 误区3：忽略距离度量的选择
**正确做法**：始终匹配embedding模型的训练度量，大多数情况用Cosine。

### 误区4：直接上生产级方案
**正确做法**：先用ChromaDB验证可行性，再考虑Milvus等企业级方案。

---

## 最小可用 vs 完整掌握

| 维度 | 最小可用 | 完整掌握 |
|------|---------|---------|
| **索引算法** | 知道HNSW存在 | 理解HNSW/IVF/PQ原理 |
| **距离度量** | 会用Cosine | 理解所有度量的数学原理 |
| **数据库选型** | 会用ChromaDB | 对比5+种向量数据库 |
| **参数调优** | 使用默认配置 | 根据场景精细调优 |
| **部署** | 单机FastAPI | 分布式高可用架构 |
| **监控** | 基本日志 | 完整监控体系 |
| **时间投入** | 1周 | 1-2个月 |

---

## 实战练习

### 练习1：构建文档问答系统（2小时）
```python
# 目标：实现一个简单的文档问答系统
# 要求：
# 1. 加载3-5篇文档
# 2. 分块并存储到ChromaDB
# 3. 实现问答接口
# 4. 返回Top 3相关片段

# 提示：
# - 使用RecursiveCharacterTextSplitter分块
# - chunk_size=512, chunk_overlap=50
# - 使用all-MiniLM-L6-v2模型
```

### 练习2：对比不同距离度量（1小时）
```python
# 目标：实验Cosine vs Euclidean vs Dot Product
# 要求：
# 1. 使用相同数据集
# 2. 分别用三种度量检索
# 3. 对比Top 5结果的差异
# 4. 分析为什么会有差异

# 提示：
# - 使用归一化和非归一化向量对比
# - 观察相似度分数的范围
```

### 练习3：参数调优实验（2小时）
```python
# 目标：理解HNSW参数的影响
# 要求：
# 1. 固定数据集（1万条）
# 2. 对比不同M值（8, 16, 32, 64）
# 3. 对比不同ef_search值（10, 50, 100, 200）
# 4. 记录召回率和延迟

# 提示：
# - 使用FAISS便于参数对比
# - 准备ground truth用于计算召回率
```

---

## 进阶方向

掌握最小可用知识后，可以根据需求选择进阶方向：

### 方向1：性能优化
- 学习IVF和PQ压缩
- 实验GPU加速（FAISS）
- 优化批量插入和查询

### 方向2：生产部署
- 学习Milvus分布式架构
- 实现高可用和负载均衡
- 配置监控和告警

### 方向3：算法深入
- 手写HNSW实现
- 研究最新的索引算法（DiskANN, SPANN）
- 阅读学术论文

### 方向4：RAG优化
- 学习混合检索（向量+关键词）
- 实现ReRank重排序
- 优化chunk策略

---

## 总结

**最小可用 = 5个核心点**：
1. 理解向量存储本质
2. 会用ChromaDB构建RAG
3. 理解Cosine vs Euclidean
4. 知道HNSW基本配置
5. 能部署简单检索服务

**学习策略**：
- 先掌握最小可用，快速上手
- 在实践中遇到问题，再深入学习
- 避免过早优化，关注数据质量

**记住**：向量存储是手段，不是目的。最小可用知识足以支撑大多数RAG应用开发。

---

## 引用来源

1. **ChromaDB官方文档**：https://docs.trychroma.com/
2. **Sentence Transformers**：https://www.sbert.net/
3. **HNSW参数指南**：https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md
4. **FastAPI文档**：https://fastapi.tiangolo.com/
5. **RAG最佳实践**：https://www.pinecone.io/learn/retrieval-augmented-generation/
