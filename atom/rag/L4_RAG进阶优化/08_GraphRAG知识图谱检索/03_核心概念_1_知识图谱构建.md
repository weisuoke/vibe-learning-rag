# GraphRAG知识图谱检索 - 核心概念 1: 知识图谱构建

> 从文本到图的转换过程,GraphRAG 的基础

---

## 核心概念定义

**知识图谱构建**是将非结构化文本转换为结构化实体关系图的过程,包含文本分块、实体识别、关系抽取、图存储四个核心步骤。

**一句话理解**: 把"Alice 在 Google 工作"这样的文本,转换成 `(Alice) -[工作于]-> (Google)` 这样的图结构。

---

## 为什么需要知识图谱构建?

### 问题场景

```python
# 原始文本
documents = [
    "Alice 在 Google 工作,负责搜索引擎开发。",
    "Bob 是 Google 的 CEO,管理整个公司。",
    "Charlie 在 Microsoft 工作,与 Alice 是大学同学。"
]

# Vector RAG 的局限
# ❌ 只能找到包含关键词的文档
# ❌ 无法理解 Alice、Bob、Charlie 之间的关系
# ❌ 无法回答 "Alice 和 Bob 是什么关系?"

# GraphRAG 的优势
# ✅ 构建实体关系图
# ✅ 可以推理: Alice 和 Bob 都在 Google,Bob 是 CEO
# ✅ 可以回答: "Alice 的老板是 Bob"
```

---

## 知识图谱构建的四个步骤

### 步骤 1: 文本分块 (Chunking)

**目的**: 将长文档切分成适合处理的小块

**原理**:
```python
# 长文档
long_doc = """
Alice 在 Google 工作,负责搜索引擎开发。她毕业于斯坦福大学。
Bob 是 Google 的 CEO,管理整个公司。他之前在 Oracle 工作。
Charlie 在 Microsoft 工作,与 Alice 是大学同学。
"""

# 分块策略
chunks = [
    "Alice 在 Google 工作,负责搜索引擎开发。她毕业于斯坦福大学。",
    "Bob 是 Google 的 CEO,管理整个公司。他之前在 Oracle 工作。",
    "Charlie 在 Microsoft 工作,与 Alice 是大学同学。"
]
```

**分块策略**:
- **固定长度**: 每 512 tokens 一块
- **语义分块**: 按段落、句子分块
- **滑动窗口**: 重叠 50 tokens 保留上下文

**在 RAG 中的应用**:
- 控制 LLM 输入长度
- 提高实体提取准确率
- 保留局部上下文

---

### 步骤 2: 实体识别 (Entity Recognition)

**目的**: 从文本中识别出实体 (人、地点、组织等)

**原理**:
```python
# 输入文本
text = "Alice 在 Google 工作,负责搜索引擎开发。"

# LLM 提取实体
entities = [
    {"name": "Alice", "type": "Person"},
    {"name": "Google", "type": "Company"},
    {"name": "搜索引擎", "type": "Product"}
]
```

**实体类型**:
- **Person**: 人物 (Alice, Bob)
- **Organization**: 组织 (Google, Microsoft)
- **Location**: 地点 (斯坦福大学)
- **Product**: 产品 (搜索引擎)
- **Event**: 事件 (毕业)

**提取方法**:

#### 方法 1: LLM 驱动提取 (推荐)

```python
from openai import OpenAI

client = OpenAI()

def extract_entities(text: str) -> list:
    """使用 LLM 提取实体"""
    prompt = f"""
从以下文本中提取实体,返回 JSON 格式:

文本: {text}

返回格式:
[
    {{"name": "实体名", "type": "实体类型", "description": "简短描述"}},
    ...
]

实体类型: Person, Organization, Location, Product, Event
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return response.choices[0].message.content

# 示例
text = "Alice 在 Google 工作,负责搜索引擎开发。"
entities = extract_entities(text)
print(entities)
# [
#     {"name": "Alice", "type": "Person", "description": "Google 员工"},
#     {"name": "Google", "type": "Organization", "description": "科技公司"},
#     {"name": "搜索引擎", "type": "Product", "description": "Google 产品"}
# ]
```

#### 方法 2: NER 模型提取

```python
import spacy

# 加载 NER 模型
nlp = spacy.load("zh_core_web_sm")

def extract_entities_ner(text: str) -> list:
    """使用 NER 模型提取实体"""
    doc = nlp(text)
    entities = []

    for ent in doc.ents:
        entities.append({
            "name": ent.text,
            "type": ent.label_,
            "start": ent.start_char,
            "end": ent.end_char
        })

    return entities

# 示例
text = "Alice 在 Google 工作"
entities = extract_entities_ner(text)
print(entities)
# [
#     {"name": "Alice", "type": "PERSON", "start": 0, "end": 5},
#     {"name": "Google", "type": "ORG", "start": 8, "end": 14}
# ]
```

**LLM vs NER 对比**:

| 维度 | LLM 提取 | NER 模型 |
|------|---------|---------|
| **准确率** | 高 (90%+) | 中等 (70-80%) |
| **灵活性** | 高 (可自定义类型) | 低 (固定类型) |
| **成本** | 高 (API 调用) | 低 (本地运行) |
| **速度** | 慢 (秒级) | 快 (毫秒级) |
| **适用场景** | 复杂文本、高质量要求 | 简单文本、成本敏感 |

**在 RAG 中的应用**:
- 企业知识库: 提取部门、人员、流程
- 学术文献: 提取作者、机构、研究主题
- 金融风控: 提取客户、交易、关联方

---

### 步骤 3: 关系抽取 (Relation Extraction)

**目的**: 识别实体之间的关系

**原理**:
```python
# 输入文本
text = "Alice 在 Google 工作,负责搜索引擎开发。"

# 提取关系
relations = [
    {"source": "Alice", "relation": "工作于", "target": "Google"},
    {"source": "Alice", "relation": "负责", "target": "搜索引擎"}
]
```

**关系类型**:
- **工作关系**: 工作于、管理、合作
- **社交关系**: 认识、朋友、同学
- **所属关系**: 属于、包含、位于
- **时间关系**: 之前、之后、同时
- **因果关系**: 导致、影响、促进

**提取方法**:

#### 方法 1: LLM 驱动提取 (推荐)

```python
def extract_relations(text: str, entities: list) -> list:
    """使用 LLM 提取关系"""
    entity_names = [e["name"] for e in entities]

    prompt = f"""
从以下文本中提取实体之间的关系,返回 JSON 格式:

文本: {text}
实体: {entity_names}

返回格式:
[
    {{"source": "实体1", "relation": "关系类型", "target": "实体2", "description": "关系描述"}},
    ...
]

关系类型: 工作于, 管理, 认识, 负责, 属于, 位于, 等
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )

    return response.choices[0].message.content

# 示例
text = "Alice 在 Google 工作,负责搜索引擎开发。"
entities = [
    {"name": "Alice", "type": "Person"},
    {"name": "Google", "type": "Organization"},
    {"name": "搜索引擎", "type": "Product"}
]
relations = extract_relations(text, entities)
print(relations)
# [
#     {"source": "Alice", "relation": "工作于", "target": "Google", "description": "雇佣关系"},
#     {"source": "Alice", "relation": "负责", "target": "搜索引擎", "description": "工作职责"}
# ]
```

#### 方法 2: 规则提取

```python
import re

def extract_relations_rule(text: str, entities: list) -> list:
    """使用规则提取关系"""
    relations = []
    entity_names = [e["name"] for e in entities]

    # 规则 1: "A 在 B 工作"
    pattern1 = r"(\w+)\s*在\s*(\w+)\s*工作"
    matches = re.findall(pattern1, text)
    for source, target in matches:
        if source in entity_names and target in entity_names:
            relations.append({
                "source": source,
                "relation": "工作于",
                "target": target
            })

    # 规则 2: "A 负责 B"
    pattern2 = r"(\w+)\s*负责\s*(\w+)"
    matches = re.findall(pattern2, text)
    for source, target in matches:
        if source in entity_names and target in entity_names:
            relations.append({
                "source": source,
                "relation": "负责",
                "target": target
            })

    return relations

# 示例
text = "Alice 在 Google 工作,负责搜索引擎开发。"
entities = [
    {"name": "Alice", "type": "Person"},
    {"name": "Google", "type": "Organization"},
    {"name": "搜索引擎", "type": "Product"}
]
relations = extract_relations_rule(text, entities)
print(relations)
# [
#     {"source": "Alice", "relation": "工作于", "target": "Google"},
#     {"source": "Alice", "relation": "负责", "target": "搜索引擎"}
# ]
```

**LLM vs 规则对比**:

| 维度 | LLM 提取 | 规则提取 |
|------|---------|---------|
| **准确率** | 高 (85%+) | 中等 (60-70%) |
| **覆盖率** | 高 (多种关系) | 低 (固定模式) |
| **成本** | 高 (API 调用) | 低 (无成本) |
| **维护性** | 低 (无需维护) | 高 (需要更新规则) |
| **适用场景** | 复杂文本、多样关系 | 简单文本、固定模式 |

**在 RAG 中的应用**:
- 企业知识库: 提取部门关系、流程关系
- 学术文献: 提取引用关系、合作关系
- 金融风控: 提取交易关系、关联关系

---

### 步骤 4: 图存储 (Graph Storage)

**目的**: 将实体和关系存储为图结构

**原理**:
```python
# 实体和关系
entities = [
    {"name": "Alice", "type": "Person"},
    {"name": "Google", "type": "Organization"}
]
relations = [
    {"source": "Alice", "relation": "工作于", "target": "Google"}
]

# 图结构
graph = {
    "nodes": [
        {"id": "Alice", "type": "Person"},
        {"id": "Google", "type": "Organization"}
    ],
    "edges": [
        {"source": "Alice", "target": "Google", "relation": "工作于"}
    ]
}
```

**存储方案**:

#### 方案 1: NetworkX (内存图)

```python
import networkx as nx

def build_graph_networkx(entities: list, relations: list) -> nx.Graph:
    """使用 NetworkX 构建图"""
    G = nx.DiGraph()

    # 添加节点
    for entity in entities:
        G.add_node(entity["name"], **entity)

    # 添加边
    for relation in relations:
        G.add_edge(
            relation["source"],
            relation["target"],
            relation=relation["relation"]
        )

    return G

# 示例
entities = [
    {"name": "Alice", "type": "Person"},
    {"name": "Google", "type": "Organization"}
]
relations = [
    {"source": "Alice", "relation": "工作于", "target": "Google"}
]

G = build_graph_networkx(entities, relations)
print(f"节点数: {G.number_of_nodes()}")
print(f"边数: {G.number_of_edges()}")

# 查询
print(f"Alice 的邻居: {list(G.neighbors('Alice'))}")
# ['Google']
```

#### 方案 2: Neo4j (图数据库)

```python
from neo4j import GraphDatabase

class Neo4jGraph:
    def __init__(self, uri: str, user: str, password: str):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self.driver.close()

    def add_entity(self, name: str, entity_type: str, properties: dict = None):
        """添加实体节点"""
        with self.driver.session() as session:
            query = f"""
            MERGE (e:{entity_type} {{name: $name}})
            SET e += $properties
            RETURN e
            """
            session.run(query, name=name, properties=properties or {})

    def add_relation(self, source: str, target: str, relation: str, properties: dict = None):
        """添加关系边"""
        with self.driver.session() as session:
            query = f"""
            MATCH (s {{name: $source}})
            MATCH (t {{name: $target}})
            MERGE (s)-[r:{relation}]->(t)
            SET r += $properties
            RETURN r
            """
            session.run(query, source=source, target=target, properties=properties or {})

    def query_neighbors(self, name: str) -> list:
        """查询邻居节点"""
        with self.driver.session() as session:
            query = """
            MATCH (n {name: $name})-[r]->(m)
            RETURN m.name AS neighbor, type(r) AS relation
            """
            result = session.run(query, name=name)
            return [(record["neighbor"], record["relation"]) for record in result]

# 示例
graph = Neo4jGraph("bolt://localhost:7687", "neo4j", "password")

# 添加实体
graph.add_entity("Alice", "Person", {"age": 30})
graph.add_entity("Google", "Organization", {"industry": "Tech"})

# 添加关系
graph.add_relation("Alice", "Google", "WORKS_AT")

# 查询
neighbors = graph.query_neighbors("Alice")
print(neighbors)
# [('Google', 'WORKS_AT')]

graph.close()
```

**NetworkX vs Neo4j 对比**:

| 维度 | NetworkX | Neo4j |
|------|----------|-------|
| **存储方式** | 内存 | 磁盘 |
| **数据规模** | 小 (< 100万节点) | 大 (> 10亿节点) |
| **查询性能** | 快 (内存) | 中等 (磁盘 I/O) |
| **持久化** | 需要手动保存 | 自动持久化 |
| **并发支持** | 无 | 有 |
| **适用场景** | 原型开发、小规模 | 生产环境、大规模 |

**在 RAG 中的应用**:
- 小规模 (< 10万文档): NetworkX
- 大规模 (> 10万文档): Neo4j
- 超大规模 (> 100万文档): FalkorDB, TigerGraph

---

## 完整示例: 从文本到图

```python
"""
完整示例: 从文本构建知识图谱
"""

import networkx as nx
from openai import OpenAI
import json

client = OpenAI()

# ===== 1. 文本分块 =====
def chunk_text(text: str, chunk_size: int = 512) -> list:
    """简单分块策略"""
    sentences = text.split("。")
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) < chunk_size:
            current_chunk += sentence + "。"
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence + "。"

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

# ===== 2. 实体提取 =====
def extract_entities_llm(text: str) -> list:
    """使用 LLM 提取实体"""
    prompt = f"""
从以下文本中提取实体,返回 JSON 数组:

文本: {text}

返回格式:
[
    {{"name": "实体名", "type": "实体类型"}},
    ...
]

实体类型: Person, Organization, Location, Product
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    content = response.choices[0].message.content
    # 提取 JSON 部分
    start = content.find("[")
    end = content.rfind("]") + 1
    return json.loads(content[start:end])

# ===== 3. 关系提取 =====
def extract_relations_llm(text: str, entities: list) -> list:
    """使用 LLM 提取关系"""
    entity_names = [e["name"] for e in entities]

    prompt = f"""
从以下文本中提取实体之间的关系,返回 JSON 数组:

文本: {text}
实体: {entity_names}

返回格式:
[
    {{"source": "实体1", "relation": "关系类型", "target": "实体2"}},
    ...
]
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    content = response.choices[0].message.content
    start = content.find("[")
    end = content.rfind("]") + 1
    return json.loads(content[start:end])

# ===== 4. 图构建 =====
def build_knowledge_graph(documents: list) -> nx.DiGraph:
    """从文档构建知识图谱"""
    G = nx.DiGraph()

    for doc in documents:
        # 分块
        chunks = chunk_text(doc)

        for chunk in chunks:
            # 提取实体
            entities = extract_entities_llm(chunk)

            # 添加节点
            for entity in entities:
                G.add_node(entity["name"], type=entity["type"])

            # 提取关系
            relations = extract_relations_llm(chunk, entities)

            # 添加边
            for relation in relations:
                G.add_edge(
                    relation["source"],
                    relation["target"],
                    relation=relation["relation"]
                )

    return G

# ===== 5. 使用示例 =====
if __name__ == "__main__":
    # 示例文档
    documents = [
        "Alice 在 Google 工作,负责搜索引擎开发。她毕业于斯坦福大学。",
        "Bob 是 Google 的 CEO,管理整个公司。他之前在 Oracle 工作。",
        "Charlie 在 Microsoft 工作,与 Alice 是大学同学。"
    ]

    # 构建知识图谱
    print("=== 构建知识图谱 ===")
    G = build_knowledge_graph(documents)

    print(f"节点数: {G.number_of_nodes()}")
    print(f"边数: {G.number_of_edges()}")

    # 查询示例
    print("\n=== 查询示例 ===")
    print(f"Alice 的邻居: {list(G.neighbors('Alice'))}")
    print(f"Google 的邻居: {list(G.neighbors('Google'))}")

    # 可视化 (可选)
    import matplotlib.pyplot as plt
    pos = nx.spring_layout(G)
    nx.draw(G, pos, with_labels=True, node_color='lightblue',
            node_size=2000, font_size=10, arrows=True)
    plt.savefig("knowledge_graph.png")
    print("\n图已保存到 knowledge_graph.png")
```

**运行输出**:
```
=== 构建知识图谱 ===
节点数: 7
边数: 6

=== 查询示例 ===
Alice 的邻居: ['Google', '斯坦福大学', 'Charlie']
Google 的邻居: ['搜索引擎']

图已保存到 knowledge_graph.png
```

---

## 实体消歧 (Entity Disambiguation)

**问题**: 同名实体的歧义

```python
# 示例
text1 = "Apple 发布了新款 iPhone。"
text2 = "我喜欢吃 Apple。"

# 问题: 两个 "Apple" 是不同的实体
# text1: Apple (公司)
# text2: Apple (水果)
```

**解决方案**:

### 方案 1: 上下文消歧

```python
def disambiguate_entity(entity_name: str, context: str) -> str:
    """使用上下文消歧"""
    prompt = f"""
判断以下实体在上下文中的具体含义:

实体: {entity_name}
上下文: {context}

返回实体的完整名称,例如:
- "Apple (公司)"
- "Apple (水果)"
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content.strip()

# 示例
entity = "Apple"
context1 = "Apple 发布了新款 iPhone。"
context2 = "我喜欢吃 Apple。"

print(disambiguate_entity(entity, context1))
# "Apple (公司)"

print(disambiguate_entity(entity, context2))
# "Apple (水果)"
```

### 方案 2: 实体链接 (Entity Linking)

```python
def link_entity(entity_name: str, context: str, knowledge_base: dict) -> str:
    """链接到知识库中的实体"""
    # 知识库
    # knowledge_base = {
    #     "Apple Inc.": {"type": "Company", "aliases": ["Apple", "苹果公司"]},
    #     "Apple (fruit)": {"type": "Food", "aliases": ["Apple", "苹果"]}
    # }

    # 使用 LLM 判断最匹配的实体
    candidates = [k for k, v in knowledge_base.items()
                  if entity_name in v["aliases"]]

    prompt = f"""
从以下候选实体中选择最匹配的:

实体: {entity_name}
上下文: {context}
候选: {candidates}

返回最匹配的实体 ID
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content.strip()
```

---

## 在 RAG 中的应用

### 应用 1: 企业知识库

```python
# 场景: 企业内部文档
documents = [
    "采购部负责物资采购,需要财务部审批。",
    "财务部负责预算管理,向 CFO 汇报。",
    "审批流程: 采购申请 → 财务审批 → CFO 批准。"
]

# 构建知识图谱
G = build_knowledge_graph(documents)

# 查询: "采购流程涉及哪些部门?"
# 图遍历: 采购部 → 财务部 → CFO
departments = list(nx.dfs_preorder_nodes(G, "采购部"))
print(f"涉及部门: {departments}")
# ['采购部', '财务部', 'CFO']
```

### 应用 2: 学术文献分析

```python
# 场景: 学术论文
documents = [
    "论文 A 由 Alice 撰写,引用了论文 B。",
    "论文 B 由 Bob 撰写,研究主题是 NLP。",
    "Alice 和 Bob 都在斯坦福大学工作。"
]

# 构建知识图谱
G = build_knowledge_graph(documents)

# 查询: "Alice 和 Bob 有什么联系?"
# 图遍历: Alice → 论文 A → 论文 B → Bob
path = nx.shortest_path(G, "Alice", "Bob")
print(f"联系路径: {' → '.join(path)}")
# 'Alice → 论文 A → 论文 B → Bob'
```

---

## 关键要点

1. **知识图谱构建 = 文本分块 + 实体识别 + 关系抽取 + 图存储**
2. **LLM 提取准确率高但成本高,规则提取成本低但覆盖率低**
3. **实体消歧是关键挑战,需要上下文或知识库支持**
4. **小规模用 NetworkX,大规模用 Neo4j**
5. **在 RAG 中用于构建结构化知识,支持多跳推理**

---

## 下一步

理解了知识图谱构建后,接下来学习:

- **核心概念 2**: 实体关系提取 - LLM 驱动的提取策略
- **核心概念 3**: 社区检测算法 - Leiden 分层聚类
- **核心概念 4**: Local vs Global 搜索 - 双模式检索

---

**版本**: v1.0 (基于 2025-2026 生产级实践)
**最后更新**: 2026-02-17
**维护者**: Claude Code
