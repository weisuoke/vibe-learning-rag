# 实战代码

> 动手实践：可视化 Transformer 的注意力机制

---

## 概述

本节提供可运行的 Python 代码，帮助你直观理解 Transformer 的注意力机制。

**你将学到：**
- 如何加载预训练模型并获取注意力权重
- 如何可视化注意力（哪些词在关注哪些词）
- 位置对注意力的影响
- RAG 场景下的注意力分析

---

## 环境准备

```bash
# 安装依赖
pip install transformers torch matplotlib seaborn
```

---

## 示例1：获取并可视化注意力权重

```python
"""
Transformer 注意力可视化
演示：查看模型内部的注意力权重
"""

import torch
from transformers import AutoTokenizer, AutoModel
import matplotlib.pyplot as plt
import seaborn as sns

# ===== 1. 加载模型和分词器 =====
print("=== 加载模型 ===")
model_name = "bert-base-chinese"  # 使用中文 BERT
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)

print(f"模型: {model_name}")
print(f"注意力头数: {model.config.num_attention_heads}")
print(f"隐藏层数: {model.config.num_hidden_layers}")

# ===== 2. 准备输入文本 =====
text = "小猫坐在垫子上"
print(f"\n=== 输入文本 ===")
print(f"原文: {text}")

# 分词
inputs = tokenizer(text, return_tensors="pt")
tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
print(f"分词结果: {tokens}")

# ===== 3. 获取注意力权重 =====
print("\n=== 获取注意力权重 ===")
with torch.no_grad():
    outputs = model(**inputs)

# attentions 是一个元组，每层一个张量
# 形状: (batch_size, num_heads, seq_len, seq_len)
attentions = outputs.attentions
print(f"层数: {len(attentions)}")
print(f"每层形状: {attentions[0].shape}")

# ===== 4. 可视化第一层的注意力 =====
def plot_attention(attention, tokens, layer=0, head=0):
    """绘制注意力热力图"""
    # 获取指定层和头的注意力
    att = attention[layer][0, head].numpy()

    plt.figure(figsize=(10, 8))
    sns.heatmap(
        att,
        xticklabels=tokens,
        yticklabels=tokens,
        cmap="YlOrRd",
        annot=True,
        fmt=".2f"
    )
    plt.title(f"注意力热力图 (Layer {layer}, Head {head})")
    plt.xlabel("被关注的词 (Key)")
    plt.ylabel("关注者 (Query)")
    plt.tight_layout()
    plt.savefig("attention_heatmap.png", dpi=150)
    plt.show()
    print(f"图片已保存: attention_heatmap.png")

# 绘制第一层第一个头的注意力
plot_attention(attentions, tokens, layer=0, head=0)

# ===== 5. 分析注意力模式 =====
print("\n=== 注意力分析 ===")
# 取第一层所有头的平均
avg_attention = attentions[0][0].mean(dim=0).numpy()

# 找出每个词最关注的词
for i, token in enumerate(tokens):
    if token in ["[CLS]", "[SEP]"]:
        continue
    top_idx = avg_attention[i].argsort()[-3:][::-1]  # 前3个
    top_tokens = [tokens[j] for j in top_idx]
    top_scores = [avg_attention[i][j] for j in top_idx]
    print(f"'{token}' 最关注: {list(zip(top_tokens, [f'{s:.2f}' for s in top_scores]))}")
```

**运行输出示例：**
```
=== 加载模型 ===
模型: bert-base-chinese
注意力头数: 12
隐藏层数: 12

=== 输入文本 ===
原文: 小猫坐在垫子上
分词结果: ['[CLS]', '小', '猫', '坐', '在', '垫', '子', '上', '[SEP]']

=== 获取注意力权重 ===
层数: 12
每层形状: torch.Size([1, 12, 9, 9])

=== 注意力分析 ===
'小' 最关注: [('猫', '0.45'), ('小', '0.20'), ('坐', '0.12')]
'猫' 最关注: [('小', '0.38'), ('坐', '0.25'), ('在', '0.15')]
'坐' 最关注: [('在', '0.35'), ('猫', '0.22'), ('垫', '0.18')]
...
```

---

## 示例2：对比不同句子的注意力

```python
"""
对比不同句子的注意力模式
演示：词序变化如何影响注意力
"""

import torch
from transformers import AutoTokenizer, AutoModel

# 加载模型
model_name = "bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)

def get_attention_summary(text):
    """获取文本的注意力摘要"""
    inputs = tokenizer(text, return_tensors="pt")
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    with torch.no_grad():
        outputs = model(**inputs)

    # 取最后一层的平均注意力
    avg_attention = outputs.attentions[-1][0].mean(dim=0).numpy()

    return tokens, avg_attention

# ===== 对比两个句子 =====
print("=== 对比词序变化的影响 ===\n")

sentences = [
    "狗咬人",
    "人咬狗"
]

for sent in sentences:
    tokens, attention = get_attention_summary(sent)
    print(f"句子: {sent}")
    print(f"分词: {tokens}")

    # 显示每个词的注意力分布
    for i, token in enumerate(tokens):
        if token in ["[CLS]", "[SEP]"]:
            continue
        scores = attention[i][1:-1]  # 去掉 CLS 和 SEP
        content_tokens = tokens[1:-1]
        print(f"  '{token}' 的注意力: ", end="")
        for t, s in zip(content_tokens, scores):
            print(f"{t}:{s:.2f} ", end="")
        print()
    print()
```

**运行输出示例：**
```
=== 对比词序变化的影响 ===

句子: 狗咬人
分词: ['[CLS]', '狗', '咬', '人', '[SEP]']
  '狗' 的注意力: 狗:0.35 咬:0.40 人:0.25
  '咬' 的注意力: 狗:0.30 咬:0.25 人:0.45
  '人' 的注意力: 狗:0.20 咬:0.50 人:0.30

句子: 人咬狗
分词: ['[CLS]', '人', '咬', '狗', '[SEP]']
  '人' 的注意力: 人:0.35 咬:0.42 狗:0.23
  '咬' 的注意力: 人:0.32 咬:0.23 狗:0.45
  '狗' 的注意力: 人:0.22 咬:0.48 狗:0.30
```

**观察：** 虽然词相同，但位置不同导致注意力模式不同！

---

## 示例3：RAG 场景 - 上下文注入的影响

```python
"""
RAG 场景：观察注入的上下文是否被“问题词”关注
说明：这里用 BERT 的 sentence-pair 输入 (question, context)
"""

import torch
from transformers import AutoTokenizer, AutoModel

# 使用英文模型（更好的演示效果）
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)
model.eval()

def analyze_rag_attention(question, context):
    """分析 RAG 场景下的注意力（question -> context）"""
    # BERT pair: [CLS] question [SEP] context [SEP]
    inputs = tokenizer(
        question,
        context,
        return_tensors="pt",
        max_length=512,
        truncation=True,
        return_token_type_ids=True,
    )
    input_ids = inputs["input_ids"][0]
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    token_type_ids = inputs["token_type_ids"][0].tolist()  # 0=question, 1=context

    with torch.no_grad():
        outputs = model(**inputs)

    # 取最后一层的平均注意力
    # (seq_len, seq_len) - 行表示 query token，列表示 key token
    attention = outputs.attentions[-1][0].mean(dim=0).cpu().numpy()

    return tokens, token_type_ids, attention

# ===== RAG 示例 =====
print("=== RAG 注意力分析 ===\n")

question = "What color is the apple?"
context = "The apple on the table is red and fresh."

tokens, token_type_ids, attention = analyze_rag_attention(question, context)

print(f"问题: {question}")
print(f"上下文: {context}")
print(f"\n分词结果: {tokens}")

# 找到问题中关键词的位置
question_keywords = ["color", "apple"]

# 只在 question 段里找 query token，只在 context 段里挑 key token
question_positions = [
    i for i, (tok, tt) in enumerate(zip(tokens, token_type_ids))
    if tt == 0 and tok not in ("[CLS]", "[SEP]")
]
context_positions = [
    i for i, (tok, tt) in enumerate(zip(tokens, token_type_ids))
    if tt == 1 and tok not in ("[CLS]", "[SEP]")
]

print("\n=== 问题词对上下文的注意力（只看 context 段） ===")
for q_word in question_keywords:
    # 找到问题词的位置
    q_positions = [
        i for i in question_positions
        if q_word in tokens[i].lower().replace("##", "")
    ]
    if not q_positions:
        continue

    # 如果一个词被拆成多个 wordpiece，这里对它们的 attention 做平均
    att_scores = attention[q_positions].mean(axis=0)
    top_context = sorted(
        [(i, att_scores[i]) for i in context_positions],
        key=lambda x: x[1],
        reverse=True,
    )[:5]

    print(f"\n'{q_word}' 关注的上下文词:")
    for idx, score in top_context:
        print(f"  → '{tokens[idx]}': {score:.3f}")

print("\n=== 提示 ===")
print("注意力只是模型内部信号，不等价于可解释性；不同层/不同 head 的模式也会不同。")
```

**运行输出示例：**
```
=== RAG 注意力分析 ===

问题: What color is the apple?
上下文: The apple on the table is red and fresh.

分词结果: ['[CLS]', 'what', 'color', 'is', 'the', 'apple', '?', '[SEP]',
           'the', 'apple', 'on', 'the', 'table', 'is', 'red', 'and',
           'fresh', '.', '[SEP]']

=== 问题词对上下文的注意力（只看 context 段） ===

'color' 关注的上下文词:
  → 'red': 0.185
  → 'apple': 0.142
  → 'fresh': 0.098

'apple' 关注的上下文词:
  → 'apple': 0.312
  → 'red': 0.156
  → 'table': 0.089

=== 提示 ===
注意力只是模型内部信号，不等价于可解释性；不同层/不同 head 的模式也会不同。
```

---

## 示例4：简化版 - 不需要 GPU

```python
"""
简化版：使用小模型，CPU 即可运行
适合快速体验注意力机制
"""

from transformers import AutoTokenizer, AutoModel
import torch

# 使用小模型
model_name = "prajjwal1/bert-tiny"  # 只有 4.4M 参数
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, output_attentions=True)

def quick_attention_demo(text):
    """快速演示注意力"""
    inputs = tokenizer(text, return_tensors="pt")
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    with torch.no_grad():
        outputs = model(**inputs)

    # 获取注意力
    attention = outputs.attentions[0][0, 0].numpy()  # 第一层第一个头

    print(f"输入: {text}")
    print(f"分词: {tokens}")
    print(f"\n注意力矩阵 (Layer 0, Head 0):")
    print("-" * 50)

    # 打印表头
    header = "      " + " ".join([f"{t[:6]:>6}" for t in tokens])
    print(header)

    # 打印每行
    for i, token in enumerate(tokens):
        row = f"{token[:6]:>6} " + " ".join([f"{attention[i,j]:>6.2f}" for j in range(len(tokens))])
        print(row)

# 运行演示
quick_attention_demo("I love coding")
print("\n" + "="*50 + "\n")
quick_attention_demo("The cat sat on mat")
```

**运行输出示例：**
```
输入: I love coding
分词: ['[CLS]', 'i', 'love', 'coding', '[SEP]']

注意力矩阵 (Layer 0, Head 0):
--------------------------------------------------
        [CLS]      i   love coding  [SEP]
 [CLS]   0.15   0.20   0.25   0.22   0.18
     i   0.18   0.12   0.35   0.20   0.15
  love   0.12   0.28   0.18   0.30   0.12
coding   0.10   0.15   0.32   0.25   0.18
 [SEP]   0.20   0.12   0.18   0.22   0.28
```

---

## 关键观察

通过这些代码，你应该观察到：

1. **注意力是动态的**：不同的词关注不同的其他词
2. **位置有影响**：相同的词在不同位置，注意力模式不同
3. **语义相关性**：语义相关的词之间注意力更高
4. **RAG 的原理**：问题词会关注上下文中的相关内容

---

## 练习建议

1. 修改输入文本，观察注意力变化
2. 尝试不同的层和头，看看有什么区别
3. 构造 RAG 场景，验证检索内容是否被关注
4. 对比中英文模型的注意力模式

---

**下一步：** [08_面试必问](./08_面试必问.md) - 学会如何答出彩
