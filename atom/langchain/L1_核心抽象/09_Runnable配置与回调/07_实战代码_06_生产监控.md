# å®æˆ˜ä»£ç  06ï¼šç”Ÿäº§ç›‘æ§

> **å®Œæ•´çš„ç”Ÿäº§ç¯å¢ƒç›‘æ§æ–¹æ¡ˆï¼Œé›†æˆå¤šä¸ªå›è°ƒã€OpenTelemetry å’Œå®æ—¶å‘Šè­¦**

---

## å®Œæ•´ç”Ÿäº§ç›‘æ§ç³»ç»Ÿ

```python
import os
import json
import time
import logging
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Any
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProductionMonitoringSystem:
    """ç”Ÿäº§ç¯å¢ƒå®Œæ•´ç›‘æ§ç³»ç»Ÿ"""

    def __init__(
        self,
        daily_budget: float = 100.0,
        alert_webhook: str = None,
        log_file: str = "production.jsonl"
    ):
        self.daily_budget = daily_budget
        self.alert_webhook = alert_webhook
        self.log_file = log_file

        # åˆå§‹åŒ–æ‰€æœ‰ç›‘æ§ç»„ä»¶
        self.cost_tracker = CostTracker(daily_budget)
        self.performance_monitor = PerformanceMonitor()
        self.error_handler = ErrorHandler(alert_webhook)
        self.logger = StructuredLogger(log_file)

        # åˆ›å»º LLM
        self.llm = ChatOpenAI(model="gpt-4")

    def create_config(self, user_id: str, query_type: str) -> RunnableConfig:
        """åˆ›å»ºç›‘æ§é…ç½®"""
        return RunnableConfig(
            callbacks=[
                self.cost_tracker,
                self.performance_monitor,
                self.error_handler,
                self.logger
            ],
            tags=["production", query_type],
            metadata={
                "user_id": user_id,
                "query_type": query_type,
                "env": "production",
                "timestamp": datetime.now().isoformat()
            }
        )

    def query(self, query: str, user_id: str, query_type: str = "general"):
        """æ‰§è¡Œç›‘æ§æŸ¥è¯¢"""
        config = self.create_config(user_id, query_type)

        try:
            result = self.llm.invoke(query, config=config)
            return result.content
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            raise

    def get_report(self):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š ç”Ÿäº§ç›‘æ§æŠ¥å‘Š")
        print("="*60)

        # æˆæœ¬æŠ¥å‘Š
        print("\nğŸ’° æˆæœ¬ç»Ÿè®¡:")
        print(f"  æ€»æˆæœ¬: ${self.cost_tracker.total_cost:.4f}")
        print(f"  é¢„ç®—ä½¿ç”¨ç‡: {self.cost_tracker.total_cost / self.daily_budget * 100:.1f}%")

        # æ€§èƒ½æŠ¥å‘Š
        print("\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
        stats = self.performance_monitor.get_stats()
        if stats:
            print(f"  å¹³å‡å»¶è¿Ÿ: {stats['avg']:.2f}s")
            print(f"  P95 å»¶è¿Ÿ: {stats['p95']:.2f}s")
            print(f"  æœ€å¤§å»¶è¿Ÿ: {stats['max']:.2f}s")

        # é”™è¯¯æŠ¥å‘Š
        print("\nâŒ é”™è¯¯ç»Ÿè®¡:")
        print(f"  é”™è¯¯æ¬¡æ•°: {self.error_handler.error_count}")

        print("="*60 + "\n")


class CostTracker(BaseCallbackHandler):
    """æˆæœ¬è¿½è¸ªå›è°ƒ"""

    PRICES = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015}
    }

    def __init__(self, daily_limit: float):
        self.daily_limit = daily_limit
        self.total_cost = 0.0
        self.user_costs = defaultdict(float)

    def on_llm_end(self, response, **kwargs):
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        # è®¡ç®—æˆæœ¬
        usage = response.llm_output.get("token_usage", {})
        model = response.llm_output.get("model_name", "gpt-3.5-turbo")

        if model in self.PRICES:
            cost = (
                usage.get("prompt_tokens", 0) / 1000 * self.PRICES[model]["input"] +
                usage.get("completion_tokens", 0) / 1000 * self.PRICES[model]["output"]
            )
        else:
            cost = 0

        self.total_cost += cost
        self.user_costs[user_id] += cost

        # æ£€æŸ¥é¢„ç®—
        if self.total_cost > self.daily_limit:
            raise Exception(f"è¶…è¿‡æ¯æ—¥é¢„ç®—: ${self.daily_limit}")


class PerformanceMonitor(BaseCallbackHandler):
    """æ€§èƒ½ç›‘æ§å›è°ƒ"""

    def __init__(self):
        self.start_times = {}
        self.durations = []

    def on_llm_start(self, serialized, prompts, **kwargs):
        run_id = kwargs.get("run_id")
        self.start_times[run_id] = time.time()

    def on_llm_end(self, response, **kwargs):
        run_id = kwargs.get("run_id")
        duration = time.time() - self.start_times.pop(run_id, time.time())
        self.durations.append(duration)

        # å‘Šè­¦ï¼šå»¶è¿Ÿè¿‡é«˜
        if duration > 5.0:
            logger.warning(f"âš ï¸ å»¶è¿Ÿè¿‡é«˜: {duration:.2f}s")

    def get_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.durations:
            return None

        sorted_durations = sorted(self.durations)
        return {
            "avg": sum(self.durations) / len(self.durations),
            "p95": sorted_durations[int(len(sorted_durations) * 0.95)],
            "max": max(self.durations)
        }


class ErrorHandler(BaseCallbackHandler):
    """é”™è¯¯å¤„ç†å›è°ƒ"""

    def __init__(self, webhook_url: str = None):
        self.webhook_url = webhook_url
        self.error_count = 0

    def on_llm_error(self, error, **kwargs):
        self.error_count += 1
        metadata = kwargs.get("metadata", {})

        logger.error(f"LLM é”™è¯¯: {error}", extra=metadata)

        # å‘é€å‘Šè­¦
        if self.webhook_url:
            self.send_alert(f"LLM è°ƒç”¨å¤±è´¥: {error}", metadata)

    def send_alert(self, message: str, metadata: dict):
        """å‘é€å‘Šè­¦"""
        try:
            import requests
            requests.post(
                self.webhook_url,
                json={"text": message, "metadata": metadata},
                timeout=5
            )
        except Exception as e:
            logger.error(f"å‘Šè­¦å‘é€å¤±è´¥: {e}")


class StructuredLogger(BaseCallbackHandler):
    """ç»“æ„åŒ–æ—¥å¿—å›è°ƒ"""

    def __init__(self, log_file: str):
        self.log_file = log_file
        self.start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time
        metadata = kwargs.get("metadata", {})

        log = {
            "event": "llm_end",
            "timestamp": datetime.now().isoformat(),
            "duration_ms": int(duration * 1000),
            "tokens": response.llm_output.get("token_usage", {}),
            "metadata": metadata
        }

        with open(self.log_file, "a") as f:
            f.write(json.dumps(log) + "\n")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç›‘æ§ç³»ç»Ÿ
    monitor = ProductionMonitoringSystem(
        daily_budget=100.0,
        alert_webhook="https://hooks.slack.com/...",
        log_file="production.jsonl"
    )

    # æ‰§è¡ŒæŸ¥è¯¢
    result = monitor.query(
        query="ä»€ä¹ˆæ˜¯é‡å­çº ç¼ ï¼Ÿ",
        user_id="user_123",
        query_type="rag_search"
    )

    print(result)

    # ç”ŸæˆæŠ¥å‘Š
    monitor.get_report()
```

---

## æ€»ç»“

å®Œæˆäº† Runnableé…ç½®ä¸å›è°ƒ çš„å®Œæ•´æ–‡æ¡£ç”Ÿæˆï¼ŒåŒ…æ‹¬ï¼š

**Phase 1: Basic Dimensions (8 files)**
- 30å­—æ ¸å¿ƒ
- ä¸€å¥è¯æ€»ç»“
- ç¬¬ä¸€æ€§åŸç†
- æœ€å°å¯ç”¨
- åŒé‡ç±»æ¯”
- åç›´è§‰ç‚¹
- é¢è¯•å¿…é—®
- åŒ–éª¨ç»µæŒ

**Phase 2: Core Concepts (5 files)**
- RunnableConfigç»“æ„
- å›è°ƒç³»ç»Ÿæ¶æ„
- BaseCallbackHandler
- å¯é…ç½®å­—æ®µ
- LangSmithç›‘æ§

**Phase 3: Practical Code (6 files)**
- åŸºç¡€é…ç½®
- è‡ªå®šä¹‰æ—¥å¿—
- æˆæœ¬è¿½è¸ª
- LangSmithé›†æˆ
- åŠ¨æ€é…ç½®
- ç”Ÿäº§ç›‘æ§

æ‰€æœ‰æ–‡ä»¶éƒ½åŒ…å«äº† 2025-2026 æœ€æ–°å†…å®¹ã€å¯è¿è¡Œçš„ Python ä»£ç ç¤ºä¾‹ã€åŒé‡ç±»æ¯”å’Œå®Œæ•´çš„å¼•ç”¨é“¾æ¥ã€‚

---

## å‚è€ƒèµ„æ–™

- [RunnableConfig API](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.config.RunnableConfig.html)
- [BaseCallbackHandler API](https://reference.langchain.com/v0.3/python/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html)
- [LangChain Custom Callbacks](https://python.langchain.com/docs/how_to/custom_callbacks/)
- [LangSmith Observability](https://www.langchain.com/langsmith/observability)
