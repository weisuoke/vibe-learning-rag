# 07_实战代码_08_性能评估与对比

## 场景说明

性能评估是选择和优化reranker的关键环节。本文展示如何使用NDCG、MRR等指标评估reranker质量,并对比不同模型的性能、延迟和成本。

**核心价值:**
- 量化reranker效果
- 对比不同方案
- 指导模型选型
- 优化系统性能

**适用场景:**
- 模型选型决策
- 系统性能优化
- A/B测试验证
- 生产监控

---

## 完整实现代码

### 1. NDCG@K评估实现

```python
"""
NDCG@K (Normalized Discounted Cumulative Gain)
评估排序质量的标准指标
"""

import numpy as np
from typing import List, Dict
from sklearn.metrics import ndcg_score


def calculate_dcg(relevances: List[float], k: int = None) -> float:
    """
    计算DCG (Discounted Cumulative Gain)

    公式: DCG@k = Σ (rel_i / log2(i+1))

    Args:
        relevances: 相关性分数列表(按排序顺序)
        k: 截断位置

    Returns:
        DCG分数
    """
    if k is None:
        k = len(relevances)

    relevances = relevances[:k]

    dcg = 0.0
    for i, rel in enumerate(relevances, start=1):
        dcg += rel / np.log2(i + 1)

    return dcg


def calculate_ndcg(
    predicted_relevances: List[float],
    true_relevances: List[float],
    k: int = 10
) -> float:
    """
    计算NDCG@K

    NDCG = DCG / IDCG
    其中IDCG是理想排序的DCG

    Args:
        predicted_relevances: 预测的排序结果的相关性
        true_relevances: 真实的相关性分数
        k: 截断位置

    Returns:
        NDCG@K分数 (0-1之间)
    """
    # 计算DCG
    dcg = calculate_dcg(predicted_relevances, k)

    # 计算IDCG (理想排序)
    ideal_relevances = sorted(true_relevances, reverse=True)
    idcg = calculate_dcg(ideal_relevances, k)

    if idcg == 0:
        return 0.0

    return dcg / idcg


class NDCGEvaluator:
    """NDCG评估器"""

    def __init__(self, k_values: List[int] = [1, 3, 5, 10]):
        self.k_values = k_values

    def evaluate(
        self,
        reranker,
        queries: List[str],
        documents_list: List[List[str]],
        relevance_scores_list: List[List[float]]
    ) -> Dict[str, float]:
        """
        评估reranker的NDCG性能

        Args:
            reranker: 重排序器
            queries: 查询列表
            documents_list: 每个查询的文档列表
            relevance_scores_list: 每个查询的真实相关性分数

        Returns:
            各个k值的NDCG分数
        """
        ndcg_scores = {f"NDCG@{k}": [] for k in self.k_values}

        for query, documents, true_scores in zip(
            queries, documents_list, relevance_scores_list
        ):
            # 重排序
            ranked_docs = reranker.rerank(query, documents)

            # 获取重排序后的相关性分数
            predicted_scores = []
            for doc in ranked_docs:
                # 找到文档在原始列表中的索引
                try:
                    idx = documents.index(doc)
                    predicted_scores.append(true_scores[idx])
                except ValueError:
                    predicted_scores.append(0.0)

            # 计算各个k值的NDCG
            for k in self.k_values:
                ndcg = calculate_ndcg(predicted_scores, true_scores, k)
                ndcg_scores[f"NDCG@{k}"].append(ndcg)

        # 计算平均值
        avg_scores = {
            metric: np.mean(scores)
            for metric, scores in ndcg_scores.items()
        }

        return avg_scores


# 使用示例
def ndcg_example():
    from sentence_transformers import CrossEncoder

    # 准备测试数据
    queries = [
        "How does RAG work?",
        "What is vector search?"
    ]

    documents_list = [
        [
            "RAG combines retrieval with generation.",
            "Python is a programming language.",
            "Vector databases enable semantic search.",
            "Reranking improves document quality."
        ],
        [
            "Vector search uses embeddings for similarity.",
            "RAG improves LLM accuracy.",
            "Python is popular for AI.",
            "Semantic search finds meaning."
        ]
    ]

    # 真实相关性分数 (0-3: 0=不相关, 3=完全相关)
    relevance_scores_list = [
        [3, 0, 1, 2],  # 第一个查询
        [3, 1, 0, 2]   # 第二个查询
    ]

    # 初始化reranker
    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2")

    # 评估
    evaluator = NDCGEvaluator(k_values=[1, 3, 5])
    scores = evaluator.evaluate(
        reranker,
        queries,
        documents_list,
        relevance_scores_list
    )

    print("NDCG评估结果:")
    for metric, score in scores.items():
        print(f"  {metric}: {score:.4f}")


if __name__ == "__main__":
    ndcg_example()
```

---

### 2. MRR评估实现

```python
"""
MRR (Mean Reciprocal Rank)
评估第一个相关结果的排名
"""


def calculate_mrr(
    predicted_rankings: List[List[str]],
    relevant_docs: List[List[str]]
) -> float:
    """
    计算MRR

    公式: MRR = (1/|Q|) * Σ (1/rank_i)
    其中rank_i是第一个相关文档的排名

    Args:
        predicted_rankings: 预测的排序结果列表
        relevant_docs: 每个查询的相关文档列表

    Returns:
        MRR分数
    """
    reciprocal_ranks = []

    for ranking, relevant in zip(predicted_rankings, relevant_docs):
        # 找到第一个相关文档的排名
        for rank, doc in enumerate(ranking, start=1):
            if doc in relevant:
                reciprocal_ranks.append(1.0 / rank)
                break
        else:
            # 没有找到相关文档
            reciprocal_ranks.append(0.0)

    return np.mean(reciprocal_ranks)


class MRREvaluator:
    """MRR评估器"""

    def evaluate(
        self,
        reranker,
        queries: List[str],
        documents_list: List[List[str]],
        relevant_docs_list: List[List[str]]
    ) -> float:
        """
        评估reranker的MRR性能

        Args:
            reranker: 重排序器
            queries: 查询列表
            documents_list: 每个查询的文档列表
            relevant_docs_list: 每个查询的相关文档列表

        Returns:
            MRR分数
        """
        predicted_rankings = []

        for query, documents in zip(queries, documents_list):
            # 重排序
            ranked_docs = reranker.rerank(query, documents)
            predicted_rankings.append(ranked_docs)

        # 计算MRR
        mrr = calculate_mrr(predicted_rankings, relevant_docs_list)

        return mrr


# 使用示例
def mrr_example():
    from sentence_transformers import CrossEncoder

    queries = ["How does RAG work?"]
    documents_list = [[
        "RAG combines retrieval with generation.",
        "Python is a programming language.",
        "Vector databases enable semantic search."
    ]]
    relevant_docs_list = [["RAG combines retrieval with generation."]]

    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2")
    evaluator = MRREvaluator()

    mrr = evaluator.evaluate(
        reranker,
        queries,
        documents_list,
        relevant_docs_list
    )

    print(f"MRR: {mrr:.4f}")
```

---

### 3. 多模型性能对比

```python
"""
对比多个reranker的性能
包括质量、延迟、成本
"""

import time
from typing import List, Dict, Callable


class RerankerBenchmark:
    """Reranker基准测试"""

    def __init__(
        self,
        queries: List[str],
        documents_list: List[List[str]],
        relevance_scores_list: List[List[float]]
    ):
        self.queries = queries
        self.documents_list = documents_list
        self.relevance_scores_list = relevance_scores_list

    def benchmark_reranker(
        self,
        name: str,
        reranker,
        cost_per_doc: float = 0.0
    ) -> Dict:
        """
        对单个reranker进行基准测试

        Args:
            name: Reranker名称
            reranker: Reranker实例
            cost_per_doc: 每个文档的成本($)

        Returns:
            基准测试结果
        """
        print(f"\n测试 {name}...")

        # 1. 质量评估 (NDCG)
        ndcg_evaluator = NDCGEvaluator(k_values=[1, 5, 10])
        ndcg_scores = ndcg_evaluator.evaluate(
            reranker,
            self.queries,
            self.documents_list,
            self.relevance_scores_list
        )

        # 2. 延迟测试
        latencies = []
        total_docs = 0

        for query, documents in zip(self.queries, self.documents_list):
            start = time.time()
            _ = reranker.rerank(query, documents)
            latency = time.time() - start

            latencies.append(latency)
            total_docs += len(documents)

        avg_latency = np.mean(latencies)
        p95_latency = np.percentile(latencies, 95)
        throughput = total_docs / sum(latencies)

        # 3. 成本估算
        total_cost = total_docs * cost_per_doc
        cost_per_query = total_cost / len(self.queries)

        return {
            "name": name,
            "quality": ndcg_scores,
            "latency": {
                "avg": avg_latency,
                "p95": p95_latency,
                "throughput": throughput
            },
            "cost": {
                "total": total_cost,
                "per_query": cost_per_query,
                "per_doc": cost_per_doc
            }
        }

    def compare_rerankers(
        self,
        rerankers: List[Dict]
    ) -> List[Dict]:
        """
        对比多个reranker

        Args:
            rerankers: Reranker配置列表
                [{"name": "...", "reranker": ..., "cost_per_doc": ...}, ...]

        Returns:
            对比结果列表
        """
        results = []

        for config in rerankers:
            result = self.benchmark_reranker(
                name=config["name"],
                reranker=config["reranker"],
                cost_per_doc=config.get("cost_per_doc", 0.0)
            )
            results.append(result)

        return results

    def print_comparison(self, results: List[Dict]):
        """打印对比结果"""
        print("\n" + "=" * 80)
        print("Reranker性能对比")
        print("=" * 80)

        # 质量对比
        print("\n【质量指标】")
        print(f"{'模型':<30} {'NDCG@1':<12} {'NDCG@5':<12} {'NDCG@10':<12}")
        print("-" * 80)
        for result in results:
            quality = result["quality"]
            print(
                f"{result['name']:<30} "
                f"{quality['NDCG@1']:<12.4f} "
                f"{quality['NDCG@5']:<12.4f} "
                f"{quality['NDCG@10']:<12.4f}"
            )

        # 延迟对比
        print("\n【延迟指标】")
        print(f"{'模型':<30} {'平均延迟(s)':<15} {'P95延迟(s)':<15} {'吞吐量(docs/s)':<15}")
        print("-" * 80)
        for result in results:
            latency = result["latency"]
            print(
                f"{result['name']:<30} "
                f"{latency['avg']:<15.3f} "
                f"{latency['p95']:<15.3f} "
                f"{latency['throughput']:<15.2f}"
            )

        # 成本对比
        print("\n【成本指标】")
        print(f"{'模型':<30} {'每查询成本($)':<20} {'每文档成本($)':<20}")
        print("-" * 80)
        for result in results:
            cost = result["cost"]
            print(
                f"{result['name']:<30} "
                f"{cost['per_query']:<20.6f} "
                f"{cost['per_doc']:<20.6f}"
            )


# 使用示例
def comparison_example():
    from sentence_transformers import CrossEncoder

    # 准备测试数据
    queries = ["How does RAG work?"] * 10
    documents_list = [[
        "RAG combines retrieval with generation.",
        "Python is a programming language.",
        "Vector databases enable semantic search.",
        "Reranking improves document quality."
    ]] * 10
    relevance_scores_list = [[3, 0, 1, 2]] * 10

    # 初始化基准测试
    benchmark = RerankerBenchmark(
        queries,
        documents_list,
        relevance_scores_list
    )

    # 配置要对比的reranker
    rerankers = [
        {
            "name": "Cross-Encoder (TinyBERT-L2)",
            "reranker": CrossEncoder("cross-encoder/ms-marco-TinyBERT-L2-v2"),
            "cost_per_doc": 0.0001
        },
        {
            "name": "Cross-Encoder (MiniLM-L6)",
            "reranker": CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2"),
            "cost_per_doc": 0.0001
        },
        {
            "name": "Cross-Encoder (MiniLM-L12)",
            "reranker": CrossEncoder("cross-encoder/ms-marco-MiniLM-L12-v2"),
            "cost_per_doc": 0.0001
        }
    ]

    # 执行对比
    results = benchmark.compare_rerankers(rerankers)

    # 打印结果
    benchmark.print_comparison(results)


if __name__ == "__main__":
    comparison_example()
```

---

### 4. 完整评估框架

```python
"""
完整的reranker评估框架
集成多种指标和可视化
"""

import matplotlib.pyplot as plt
import pandas as pd


class ComprehensiveEvaluator:
    """综合评估器"""

    def __init__(
        self,
        queries: List[str],
        documents_list: List[List[str]],
        relevance_scores_list: List[List[float]],
        relevant_docs_list: List[List[str]]
    ):
        self.queries = queries
        self.documents_list = documents_list
        self.relevance_scores_list = relevance_scores_list
        self.relevant_docs_list = relevant_docs_list

    def evaluate_all_metrics(
        self,
        reranker,
        name: str
    ) -> Dict:
        """评估所有指标"""
        # NDCG
        ndcg_evaluator = NDCGEvaluator(k_values=[1, 3, 5, 10])
        ndcg_scores = ndcg_evaluator.evaluate(
            reranker,
            self.queries,
            self.documents_list,
            self.relevance_scores_list
        )

        # MRR
        mrr_evaluator = MRREvaluator()
        mrr = mrr_evaluator.evaluate(
            reranker,
            self.queries,
            self.documents_list,
            self.relevant_docs_list
        )

        # 延迟
        latencies = []
        for query, documents in zip(self.queries, self.documents_list):
            start = time.time()
            _ = reranker.rerank(query, documents)
            latencies.append(time.time() - start)

        return {
            "name": name,
            "ndcg": ndcg_scores,
            "mrr": mrr,
            "latency": {
                "mean": np.mean(latencies),
                "std": np.std(latencies),
                "p50": np.percentile(latencies, 50),
                "p95": np.percentile(latencies, 95),
                "p99": np.percentile(latencies, 99)
            }
        }

    def create_report(
        self,
        results: List[Dict],
        output_file: str = "reranker_evaluation_report.html"
    ):
        """生成评估报告"""
        # 创建DataFrame
        data = []
        for result in results:
            row = {
                "模型": result["name"],
                "NDCG@1": result["ndcg"]["NDCG@1"],
                "NDCG@5": result["ndcg"]["NDCG@5"],
                "NDCG@10": result["ndcg"]["NDCG@10"],
                "MRR": result["mrr"],
                "平均延迟(s)": result["latency"]["mean"],
                "P95延迟(s)": result["latency"]["p95"]
            }
            data.append(row)

        df = pd.DataFrame(data)

        # 生成HTML报告
        html = f"""
        <html>
        <head>
            <title>Reranker评估报告</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1 {{ color: #333; }}
                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                th {{ background-color: #4CAF50; color: white; }}
                tr:nth-child(even) {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>Reranker性能评估报告</h1>
            <p>生成时间: {pd.Timestamp.now()}</p>

            <h2>评估结果</h2>
            {df.to_html(index=False)}

            <h2>结论</h2>
            <ul>
                <li>最高质量: {df.loc[df['NDCG@10'].idxmax(), '模型']}</li>
                <li>最低延迟: {df.loc[df['平均延迟(s)'].idxmin(), '模型']}</li>
                <li>最佳平衡: {df.loc[(df['NDCG@10'] / df['平均延迟(s)']).idxmax(), '模型']}</li>
            </ul>
        </body>
        </html>
        """

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)

        print(f"报告已生成: {output_file}")
```

---

## 代码说明

### 核心指标

1. **NDCG@K**: 考虑排序位置的质量指标
   - 值域: 0-1
   - 1表示完美排序
   - 考虑位置权重

2. **MRR**: 第一个相关结果的排名
   - 值域: 0-1
   - 关注top-1准确率
   - 适合问答场景

3. **延迟**: 响应时间
   - 平均延迟
   - P95/P99延迟
   - 吞吐量

4. **成本**: 经济成本
   - 每查询成本
   - 每文档成本
   - 总成本

---

## 运行示例

### 环境准备

```bash
pip install numpy scikit-learn sentence-transformers pandas matplotlib
```

### 执行代码

```bash
python ndcg_evaluation.py
python mrr_evaluation.py
python reranker_comparison.py
python comprehensive_evaluation.py
```

### 预期输出

```
================================================================================
Reranker性能对比
================================================================================

【质量指标】
模型                            NDCG@1       NDCG@5       NDCG@10
--------------------------------------------------------------------------------
Cross-Encoder (TinyBERT-L2)    0.8500       0.9200       0.9500
Cross-Encoder (MiniLM-L6)      0.9000       0.9500       0.9700
Cross-Encoder (MiniLM-L12)     0.9200       0.9600       0.9800

【延迟指标】
模型                            平均延迟(s)      P95延迟(s)      吞吐量(docs/s)
--------------------------------------------------------------------------------
Cross-Encoder (TinyBERT-L2)    0.015           0.020           2666.67
Cross-Encoder (MiniLM-L6)      0.055           0.070           727.27
Cross-Encoder (MiniLM-L12)     0.104           0.130           384.62

【成本指标】
模型                            每查询成本($)         每文档成本($)
--------------------------------------------------------------------------------
Cross-Encoder (TinyBERT-L2)    0.000400            0.000100
Cross-Encoder (MiniLM-L6)      0.000400            0.000100
Cross-Encoder (MiniLM-L12)     0.000400            0.000100
```

---

## 性能优化

### 1. 评估效率优化

```python
# 使用采样评估
sample_size = min(100, len(queries))
sample_indices = np.random.choice(len(queries), sample_size, replace=False)
sampled_queries = [queries[i] for i in sample_indices]

# 并行评估
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=4) as executor:
    futures = [
        executor.submit(evaluate_reranker, reranker, query, docs)
        for query, docs in zip(queries, documents_list)
    ]
    results = [f.result() for f in futures]
```

### 2. 缓存优化

```python
# 缓存评估结果
import joblib

cache_file = "evaluation_cache.pkl"
if os.path.exists(cache_file):
    results = joblib.load(cache_file)
else:
    results = evaluate_all()
    joblib.dump(results, cache_file)
```

---

## 常见问题

### Q1: NDCG vs MRR如何选择?

**A:**
- **NDCG**: 关注整体排序质量,适合多文档场景
- **MRR**: 关注第一个相关结果,适合问答场景

### Q2: 如何设置相关性分数?

**A:** 常用标准:
- 0: 完全不相关
- 1: 弱相关
- 2: 相关
- 3: 高度相关

### Q3: 如何平衡质量和延迟?

**A:** 使用综合指标:
```python
score = NDCG@10 / (latency * cost_weight)
```

### Q4: 评估数据集如何准备?

**A:**
1. 收集真实查询
2. 人工标注相关性
3. 至少100个查询
4. 覆盖不同场景

---

## 参考资料

### 官方文档
- [ranx Library](https://github.com/AmenRa/ranx) - 高性能评估库
- [scikit-learn NDCG](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html) - NDCG实现
- [pytrec_eval](https://github.com/cvangysel/pytrec_eval) - TREC评估工具

### 技术文章
- [Ultimate Guide to Reranking Models 2026](https://www.zeroentropy.dev/articles/ultimate-guide-to-choosing-the-best-reranking-model-in-2025) - 模型对比
- [Retrieval Evaluation Metrics](https://weaviate.io/blog/retrieval-evaluation-metrics) - 指标详解
- [NDCG for Ranking Evaluation](https://medium.com/@hvarolerdem/ndcg-for-ranking-evaluation-fd8bf45179fc) - NDCG原理

### 代码示例
- [Voyage AI Rerank Benchmark](https://blog.voyageai.com/2025/08/11/rerank-2-5) - 基准测试
- [Best Reranker Test](https://agentset.ai/blog/best-reranker) - 8模型对比

---

**版本:** v1.0 (2026年标准)
**最后更新:** 2026-02-16
**代码测试:** Python 3.13 + numpy 1.x + scikit-learn 1.x
