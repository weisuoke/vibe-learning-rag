# 最小可用

> 掌握20%核心知识，解决80%问题

---

## 核心理念

掌握以下内容，就能开始实现对话式RAG系统：
- Session管理的基础实现
- 滑动窗口历史管理
- LLM Query重写
- 历史注入到Prompt

**这些知识足以：**
- ✅ 实现一个能记住上下文的RAG聊天机器人
- ✅ 处理简单的指代问题（"它"、"这个"）
- ✅ 避免Context Window溢出
- ✅ 为后续学习高级策略打基础

---

## 核心知识点1：Session管理

### 概念

**Session管理 = 为每个用户维护独立的对话会话**

**核心数据结构：**
```python
sessions = {
    "user_123": [Message1, Message2, Message3],
    "user_456": [Message4, Message5]
}
```

### 最小实现

```python
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List

@dataclass
class Message:
    role: str  # "user" 或 "assistant"
    content: str
    timestamp: datetime

class SimpleSessionManager:
    def __init__(self):
        self.sessions: Dict[str, List[Message]] = {}

    def add_message(self, session_id: str, role: str, content: str):
        """添加消息"""
        if session_id not in self.sessions:
            self.sessions[session_id] = []

        msg = Message(role=role, content=content, timestamp=datetime.now())
        self.sessions[session_id].append(msg)

    def get_history(self, session_id: str) -> List[Message]:
        """获取历史"""
        return self.sessions.get(session_id, [])

# 使用
manager = SimpleSessionManager()
manager.add_message("user_123", "user", "什么是RAG？")
manager.add_message("user_123", "assistant", "RAG是...")
history = manager.get_history("user_123")
```

### 在RAG中的应用

```python
def chat(session_id: str, query: str) -> str:
    # 1. 获取历史
    history = manager.get_history(session_id)

    # 2. RAG处理...
    answer = rag_process(query, history)

    # 3. 保存历史
    manager.add_message(session_id, "user", query)
    manager.add_message(session_id, "assistant", answer)

    return answer
```

**关键要点：**
- Session ID通常是`user_id`或`user_id_timestamp`
- 每个Session独立存储，互不干扰
- 这是对话式RAG的基础

---

## 核心知识点2：滑动窗口历史管理

### 概念

**滑动窗口 = 只保留最近N轮对话，自动删除更早的**

**为什么需要：**
- Context Window有限（如GPT-4的128K tokens）
- 历史越长，成本越高
- 滑动窗口是最简单的控制方法

### 最小实现

```python
class SessionManagerWithWindow:
    def __init__(self, max_history: int = 20):
        self.sessions: Dict[str, List[Message]] = {}
        self.max_history = max_history  # 最多保留20条消息（10轮）

    def add_message(self, session_id: str, role: str, content: str):
        """添加消息（自动滑动窗口）"""
        if session_id not in self.sessions:
            self.sessions[session_id] = []

        msg = Message(role=role, content=content, timestamp=datetime.now())
        self.sessions[session_id].append(msg)

        # 滑动窗口：超过max_history就删除最早的
        if len(self.sessions[session_id]) > self.max_history:
            self.sessions[session_id] = self.sessions[session_id][-self.max_history:]

    def get_recent(self, session_id: str, last_n: int = 5) -> List[Message]:
        """获取最近N轮"""
        messages = self.sessions.get(session_id, [])
        return messages[-(last_n * 2):]  # 每轮=2条消息
```

### 窗口大小选择

```python
# 根据场景选择
window_sizes = {
    "简单问答": 10,      # 5轮
    "文档助手": 20,      # 10轮
    "代码助手": 30,      # 15轮
    "智能客服": 20,      # 10轮
}
```

**关键要点：**
- 窗口太小（<5轮）：丢失重要上下文
- 窗口太大（>20轮）：成本高，可能超限
- 推荐：10-20条消息（5-10轮）

---

## 核心知识点3：LLM Query重写

### 概念

**Query重写 = 使用LLM将含指代的问题重写为独立完整的问题**

**为什么需要：**
```
用户: "它有什么优势？"
向量检索: "它"没有明确语义 → 检索失败

重写后: "RAG有什么优势？"
向量检索: "RAG"有明确语义 → 检索成功
```

### 最小实现

```python
from openai import OpenAI

class SimpleQueryRewriter:
    def __init__(self):
        self.client = OpenAI()

    def rewrite(self, query: str, history: List[Message]) -> str:
        """重写Query"""
        if not history:
            return query  # 没有历史，不需要重写

        # 构建上下文（最近3轮）
        context = "\n".join([
            f"{msg.role}: {msg.content}"
            for msg in history[-6:]  # 最近3轮=6条消息
        ])

        # 调用LLM重写
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{
                "role": "system",
                "content": "根据对话历史，将问题重写为独立完整的问题。只输出重写后的问题。"
            }, {
                "role": "user",
                "content": f"对话历史:\n{context}\n\n当前问题: {query}\n\n重写后:"
            }],
            temperature=0.3,
            max_tokens=100
        )

        return response.choices[0].message.content.strip()

# 使用
rewriter = SimpleQueryRewriter()
history = [
    Message(role="user", content="什么是RAG？", timestamp=datetime.now()),
    Message(role="assistant", content="RAG是...", timestamp=datetime.now()),
]
resolved = rewriter.rewrite("它有什么优势？", history)
# 输出: "RAG有什么优势？"
```

### 在RAG中的应用

```python
def chat_with_resolution(session_id: str, query: str) -> str:
    # 1. 获取历史
    history = manager.get_history(session_id)

    # 2. 重写Query
    resolved_query = rewriter.rewrite(query, history)

    # 3. 使用resolved_query进行检索
    docs = vector_store.search(resolved_query)

    # 4. 生成回答
    answer = llm.generate(resolved_query, docs)

    # 5. 保存历史
    manager.add_message(session_id, "user", query)
    manager.add_message(session_id, "assistant", answer)

    return answer
```

**关键要点：**
- 必须在向量检索前完成
- 使用最近3-5轮历史即可
- 成本：约$0.001-0.01/次

---

## 核心知识点4：历史注入到Prompt

### 概念

**历史注入 = 在生成回答时，将对话历史添加到Prompt中**

**为什么需要：**
- LLM需要知道对话上下文
- 才能生成连贯的回答

### 最小实现

```python
def generate_with_history(query: str, docs: str, history: List[Message]) -> str:
    """生成回答（注入历史）"""
    client = OpenAI()

    # 构建历史文本（最近3轮）
    history_text = "\n".join([
        f"{msg.role}: {msg.content}"
        for msg in history[-6:]
    ])

    # 构建Prompt
    messages = [
        {
            "role": "system",
            "content": "你是一个智能助手，基于对话历史和参考文档回答问题。"
        },
        {
            "role": "system",
            "content": f"对话历史:\n{history_text}"
        },
        {
            "role": "system",
            "content": f"参考文档:\n{docs}"
        },
        {
            "role": "user",
            "content": query
        }
    ]

    # 调用LLM
    response = client.chat.completions.create(
        model="gpt-4",
        messages=messages
    )

    return response.choices[0].message.content

# 使用
history = [...]  # 对话历史
docs = "RAG是检索增强生成..."  # 检索到的文档
answer = generate_with_history("它有什么优势？", docs, history)
```

### 注入策略

```python
# 策略1：只注入最近N轮
recent_history = history[-6:]  # 最近3轮

# 策略2：注入摘要+最近N轮
if len(history) > 10:
    summary = summarize(history[:-6])  # 摘要前面的
    recent = history[-6:]               # 最近3轮
    final_history = [summary] + recent
else:
    final_history = history
```

**关键要点：**
- 注入最近3-5轮即可
- 太多历史会增加成本和噪音
- 历史要放在system message中

---

## 完整最小可用示例

### 整合所有知识点

```python
"""
最小可用对话式RAG系统
包含：Session管理 + 滑动窗口 + Query重写 + 历史注入
"""

from openai import OpenAI
from typing import List, Dict
from dataclasses import dataclass
from datetime import datetime

@dataclass
class Message:
    role: str
    content: str
    timestamp: datetime

class MinimalConversationalRAG:
    """最小可用对话式RAG"""

    def __init__(self):
        self.client = OpenAI()
        self.sessions: Dict[str, List[Message]] = {}
        self.max_history = 20  # 最多保留10轮

    def chat(self, session_id: str, query: str) -> str:
        """对话接口"""
        # 1. 获取历史
        history = self.sessions.get(session_id, [])

        # 2. Query重写（如果有历史）
        if history:
            resolved_query = self._rewrite_query(query, history)
        else:
            resolved_query = query

        # 3. 向量检索（这里简化，实际需要向量数据库）
        docs = self._mock_retrieve(resolved_query)

        # 4. 生成回答（注入历史）
        answer = self._generate(resolved_query, docs, history)

        # 5. 保存历史（滑动窗口）
        self._add_message(session_id, "user", query)
        self._add_message(session_id, "assistant", answer)

        return answer

    def _rewrite_query(self, query: str, history: List[Message]) -> str:
        """重写Query"""
        context = "\n".join([f"{m.role}: {m.content}" for m in history[-6:]])

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{
                "role": "system",
                "content": "根据对话历史，将问题重写为独立完整的问题。"
            }, {
                "role": "user",
                "content": f"对话历史:\n{context}\n\n当前问题: {query}\n\n重写后:"
            }],
            temperature=0.3,
            max_tokens=100
        )

        return response.choices[0].message.content.strip()

    def _mock_retrieve(self, query: str) -> str:
        """模拟检索（实际需要向量数据库）"""
        return f"关于'{query}'的相关文档..."

    def _generate(self, query: str, docs: str, history: List[Message]) -> str:
        """生成回答"""
        history_text = "\n".join([f"{m.role}: {m.content}" for m in history[-6:]])

        messages = [
            {"role": "system", "content": "你是一个智能助手。"},
            {"role": "system", "content": f"对话历史:\n{history_text}"},
            {"role": "system", "content": f"参考文档:\n{docs}"},
            {"role": "user", "content": query}
        ]

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=messages
        )

        return response.choices[0].message.content

    def _add_message(self, session_id: str, role: str, content: str):
        """添加消息（滑动窗口）"""
        if session_id not in self.sessions:
            self.sessions[session_id] = []

        msg = Message(role=role, content=content, timestamp=datetime.now())
        self.sessions[session_id].append(msg)

        # 滑动窗口
        if len(self.sessions[session_id]) > self.max_history:
            self.sessions[session_id] = self.sessions[session_id][-self.max_history:]

# ===== 使用示例 =====
rag = MinimalConversationalRAG()

# 第1轮
answer1 = rag.chat("user_123", "什么是RAG？")
print(f"回答1: {answer1}")

# 第2轮（含指代）
answer2 = rag.chat("user_123", "它有什么优势？")
print(f"回答2: {answer2}")

# 第3轮
answer3 = rag.chat("user_123", "如何实现它？")
print(f"回答3: {answer3}")
```

---

## 这些知识足以做什么？

### ✅ 能做的事

1. **实现基础对话式RAG**
   - 支持多轮对话
   - 记住历史上下文
   - 理解简单指代

2. **处理常见场景**
   - 文档问答助手
   - 简单的智能客服
   - 代码助手原型

3. **控制成本**
   - 滑动窗口控制历史长度
   - 避免Context Window溢出

### ❌ 暂时做不了的事

1. **长对话场景**
   - 需要历史压缩（LLM摘要）
   - 需要关键信息提取

2. **复杂指代**
   - 嵌套指代："前面提到的第一个"
   - 需要更复杂的消解策略

3. **生产环境**
   - 需要数据库持久化
   - 需要监控和日志
   - 需要性能优化

---

## 快速上手步骤

### 步骤1：安装依赖

```bash
pip install openai chromadb
```

### 步骤2：设置API Key

```python
import os
os.environ["OPENAI_API_KEY"] = "your-api-key"
```

### 步骤3：复制最小可用代码

复制上面的`MinimalConversationalRAG`类

### 步骤4：开始使用

```python
rag = MinimalConversationalRAG()
answer = rag.chat("user_123", "你的问题")
```

### 步骤5：添加向量检索

```python
import chromadb

# 初始化向量数据库
client = chromadb.Client()
collection = client.create_collection("docs")

# 添加文档
collection.add(
    documents=["RAG是检索增强生成..."],
    ids=["doc1"]
)

# 在_mock_retrieve中使用真实检索
def _retrieve(self, query: str) -> str:
    results = collection.query(query_texts=[query], n_results=3)
    return "\n".join(results['documents'][0])
```

---

## 常见问题

### Q1: 为什么只保留最近N轮？

**A:** Context Window有限，成本会线性增长。最近N轮已经足够理解上下文。

### Q2: Query重写成本高吗？

**A:** 约$0.001-0.01/次，相比生成成本（$0.05/次）可以接受。

### Q3: 必须用GPT-4吗？

**A:** 不是。可以用GPT-3.5-turbo（便宜10倍），但准确率略低。

### Q4: 如何处理多用户？

**A:** 使用不同的session_id即可，系统会自动隔离。

### Q5: 如何持久化历史？

**A:** 将sessions存储到数据库（PostgreSQL、MongoDB等）。

---

## 下一步学习

### 掌握了最小可用知识后，可以：

1. **学习高级压缩策略**
   - 阅读 `04_核心概念_历史压缩.md`
   - 学习LLM摘要、关键信息提取

2. **学习复杂指代消解**
   - 阅读 `05_核心概念_指代消解.md`
   - 处理嵌套指代、多实体指代

3. **运行完整示例**
   - 阅读 `12_实战代码_场景4_完整对话式RAG.md`
   - 看完整的端到端实现

4. **生产环境部署**
   - 数据库持久化
   - FastAPI封装
   - 监控和日志

---

## 总结

**最小可用知识（20%）：**
1. Session管理：隔离不同用户
2. 滑动窗口：控制历史长度
3. Query重写：理解指代
4. 历史注入：生成连贯回答

**能解决的问题（80%）：**
- ✅ 基础对话式RAG
- ✅ 简单指代消解
- ✅ 成本控制
- ✅ 常见应用场景

**记住：** 先掌握这20%，能解决80%的问题。遇到复杂场景再学习高级策略。
