# 核心概念 - 性能与成本优化策略

## 概述

性能与成本优化是RAG系统生产化的关键，2025-2026年行业标准要求在保证质量的前提下，实现延迟降低20-40%、成本节省15-20%。

---

## 一、延迟优化 (Latency Optimization)

### 1. 缓存策略

**语义缓存 (Semantic Cache)**

```python
from sentence_transformers import SentenceTransformer, util
import hashlib

class SemanticCache:
    """语义缓存：基于语义相似度的缓存"""

    def __init__(self, similarity_threshold=0.95):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.cache = {}  # {embedding: (question, answer)}
        self.similarity_threshold = similarity_threshold

    def get(self, question):
        """获取缓存"""
        question_emb = self.model.encode(question, convert_to_tensor=True)

        # 查找相似问题
        for cached_emb, (cached_q, cached_a) in self.cache.items():
            similarity = util.cos_sim(question_emb, cached_emb).item()

            if similarity >= self.similarity_threshold:
                print(f"Cache hit! Similarity: {similarity:.3f}")
                return cached_a

        return None

    def set(self, question, answer):
        """设置缓存"""
        question_emb = self.model.encode(question, convert_to_tensor=True)
        self.cache[question_emb] = (question, answer)

# 使用示例
cache = SemanticCache()

# 第1次查询 (缓存未命中)
question1 = "公司年假多少天？"
answer1 = cache.get(question1)
if not answer1:
    answer1 = rag_system.query(question1)  # 调用RAG系统
    cache.set(question1, answer1)

# 第2次查询 (语义相似，缓存命中)
question2 = "公司的年假是几天？"
answer2 = cache.get(question2)  # Cache hit!
```

**精确缓存 (Exact Cache)**

```python
import hashlib
import json

class ExactCache:
    """精确缓存：基于问题哈希的缓存"""

    def __init__(self):
        self.cache = {}

    def _get_key(self, question):
        """生成缓存键"""
        return hashlib.md5(question.encode()).hexdigest()

    def get(self, question):
        """获取缓存"""
        key = self._get_key(question)
        return self.cache.get(key)

    def set(self, question, answer):
        """设置缓存"""
        key = self._get_key(question)
        self.cache[key] = answer

# 使用示例
cache = ExactCache()

# 完全相同的问题会命中缓存
question = "公司年假多少天？"
answer = cache.get(question)
if not answer:
    answer = rag_system.query(question)
    cache.set(question, answer)
```

**缓存效果对比**

```python
# 性能对比
performance_comparison = {
    '无缓存': {
        'latency': '3.0秒',
        'cost': '$0.05/query'
    },
    '精确缓存 (30%命中率)': {
        'latency': '2.1秒 (降低30%)',
        'cost': '$0.035/query (降低30%)'
    },
    '语义缓存 (70%命中率)': {
        'latency': '0.97秒 (降低68%)',
        'cost': '$0.015/query (降低70%)'
    }
}
```

### 2. 批处理 (Batching)

**Embedding批处理**

```python
def batch_embed(texts, batch_size=32):
    """批量Embedding，提升吞吐量"""
    from sentence_transformers import SentenceTransformer

    model = SentenceTransformer('all-MiniLM-L6-v2')

    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_emb = model.encode(batch)
        embeddings.extend(batch_emb)

    return embeddings

# 性能对比
# 串行处理: 100个文本 = 10秒
# 批处理(batch_size=32): 100个文本 = 2秒 (提升5倍)
```

**LLM批处理**

```python
from openai import OpenAI
import asyncio

async def batch_generate(questions, batch_size=5):
    """批量生成答案"""
    client = OpenAI()

    async def generate_one(question):
        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": question}]
        )
        return response.choices[0].message.content

    results = []
    for i in range(0, len(questions), batch_size):
        batch = questions[i:i+batch_size]
        batch_results = await asyncio.gather(*[generate_one(q) for q in batch])
        results.extend(batch_results)

    return results

# 使用
questions = ["问题1", "问题2", "问题3", ...]
answers = asyncio.run(batch_generate(questions))
```

### 3. 并行处理 (Parallel Processing)

**检索与生成并行**

```python
import asyncio

async def rag_query_parallel(question):
    """并行处理检索和生成"""

    async def retrieve(q):
        # 检索操作
        await asyncio.sleep(1)  # 模拟检索延迟
        return ["doc1", "doc2", "doc3"]

    async def generate(q, docs):
        # 生成操作
        await asyncio.sleep(1.5)  # 模拟生成延迟
        return f"基于{docs}的答案"

    # 串行处理: 1 + 1.5 = 2.5秒
    # docs = await retrieve(question)
    # answer = await generate(question, docs)

    # 并行处理: max(1, 1.5) = 1.5秒 (提升40%)
    docs, _ = await asyncio.gather(
        retrieve(question),
        asyncio.sleep(0)  # 占位符
    )
    answer = await generate(question, docs)

    return answer
```

### 4. 模型选择优化

**动态模型路由**

```python
class DynamicModelRouter:
    """动态模型路由：根据问题复杂度选择模型"""

    def __init__(self):
        self.simple_model = "gpt-4o-mini"
        self.complex_model = "gpt-4o"

    def classify_complexity(self, question):
        """判断问题复杂度"""
        # 简单规则
        if len(question) < 20:
            return "simple"
        elif any(keyword in question for keyword in ["为什么", "如何", "分析"]):
            return "complex"
        else:
            return "simple"

    def route(self, question):
        """路由到合适的模型"""
        complexity = self.classify_complexity(question)

        if complexity == "simple":
            return self.simple_model
        else:
            return self.complex_model

# 使用示例
router = DynamicModelRouter()

question = "公司年假多少天？"
model = router.route(question)  # gpt-4o-mini

# 成本对比
# 全部用gpt-4o: $0.05/query
# 动态路由(70%简单): 0.7 * $0.002 + 0.3 * $0.05 = $0.0164/query (降低67%)
```

---

## 二、吞吐量优化 (Throughput Optimization)

### 1. 负载均衡

**轮询负载均衡**

```python
class LoadBalancer:
    """负载均衡器"""

    def __init__(self, servers):
        self.servers = servers
        self.current_index = 0

    def get_next_server(self):
        """轮询获取下一个服务器"""
        server = self.servers[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.servers)
        return server

# 使用示例
servers = [
    "http://rag-server-1:8000",
    "http://rag-server-2:8000",
    "http://rag-server-3:8000"
]

lb = LoadBalancer(servers)

# 请求分发
for i in range(10):
    server = lb.get_next_server()
    print(f"Request {i} -> {server}")

# 吞吐量提升: 100 QPS -> 300 QPS (3倍)
```

### 2. 异步处理

**异步RAG系统**

```python
import asyncio
from fastapi import FastAPI

app = FastAPI()

class AsyncRAGSystem:
    """异步RAG系统"""

    async def embed(self, text):
        """异步Embedding"""
        await asyncio.sleep(0.5)  # 模拟Embedding
        return [0.1, 0.2, 0.3]

    async def retrieve(self, embedding):
        """异步检索"""
        await asyncio.sleep(1.0)  # 模拟检索
        return ["doc1", "doc2", "doc3"]

    async def generate(self, question, docs):
        """异步生成"""
        await asyncio.sleep(1.5)  # 模拟生成
        return f"基于{docs}的答案"

    async def query(self, question):
        """异步查询"""
        embedding = await self.embed(question)
        docs = await self.retrieve(embedding)
        answer = await self.generate(question, docs)
        return answer

rag = AsyncRAGSystem()

@app.post("/query")
async def query_endpoint(question: str):
    """异步API端点"""
    answer = await rag.query(question)
    return {"answer": answer}

# 吞吐量对比
# 同步: 10 QPS
# 异步: 100 QPS (提升10倍)
```

---

## 三、成本优化 (Cost Optimization)

### 1. Token压缩

**上下文压缩**

```python
def compress_context(docs, max_tokens=500):
    """压缩上下文，减少Token消耗"""

    # 方法1: 截断
    compressed = []
    total_tokens = 0

    for doc in docs:
        doc_tokens = len(doc.split())
        if total_tokens + doc_tokens <= max_tokens:
            compressed.append(doc)
            total_tokens += doc_tokens
        else:
            break

    return compressed

# 方法2: 摘要
from openai import OpenAI

def summarize_context(docs):
    """使用LLM摘要上下文"""
    client = OpenAI()

    combined = "\n\n".join(docs)

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"请将以下内容压缩为200字以内的摘要：\n\n{combined}"
        }]
    )

    return response.choices[0].message.content

# 成本对比
# 原始上下文: 2000 tokens -> $0.01
# 压缩后: 500 tokens -> $0.0025 (降低75%)
```

### 2. 模型降级

**成本分级策略**

```python
class CostTierStrategy:
    """成本分级策略"""

    def __init__(self):
        self.tiers = {
            'premium': {
                'model': 'gpt-4o',
                'cost_per_1k': 0.015,
                'use_case': '复杂问题、高价值用户'
            },
            'standard': {
                'model': 'gpt-4o-mini',
                'cost_per_1k': 0.0006,
                'use_case': '一般问题、普通用户'
            },
            'economy': {
                'model': 'gpt-3.5-turbo',
                'cost_per_1k': 0.0005,
                'use_case': '简单问题、免费用户'
            }
        }

    def select_tier(self, user_type, question_complexity):
        """选择成本层级"""
        if user_type == 'premium' or question_complexity == 'high':
            return 'premium'
        elif question_complexity == 'medium':
            return 'standard'
        else:
            return 'economy'

# 使用示例
strategy = CostTierStrategy()

# 场景1: 付费用户 + 复杂问题
tier = strategy.select_tier('premium', 'high')  # gpt-4o

# 场景2: 免费用户 + 简单问题
tier = strategy.select_tier('free', 'low')  # gpt-3.5-turbo

# 成本节省: 平均降低60%
```

### 3. 请求去重

**去重策略**

```python
import hashlib
from datetime import datetime, timedelta

class RequestDeduplicator:
    """请求去重器"""

    def __init__(self, ttl_seconds=300):
        self.cache = {}  # {hash: (answer, timestamp)}
        self.ttl = timedelta(seconds=ttl_seconds)

    def _get_hash(self, question):
        """生成问题哈希"""
        return hashlib.md5(question.encode()).hexdigest()

    def is_duplicate(self, question):
        """检查是否重复"""
        hash_key = self._get_hash(question)

        if hash_key in self.cache:
            answer, timestamp = self.cache[hash_key]

            # 检查是否过期
            if datetime.now() - timestamp < self.ttl:
                return True, answer

        return False, None

    def add(self, question, answer):
        """添加到缓存"""
        hash_key = self._get_hash(question)
        self.cache[hash_key] = (answer, datetime.now())

# 使用示例
dedup = RequestDeduplicator()

question = "公司年假多少天？"

# 检查重复
is_dup, cached_answer = dedup.is_duplicate(question)

if is_dup:
    print(f"重复请求，使用缓存: {cached_answer}")
else:
    answer = rag_system.query(question)
    dedup.add(question, answer)

# 成本节省: 20-30% (取决于重复率)
```

---

## 四、综合优化策略

### 生产级优化系统

```python
import time
from dataclasses import dataclass
from typing import Optional

@dataclass
class OptimizationMetrics:
    """优化指标"""
    latency: float
    cost: float
    cache_hit_rate: float
    throughput: float

class ProductionRAGSystem:
    """生产级RAG系统（含优化）"""

    def __init__(self):
        # 缓存
        self.semantic_cache = SemanticCache()

        # 模型路由
        self.model_router = DynamicModelRouter()

        # 成本追踪
        self.total_cost = 0.0
        self.query_count = 0
        self.cache_hits = 0

    def query(self, question: str) -> tuple[str, OptimizationMetrics]:
        """优化后的查询"""
        start_time = time.time()

        # 1. 检查缓存
        cached_answer = self.semantic_cache.get(question)
        if cached_answer:
            self.cache_hits += 1
            latency = time.time() - start_time

            return cached_answer, OptimizationMetrics(
                latency=latency,
                cost=0.0,
                cache_hit_rate=self.cache_hits / (self.query_count + 1),
                throughput=1.0 / latency
            )

        # 2. 动态模型选择
        model = self.model_router.route(question)

        # 3. 执行RAG
        answer, cost = self._execute_rag(question, model)

        # 4. 更新缓存
        self.semantic_cache.set(question, answer)

        # 5. 更新指标
        self.total_cost += cost
        self.query_count += 1
        latency = time.time() - start_time

        metrics = OptimizationMetrics(
            latency=latency,
            cost=cost,
            cache_hit_rate=self.cache_hits / self.query_count,
            throughput=1.0 / latency
        )

        return answer, metrics

    def _execute_rag(self, question: str, model: str) -> tuple[str, float]:
        """执行RAG查询"""
        # 模拟RAG流程
        time.sleep(1.5)  # 模拟延迟

        # 计算成本
        if model == "gpt-4o-mini":
            cost = 0.002
        else:
            cost = 0.05

        return f"答案（使用{model}）", cost

    def get_stats(self):
        """获取统计信息"""
        return {
            'total_queries': self.query_count,
            'cache_hits': self.cache_hits,
            'cache_hit_rate': self.cache_hits / self.query_count if self.query_count > 0 else 0,
            'total_cost': self.total_cost,
            'avg_cost_per_query': self.total_cost / self.query_count if self.query_count > 0 else 0
        }

# 使用示例
rag = ProductionRAGSystem()

# 查询1
answer1, metrics1 = rag.query("公司年假多少天？")
print(f"Latency: {metrics1.latency:.3f}s, Cost: ${metrics1.cost:.4f}")

# 查询2 (相似问题，缓存命中)
answer2, metrics2 = rag.query("公司的年假是几天？")
print(f"Latency: {metrics2.latency:.3f}s, Cost: ${metrics2.cost:.4f}")

# 统计
stats = rag.get_stats()
print(f"Cache Hit Rate: {stats['cache_hit_rate']:.1%}")
print(f"Avg Cost: ${stats['avg_cost_per_query']:.4f}")
```

---

## 五、2025-2026年优化基准

### 行业标准

```python
optimization_benchmarks = {
    '延迟优化': {
        'baseline': '3.0秒',
        'target': '1.8秒',
        'improvement': '40%',
        'methods': ['缓存', '并行处理', '模型优化']
    },
    '成本优化': {
        'baseline': '$0.05/query',
        'target': '$0.04/query',
        'improvement': '20%',
        'methods': ['模型降级', 'Token压缩', '缓存']
    },
    '吞吐量优化': {
        'baseline': '100 QPS',
        'target': '150 QPS',
        'improvement': '50%',
        'methods': ['负载均衡', '异步处理', '批处理']
    }
}
```

### 优化效果对比

| 优化策略 | 延迟改善 | 成本节省 | 吞吐量提升 | 实施难度 |
|----------|----------|----------|------------|----------|
| **语义缓存** | 60-70% | 60-70% | 2-3x | 低 |
| **批处理** | 20-30% | 10-15% | 3-5x | 中 |
| **并行处理** | 30-40% | 0% | 1.5-2x | 中 |
| **模型路由** | 10-20% | 50-70% | 1x | 低 |
| **Token压缩** | 5-10% | 30-50% | 1x | 中 |
| **负载均衡** | 0-10% | 0% | 2-3x | 高 |

---

## 六、实践建议

### 优化优先级

```python
optimization_priority = {
    '第一优先级': {
        'strategies': ['语义缓存', '模型路由'],
        'reason': '低成本、高收益、易实施',
        'expected_improvement': '延迟降低50%，成本降低60%'
    },
    '第二优先级': {
        'strategies': ['批处理', '并行处理'],
        'reason': '中等成本、高收益',
        'expected_improvement': '吞吐量提升3-5倍'
    },
    '第三优先级': {
        'strategies': ['Token压缩', '负载均衡'],
        'reason': '高成本、中等收益',
        'expected_improvement': '成本降低30%，吞吐量提升2-3倍'
    }
}
```

### 监控指标

```python
monitoring_metrics = {
    '延迟指标': {
        'P50': '< 1.5秒',
        'P95': '< 3.0秒',
        'P99': '< 5.0秒'
    },
    '成本指标': {
        '每次查询成本': '< $0.05',
        '日均成本': '< $100',
        'Token利用率': '> 80%'
    },
    '吞吐量指标': {
        'QPS': '> 100',
        '并发数': '> 50',
        '可用性': '> 99.9%'
    },
    '缓存指标': {
        '命中率': '> 70%',
        '缓存大小': '< 1GB',
        'TTL': '5-10分钟'
    }
}
```

---

## 七、总结

### 核心要点

1. **延迟优化**: 缓存 + 并行 + 模型选择
2. **成本优化**: 模型降级 + Token压缩 + 去重
3. **吞吐量优化**: 负载均衡 + 异步 + 批处理
4. **综合优化**: 多策略组合，实现最佳平衡
5. **持续监控**: 实时追踪指标，动态调整策略

### 2025-2026年标准

```python
production_standards = {
    'latency': {
        'P95': '< 2秒',
        'improvement': '降低20-40%'
    },
    'cost': {
        'per_query': '< $0.04',
        'improvement': '节省15-20%'
    },
    'throughput': {
        'QPS': '> 500',
        'improvement': '提升30-50%'
    },
    'cache': {
        'hit_rate': '> 70%',
        'semantic_cache': '标准配置'
    }
}
```

---

**参考资料**:
- https://medium.com/@robi.tomar72/ai-performance-engineering-2025-2026-edition-latency-throughput-cost-optimization-142eec0daece
- https://redis.io/blog/rag-at-scale
- https://aws.amazon.com/blogs/machine-learning/effective-cost-optimization-strategies-for-amazon-bedrock
- https://hackernoon.com/designing-production-ready-rag-pipelines-tackling-latency-hallucinations-and-cost-at-scale
