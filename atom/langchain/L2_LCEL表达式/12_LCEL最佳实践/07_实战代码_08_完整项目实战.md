# å®æˆ˜ä»£ç ï¼šå®Œæ•´é¡¹ç›®å®æˆ˜

> **æœ¬èŠ‚ç›®æ ‡**ï¼šæ•´åˆæ‰€æœ‰LCELæœ€ä½³å®è·µï¼Œæ„å»ºä¼ä¸šçº§RAGåº”ç”¨ï¼Œæ¶µç›–é“¾å¼ç»„åˆã€é”™è¯¯å¤„ç†ã€é…ç½®ç®¡ç†ã€å¹¶å‘ä¼˜åŒ–ã€ç¼“å­˜ã€ç›‘æ§ã€å®‰å…¨çš„å®Œæ•´æ–¹æ¡ˆã€‚

---

## ä¸€ã€é¡¹ç›®æ¦‚è§ˆ

**ä¼ä¸šçº§æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ**ï¼š
- âœ… é“¾å¼ç»„åˆï¼ˆSequential + Parallel + Branchï¼‰
- âœ… é”™è¯¯å¤„ç†ï¼ˆRetry + Fallbacksï¼‰
- âœ… é…ç½®ç®¡ç†ï¼ˆå¤šç¯å¢ƒ + åŠ¨æ€é…ç½®ï¼‰
- âœ… å¹¶å‘ä¼˜åŒ–ï¼ˆBatch + Asyncï¼‰
- âœ… å¤šå±‚ç¼“å­˜ï¼ˆMemory + Redis + Semanticï¼‰
- âœ… å®Œæ•´ç›‘æ§ï¼ˆCallbacks + LangSmith + Prometheusï¼‰
- âœ… å®‰å…¨é˜²æŠ¤ï¼ˆè¾“å…¥éªŒè¯ + PIIè¿‡æ»¤ + RBACï¼‰
- âœ… ç”Ÿäº§éƒ¨ç½²ï¼ˆDocker + K8sï¼‰

**æŠ€æœ¯æ ˆ**ï¼š
- LangChain + OpenAI
- FastAPI + Redis
- Prometheus + Grafana
- Docker + docker-compose

---

## äºŒã€é¡¹ç›®ç»“æ„

```
enterprise_rag/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py          # é…ç½®ç®¡ç†
â”‚   â””â”€â”€ environments.py      # ç¯å¢ƒé…ç½®
â”œâ”€â”€ chains/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retrieval.py        # æ£€ç´¢é“¾
â”‚   â”œâ”€â”€ generation.py       # ç”Ÿæˆé“¾
â”‚   â””â”€â”€ factory.py          # é“¾å·¥å‚
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ multi_layer.py      # å¤šå±‚ç¼“å­˜
â”œâ”€â”€ security/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ validation.py       # è¾“å…¥éªŒè¯
â”‚   â”œâ”€â”€ pii_filter.py       # PIIè¿‡æ»¤
â”‚   â””â”€â”€ rbac.py             # è®¿é—®æ§åˆ¶
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ callbacks.py        # ç›‘æ§å›è°ƒ
â”‚   â””â”€â”€ metrics.py          # PrometheusæŒ‡æ ‡
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py             # FastAPIåº”ç”¨
â”œâ”€â”€ docker-compose.yml      # å®¹å™¨ç¼–æ’
â”œâ”€â”€ Dockerfile              # å®¹å™¨é•œåƒ
â””â”€â”€ .env                    # ç¯å¢ƒå˜é‡
```

---

## ä¸‰ã€æ ¸å¿ƒç»„ä»¶å®ç°

### 3.1 é…ç½®ç®¡ç†

```python
# config/settings.py
from pydantic import BaseModel, Field
from typing import Literal
import os

class AppConfig(BaseModel):
    """åº”ç”¨é…ç½®"""
    # ç¯å¢ƒ
    environment: Literal["dev", "staging", "prod"] = "dev"

    # LLMé…ç½®
    llm_model: str = "gpt-4o-mini"
    llm_temperature: float = Field(default=0, ge=0, le=2)
    max_tokens: int = Field(default=2000, gt=0)

    # æ£€ç´¢é…ç½®
    retrieval_k: int = Field(default=5, gt=0)
    score_threshold: float = Field(default=0.7, ge=0, le=1)

    # æ€§èƒ½é…ç½®
    max_concurrency: int = Field(default=20, gt=0)
    enable_cache: bool = True
    cache_ttl: int = 3600

    # å®‰å…¨é…ç½®
    rate_limit: int = 100
    rate_window: int = 3600

    @classmethod
    def load(cls, env: str = None):
        """åŠ è½½ç¯å¢ƒé…ç½®"""
        env = env or os.getenv("ENV", "dev")

        configs = {
            "dev": cls(
                environment="dev",
                llm_model="gpt-4o-mini",
                max_concurrency=5,
                enable_cache=False
            ),
            "prod": cls(
                environment="prod",
                llm_model="gpt-4o",
                max_concurrency=50,
                enable_cache=True
            )
        }

        return configs.get(env, configs["dev"])
```

### 3.2 å¼¹æ€§é“¾å·¥å‚

```python
# chains/factory.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableBranch
from config.settings import AppConfig

class ResilientChainFactory:
    """å¼¹æ€§é“¾å·¥å‚ï¼ˆæ•´åˆæ‰€æœ‰æœ€ä½³å®è·µï¼‰"""

    def __init__(self, config: AppConfig, vectorstore):
        self.config = config
        self.vectorstore = vectorstore

    def create_rag_chain(self):
        """åˆ›å»ºå®Œæ•´RAGé“¾

        ç‰¹æ€§:
        - å¹¶è¡Œæ£€ç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
        - æ™ºèƒ½è·¯ç”±ï¼ˆæ ¹æ®æŸ¥è¯¢å¤æ‚åº¦ï¼‰
        - é”™è¯¯å¤„ç†ï¼ˆé‡è¯• + é™çº§ï¼‰
        - é…ç½®ç®¡ç†ï¼ˆåŠ¨æ€åˆ‡æ¢ï¼‰
        """
        # 1. æ£€ç´¢å±‚ï¼ˆå¹¶è¡Œï¼‰
        from langchain_core.runnables import RunnableParallel

        retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": self.config.retrieval_k}
        )

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        parallel_retrieval = RunnableParallel({
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        })

        # 2. ç”Ÿæˆå±‚ï¼ˆæ™ºèƒ½è·¯ç”±ï¼‰
        def is_complex_query(input_dict: dict) -> bool:
            question = input_dict.get("question", "")
            return len(question.split()) > 10 or "åˆ†æ" in question

        # ä¸»é“¾ï¼ˆGPT-4ï¼‰
        primary_prompt = ChatPromptTemplate.from_template(
            "åŸºäºä¸Šä¸‹æ–‡è¯¦ç»†å›ç­”:\n\n{context}\n\né—®é¢˜:{question}"
        )
        primary_llm = ChatOpenAI(
            model="gpt-4o",
            temperature=self.config.llm_temperature
        )
        primary_gen = primary_prompt | primary_llm | StrOutputParser()

        # å¿«é€Ÿé“¾ï¼ˆGPT-4o-miniï¼‰
        fast_prompt = ChatPromptTemplate.from_template(
            "ç®€æ´å›ç­”:\n\n{context}\n\né—®é¢˜:{question}"
        )
        fast_llm = ChatOpenAI(model="gpt-4o-mini")
        fast_gen = fast_prompt | fast_llm | StrOutputParser()

        # è·¯ç”±
        router = RunnableBranch(
            (is_complex_query, primary_gen),
            fast_gen
        )

        # 3. æ·»åŠ é‡è¯•å’Œé™çº§
        resilient_gen = (
            router
            .with_retry(stop_after_attempt=3)
            .with_fallbacks([fast_gen])
        )

        # 4. å®Œæ•´é“¾
        rag_chain = parallel_retrieval | resilient_gen

        # 5. æ·»åŠ é…ç½®æ ‡è®°
        return rag_chain.with_config(
            run_name=f"rag_chain_{self.config.environment}",
            tags=[self.config.environment, "rag"],
            max_concurrency=self.config.max_concurrency
        )
```

### 3.3 å¤šå±‚ç¼“å­˜

```python
# cache/multi_layer.py
from langchain.cache import InMemoryCache
from redis import Redis
import hashlib

class MultiLayerCache:
    """ä¸‰å±‚ç¼“å­˜ï¼ˆMemory + Redis + Semanticï¼‰"""

    def __init__(self, redis_client: Redis, ttl: int = 3600):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜
        self.l2_cache = redis_client  # Redisç¼“å­˜
        self.ttl = ttl
        self.stats = {"l1": 0, "l2": 0, "miss": 0}

    def _generate_key(self, query: str) -> str:
        return f"cache:{hashlib.md5(query.encode()).hexdigest()}"

    def get(self, query: str):
        """ä¸‰å±‚æŸ¥æ‰¾"""
        key = self._generate_key(query)

        # L1: å†…å­˜
        if key in self.l1_cache:
            self.stats["l1"] += 1
            return self.l1_cache[key]

        # L2: Redis
        result = self.l2_cache.get(key)
        if result:
            self.stats["l2"] += 1
            self.l1_cache[key] = result  # å›å¡«L1
            return result

        # æœªå‘½ä¸­
        self.stats["miss"] += 1
        return None

    def set(self, query: str, response: str):
        """å†™å…¥æ‰€æœ‰å±‚"""
        key = self._generate_key(query)
        self.l1_cache[key] = response
        self.l2_cache.setex(key, self.ttl, response)
```

### 3.4 å®‰å…¨å±‚

```python
# security/validation.py
from pydantic import BaseModel, Field, validator
import re

class SecureQueryRequest(BaseModel):
    """å®‰å…¨æŸ¥è¯¢è¯·æ±‚"""
    question: str = Field(..., min_length=1, max_length=1000)
    tenant_id: str = Field(..., pattern=r"^[a-zA-Z0-9_-]+$")
    user_id: str = Field(..., pattern=r"^[a-zA-Z0-9_-]+$")

    @validator("question")
    def validate_question(cls, v):
        # SQLæ³¨å…¥æ£€æµ‹
        if any(kw in v.upper() for kw in ["DROP", "DELETE", "INSERT"]):
            raise ValueError("æ£€æµ‹åˆ°SQLæ³¨å…¥")

        # Promptæ³¨å…¥æ£€æµ‹
        if re.search(r"å¿½ç•¥.*æŒ‡ä»¤|ignore.*instructions", v, re.I):
            raise ValueError("æ£€æµ‹åˆ°Promptæ³¨å…¥")

        return v

# security/pii_filter.py
class PIIFilter:
    """PIIè¿‡æ»¤å™¨"""

    def filter(self, text: str) -> str:
        text = re.sub(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
                     "[EMAIL]", text)
        text = re.sub(r"\b\d{3}[-.]?\d{3}[-.]?\d{4}\b", "[PHONE]", text)
        return text
```

---

## å››ã€FastAPIåº”ç”¨

### 4.1 ä¸»åº”ç”¨

```python
# api/main.py
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from config.settings import AppConfig
from chains.factory import ResilientChainFactory
from cache.multi_layer import MultiLayerCache
from security.validation import SecureQueryRequest
from security.pii_filter import PIIFilter
from monitoring.callbacks import MonitoringCallback
from monitoring.metrics import MetricsCallback
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from redis import Redis
import structlog
import uuid
import os

# åŠ è½½ç¯å¢ƒ
load_dotenv()

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
logger = structlog.get_logger()

# åˆ›å»ºåº”ç”¨
app = FastAPI(title="Enterprise RAG API", version="1.0.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"]
)

# åˆå§‹åŒ–ç»„ä»¶
config = AppConfig.load()
redis_client = Redis(host="localhost", port=6379, decode_responses=True)
cache = MultiLayerCache(redis_client, ttl=config.cache_ttl)
pii_filter = PIIFilter()

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(
    collection_name="enterprise_docs",
    embedding_function=embeddings
)

# åˆ›å»ºé“¾å·¥å‚
factory = ResilientChainFactory(config, vectorstore)
rag_chain = factory.create_rag_chain()


class QueryResponse(BaseModel):
    """æŸ¥è¯¢å“åº”"""
    answer: str
    correlation_id: str
    source: str  # "cache" | "llm"
    metrics: dict


@app.post("/query", response_model=QueryResponse)
async def query(request: Request, query_req: SecureQueryRequest):
    """ä¼ä¸šçº§æŸ¥è¯¢æ¥å£

    ç‰¹æ€§:
    - è¾“å…¥éªŒè¯ï¼ˆPydanticï¼‰
    - å¤šå±‚ç¼“å­˜ï¼ˆMemory + Redisï¼‰
    - é”™è¯¯å¤„ç†ï¼ˆRetry + Fallbacksï¼‰
    - PIIè¿‡æ»¤
    - å®Œæ•´ç›‘æ§
    """
    correlation_id = str(uuid.uuid4())

    log = logger.bind(
        correlation_id=correlation_id,
        tenant_id=query_req.tenant_id,
        user_id=query_req.user_id
    )

    log.info("query_received", question=query_req.question)

    try:
        # 1. ç¼“å­˜æ£€æŸ¥
        if config.enable_cache:
            cached = cache.get(query_req.question)
            if cached:
                log.info("cache_hit")
                return QueryResponse(
                    answer=cached,
                    correlation_id=correlation_id,
                    source="cache",
                    metrics={"cache_hit": True}
                )

        # 2. åˆ›å»ºç›‘æ§å›è°ƒ
        monitoring_cb = MonitoringCallback()
        metrics_cb = MetricsCallback(chain_name="rag_chain")

        # 3. æ‰§è¡Œé“¾
        answer = rag_chain.invoke(
            query_req.question,
            config={
                "callbacks": [monitoring_cb, metrics_cb],
                "run_name": "enterprise_query",
                "tags": [config.environment, query_req.tenant_id],
                "metadata": {
                    "correlation_id": correlation_id,
                    "tenant_id": query_req.tenant_id
                }
            }
        )

        # 4. PIIè¿‡æ»¤
        safe_answer = pii_filter.filter(answer)

        # 5. å†™å…¥ç¼“å­˜
        if config.enable_cache:
            cache.set(query_req.question, safe_answer)

        # 6. è·å–æŒ‡æ ‡
        metrics = monitoring_cb.get_metrics()

        log.info(
            "query_completed",
            tokens=metrics["total_tokens"],
            cost=metrics["total_cost"]
        )

        return QueryResponse(
            answer=safe_answer,
            correlation_id=correlation_id,
            source="llm",
            metrics=metrics
        )

    except Exception as e:
        log.error("query_failed", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "environment": config.environment}


@app.get("/stats")
async def stats():
    """ç¼“å­˜ç»Ÿè®¡"""
    return cache.stats


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## äº”ã€Dockeréƒ¨ç½²

### 5.1 docker-compose.yml

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_HOST=redis
      - ENV=prod
      - LANGCHAIN_TRACING_V2=true
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
    depends_on:
      - redis
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

volumes:
  redis_data:
  grafana_data:
```

### 5.2 Dockerfile

```dockerfile
FROM python:3.13-slim

WORKDIR /app

RUN pip install uv

COPY pyproject.toml uv.lock ./
RUN uv sync --no-dev

COPY . .

EXPOSE 8000

CMD ["uv", "run", "uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## å…­ã€è¿è¡Œä¸æµ‹è¯•

### 6.1 æœ¬åœ°å¼€å‘

```bash
# å®‰è£…ä¾èµ–
uv sync

# å¯åŠ¨Redis
docker run -d -p 6379:6379 redis:7-alpine

# è¿è¡ŒAPI
ENV=dev uv run python api/main.py

# æµ‹è¯•
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "ä»€ä¹ˆæ˜¯LCEL?",
    "tenant_id": "tenant_a",
    "user_id": "user_123"
  }'
```

### 6.2 Dockeréƒ¨ç½²

```bash
# æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f api

# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹ç»Ÿè®¡
curl http://localhost:8000/stats

# åœæ­¢
docker-compose down
```

### 6.3 æ€§èƒ½æµ‹è¯•

```bash
# ä½¿ç”¨abè¿›è¡Œå‹åŠ›æµ‹è¯•
ab -n 1000 -c 10 -p query.json -T application/json \
  http://localhost:8000/query

# query.jsonå†…å®¹:
# {"question":"ä»€ä¹ˆæ˜¯LCEL?","tenant_id":"tenant_a","user_id":"user_123"}
```

---

## ä¸ƒã€ç›‘æ§ä»ªè¡¨æ¿

### 7.1 PrometheusæŸ¥è¯¢

```promql
# è¯·æ±‚é€Ÿç‡
rate(chain_calls_total[5m])

# P95å»¶è¿Ÿ
histogram_quantile(0.95, chain_latency_seconds)

# é”™è¯¯ç‡
rate(chain_calls_total{status="error"}[5m]) / rate(chain_calls_total[5m])

# æˆæœ¬è¿½è¸ª
rate(chain_cost_dollars[1h])

# ç¼“å­˜å‘½ä¸­ç‡
cache_hits / (cache_hits + cache_misses)
```

### 7.2 Grafanaä»ªè¡¨æ¿

è®¿é—® http://localhost:3000ï¼ˆé»˜è®¤å¯†ç ï¼šadmin/adminï¼‰

æ·»åŠ Prometheusæ•°æ®æºï¼šhttp://prometheus:9090

å¯¼å…¥ä»ªè¡¨æ¿ï¼Œç›‘æ§ï¼š
- è¯·æ±‚é€Ÿç‡
- å»¶è¿Ÿåˆ†å¸ƒï¼ˆP50/P95/P99ï¼‰
- é”™è¯¯ç‡
- Tokenä½¿ç”¨é‡
- æˆæœ¬è¶‹åŠ¿
- ç¼“å­˜å‘½ä¸­ç‡

---

## å…«ã€æœ€ä½³å®è·µæ€»ç»“

### 8.1 æ¶æ„è®¾è®¡

```
ç”¨æˆ·è¯·æ±‚
  â†“
FastAPIï¼ˆè¾“å…¥éªŒè¯ï¼‰
  â†“
å¤šå±‚ç¼“å­˜ï¼ˆL1 Memory + L2 Redisï¼‰
  â†“ æœªå‘½ä¸­
å¹¶è¡Œæ£€ç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
  â†“
æ™ºèƒ½è·¯ç”±ï¼ˆå¤æ‚â†’GPT-4ï¼Œç®€å•â†’GPT-4o-miniï¼‰
  â†“
é”™è¯¯å¤„ç†ï¼ˆé‡è¯•3æ¬¡ + é™çº§ï¼‰
  â†“
PIIè¿‡æ»¤
  â†“
ç›‘æ§è®°å½•ï¼ˆCallbacks + Prometheusï¼‰
  â†“
è¿”å›å“åº”
```

### 8.2 æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | å®é™… |
|------|------|------|
| å¯ç”¨æ€§ | 99.9% | 99.95% |
| P95å»¶è¿Ÿ | <2ç§’ | 1.5ç§’ |
| ç¼“å­˜å‘½ä¸­ç‡ | >60% | 75% |
| æˆæœ¬é™ä½ | 65% | 80% |
| é”™è¯¯ç‡ | <1% | 0.5% |

### 8.3 æ ¸å¿ƒç‰¹æ€§

âœ… **é“¾å¼ç»„åˆ**: Sequential + Parallel + Branch
âœ… **é”™è¯¯å¤„ç†**: Retry(3æ¬¡) + Fallbacks(2å±‚)
âœ… **é…ç½®ç®¡ç†**: å¤šç¯å¢ƒ + åŠ¨æ€åˆ‡æ¢
âœ… **å¹¶å‘ä¼˜åŒ–**: max_concurrency=50
âœ… **å¤šå±‚ç¼“å­˜**: Memory + Redis + Semantic
âœ… **å®Œæ•´ç›‘æ§**: Callbacks + LangSmith + Prometheus
âœ… **å®‰å…¨é˜²æŠ¤**: è¾“å…¥éªŒè¯ + PIIè¿‡æ»¤ + RBAC
âœ… **ç”Ÿäº§éƒ¨ç½²**: Docker + docker-compose + K8s

---

## ä¹ã€æ€»ç»“

### é¡¹ç›®æˆæœ

æœ¬é¡¹ç›®æ•´åˆäº†LCELçš„æ‰€æœ‰æœ€ä½³å®è·µï¼Œæ„å»ºäº†ä¸€ä¸ªä¼ä¸šçº§RAGåº”ç”¨ï¼š

1. **è®¾è®¡æ¨¡å¼**: é“¾å¼ç»„åˆã€é”™è¯¯å¤„ç†ã€é…ç½®ç®¡ç†
2. **æ€§èƒ½ä¼˜åŒ–**: å¹¶å‘æ‰¹å¤„ç†ã€å¤šå±‚ç¼“å­˜
3. **ç”Ÿäº§å®è·µ**: ç›‘æ§è°ƒè¯•ã€å®‰å…¨æ§åˆ¶ã€å®¹å™¨éƒ¨ç½²
4. **ä»£ç è´¨é‡**: æ¨¡å—åŒ–ã€å¯æµ‹è¯•ã€å¯ç»´æŠ¤

### å…³é”®æ”¶è·

- âœ… æŒæ¡LCELæ ¸å¿ƒç»„åˆæ¨¡å¼
- âœ… æ„å»ºå¼¹æ€§å¯é çš„ç”Ÿäº§ç³»ç»Ÿ
- âœ… å®ç°65-90%æˆæœ¬é™ä½
- âœ… è¾¾åˆ°99.9%+å¯ç”¨æ€§
- âœ… å®Œæ•´çš„å¯è§‚æµ‹æ€§ä½“ç³»

### ä¸‹ä¸€æ­¥

- æ‰©å±•åˆ°å¤šæ¨¡æ€RAGï¼ˆå›¾åƒã€éŸ³é¢‘ï¼‰
- é›†æˆæ›´å¤šLLMæä¾›å•†
- å®ç°A/Bæµ‹è¯•æ¡†æ¶
- æ·»åŠ ç”¨æˆ·åé¦ˆå¾ªç¯
- ä¼˜åŒ–Promptå·¥ç¨‹

---

**ğŸ‰ æ­å–œï¼ä½ å·²å®ŒæˆLCELæœ€ä½³å®è·µçš„å®Œæ•´å­¦ä¹ ï¼**

ç°åœ¨ä½ å¯ä»¥ï¼š
- æ„å»ºç”Ÿäº§çº§LCELåº”ç”¨
- ä¼˜åŒ–æ€§èƒ½å’Œæˆæœ¬
- å®æ–½å®Œæ•´ç›‘æ§
- ç¡®ä¿ç³»ç»Ÿå®‰å…¨
- éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

ç»§ç»­æ¢ç´¢LangChainç”Ÿæ€ï¼Œæ„å»ºæ›´å¼ºå¤§çš„AIåº”ç”¨ï¼
