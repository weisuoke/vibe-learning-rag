# 核心概念

掌握三个最核心的 Query改写技术，覆盖 90% 的 RAG 查询优化场景。

---

## 核心概念1：HyDE（假设文档嵌入）

**HyDE（Hypothetical Document Embeddings）是让 LLM 先生成一个假设的答案文档，然后用这个假设文档的 Embedding 去检索真实文档的技术。**

### HyDE 的核心思想

```
传统检索的问题：
┌─────────────────────────────────────────────────────────────┐
│  用户查询（问题形式）                                          │
│  "Python 怎么实现单例模式？"                                   │
│         ↓                                                   │
│  问题的 Embedding                                            │
│         ↓                                                   │
│  检索文档（答案形式）                                          │
│  "Python 单例模式可以通过 __new__ 方法实现..."                  │
│                                                             │
│  问题：问题和答案在向量空间中距离可能较远                        │
└─────────────────────────────────────────────────────────────┘

HyDE 的解决方案：
┌─────────────────────────────────────────────────────────────┐
│  用户查询: "Python 怎么实现单例模式？"                          │
│         ↓                                                   │
│  LLM 生成假设答案:                                            │
│  "Python 实现单例模式有多种方式：                               │
│   1. 使用 __new__ 方法控制实例创建                             │
│   2. 使用装饰器包装类                                          │
│   3. 使用元类 metaclass..."                                   │
│         ↓                                                   │
│  用假设答案的 Embedding 去检索                                 │
│         ↓                                                   │
│  假设答案和真实文档都是"答案形式"，向量距离更近！                 │
└─────────────────────────────────────────────────────────────┘
```

### Python 实现

```python
from openai import OpenAI

client = OpenAI()

def hyde_query_rewrite(query: str) -> str:
    """
    HyDE: 生成假设文档用于检索

    步骤：
    1. 让 LLM 生成一个假设的答案
    2. 用这个假设答案去检索（而不是原始问题）
    """
    prompt = f"""请回答以下问题，生成一个详细的答案段落。
这个答案将用于检索相关文档，所以请包含关键术语和概念。

问题：{query}

答案："""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=300
    )

    hypothetical_doc = response.choices[0].message.content
    return hypothetical_doc

# 示例
query = "Python 怎么实现异步编程？"
hyde_doc = hyde_query_rewrite(query)
print(f"原始查询: {query}")
print(f"假设文档: {hyde_doc}")

# 输出示例：
# 原始查询: Python 怎么实现异步编程？
# 假设文档: Python 异步编程主要通过 asyncio 库实现。asyncio 是 Python 3.4
#          引入的标准库，提供了 async/await 语法来编写并发代码。核心概念包括：
#          协程(coroutine)、事件循环(event loop)、任务(Task)和 Future 对象...
```

### HyDE 的完整流程

```python
import numpy as np
from openai import OpenAI

client = OpenAI()

def get_embedding(text: str) -> list[float]:
    """获取文本的 Embedding"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

def cosine_similarity(a: list, b: list) -> float:
    """计算余弦相似度"""
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def hyde_search(query: str, documents: list[str], doc_embeddings: list) -> list:
    """
    HyDE 检索完整流程
    """
    # 1. 生成假设文档
    hyde_doc = hyde_query_rewrite(query)

    # 2. 获取假设文档的 Embedding
    hyde_embedding = get_embedding(hyde_doc)

    # 3. 计算与所有文档的相似度
    similarities = [
        cosine_similarity(hyde_embedding, doc_emb)
        for doc_emb in doc_embeddings
    ]

    # 4. 排序返回
    results = sorted(
        zip(documents, similarities),
        key=lambda x: x[1],
        reverse=True
    )

    return results

# 使用示例
documents = [
    "Python asyncio 库提供了异步 IO 支持，使用 async/await 语法",
    "多线程编程使用 threading 模块，适合 IO 密集型任务",
    "JavaScript 的 Promise 和 async/await 实现异步编程",
    "Python 的 GIL 限制了多线程的并行执行"
]

# 预计算文档 Embedding
doc_embeddings = [get_embedding(doc) for doc in documents]

# HyDE 检索
query = "Python 怎么写异步代码"
results = hyde_search(query, documents, doc_embeddings)

for doc, score in results[:3]:
    print(f"{score:.3f}: {doc}")
```

### HyDE 的优缺点

| 优点 | 缺点 |
|------|------|
| ✅ 显著提升语义检索效果 | ❌ 增加一次 LLM 调用延迟 |
| ✅ 用答案找答案，更准确 | ❌ 假设答案可能有错误 |
| ✅ 对开放性问题效果好 | ❌ 对精确查询可能引入噪音 |
| ✅ 实现简单 | ❌ 增加 API 成本 |

### 在 RAG 开发中的应用

```python
# HyDE 适合的场景
queries_good_for_hyde = [
    "如何优化 Python 代码性能",      # 开放性问题
    "微服务架构的最佳实践",          # 需要综合性答案
    "怎么设计一个缓存系统",          # 设计类问题
]

# HyDE 不适合的场景
queries_bad_for_hyde = [
    "Python 3.12 发布日期",          # 事实性问题，假设答案可能错
    "error code E001 含义",          # 精确查询
    "pip install langchain 命令",    # 命令查询
]
```

---

## 核心概念2：Multi-Query（多查询）

**Multi-Query 是让 LLM 生成原始查询的多个变体版本，然后并行检索并合并结果的技术。**

### Multi-Query 的核心思想

```
问题：单一查询可能遗漏相关文档
┌─────────────────────────────────────────────────────────────┐
│  用户查询: "Python 性能优化"                                   │
│         ↓                                                   │
│  只能匹配包含"性能优化"的文档                                   │
│         ↓                                                   │
│  遗漏了：                                                     │
│  - "Python 代码加速技巧"                                      │
│  - "提升 Python 运行效率"                                     │
│  - "Python profiling 指南"                                   │
└─────────────────────────────────────────────────────────────┘

Multi-Query 的解决方案：
┌─────────────────────────────────────────────────────────────┐
│  用户查询: "Python 性能优化"                                   │
│         ↓                                                   │
│  LLM 生成多个变体：                                            │
│  1. "Python 代码性能优化方法"                                  │
│  2. "如何提升 Python 程序运行速度"                              │
│  3. "Python profiling 和性能分析"                             │
│  4. "Python 代码加速技巧"                                     │
│         ↓                                                   │
│  并行检索，合并去重                                            │
│         ↓                                                   │
│  覆盖更多相关文档！                                            │
└─────────────────────────────────────────────────────────────┘
```

### Python 实现

```python
from openai import OpenAI

client = OpenAI()

def multi_query_rewrite(query: str, num_queries: int = 3) -> list[str]:
    """
    Multi-Query: 生成多个查询变体

    参数：
    - query: 原始查询
    - num_queries: 生成的变体数量
    """
    prompt = f"""你是一个查询改写专家。请将以下查询改写成 {num_queries} 个不同的版本。
每个版本应该：
1. 保持原始查询的核心意图
2. 使用不同的表达方式或角度
3. 可能包含同义词或相关术语

原始查询：{query}

请直接输出 {num_queries} 个改写后的查询，每行一个，不要编号："""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.8,  # 稍高的温度增加多样性
        max_tokens=200
    )

    # 解析输出
    rewritten = response.choices[0].message.content.strip().split('\n')
    # 过滤空行，保留有效查询
    rewritten = [q.strip() for q in rewritten if q.strip()]

    # 加上原始查询
    all_queries = [query] + rewritten[:num_queries]

    return all_queries

# 示例
query = "Python 性能优化"
queries = multi_query_rewrite(query)
print("生成的查询变体：")
for i, q in enumerate(queries):
    print(f"  {i+1}. {q}")

# 输出示例：
# 生成的查询变体：
#   1. Python 性能优化
#   2. 如何提升 Python 代码运行速度
#   3. Python 程序性能调优技巧
#   4. Python profiling 和代码优化方法
```

### Multi-Query 的完整流程

```python
def multi_query_search(
    query: str,
    vector_store,
    num_queries: int = 3,
    top_k: int = 5
) -> list:
    """
    Multi-Query 检索完整流程
    """
    # 1. 生成多个查询变体
    queries = multi_query_rewrite(query, num_queries)

    # 2. 并行检索（这里简化为串行）
    all_results = []
    seen_ids = set()

    for q in queries:
        results = vector_store.search(q, top_k=top_k)
        for doc in results:
            if doc['id'] not in seen_ids:
                all_results.append(doc)
                seen_ids.add(doc['id'])

    # 3. 可选：使用 RRF 融合排序
    # 这里简化为按出现频率排序

    return all_results[:top_k]
```

### Multi-Query 的优缺点

| 优点 | 缺点 |
|------|------|
| ✅ 提高召回率 | ❌ 增加检索次数 |
| ✅ 覆盖更多表达方式 | ❌ 需要结果去重和融合 |
| ✅ 对模糊查询效果好 | ❌ 增加 LLM 调用成本 |
| ✅ 可以并行检索 | ❌ 变体质量依赖 LLM |

### 在 RAG 开发中的应用

```python
# Multi-Query 适合的场景
queries_good_for_multi = [
    "数据库优化",              # 可以扩展为 SQL优化、索引优化、查询优化
    "代码重构",                # 可以扩展为 重构技巧、设计模式、代码整洁
    "API 设计",                # 可以扩展为 RESTful、接口设计、API 规范
]

# Multi-Query 不适合的场景
queries_bad_for_multi = [
    "asyncio.gather 用法",     # 已经很具体，不需要扩展
    "Python 3.12.1 changelog", # 精确查询
]
```

---

## 核心概念3：Query Decomposition（查询分解）

**Query Decomposition 是将复杂的复合问题拆解为多个简单子问题，分别检索后综合答案的技术。**

### Query Decomposition 的核心思想

```
问题：复杂问题难以一次检索到完整答案
┌─────────────────────────────────────────────────────────────┐
│  用户查询: "比较 Python 和 Go 在微服务开发中的优缺点"            │
│         ↓                                                   │
│  这个问题包含多个子问题：                                       │
│  - Python 在微服务中的优点是什么？                              │
│  - Python 在微服务中的缺点是什么？                              │
│  - Go 在微服务中的优点是什么？                                  │
│  - Go 在微服务中的缺点是什么？                                  │
│         ↓                                                   │
│  单次检索很难找到同时覆盖所有方面的文档                          │
└─────────────────────────────────────────────────────────────┘

Query Decomposition 的解决方案：
┌─────────────────────────────────────────────────────────────┐
│  原始查询: "比较 Python 和 Go 在微服务开发中的优缺点"            │
│         ↓                                                   │
│  LLM 分解为子问题：                                            │
│  Q1: "Python 微服务开发优势"                                   │
│  Q2: "Python 微服务开发劣势"                                   │
│  Q3: "Go 微服务开发优势"                                       │
│  Q4: "Go 微服务开发劣势"                                       │
│         ↓                                                   │
│  分别检索，获取各自的相关文档                                    │
│         ↓                                                   │
│  综合所有检索结果，生成完整答案                                  │
└─────────────────────────────────────────────────────────────┘
```

### Python 实现

```python
from openai import OpenAI
import json

client = OpenAI()

def decompose_query(query: str) -> list[str]:
    """
    Query Decomposition: 将复杂问题分解为子问题
    """
    prompt = f"""你是一个问题分析专家。请将以下复杂问题分解为多个简单的子问题。

要求：
1. 每个子问题应该是独立的、可以单独回答的
2. 子问题的答案组合起来应该能回答原始问题
3. 子问题数量控制在 2-5 个

原始问题：{query}

请以 JSON 数组格式输出子问题列表："""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=300
    )

    # 解析 JSON
    content = response.choices[0].message.content
    # 提取 JSON 部分
    try:
        # 尝试直接解析
        sub_queries = json.loads(content)
    except json.JSONDecodeError:
        # 如果失败，尝试提取 JSON 数组
        import re
        match = re.search(r'\[.*\]', content, re.DOTALL)
        if match:
            sub_queries = json.loads(match.group())
        else:
            # 回退：按行分割
            sub_queries = [q.strip() for q in content.split('\n') if q.strip()]

    return sub_queries

# 示例
query = "比较 Redis 和 Memcached 在缓存场景下的性能和功能差异"
sub_queries = decompose_query(query)
print("分解后的子问题：")
for i, q in enumerate(sub_queries, 1):
    print(f"  {i}. {q}")

# 输出示例：
# 分解后的子问题：
#   1. Redis 的主要功能特性有哪些？
#   2. Memcached 的主要功能特性有哪些？
#   3. Redis 的性能表现如何？
#   4. Memcached 的性能表现如何？
#   5. Redis 和 Memcached 各自适合什么场景？
```

### Query Decomposition 的完整流程

```python
def decomposition_search(
    query: str,
    vector_store,
    top_k_per_query: int = 3
) -> dict:
    """
    Query Decomposition 检索完整流程
    """
    # 1. 分解问题
    sub_queries = decompose_query(query)

    # 2. 分别检索
    results = {}
    for sub_q in sub_queries:
        docs = vector_store.search(sub_q, top_k=top_k_per_query)
        results[sub_q] = docs

    # 3. 返回结构化结果
    return {
        "original_query": query,
        "sub_queries": sub_queries,
        "results": results
    }

# 生成答案时，可以这样组织 context
def build_context_from_decomposition(search_results: dict) -> str:
    """构建用于生成答案的上下文"""
    context_parts = []

    for sub_q, docs in search_results["results"].items():
        context_parts.append(f"### 关于：{sub_q}")
        for doc in docs:
            context_parts.append(f"- {doc['content']}")
        context_parts.append("")

    return "\n".join(context_parts)
```

### Query Decomposition 的优缺点

| 优点 | 缺点 |
|------|------|
| ✅ 处理复杂复合问题 | ❌ 增加多次检索 |
| ✅ 答案更全面 | ❌ 分解质量依赖 LLM |
| ✅ 结构化的检索结果 | ❌ 需要综合多个结果 |
| ✅ 可追溯每个子问题的来源 | ❌ 简单问题不需要分解 |

### 在 RAG 开发中的应用

```python
# Query Decomposition 适合的场景
queries_good_for_decomposition = [
    "比较 A 和 B 的优缺点",           # 比较类问题
    "实现 X 功能需要哪些步骤",         # 步骤类问题
    "分析 Y 系统的架构和性能",         # 多维度分析
]

# Query Decomposition 不适合的场景
queries_bad_for_decomposition = [
    "Python 怎么排序列表",            # 简单问题
    "asyncio 是什么",                 # 定义类问题
]
```

---

## 三种技术的对比与选择

| 技术 | 核心思想 | 适用场景 | 延迟影响 |
|------|---------|---------|---------|
| **HyDE** | 用假设答案检索 | 开放性问题 | +1 次 LLM |
| **Multi-Query** | 多变体并行检索 | 模糊/多义查询 | +1 次 LLM + N 次检索 |
| **Decomposition** | 拆解为子问题 | 复杂复合问题 | +1 次 LLM + N 次检索 |

### 选择决策树

```
用户查询
    │
    ├─→ 是否是复杂的复合问题？
    │       │
    │       ├─→ 是 → Query Decomposition
    │       │
    │       └─→ 否 ↓
    │
    ├─→ 是否是开放性/概念性问题？
    │       │
    │       ├─→ 是 → HyDE
    │       │
    │       └─→ 否 ↓
    │
    ├─→ 是否表达模糊/可能有多种理解？
    │       │
    │       ├─→ 是 → Multi-Query
    │       │
    │       └─→ 否 → 直接检索（不改写）
    │
    └─→ 是否是精确查询（错误码、版本号等）？
            │
            └─→ 是 → 直接检索（不改写）
```

---

## 扩展概念：Step-back Prompting 和 Query Expansion

### Step-back Prompting（后退提问）

**先问一个更抽象、更基础的问题，获取背景知识后再回答原始问题。**

```python
def step_back_query(query: str) -> str:
    """生成后退问题"""
    prompt = f"""请为以下问题生成一个更抽象、更基础的背景问题。
这个背景问题应该能帮助理解原始问题的上下文。

原始问题：{query}

背景问题："""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )

    return response.choices[0].message.content.strip()

# 示例
query = "为什么 Python 的 GIL 会影响多线程性能？"
step_back = step_back_query(query)
# 背景问题: "什么是 GIL（全局解释器锁）？它的作用是什么？"
```

### Query Expansion（查询扩展）

**添加同义词、相关术语来扩展查询。**

```python
def expand_query(query: str) -> str:
    """扩展查询，添加同义词和相关术语"""
    prompt = f"""请扩展以下查询，添加相关的同义词和术语。
保持原始查询的意图，但增加更多相关关键词。

原始查询：{query}

扩展后的查询（一行）："""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )

    return response.choices[0].message.content.strip()

# 示例
query = "Python 性能优化"
expanded = expand_query(query)
# 扩展后: "Python 性能优化 代码加速 运行效率 profiling 性能分析 优化技巧"
```

---

**下一步：** [04_最小可用](./04_最小可用.md) - 掌握 20% 核心知识解决 80% 问题
