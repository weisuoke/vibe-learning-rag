# 大模型API调用 - 化骨绵掌

10 个 2 分钟知识卡片，每个独立完整，可单独学习。

---

## 卡片1：什么是大模型 API？

**一句话：** 大模型 API 是通过网络请求使用云端 LLM 的标准方式。

**类比：**
- 就像打电话咨询专家：你问问题，专家回答
- 就像前端调用后端 API：发请求，收响应

**核心结构：**
```python
response = client.chat.completions.create(
    model="gpt-4o",      # 选择哪个专家
    messages=[...]       # 你的问题
)
answer = response.choices[0].message.content  # 专家的回答
```

**应用：** RAG 系统的生成模块就是调用 LLM API 生成最终答案。

---

## 卡片2：消息结构 (messages)

**一句话：** messages 是一个消息列表，包含对话历史和当前问题。

**三种角色：**
| 角色 | 作用 | 示例 |
|------|------|------|
| system | 设定模型行为 | "你是Python专家" |
| user | 用户输入 | "什么是RAG？" |
| assistant | 模型回复 | "RAG是..." |

**代码示例：**
```python
messages = [
    {"role": "system", "content": "你是RAG专家"},
    {"role": "user", "content": "什么是向量检索？"}
]
```

**应用：** RAG 中，system 设定回答规则，user 包含检索结果和问题。

---

## 卡片3：temperature 参数

**一句话：** temperature 控制输出的随机性，值越低越确定。

**取值指南：**
| 值 | 效果 | 适用场景 |
|----|------|----------|
| 0 | 最确定 | RAG、事实问答 |
| 0.7 | 平衡 | 通用对话 |
| 1.5 | 高随机 | 创意写作 |

**代码示例：**
```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    temperature=0  # RAG 推荐
)
```

**应用：** RAG 场景用 0-0.3，确保答案基于检索内容。

---

## 卡片4：max_tokens 参数

**一句话：** max_tokens 限制模型最多生成多少个 token。

**注意事项：**
- 设太小：答案被截断
- 设太大：可能浪费成本
- 超出上下文限制会报错

**代码示例：**
```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    max_tokens=500  # 根据预期答案长度设置
)

# 检查是否被截断
if response.choices[0].finish_reason == "length":
    print("答案被截断了！")
```

**应用：** RAG 问答通常 300-500 tokens 足够。

---

## 卡片5：流式输出 (Streaming)

**一句话：** 流式输出让模型边生成边返回，提升用户体验。

**对比：**
| 方式 | 体验 | 适用 |
|------|------|------|
| 非流式 | 等待→一次显示 | 后台处理 |
| 流式 | 逐字显示 | 聊天界面 |

**代码示例：**
```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    stream=True  # 开启流式
)

for chunk in stream:
    content = chunk.choices[0].delta.content
    if content:
        print(content, end="")
```

**应用：** RAG 聊天界面必备，用户不用干等。

---

## 卡片6：错误处理

**一句话：** 生产环境必须处理网络错误、限流、超时等异常。

**三种常见错误：**
```python
from openai import RateLimitError, APIConnectionError, APIError

try:
    response = client.chat.completions.create(...)
except RateLimitError:
    print("请求太频繁")  # 等待重试
except APIConnectionError:
    print("网络错误")    # 检查网络
except APIError as e:
    print(f"API错误: {e}")
```

**应用：** RAG 系统要有重试机制，避免单次失败影响用户。

---

## 卡片7：Token 计费

**一句话：** API 按 token 数量计费，输入输出分开计算。

**计费公式：**
```
成本 = 输入tokens × 输入单价 + 输出tokens × 输出单价
```

**查看使用量：**
```python
response = client.chat.completions.create(...)

print(f"输入: {response.usage.prompt_tokens}")
print(f"输出: {response.usage.completion_tokens}")
print(f"总计: {response.usage.total_tokens}")
```

**应用：** RAG 要控制检索结果数量，避免输入 token 过多。

---

## 卡片8：OpenAI vs Anthropic

**一句话：** 两大主流 API，设计相似但有细节差异。

**主要区别：**
| 特性 | OpenAI | Anthropic |
|------|--------|-----------|
| System | messages 中 | 独立参数 |
| max_tokens | 可选 | 必填 |
| 响应 | choices[0].message.content | content[0].text |

**Anthropic 示例：**
```python
from anthropic import Anthropic
client = Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-20250514",
    max_tokens=500,
    system="你是RAG专家",
    messages=[{"role": "user", "content": "..."}]
)
```

**应用：** 建议封装统一接口，支持多模型切换。

---

## 卡片9：RAG 生成模块

**一句话：** RAG 的生成模块就是将检索结果注入 Prompt，调用 LLM 生成答案。

**核心代码：**
```python
def rag_generate(question: str, docs: list) -> str:
    context = "\n".join(docs)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "基于参考资料回答"},
            {"role": "user", "content": f"资料：{context}\n问题：{question}"}
        ],
        temperature=0.1
    )

    return response.choices[0].message.content
```

**应用：** 这是 RAG 系统的最后一环，决定最终输出质量。

---

## 卡片10：最佳实践总结

**一句话：** 掌握这些最佳实践，构建生产级 RAG 系统。

**检查清单：**
- [ ] 使用环境变量管理 API Key
- [ ] 设置合理的 temperature（RAG 用 0-0.3）
- [ ] 根据场景设置 max_tokens
- [ ] 实现错误处理和重试机制
- [ ] 使用流式输出提升体验
- [ ] 监控 token 使用量和成本
- [ ] 封装统一接口支持多模型

**下一步学习：**
- Token 与 Context Window（理解 LLM 的核心限制）
- Prompt Engineering（优化 RAG 的输出质量）

---

**上一节：** [08_面试必问.md](./08_面试必问.md)
**下一节：** [10_一句话总结.md](./10_一句话总结.md) - 最终总结
