# 实战代码3：生产级模式

## 概述

本节提供生产环境中使用管道操作符的最佳实践和高级模式，包括并行执行、条件路由、错误处理、监控等。

**学习目标**：
- 掌握 RunnableParallel 并行执行
- 理解 RunnableBranch 条件路由
- 能够实现完整的错误处理和降级
- 掌握生产环境监控和优化

---

## 1. 并行执行模式（RunnableParallel）

### 1.1 完整示例

```python
"""
并行执行模式
演示：使用 RunnableParallel 提升性能
"""

import time
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel

# ===== 1. 创建多个独立任务 =====
print("=== 创建独立任务 ===")

# 任务1: 生成摘要
summary_prompt = ChatPromptTemplate.from_template("用一句话总结：{text}")
summary_model = ChatOpenAI(model="gpt-4o-mini")
summary_parser = StrOutputParser()
summary_chain = summary_prompt | summary_model | summary_parser

# 任务2: 翻译
translation_prompt = ChatPromptTemplate.from_template("将以下文本翻译成英文：{text}")
translation_model = ChatOpenAI(model="gpt-4o-mini")
translation_parser = StrOutputParser()
translation_chain = translation_prompt | translation_model | translation_parser

# 任务3: 情感分析
sentiment_prompt = ChatPromptTemplate.from_template("分析以下文本的情感（正面/负面/中性）：{text}")
sentiment_model = ChatOpenAI(model="gpt-4o-mini")
sentiment_parser = StrOutputParser()
sentiment_chain = sentiment_prompt | sentiment_model | sentiment_parser

# ===== 2. 串行执行（慢） =====
print("\n=== 串行执行 ===")

text = "LangChain 是一个强大的 AI 应用开发框架，提供了丰富的组件和工具。"

start = time.time()
summary = summary_chain.invoke({"text": text})
translation = translation_chain.invoke({"text": text})
sentiment = sentiment_chain.invoke({"text": text})
serial_time = time.time() - start

print(f"摘要: {summary}")
print(f"翻译: {translation}")
print(f"情感: {sentiment}")
print(f"串行耗时: {serial_time:.2f}秒")

# ===== 3. 并行执行（快） =====
print("\n=== 并行执行 ===")

parallel_chain = RunnableParallel(
    summary=summary_chain,
    translation=translation_chain,
    sentiment=sentiment_chain
)

start = time.time()
result = parallel_chain.invoke({"text": text})
parallel_time = time.time() - start

print(f"摘要: {result['summary']}")
print(f"翻译: {result['translation']}")
print(f"情感: {result['sentiment']}")
print(f"并行耗时: {parallel_time:.2f}秒")

# ===== 4. 性能对比 =====
print(f"\n=== 性能对比 ===")
print(f"串行: {serial_time:.2f}秒")
print(f"并行: {parallel_time:.2f}秒")
print(f"性能提升: {serial_time / parallel_time:.2f}x")
```

---

## 2. 条件路由模式（RunnableBranch）

### 2.1 完整示例

```python
"""
条件路由模式
演示：根据条件选择不同的处理路径
"""

from langchain_core.runnables import RunnableBranch, RunnableLambda

# ===== 1. 定义不同的处理路径 =====

# 简单任务处理器
simple_prompt = ChatPromptTemplate.from_template("简单回答：{question}")
simple_model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
simple_chain = simple_prompt | simple_model | StrOutputParser()

# 复杂任务处理器
complex_prompt = ChatPromptTemplate.from_template(
    "详细回答以下问题，包括背景、原理和应用：{question}"
)
complex_model = ChatOpenAI(model="gpt-4", temperature=0.3)
complex_chain = complex_prompt | complex_model | StrOutputParser()

# 专业任务处理器
expert_prompt = ChatPromptTemplate.from_template(
    "作为专家，深入分析以下问题：{question}"
)
expert_model = ChatOpenAI(model="gpt-4", temperature=0.5)
expert_chain = expert_prompt | expert_model | StrOutputParser()

# ===== 2. 定义路由条件 =====

def is_simple(input_dict):
    """判断是否为简单问题"""
    question = input_dict["question"]
    return len(question) < 20 and "什么是" in question

def is_complex(input_dict):
    """判断是否为复杂问题"""
    question = input_dict["question"]
    return len(question) > 50 or "如何" in question or "为什么" in question

# ===== 3. 构建条件路由 =====

router = RunnableBranch(
    (is_simple, simple_chain),    # 简单问题 → 简单处理
    (is_complex, complex_chain),  # 复杂问题 → 复杂处理
    expert_chain                   # 默认 → 专家处理
)

# ===== 4. 测试不同类型的问题 =====
print("=== 测试条件路由 ===")

test_questions = [
    {"question": "什么是 AI？"},  # 简单问题
    {"question": "如何构建一个生产级的 RAG 系统？请详细说明架构设计、技术选型和优化策略。"},  # 复杂问题
    {"question": "LangChain 的设计哲学"},  # 专家问题
]

for i, q in enumerate(test_questions):
    print(f"\n问题 {i+1}: {q['question']}")
    answer = router.invoke(q)
    print(f"答案: {answer[:100]}...")
```

---

## 3. 错误处理与降级模式

### 3.1 完整示例

```python
"""
错误处理与降级模式
演示：多级降级策略
"""

from langchain_core.runnables import RunnableLambda

# ===== 1. 定义多级处理链 =====

# 主路径: GPT-4（最强但最贵）
primary_prompt = ChatPromptTemplate.from_template("回答：{question}")
primary_model = ChatOpenAI(model="gpt-4", temperature=0)
primary_chain = primary_prompt | primary_model | StrOutputParser()

# 降级路径1: GPT-4o-mini（平衡）
fallback1_prompt = ChatPromptTemplate.from_template("回答：{question}")
fallback1_model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
fallback1_chain = fallback1_prompt | fallback1_model | StrOutputParser()

# 降级路径2: 缓存响应（最快但可能不准确）
cache = {
    "什么是 AI？": "AI 是人工智能的缩写。",
    "什么是 LangChain？": "LangChain 是一个 AI 应用开发框架。"
}

def cached_response(input_dict):
    question = input_dict["question"]
    return cache.get(question, "抱歉，我现在无法回答这个问题。")

fallback2_chain = RunnableLambda(cached_response)

# ===== 2. 实现多级降级 =====

def with_fallback_chain(primary, fallback1, fallback2):
    """多级降级链"""
    def invoke_with_fallback(input_dict):
        # 尝试主路径
        try:
            print("尝试主路径（GPT-4）...")
            return primary.invoke(input_dict)
        except Exception as e:
            print(f"主路径失败: {e}")

            # 尝试降级路径1
            try:
                print("尝试降级路径1（GPT-4o-mini）...")
                return fallback1.invoke(input_dict)
            except Exception as e:
                print(f"降级路径1失败: {e}")

                # 尝试降级路径2
                try:
                    print("使用降级路径2（缓存）...")
                    return fallback2.invoke(input_dict)
                except Exception as e:
                    print(f"降级路径2失败: {e}")
                    return "所有路径都失败了，请稍后重试。"

    return RunnableLambda(invoke_with_fallback)

# ===== 3. 构建弹性链 =====

resilient_chain = with_fallback_chain(
    primary_chain,
    fallback1_chain,
    fallback2_chain
)

# ===== 4. 测试 =====
print("=== 测试弹性链 ===")

questions = [
    {"question": "什么是 AI？"},
    {"question": "什么是量子计算？"}
]

for q in questions:
    print(f"\n问题: {q['question']}")
    answer = resilient_chain.invoke(q)
    print(f"答案: {answer}")
```

---

## 4. 重试模式

### 4.1 完整示例

```python
"""
重试模式
演示：自动重试失败的请求
"""

import time
from langchain_core.runnables import RunnableLambda

# ===== 1. 实现重试逻辑 =====

def with_retry(runnable, max_retries=3, delay=1, backoff=2):
    """
    添加重试功能

    Args:
        runnable: 要包装的 Runnable
        max_retries: 最大重试次数
        delay: 初始延迟（秒）
        backoff: 延迟倍数（指数退避）
    """
    def retry_invoke(input_dict):
        current_delay = delay

        for attempt in range(max_retries):
            try:
                print(f"尝试 {attempt + 1}/{max_retries}...")
                return runnable.invoke(input_dict)
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"所有重试都失败了: {e}")
                    raise

                print(f"失败: {e}")
                print(f"等待 {current_delay} 秒后重试...")
                time.sleep(current_delay)
                current_delay *= backoff  # 指数退避

        return None

    return RunnableLambda(retry_invoke)

# ===== 2. 创建可能失败的链 =====

# 模拟不稳定的服务
class UnstableService:
    def __init__(self, failure_rate=0.5):
        self.failure_rate = failure_rate
        self.call_count = 0

    def invoke(self, input_dict):
        self.call_count += 1
        import random
        if random.random() < self.failure_rate:
            raise Exception("服务暂时不可用")
        return f"成功处理: {input_dict['question']}"

unstable_service = UnstableService(failure_rate=0.7)
unstable_chain = RunnableLambda(unstable_service.invoke)

# ===== 3. 包装重试逻辑 =====

retry_chain = with_retry(
    unstable_chain,
    max_retries=5,
    delay=0.5,
    backoff=2
)

# ===== 4. 测试 =====
print("=== 测试重试模式 ===")

try:
    result = retry_chain.invoke({"question": "测试问题"})
    print(f"\n最终结果: {result}")
except Exception as e:
    print(f"\n最终失败: {e}")
```

---

## 5. 监控与日志模式

### 5.1 完整示例

```python
"""
监控与日志模式
演示：完整的监控和日志系统
"""

import time
from datetime import datetime
from langchain.callbacks.base import BaseCallbackHandler

# ===== 1. 定义监控回调 =====

class ProductionMonitor(BaseCallbackHandler):
    """生产环境监控器"""

    def __init__(self):
        self.metrics = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "total_tokens": 0,
            "total_cost": 0,
            "avg_latency": 0,
            "errors": []
        }
        self.start_times = {}

    def on_chain_start(self, serialized, inputs, run_id, **kwargs):
        """链开始"""
        self.start_times[run_id] = time.time()
        print(f"[{datetime.now()}] 链开始: {inputs}")

    def on_chain_end(self, outputs, run_id, **kwargs):
        """链结束"""
        latency = time.time() - self.start_times.get(run_id, time.time())
        self.metrics["successful_calls"] += 1

        # 更新平均延迟
        total_calls = self.metrics["successful_calls"]
        self.metrics["avg_latency"] = (
            (self.metrics["avg_latency"] * (total_calls - 1) + latency) / total_calls
        )

        print(f"[{datetime.now()}] 链结束: 延迟 {latency:.2f}秒")

    def on_chain_error(self, error, run_id, **kwargs):
        """链错误"""
        self.metrics["failed_calls"] += 1
        self.metrics["errors"].append({
            "time": datetime.now(),
            "error": str(error)
        })
        print(f"[{datetime.now()}] 链错误: {error}")

    def on_llm_start(self, serialized, prompts, run_id, **kwargs):
        """LLM 开始"""
        self.metrics["total_calls"] += 1
        print(f"[{datetime.now()}] LLM 调用开始")

    def on_llm_end(self, response, run_id, **kwargs):
        """LLM 结束"""
        # 计算 token 和成本
        usage = response.llm_output.get("token_usage", {})
        tokens = usage.get("total_tokens", 0)
        self.metrics["total_tokens"] += tokens
        self.metrics["total_cost"] += tokens * 0.00002  # 假设价格

        print(f"[{datetime.now()}] LLM 调用结束: {tokens} tokens")

    def get_dashboard(self):
        """获取监控面板"""
        return {
            "总调用次数": self.metrics["total_calls"],
            "成功次数": self.metrics["successful_calls"],
            "失败次数": self.metrics["failed_calls"],
            "总 Token 数": self.metrics["total_tokens"],
            "总成本": f"${self.metrics['total_cost']:.4f}",
            "平均延迟": f"{self.metrics['avg_latency']:.2f}秒",
            "错误列表": self.metrics["errors"]
        }

# ===== 2. 使用监控 =====

monitor = ProductionMonitor()

prompt = ChatPromptTemplate.from_template("回答：{question}")
model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | model | parser

# ===== 3. 执行并监控 =====
print("=== 执行并监控 ===")

questions = [
    {"question": "什么是 AI？"},
    {"question": "什么是机器学习？"},
    {"question": "什么是深度学习？"}
]

for q in questions:
    try:
        result = chain.invoke(
            q,
            config={"callbacks": [monitor]}
        )
        print(f"答案: {result[:50]}...\n")
    except Exception as e:
        print(f"错误: {e}\n")

# ===== 4. 显示监控面板 =====
print("\n=== 监控面板 ===")
dashboard = monitor.get_dashboard()
for key, value in dashboard.items():
    if key != "错误列表":
        print(f"{key}: {value}")

if dashboard["错误列表"]:
    print("\n错误列表:")
    for error in dashboard["错误列表"]:
        print(f"  - {error['time']}: {error['error']}")
```

---

## 6. 缓存模式

### 6.1 完整示例

```python
"""
缓存模式
演示：多级缓存策略
"""

from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache
from langchain_core.runnables import RunnableLambda

# ===== 1. 全局 LLM 缓存 =====
print("=== 全局 LLM 缓存 ===")

set_llm_cache(InMemoryCache())

chain = prompt | model | parser

# 第一次调用（缓存未命中）
print("第一次调用...")
start = time.time()
result1 = chain.invoke({"question": "什么是 AI？"})
time1 = time.time() - start
print(f"耗时: {time1:.2f}秒")

# 第二次调用（缓存命中）
print("\n第二次调用（相同输入）...")
start = time.time()
result2 = chain.invoke({"question": "什么是 AI？"})
time2 = time.time() - start
print(f"耗时: {time2:.2f}秒")

print(f"\n性能提升: {time1 / time2:.2f}x")

# ===== 2. 自定义缓存层 =====
print("\n=== 自定义缓存层 ===")

class SmartCache:
    """智能缓存"""

    def __init__(self, ttl=3600):
        self.cache = {}
        self.ttl = ttl  # 缓存过期时间（秒）

    def get(self, key):
        """获取缓存"""
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                print(f"缓存命中: {key}")
                return value
            else:
                print(f"缓存过期: {key}")
                del self.cache[key]
        return None

    def set(self, key, value):
        """设置缓存"""
        self.cache[key] = (value, time.time())
        print(f"缓存保存: {key}")

    def clear(self):
        """清空缓存"""
        self.cache.clear()
        print("缓存已清空")

# 创建缓存实例
smart_cache = SmartCache(ttl=60)

def cached_chain(input_dict):
    """带缓存的链"""
    key = str(input_dict)

    # 检查缓存
    cached_result = smart_cache.get(key)
    if cached_result:
        return cached_result

    # 缓存未命中，执行链
    result = chain.invoke(input_dict)

    # 保存到缓存
    smart_cache.set(key, result)

    return result

cached_runnable = RunnableLambda(cached_chain)

# 测试
print("\n测试智能缓存:")
result1 = cached_runnable.invoke({"question": "什么是 LangChain？"})
result2 = cached_runnable.invoke({"question": "什么是 LangChain？"})  # 缓存命中
```

---

## 7. 批处理优化模式

### 7.1 完整示例

```python
"""
批处理优化模式
演示：智能批处理策略
"""

from typing import List

# ===== 1. 动态批处理 =====

class DynamicBatcher:
    """动态批处理器"""

    def __init__(self, chain, batch_size=10, timeout=5):
        self.chain = chain
        self.batch_size = batch_size
        self.timeout = timeout
        self.buffer = []
        self.last_flush = time.time()

    def add(self, input_dict):
        """添加到批处理缓冲区"""
        self.buffer.append(input_dict)

        # 检查是否需要刷新
        if len(self.buffer) >= self.batch_size:
            return self.flush()
        elif time.time() - self.last_flush >= self.timeout:
            return self.flush()

        return None

    def flush(self):
        """刷新缓冲区"""
        if not self.buffer:
            return []

        print(f"批处理 {len(self.buffer)} 个请求...")
        results = self.chain.batch(self.buffer)
        self.buffer = []
        self.last_flush = time.time()
        return results

# ===== 2. 使用动态批处理 =====

batcher = DynamicBatcher(chain, batch_size=3, timeout=2)

print("=== 动态批处理 ===")

# 添加请求
for i in range(5):
    input_dict = {"question": f"问题 {i+1}"}
    results = batcher.add(input_dict)
    if results:
        print(f"批处理完成，返回 {len(results)} 个结果")

# 刷新剩余请求
remaining = batcher.flush()
if remaining:
    print(f"刷新剩余 {len(remaining)} 个结果")
```

---

## 8. 完整的生产级系统

### 8.1 完整示例

```python
"""
完整的生产级系统
演示：集成所有生产级模式
"""

from typing import Dict, Any

class ProductionRAGSystem:
    """生产级 RAG 系统"""

    def __init__(self, documents: List[str]):
        # 初始化组件
        self.vector_store = Chroma.from_texts(
            documents,
            embedding=OpenAIEmbeddings()
        )
        self.retriever = self.vector_store.as_retriever()

        # 初始化监控
        self.monitor = ProductionMonitor()

        # 初始化缓存
        self.cache = SmartCache(ttl=3600)

        # 构建链
        self.chain = self._build_chain()

    def _build_chain(self):
        """构建生产级链"""
        # 主链
        prompt = ChatPromptTemplate.from_template(
            "基于上下文回答：{context}\n\n问题：{question}"
        )
        model = ChatOpenAI(model="gpt-4o-mini")
        parser = StrOutputParser()

        # 带重试的主链
        main_chain = with_retry(
            prompt | model | parser,
            max_retries=3
        )

        # 降级链
        fallback_chain = ChatPromptTemplate.from_template(
            "简单回答：{question}"
        ) | ChatOpenAI(model="gpt-3.5-turbo") | StrOutputParser()

        # 组合
        return with_fallback_chain(
            main_chain,
            fallback_chain,
            RunnableLambda(lambda x: "抱歉，系统暂时不可用")
        )

    def query(self, question: str) -> Dict[str, Any]:
        """查询（带缓存、监控）"""
        # 检查缓存
        cache_key = f"query:{question}"
        cached_result = self.cache.get(cache_key)
        if cached_result:
            return {
                "question": question,
                "answer": cached_result,
                "from_cache": True
            }

        # 检索上下文
        docs = self.retriever.invoke(question)
        context = "\n\n".join([doc.page_content for doc in docs])

        # 生成答案（带监控）
        answer = self.chain.invoke(
            {"context": context, "question": question},
            config={"callbacks": [self.monitor]}
        )

        # 保存到缓存
        self.cache.set(cache_key, answer)

        return {
            "question": question,
            "answer": answer,
            "sources": [doc.page_content for doc in docs],
            "from_cache": False
        }

    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:
        """批量查询"""
        return [self.query(q) for q in questions]

    def get_metrics(self) -> Dict:
        """获取指标"""
        return self.monitor.get_dashboard()

# ===== 使用生产级系统 =====

documents = [
    "LangChain 是一个 AI 应用开发框架。",
    "LCEL 是 LangChain 的表达式语言。",
    "管道操作符 | 用于组合组件。"
]

system = ProductionRAGSystem(documents)

# 查询
print("=== 查询1 ===")
result1 = system.query("什么是 LangChain？")
print(f"答案: {result1['answer']}")
print(f"来自缓存: {result1['from_cache']}")

# 再次查询（缓存命中）
print("\n=== 查询2（相同问题） ===")
result2 = system.query("什么是 LangChain？")
print(f"答案: {result2['answer']}")
print(f"来自缓存: {result2['from_cache']}")

# 查看指标
print("\n=== 系统指标 ===")
metrics = system.get_metrics()
for key, value in metrics.items():
    if key != "错误列表":
        print(f"{key}: {value}")
```

---

## 学习检查

完成本节后，检查是否掌握：

- [ ] 能使用 RunnableParallel 实现并行执行
- [ ] 能使用 RunnableBranch 实现条件路由
- [ ] 能实现多级错误降级策略
- [ ] 能实现自动重试机制
- [ ] 能实现完整的监控和日志系统
- [ ] 能实现多级缓存策略
- [ ] 能实现智能批处理
- [ ] 能构建完整的生产级系统

---

[Source: LangChain Official Docs - https://python.langchain.com/docs/concepts/]
[Source: Medium Production Guide - https://medium.com/@sajo02/building-production-ready-ai-pipelines-with-langchain-runnables-a-complete-lcel-guide-2f9b27f6d557]
