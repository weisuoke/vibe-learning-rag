# 最小可用：5分钟上手上下文管理

> **目标**：用最少的代码实现基础的上下文管理功能，快速理解核心概念

---

## 核心功能

实现一个最简单的上下文管理系统，包含：
1. **Token计数**：计算文本的Token数量
2. **上下文截断**：超过限制时智能截断
3. **基础RAG流程**：检索 + 上下文管理 + 生成

---

## 环境准备

```bash
# 安装必需库
pip install openai tiktoken python-dotenv

# 配置API密钥
echo "OPENAI_API_KEY=your_key_here" > .env
```

---

## 完整代码（50行）

```python
"""
最小可用上下文管理系统
功能：Token计数、智能截断、基础RAG
"""

import os
import tiktoken
from openai import OpenAI
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

# 初始化
client = OpenAI()
encoding = tiktoken.encoding_for_model("gpt-4")

def count_tokens(text: str) -> int:
    """计算文本的Token数量"""
    return len(encoding.encode(text))

def truncate_context(documents: list[str], max_tokens: int = 4000) -> str:
    """
    智能截断上下文
    策略：优先保留前面的文档，确保不超过max_tokens
    """
    context_parts = []
    current_tokens = 0

    for doc in documents:
        doc_tokens = count_tokens(doc)
        if current_tokens + doc_tokens <= max_tokens:
            context_parts.append(doc)
            current_tokens += doc_tokens
        else:
            # 超过限制，停止添加
            break

    return "\n\n---\n\n".join(context_parts)

def simple_rag(query: str, documents: list[str], max_tokens: int = 4000) -> str:
    """
    简单的RAG流程
    1. 截断上下文
    2. 构建Prompt
    3. 调用LLM生成答案
    """
    # 1. 截断上下文
    context = truncate_context(documents, max_tokens)

    # 2. 构建Prompt
    prompt = f"""基于以下上下文回答问题。

上下文：
{context}

问题：{query}

答案："""

    # 3. 调用LLM
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "你是一个helpful的助手，基于提供的上下文回答问题。"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=500
    )

    return response.choices[0].message.content

# ============ 使用示例 ============

if __name__ == "__main__":
    # 模拟检索到的文档
    documents = [
        "RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。它先从知识库中检索相关文档，然后将这些文档作为上下文传递给LLM生成答案。",
        "上下文管理是RAG系统的核心能力。由于LLM的Context Window有限（如GPT-4的128K tokens），需要智能地选择和压缩上下文。",
        "Token是LLM处理文本的基本单位。一个Token大约对应0.75个英文单词或0.5个中文字符。Token数量直接影响成本和延迟。",
        "Lost in the Middle是指LLM对长上下文中间部分的内容召回率较低的现象。解决方案包括文档排序、ReRank等。",
        "LLMLingua是微软研究院提出的上下文压缩技术，可以实现20x压缩比，同时保持甚至提升性能。"
    ]

    # 查询
    query = "什么是RAG？为什么需要上下文管理？"

    # 1. 计算Token
    print("=== Token计数 ===")
    for i, doc in enumerate(documents, 1):
        tokens = count_tokens(doc)
        print(f"文档{i}: {tokens} tokens")

    total_tokens = sum(count_tokens(doc) for doc in documents)
    print(f"\n总Token数: {total_tokens}")

    # 2. 截断上下文
    print("\n=== 上下文截断 ===")
    max_tokens = 200  # 设置较小的限制用于演示
    truncated = truncate_context(documents, max_tokens)
    print(f"截断后Token数: {count_tokens(truncated)}")
    print(f"保留文档数: {truncated.count('---') + 1}")

    # 3. RAG查询
    print("\n=== RAG查询 ===")
    print(f"问题: {query}")
    answer = simple_rag(query, documents, max_tokens=4000)
    print(f"\n答案:\n{answer}")

    # 4. 成本估算
    print("\n=== 成本估算 ===")
    prompt_tokens = count_tokens(truncate_context(documents, 4000)) + count_tokens(query)
    completion_tokens = 500  # 假设
    cost = (prompt_tokens / 1_000_000) * 10 + (completion_tokens / 1_000_000) * 30
    print(f"Prompt tokens: {prompt_tokens}")
    print(f"Completion tokens: {completion_tokens}")
    print(f"预估成本: ${cost:.4f}")
```

---

## 代码解析

### 1. Token计数

```python
def count_tokens(text: str) -> int:
    """计算文本的Token数量"""
    return len(encoding.encode(text))
```

**核心**：
- 使用`tiktoken`库精确计算Token数
- 不同模型有不同的编码方式
- Token数 ≠ 字符数

**为什么重要**：
- Token是计费单位
- Context Window以Token为单位
- 需要精确控制

### 2. 智能截断

```python
def truncate_context(documents: list[str], max_tokens: int = 4000) -> str:
    """智能截断上下文"""
    context_parts = []
    current_tokens = 0

    for doc in documents:
        doc_tokens = count_tokens(doc)
        if current_tokens + doc_tokens <= max_tokens:
            context_parts.append(doc)
            current_tokens += doc_tokens
        else:
            break

    return "\n\n---\n\n".join(context_parts)
```

**策略**：
- 优先保留前面的文档（假设已按相关性排序）
- 累计计算Token数
- 超过限制立即停止

**改进空间**：
- 可以实现更智能的选择策略
- 可以部分截断最后一个文档
- 可以根据重要性排序

### 3. 简单RAG流程

```python
def simple_rag(query: str, documents: list[str], max_tokens: int = 4000) -> str:
    """简单的RAG流程"""
    # 1. 截断上下文
    context = truncate_context(documents, max_tokens)

    # 2. 构建Prompt
    prompt = f"""基于以下上下文回答问题。

上下文：
{context}

问题：{query}

答案："""

    # 3. 调用LLM
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "你是一个helpful的助手，基于提供的上下文回答问题。"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=500
    )

    return response.choices[0].message.content
```

**流程**：
1. **截断上下文**：确保不超过限制
2. **构建Prompt**：清晰的指令 + 上下文 + 问题
3. **调用LLM**：生成答案

**关键点**：
- System message定义角色
- 温度控制创造性
- max_tokens控制输出长度

---

## 运行结果示例

```
=== Token计数 ===
文档1: 89 tokens
文档2: 78 tokens
文档3: 67 tokens
文档4: 72 tokens
文档5: 65 tokens

总Token数: 371

=== 上下文截断 ===
截断后Token数: 167
保留文档数: 2

=== RAG查询 ===
问题: 什么是RAG？为什么需要上下文管理？

答案:
RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。它的工作流程是：
1. 先从知识库中检索相关文档
2. 将这些文档作为上下文传递给LLM
3. LLM基于上下文生成答案

需要上下文管理的原因：
1. **Context Window限制**：LLM的上下文窗口有限（如GPT-4的128K tokens），无法处理所有检索到的文档
2. **成本控制**：Token是按量计费的，上下文越长成本越高
3. **性能优化**：上下文越长，延迟越高，需要智能选择和压缩

=== 成本估算 ===
Prompt tokens: 187
Completion tokens: 500
预估成本: $0.0169
```

---

## 核心概念理解

### 1. Token vs 字符

```python
# 示例
text_en = "Hello, world!"
text_cn = "你好，世界！"

tokens_en = count_tokens(text_en)  # ~3 tokens
tokens_cn = count_tokens(text_cn)  # ~6 tokens

print(f"英文: {len(text_en)} 字符 = {tokens_en} tokens")
print(f"中文: {len(text_cn)} 字符 = {tokens_cn} tokens")
```

**规律**：
- 英文：1 token ≈ 0.75 单词 ≈ 4 字符
- 中文：1 token ≈ 0.5 字符
- 标点符号通常是1个token

### 2. Context Window限制

```python
# 不同模型的限制
context_limits = {
    "gpt-3.5-turbo": 16_385,
    "gpt-4": 8_192,
    "gpt-4-turbo": 128_000,
    "claude-3-sonnet": 200_000,
    "gemini-1.5-pro": 1_000_000
}

# 实际可用空间
def available_context(model: str, system_tokens: int, output_tokens: int) -> int:
    """计算实际可用的上下文空间"""
    total = context_limits[model]
    return total - system_tokens - output_tokens

# 示例
model = "gpt-4-turbo"
system_tokens = 100  # System message
output_tokens = 500  # 预留给输出
available = available_context(model, system_tokens, output_tokens)
print(f"{model} 可用上下文: {available} tokens")
```

### 3. 成本计算

```python
def calculate_cost(prompt_tokens: int, completion_tokens: int, model: str = "gpt-4") -> float:
    """计算API调用成本"""
    # 2026年价格（美元/1M tokens）
    prices = {
        "gpt-4": {"input": 10, "output": 30},
        "gpt-4-turbo": {"input": 10, "output": 30},
        "gpt-3.5-turbo": {"input": 0.5, "output": 1.5}
    }

    price = prices[model]
    cost = (prompt_tokens / 1_000_000) * price["input"] + \
           (completion_tokens / 1_000_000) * price["output"]

    return cost

# 示例
prompt_tokens = 5000
completion_tokens = 500
cost = calculate_cost(prompt_tokens, completion_tokens, "gpt-4")
print(f"单次查询成本: ${cost:.4f}")

# 月度成本估算
queries_per_day = 1000
monthly_cost = cost * queries_per_day * 30
print(f"月度成本（1000查询/天）: ${monthly_cost:.2f}")
```

---

## 快速优化

### 优化1：添加相关性排序

```python
def truncate_context_with_scores(
    documents: list[str],
    scores: list[float],  # 相关性分数
    max_tokens: int = 4000
) -> str:
    """基于相关性分数截断上下文"""
    # 按分数排序
    sorted_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)

    context_parts = []
    current_tokens = 0

    for doc, score in sorted_docs:
        doc_tokens = count_tokens(doc)
        if current_tokens + doc_tokens <= max_tokens:
            context_parts.append(doc)
            current_tokens += doc_tokens
        else:
            break

    return "\n\n---\n\n".join(context_parts)
```

### 优化2：添加Token监控

```python
class TokenMonitor:
    """Token使用监控"""
    def __init__(self):
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.total_cost = 0.0

    def log_usage(self, prompt_tokens: int, completion_tokens: int, model: str = "gpt-4"):
        """记录Token使用"""
        self.total_prompt_tokens += prompt_tokens
        self.total_completion_tokens += completion_tokens
        cost = calculate_cost(prompt_tokens, completion_tokens, model)
        self.total_cost += cost

    def report(self):
        """生成报告"""
        print(f"=== Token使用报告 ===")
        print(f"Prompt tokens: {self.total_prompt_tokens:,}")
        print(f"Completion tokens: {self.total_completion_tokens:,}")
        print(f"总成本: ${self.total_cost:.2f}")

# 使用
monitor = TokenMonitor()
# ... 在每次API调用后
monitor.log_usage(prompt_tokens, completion_tokens)
monitor.report()
```

### 优化3：添加缓存

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def count_tokens_cached(text: str) -> int:
    """带缓存的Token计数"""
    return len(encoding.encode(text))

# 对于重复的文档，缓存可以显著提升性能
```

---

## 常见问题

### Q1: 为什么不直接用字符数？

**A**: Token数 ≠ 字符数
- 英文：1 token ≈ 4 字符
- 中文：1 token ≈ 0.5 字符
- 使用字符数会导致不准确的成本估算和截断

### Q2: 如何选择max_tokens？

**A**: 根据场景权衡
- **简单查询**：2K-4K tokens
- **中等查询**：4K-8K tokens
- **复杂查询**：8K-16K tokens
- **避免**：盲目使用最大窗口

### Q3: 截断会丢失信息吗？

**A**: 会，但可以优化
- **基础版本**：简单截断，可能丢失重要信息
- **优化版本**：基于相关性排序，优先保留重要内容
- **高级版本**：使用压缩技术（LLMLingua）

### Q4: 如何处理超长文档？

**A**: 分块处理
```python
def split_long_document(doc: str, max_tokens: int = 1000) -> list[str]:
    """将超长文档分块"""
    chunks = []
    current_chunk = []
    current_tokens = 0

    for sentence in doc.split('. '):
        sentence_tokens = count_tokens(sentence)
        if current_tokens + sentence_tokens <= max_tokens:
            current_chunk.append(sentence)
            current_tokens += sentence_tokens
        else:
            chunks.append('. '.join(current_chunk) + '.')
            current_chunk = [sentence]
            current_tokens = sentence_tokens

    if current_chunk:
        chunks.append('. '.join(current_chunk) + '.')

    return chunks
```

---

## 下一步学习

掌握了最小可用版本后，可以学习：

1. **核心概念**：深入理解10个关键技术
   - Context Window基础
   - Token优化技术
   - 上下文压缩（LLMLingua）
   - Lost in the Middle问题
   - 文档排序策略
   - ReRank重排序
   - 动态上下文窗口
   - 上下文工程
   - MCP协议集成
   - 生产级优化

2. **实战代码**：7个完整场景
   - LLMLingua压缩
   - 文档排序优化
   - ReRank实现
   - 动态窗口管理
   - Lost in Middle解决
   - 完整RAG流程

3. **生产实践**：
   - 监控与调试
   - A/B测试
   - 成本优化
   - 性能优化

---

## 总结

### 核心要点

1. **Token计数**：使用tiktoken精确计算
2. **智能截断**：基于Token限制选择文档
3. **简单RAG**：检索 + 截断 + 生成
4. **成本意识**：Token = 成本，需要精确控制

### 记忆口诀

**"计截构调"**：
- **计**：计算Token数
- **截**：截断上下文
- **构**：构建Prompt
- **调**：调用LLM

### 实战建议

1. **先跑通流程**：使用最小可用版本
2. **监控指标**：Token使用、成本、延迟
3. **逐步优化**：相关性排序 → 压缩 → ReRank
4. **持续迭代**：根据实际效果调整策略

---

**记住**：上下文管理不是一次性的，而是持续优化的过程！
