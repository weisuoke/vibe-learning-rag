# RunnableParallel并行执行 - 实战代码3：错误处理场景

> 通过完整可运行的代码掌握错误处理场景

---

## 场景1：部分失败容错

### 场景描述

在多模型并行调用中，某个模型可能失败，但不应影响其他模型的结果。

**业务需求**：
- 同时调用3个模型
- 即使1-2个模型失败，也要返回成功的结果
- 记录失败信息用于监控
- 用户体验不受影响

---

### 完整代码

```python
"""
场景1：部分失败容错
演示：多模型调用中的部分失败处理
"""

import asyncio
import random
from typing import Dict, Any
from langchain_core.runnables import RunnableParallel, RunnableLambda

# ===== 1. 模拟不稳定的模型 =====
async def unstable_model_1(text: str) -> str:
    """模型1（70%成功率）"""
    await asyncio.sleep(0.5)
    if random.random() < 0.7:
        return "模型1的回答：这是一个很好的问题"
    else:
        raise ConnectionError("模型1连接失败")

async def unstable_model_2(text: str) -> str:
    """模型2（80%成功率）"""
    await asyncio.sleep(0.5)
    if random.random() < 0.8:
        return "模型2的回答：让我来解释一下"
    else:
        raise TimeoutError("模型2超时")

async def unstable_model_3(text: str) -> str:
    """模型3（90%成功率）"""
    await asyncio.sleep(0.5)
    if random.random() < 0.9:
        return "模型3的回答：这个问题很有意思"
    else:
        raise ValueError("模型3返回错误")

# ===== 2. 方案1：使用 fallback（推荐）=====
print("=== 方案1：使用 fallback ===\n")

# 为每个模型添加 fallback
model1_with_fallback = RunnableLambda(unstable_model_1).with_fallback([
    RunnableLambda(lambda x: "模型1失败，使用默认回答")
])

model2_with_fallback = RunnableLambda(unstable_model_2).with_fallback([
    RunnableLambda(lambda x: "模型2失败，使用默认回答")
])

model3_with_fallback = RunnableLambda(unstable_model_3).with_fallback([
    RunnableLambda(lambda x: "模型3失败，使用默认回答")
])

# 并行执行（带容错）
parallel_with_fallback = RunnableParallel(
    model1=model1_with_fallback,
    model2=model2_with_fallback,
    model3=model3_with_fallback
)

async def test_fallback():
    """测试 fallback 方案"""
    print("测试 fallback 方案（运行5次）：\n")

    for i in range(5):
        print(f"第 {i+1} 次尝试:")
        result = await parallel_with_fallback.ainvoke("测试问题")

        for model, answer in result.items():
            status = "✓" if "失败" not in answer else "✗"
            print(f"  {status} {model}: {answer}")
        print()

# ===== 3. 方案2：使用 return_exceptions ===
print("\n=== 方案2：使用 return_exceptions ===\n")

async def parallel_with_exceptions(text: str) -> Dict[str, Any]:
    """并行执行，捕获异常"""
    tasks = {
        "model1": unstable_model_1(text),
        "model2": unstable_model_2(text),
        "model3": unstable_model_3(text)
    }

    # 使用 return_exceptions=True
    results = await asyncio.gather(
        *tasks.values(),
        return_exceptions=True
    )

    # 处理结果
    output = {}
    for key, result in zip(tasks.keys(), results):
        if isinstance(result, Exception):
            output[key] = {
                "success": False,
                "error": str(result),
                "answer": "模型失败，使用默认回答"
            }
        else:
            output[key] = {
                "success": True,
                "error": None,
                "answer": result
            }

    return output

async def test_exceptions():
    """测试 exceptions 方案"""
    print("测试 exceptions 方案（运行5次）：\n")

    for i in range(5):
        print(f"第 {i+1} 次尝试:")
        result = await parallel_with_exceptions("测试问题")

        for model, data in result.items():
            status = "✓" if data["success"] else "✗"
            print(f"  {status} {model}: {data['answer']}")
            if not data["success"]:
                print(f"     错误: {data['error']}")
        print()

# ===== 4. 方案3：手动容错包装器 ===
print("\n=== 方案3：手动容错包装器 ===\n")

class FaultTolerantWrapper:
    """容错包装器"""

    def __init__(self, func, default_value="默认回答"):
        self.func = func
        self.default_value = default_value
        self.error_count = 0
        self.success_count = 0

    async def __call__(self, input):
        """执行函数，失败时返回默认值"""
        try:
            result = await self.func(input)
            self.success_count += 1
            return {
                "success": True,
                "result": result,
                "error": None
            }
        except Exception as e:
            self.error_count += 1
            return {
                "success": False,
                "result": self.default_value,
                "error": str(e)
            }

    def get_stats(self):
        """获取统计信息"""
        total = self.success_count + self.error_count
        success_rate = self.success_count / total if total > 0 else 0
        return {
            "success_count": self.success_count,
            "error_count": self.error_count,
            "success_rate": success_rate
        }

# 包装模型
wrapped_model1 = FaultTolerantWrapper(unstable_model_1)
wrapped_model2 = FaultTolerantWrapper(unstable_model_2)
wrapped_model3 = FaultTolerantWrapper(unstable_model_3)

parallel_wrapped = RunnableParallel(
    model1=RunnableLambda(wrapped_model1),
    model2=RunnableLambda(wrapped_model2),
    model3=RunnableLambda(wrapped_model3)
)

async def test_wrapper():
    """测试包装器方案"""
    print("测试包装器方案（运行10次）：\n")

    for i in range(10):
        result = await parallel_wrapped.ainvoke("测试问题")

        if i < 3:  # 只打印前3次
            print(f"第 {i+1} 次尝试:")
            for model, data in result.items():
                status = "✓" if data["success"] else "✗"
                print(f"  {status} {model}: {data['result']}")
            print()

    # 打印统计信息
    print("=== 统计信息 ===")
    for model, wrapper in [
        ("model1", wrapped_model1),
        ("model2", wrapped_model2),
        ("model3", wrapped_model3)
    ]:
        stats = wrapper.get_stats()
        print(f"{model}:")
        print(f"  成功: {stats['success_count']}")
        print(f"  失败: {stats['error_count']}")
        print(f"  成功率: {stats['success_rate']*100:.1f}%")

# ===== 5. 主函数 =====
async def main():
    """主函数"""
    # 测试方案1
    await test_fallback()

    # 测试方案2
    await test_exceptions()

    # 测试方案3
    await test_wrapper()

# 运行
if __name__ == "__main__":
    asyncio.run(main())
```

---

### 运行结果

```
=== 方案1：使用 fallback ===

测试 fallback 方案（运行5次）：

第 1 次尝试:
  ✓ model1: 模型1的回答：这是一个很好的问题
  ✓ model2: 模型2的回答：让我来解释一下
  ✓ model3: 模型3的回答：这个问题很有意思

第 2 次尝试:
  ✗ model1: 模型1失败，使用默认回答
  ✓ model2: 模型2的回答：让我来解释一下
  ✓ model3: 模型3的回答：这个问题很有意思

第 3 次尝试:
  ✓ model1: 模型1的回答：这是一个很好的问题
  ✗ model2: 模型2失败，使用默认回答
  ✓ model3: 模型3的回答：这个问题很有意思

...

=== 统计信息 ===
model1:
  成功: 7
  失败: 3
  成功率: 70.0%
model2:
  成功: 8
  失败: 2
  成功率: 80.0%
model3:
  成功: 9
  失败: 1
  成功率: 90.0%
```

**容错效果**：
- 即使部分模型失败，系统仍能返回结果
- 用户体验不受影响
- 可以监控每个模型的成功率

---

## 场景2：超时处理

### 场景描述

某些模型响应很慢，需要设置超时避免无限等待。

**业务需求**：
- 每个模型最多等待3秒
- 超时后使用备用方案
- 记录超时信息

---

### 完整代码

```python
"""
场景2：超时处理
演示：并行执行中的超时控制
"""

import asyncio
import random
from typing import Dict, Any
from langchain_core.runnables import RunnableParallel, RunnableLambda

# ===== 1. 模拟慢速模型 =====
async def slow_model_1(text: str) -> str:
    """模型1（随机延迟1-5秒）"""
    delay = random.uniform(1, 5)
    print(f"  [模型1] 预计耗时: {delay:.1f}秒")
    await asyncio.sleep(delay)
    return f"模型1的回答（耗时{delay:.1f}秒）"

async def slow_model_2(text: str) -> str:
    """模型2（随机延迟1-5秒）"""
    delay = random.uniform(1, 5)
    print(f"  [模型2] 预计耗时: {delay:.1f}秒")
    await asyncio.sleep(delay)
    return f"模型2的回答（耗时{delay:.1f}秒）"

async def slow_model_3(text: str) -> str:
    """模型3（随机延迟1-5秒）"""
    delay = random.uniform(1, 5)
    print(f"  [模型3] 预计耗时: {delay:.1f}秒")
    await asyncio.sleep(delay)
    return f"模型3的回答（耗时{delay:.1f}秒）"

# ===== 2. 超时包装器 =====
class TimeoutWrapper:
    """超时包装器"""

    def __init__(self, func, timeout: float, default_value: str):
        self.func = func
        self.timeout = timeout
        self.default_value = default_value
        self.timeout_count = 0
        self.success_count = 0

    async def __call__(self, input):
        """执行函数，超时则返回默认值"""
        try:
            result = await asyncio.wait_for(
                self.func(input),
                timeout=self.timeout
            )
            self.success_count += 1
            return {
                "success": True,
                "result": result,
                "timeout": False
            }
        except asyncio.TimeoutError:
            self.timeout_count += 1
            return {
                "success": False,
                "result": self.default_value,
                "timeout": True
            }

    def get_stats(self):
        """获取统计信息"""
        total = self.success_count + self.timeout_count
        success_rate = self.success_count / total if total > 0 else 0
        return {
            "success_count": self.success_count,
            "timeout_count": self.timeout_count,
            "success_rate": success_rate
        }

# ===== 3. 创建带超时的并行链 =====
print("=== 创建带超时的并行链 ===\n")

# 包装模型（3秒超时）
timeout_model1 = TimeoutWrapper(slow_model_1, timeout=3.0, default_value="模型1超时，使用缓存")
timeout_model2 = TimeoutWrapper(slow_model_2, timeout=3.0, default_value="模型2超时，使用缓存")
timeout_model3 = TimeoutWrapper(slow_model_3, timeout=3.0, default_value="模型3超时，使用缓存")

parallel_with_timeout = RunnableParallel(
    model1=RunnableLambda(timeout_model1),
    model2=RunnableLambda(timeout_model2),
    model3=RunnableLambda(timeout_model3)
)

# ===== 4. 测试超时处理 =====
async def test_timeout():
    """测试超时处理"""
    print("测试超时处理（运行5次）：\n")

    for i in range(5):
        print(f"第 {i+1} 次尝试:")

        import time
        start = time.time()

        result = await parallel_with_timeout.ainvoke("测试问题")

        elapsed = time.time() - start
        print(f"  总耗时: {elapsed:.2f}秒")

        for model, data in result.items():
            if data["timeout"]:
                print(f"  ⏱ {model}: {data['result']} (超时)")
            else:
                print(f"  ✓ {model}: {data['result']}")
        print()

    # 打印统计信息
    print("=== 统计信息 ===")
    for model, wrapper in [
        ("model1", timeout_model1),
        ("model2", timeout_model2),
        ("model3", timeout_model3)
    ]:
        stats = wrapper.get_stats()
        print(f"{model}:")
        print(f"  成功: {stats['success_count']}")
        print(f"  超时: {stats['timeout_count']}")
        print(f"  成功率: {stats['success_rate']*100:.1f}%")

# ===== 5. 主函数 =====
async def main():
    """主函数"""
    await test_timeout()

# 运行
if __name__ == "__main__":
    asyncio.run(main())
```

---

### 运行结果

```
=== 创建带超时的并行链 ===

测试超时处理（运行5次）：

第 1 次尝试:
  [模型1] 预计耗时: 2.3秒
  [模型2] 预计耗时: 4.1秒
  [模型3] 预计耗时: 1.8秒
  总耗时: 3.01秒
  ✓ model1: 模型1的回答（耗时2.3秒）
  ⏱ model2: 模型2超时，使用缓存 (超时)
  ✓ model3: 模型3的回答（耗时1.8秒）

第 2 次尝试:
  [模型1] 预计耗时: 4.5秒
  [模型2] 预计耗时: 2.1秒
  [模型3] 预计耗时: 3.8秒
  总耗时: 3.01秒
  ⏱ model1: 模型1超时，使用缓存 (超时)
  ✓ model2: 模型2的回答（耗时2.1秒）
  ⏱ model3: 模型3超时，使用缓存 (超时)

...

=== 统计信息 ===
model1:
  成功: 3
  超时: 2
  成功率: 60.0%
model2:
  成功: 2
  超时: 3
  成功率: 40.0%
model3:
  成功: 4
  超时: 1
  成功率: 80.0%
```

**超时控制效果**：
- 总耗时始终 ≤ 3秒
- 超时任务使用备用方案
- 不影响其他任务执行

---

## 场景3：重试与降级

### 场景描述

模型调用失败时，先重试，重试失败后降级到更便宜的模型。

**业务需求**：
- 主模型失败后重试3次
- 重试失败后降级到备用模型
- 记录重试和降级信息

---

### 完整代码

```python
"""
场景3：重试与降级
演示：失败重试和模型降级策略
"""

import asyncio
import random
from typing import Dict, Any
from langchain_core.runnables import RunnableParallel, RunnableLambda

# ===== 1. 模拟主模型和备用模型 =====
async def primary_model(text: str) -> str:
    """主模型（50%成功率，昂贵）"""
    await asyncio.sleep(0.5)
    if random.random() < 0.5:
        return "主模型回答（高质量，$0.01）"
    else:
        raise ConnectionError("主模型连接失败")

async def backup_model(text: str) -> str:
    """备用模型（90%成功率，便宜）"""
    await asyncio.sleep(0.3)
    if random.random() < 0.9:
        return "备用模型回答（中等质量，$0.001）"
    else:
        raise ConnectionError("备用模型连接失败")

async def fallback_model(text: str) -> str:
    """兜底模型（100%成功率，最便宜）"""
    await asyncio.sleep(0.1)
    return "兜底模型回答（基础质量，$0.0001）"

# ===== 2. 重试包装器 =====
class RetryWrapper:
    """重试包装器"""

    def __init__(self, func, max_retries: int = 3, backoff: float = 1.0):
        self.func = func
        self.max_retries = max_retries
        self.backoff = backoff
        self.retry_count = 0
        self.success_count = 0
        self.failure_count = 0

    async def __call__(self, input):
        """执行函数，失败时重试"""
        for attempt in range(self.max_retries):
            try:
                result = await self.func(input)
                self.success_count += 1
                return {
                    "success": True,
                    "result": result,
                    "attempts": attempt + 1
                }
            except Exception as e:
                self.retry_count += 1
                if attempt < self.max_retries - 1:
                    # 指数退避
                    wait_time = self.backoff * (2 ** attempt)
                    await asyncio.sleep(wait_time)
                else:
                    # 最后一次尝试失败
                    self.failure_count += 1
                    raise

    def get_stats(self):
        """获取统计信息"""
        return {
            "success_count": self.success_count,
            "failure_count": self.failure_count,
            "retry_count": self.retry_count
        }

# ===== 3. 创建重试+降级链 =====
print("=== 创建重试+降级链 ===\n")

# 主模型（带重试）
primary_with_retry = RetryWrapper(primary_model, max_retries=3)

# 创建降级链：主模型 → 备用模型 → 兜底模型
degraded_chain = (
    RunnableLambda(primary_with_retry)
    .with_fallback([
        RunnableLambda(backup_model),
        RunnableLambda(fallback_model)
    ])
)

# 并行执行3个降级链
parallel_with_degradation = RunnableParallel(
    task1=degraded_chain,
    task2=degraded_chain,
    task3=degraded_chain
)

# ===== 4. 测试重试与降级 =====
async def test_retry_degradation():
    """测试重试与降级"""
    print("测试重试与降级（运行10次）：\n")

    results_summary = {
        "primary": 0,
        "backup": 0,
        "fallback": 0
    }

    for i in range(10):
        result = await parallel_with_degradation.ainvoke("测试问题")

        if i < 3:  # 只打印前3次
            print(f"第 {i+1} 次尝试:")
            for task, data in result.items():
                if isinstance(data, dict):
                    print(f"  {task}: {data['result']} (尝试{data['attempts']}次)")
                else:
                    print(f"  {task}: {data}")

        # 统计使用的模型
        for data in result.values():
            if isinstance(data, dict):
                answer = data['result']
            else:
                answer = data

            if "主模型" in answer:
                results_summary["primary"] += 1
            elif "备用模型" in answer:
                results_summary["backup"] += 1
            elif "兜底模型" in answer:
                results_summary["fallback"] += 1

        if i < 3:
            print()

    # 打印统计信息
    print("=== 统计信息 ===")
    total = sum(results_summary.values())
    print(f"总调用次数: {total}")
    print(f"主模型使用: {results_summary['primary']} ({results_summary['primary']/total*100:.1f}%)")
    print(f"备用模型使用: {results_summary['backup']} ({results_summary['backup']/total*100:.1f}%)")
    print(f"兜底模型使用: {results_summary['fallback']} ({results_summary['fallback']/total*100:.1f}%)")

    # 重试统计
    stats = primary_with_retry.get_stats()
    print(f"\n主模型重试统计:")
    print(f"  成功: {stats['success_count']}")
    print(f"  失败: {stats['failure_count']}")
    print(f"  重试次数: {stats['retry_count']}")

# ===== 5. 主函数 =====
async def main():
    """主函数"""
    await test_retry_degradation()

# 运行
if __name__ == "__main__":
    asyncio.run(main())
```

---

### 运行结果

```
=== 创建重试+降级链 ===

测试重试与降级（运行10次）：

第 1 次尝试:
  task1: 主模型回答（高质量，$0.01） (尝试2次)
  task2: 备用模型回答（中等质量，$0.001）
  task3: 主模型回答（高质量，$0.01） (尝试1次)

第 2 次尝试:
  task1: 备用模型回答（中等质量，$0.001）
  task2: 主模型回答（高质量，$0.01） (尝试3次)
  task3: 备用模型回答（中等质量，$0.001）

第 3 次尝试:
  task1: 主模型回答（高质量，$0.01） (尝试1次)
  task2: 备用模型回答（中等质量，$0.001）
  task3: 兜底模型回答（基础质量，$0.0001）

=== 统计信息 ===
总调用次数: 30
主模型使用: 15 (50.0%)
备用模型使用: 13 (43.3%)
兜底模型使用: 2 (6.7%)

主模型重试统计:
  成功: 15
  失败: 15
  重试次数: 28
```

**重试与降级效果**：
- 主模型失败后自动重试
- 重试失败后降级到备用模型
- 确保100%成功率
- 优化成本（优先使用主模型）

---

## 学习检查清单

- [ ] 能够实现部分失败容错
- [ ] 能够实现超时控制
- [ ] 能够实现重试机制
- [ ] 能够实现模型降级
- [ ] 理解错误处理的最佳实践
- [ ] 掌握监控和统计方法

---

## 参考资料

[来源: LangChain Best Practices - https://www.swarnendu.de/blog/langchain-best-practices, 访问日期: 2026-02-18]

[来源: 7 LangChain Retry & Timeout Patterns for Flaky Tools - https://medium.com/@connect.hashblock/7-langchain-retry-timeout-patterns-for-flaky-tools-a371c3edc1d3, 访问日期: 2026-02-18]

---

**版本**: v1.0
**最后更新**: 2026-02-18
**场景数量**: 3个错误处理场景
