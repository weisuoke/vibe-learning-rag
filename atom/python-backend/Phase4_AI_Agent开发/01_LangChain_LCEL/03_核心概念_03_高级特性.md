# 核心概念 03 - 高级特性

> 掌握 LCEL 的高级特性：并行执行、条件分支、错误处理、重试机制

---

## 核心概念1：并行执行 - RunnableParallel

### 一句话定义

**RunnableParallel 用于并行执行多个独立任务，通过字典语法 `{}` 自动触发，显著提升性能。**

---

### 基础用法

```python
from langchain_core.runnables import RunnableParallel

# 方式1：显式使用 RunnableParallel
parallel = RunnableParallel({
    "task1": runnable1,
    "task2": runnable2,
    "task3": runnable3
})

# 方式2：字典语法（推荐，更简洁）
parallel = {
    "task1": runnable1,
    "task2": runnable2,
    "task3": runnable3
}

# 调用（自动并行执行）
result = parallel.invoke(input)
# result = {"task1": result1, "task2": result2, "task3": result3}
```

---

### 性能对比

```python
import time
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI()

# 串行执行（慢）
start = time.time()
result1 = llm.invoke([{"role": "user", "content": "讲一个笑话"}])
result2 = llm.invoke([{"role": "user", "content": "讲一个故事"}])
result3 = llm.invoke([{"role": "user", "content": "讲一个谜语"}])
print(f"串行耗时: {time.time() - start:.1f}秒")  # 约 6 秒

# 并行执行（快）
start = time.time()
parallel = {
    "joke": ChatPromptTemplate.from_template("讲一个笑话") | llm,
    "story": ChatPromptTemplate.from_template("讲一个故事") | llm,
    "riddle": ChatPromptTemplate.from_template("讲一个谜语") | llm
}
result = await parallel.ainvoke({})
print(f"并行耗时: {time.time() - start:.1f}秒")  # 约 2 秒
```

**性能提升**：3倍（3个任务并行执行）

---

### 手写实现：RunnableParallel

```python
import asyncio
from typing import Dict, Any


class RunnableParallel:
    """并行执行多个 Runnable"""

    def __init__(self, steps: Dict[str, Any]):
        self.steps = steps

    def invoke(self, input: Any) -> Dict[str, Any]:
        """同步调用（实际上是串行）"""
        return {
            key: runnable.invoke(input)
            for key, runnable in self.steps.items()
        }

    async def ainvoke(self, input: Any) -> Dict[str, Any]:
        """异步调用（真正的并行）"""
        # 创建所有任务
        tasks = {
            key: runnable.ainvoke(input)
            for key, runnable in self.steps.items()
        }

        # 并行执行
        results = await asyncio.gather(*tasks.values())

        # 返回字典结果
        return dict(zip(tasks.keys(), results))


# 测试
class SlowTask:
    def __init__(self, name: str, delay: float):
        self.name = name
        self.delay = delay

    async def ainvoke(self, input: Any) -> str:
        await asyncio.sleep(self.delay)
        return f"{self.name} 完成"


async def test():
    parallel = RunnableParallel({
        "task1": SlowTask("任务1", 2.0),
        "task2": SlowTask("任务2", 2.0),
        "task3": SlowTask("任务3", 2.0)
    })

    import time
    start = time.time()
    result = await parallel.ainvoke("输入")
    print(f"耗时: {time.time() - start:.1f}秒")  # 2秒（并行）
    print(result)

asyncio.run(test())
```

---

### 在实际应用中

**场景1：RAG 多路检索**

```python
# 同时从多个数据源检索
parallel_retrieval = {
    "docs": doc_retriever,           # 文档检索
    "code": code_retriever,          # 代码检索
    "qa": qa_retriever               # 问答对检索
} | combine_results

result = parallel_retrieval.invoke("什么是 LCEL？")
```

**场景2：多模型对比**

```python
# 同时调用多个模型
multi_model = {
    "gpt35": ChatOpenAI(model="gpt-3.5-turbo") | parser,
    "gpt4": ChatOpenAI(model="gpt-4") | parser,
    "claude": ChatAnthropic(model="claude-3-sonnet") | parser
} | compare_and_select_best

result = multi_model.invoke(prompt)
```

**场景3：多维度分析**

```python
# 同时进行多个分析任务
analysis = {
    "summary": summarizer,           # 总结
    "keywords": keyword_extractor,   # 关键词提取
    "sentiment": sentiment_analyzer, # 情感分析
    "entities": entity_extractor     # 实体提取
} | format_report

result = analysis.invoke(text)
```

---

## 核心概念2：条件分支 - RunnableBranch

### 一句话定义

**RunnableBranch 根据条件选择不同的执行路径，类似于 if-elif-else 语句。**

---

### 基础用法

```python
from langchain_core.runnables import RunnableBranch

# 创建条件分支
branch = RunnableBranch(
    (lambda x: len(x) < 100, simple_chain),    # 条件1：简单问题
    (lambda x: len(x) < 500, normal_chain),    # 条件2：普通问题
    complex_chain                               # 默认：复杂问题
)

# 使用
result = branch.invoke("这是一个问题")
# 根据问题长度自动选择合适的链
```

---

### 完整示例

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch

# 定义三个不同复杂度的链
simple_chain = (
    ChatPromptTemplate.from_template("简单回答：{question}")
    | ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    | StrOutputParser()
)

normal_chain = (
    ChatPromptTemplate.from_template("详细回答：{question}")
    | ChatOpenAI(model="gpt-3.5-turbo", temperature=0.5)
    | StrOutputParser()
)

complex_chain = (
    ChatPromptTemplate.from_template("深入分析：{question}")
    | ChatOpenAI(model="gpt-4", temperature=0.7)
    | StrOutputParser()
)

# 创建条件分支
branch = RunnableBranch(
    (lambda x: len(x["question"]) < 50, simple_chain),
    (lambda x: len(x["question"]) < 200, normal_chain),
    complex_chain
)

# 测试
questions = [
    "什么是 LCEL？",  # 简单问题 → simple_chain
    "请详细解释 LCEL 的工作原理和应用场景。",  # 普通问题 → normal_chain
    "请深入分析 LCEL 的设计理念、实现原理、性能优化策略，以及在生产环境中的最佳实践。"  # 复杂问题 → complex_chain
]

for q in questions:
    result = branch.invoke({"question": q})
    print(f"问题长度: {len(q)}")
    print(f"回答: {result}\n")
```

---

### 手写实现：RunnableBranch

```python
from typing import Callable, Any, List, Tuple


class RunnableBranch:
    """条件分支执行"""

    def __init__(self, *branches):
        """
        branches: 元组列表，每个元组包含 (条件函数, Runnable)
        最后一个可以是默认 Runnable（没有条件）
        """
        self.branches = []
        self.default = None

        for item in branches:
            if isinstance(item, tuple):
                # (条件, Runnable)
                condition, runnable = item
                self.branches.append((condition, runnable))
            else:
                # 默认 Runnable
                self.default = item

    def invoke(self, input: Any) -> Any:
        """根据条件选择执行路径"""
        # 依次检查条件
        for condition, runnable in self.branches:
            if condition(input):
                return runnable.invoke(input)

        # 没有匹配的条件，执行默认
        if self.default:
            return self.default.invoke(input)

        raise ValueError("没有匹配的分支，也没有默认分支")

    async def ainvoke(self, input: Any) -> Any:
        """异步版本"""
        for condition, runnable in self.branches:
            if condition(input):
                return await runnable.ainvoke(input)

        if self.default:
            return await self.default.ainvoke(input)

        raise ValueError("没有匹配的分支，也没有默认分支")


# 测试
class EchoRunnable:
    def __init__(self, prefix: str):
        self.prefix = prefix

    def invoke(self, input: Any) -> str:
        return f"{self.prefix}: {input}"


branch = RunnableBranch(
    (lambda x: x < 10, EchoRunnable("小于10")),
    (lambda x: x < 100, EchoRunnable("小于100")),
    EchoRunnable("大于等于100")
)

print(branch.invoke(5))    # "小于10: 5"
print(branch.invoke(50))   # "小于100: 50"
print(branch.invoke(500))  # "大于等于100: 500"
```

---

### 在实际应用中

**场景1：根据用户类型选择模型**

```python
# 根据用户类型选择不同的模型
user_based_chain = RunnableBranch(
    (lambda x: x["user_type"] == "premium", gpt4_chain),
    (lambda x: x["user_type"] == "standard", gpt35_chain),
    free_chain
)

result = user_based_chain.invoke({
    "user_type": "premium",
    "question": "什么是 LCEL？"
})
```

**场景2：根据问题类型路由**

```python
# 根据问题类型选择不同的处理链
question_router = RunnableBranch(
    (lambda x: "代码" in x["question"], code_qa_chain),
    (lambda x: "文档" in x["question"], doc_qa_chain),
    (lambda x: "数据" in x["question"], data_qa_chain),
    general_qa_chain
)

result = question_router.invoke({"question": "如何写代码？"})
```

**场景3：根据语言选择 Prompt**

```python
# 根据语言选择不同的 Prompt
language_router = RunnableBranch(
    (lambda x: x["language"] == "zh", chinese_prompt | llm | parser),
    (lambda x: x["language"] == "en", english_prompt | llm | parser),
    default_prompt | llm | parser
)

result = language_router.invoke({
    "language": "zh",
    "question": "什么是 LCEL？"
})
```

---

## 核心概念3：错误处理 - with_fallbacks

### 一句话定义

**with_fallbacks 提供降级策略，当主链失败时自动切换到备用链，保证系统可用性。**

---

### 基础用法

```python
from langchain_openai import ChatOpenAI

# 主模型
primary_llm = ChatOpenAI(model="gpt-4")

# 备用模型
backup_llm = ChatOpenAI(model="gpt-3.5-turbo")

# 添加降级策略
llm_with_fallback = primary_llm.with_fallbacks([backup_llm])

# 使用
chain = prompt | llm_with_fallback | parser

# 如果 gpt-4 失败，自动切换到 gpt-3.5-turbo
result = chain.invoke({"question": "什么是 LCEL？"})
```

---

### 多级降级

```python
# 多级降级策略
llm_with_fallbacks = primary_llm.with_fallbacks([
    backup_llm_1,  # 第一备用
    backup_llm_2,  # 第二备用
    backup_llm_3   # 第三备用
])

# 依次尝试：primary → backup_1 → backup_2 → backup_3
```

---

### 手写实现：with_fallbacks

```python
from typing import List, Any


class RunnableWithFallbacks:
    """带降级策略的 Runnable"""

    def __init__(self, runnable: Any, fallbacks: List[Any]):
        self.runnable = runnable
        self.fallbacks = fallbacks

    def invoke(self, input: Any) -> Any:
        """依次尝试主链和备用链"""
        # 尝试主链
        try:
            return self.runnable.invoke(input)
        except Exception as e:
            print(f"主链失败: {e}")

        # 依次尝试备用链
        for i, fallback in enumerate(self.fallbacks):
            try:
                print(f"尝试备用链 {i+1}")
                return fallback.invoke(input)
            except Exception as e:
                print(f"备用链 {i+1} 失败: {e}")

        # 所有链都失败
        raise Exception("所有链都失败了")

    async def ainvoke(self, input: Any) -> Any:
        """异步版本"""
        try:
            return await self.runnable.ainvoke(input)
        except Exception as e:
            print(f"主链失败: {e}")

        for i, fallback in enumerate(self.fallbacks):
            try:
                print(f"尝试备用链 {i+1}")
                return await fallback.ainvoke(input)
            except Exception as e:
                print(f"备用链 {i+1} 失败: {e}")

        raise Exception("所有链都失败了")


# 测试
class FailingRunnable:
    def invoke(self, input: Any) -> str:
        raise Exception("故意失败")


class SuccessRunnable:
    def __init__(self, name: str):
        self.name = name

    def invoke(self, input: Any) -> str:
        return f"{self.name} 成功: {input}"


runnable = RunnableWithFallbacks(
    runnable=FailingRunnable(),
    fallbacks=[
        FailingRunnable(),
        SuccessRunnable("备用链2")
    ]
)

result = runnable.invoke("测试")
print(result)  # "备用链2 成功: 测试"
```

---

### 在实际应用中

**场景1：模型降级**

```python
# 主模型失败时降级到更便宜的模型
chain = prompt | ChatOpenAI(model="gpt-4").with_fallbacks([
    ChatOpenAI(model="gpt-3.5-turbo"),
    ChatOpenAI(model="gpt-3.5-turbo-16k")
]) | parser

# 保证服务可用性
```

**场景2：检索降级**

```python
# 向量检索失败时降级到关键词检索
retriever = vector_retriever.with_fallbacks([
    keyword_retriever,
    fuzzy_retriever
])

chain = {
    "context": retriever,
    "question": RunnablePassthrough()
} | prompt | llm | parser
```

**场景3：API 降级**

```python
# 主 API 失败时切换到备用 API
api_chain = primary_api.with_fallbacks([
    backup_api_1,
    backup_api_2,
    local_cache  # 最后降级到本地缓存
])
```

---

## 核心概念4：重试机制 - with_retry

### 一句话定义

**with_retry 在失败时自动重试，适用于临时性错误（网络波动、API 限流等）。**

---

### 基础用法

```python
from langchain_openai import ChatOpenAI

# 添加重试机制
llm = ChatOpenAI().with_retry(
    stop_after_attempt=3,  # 最多重试3次
    wait_exponential_jitter=True  # 指数退避 + 随机抖动
)

# 使用
chain = prompt | llm | parser

# 如果失败，自动重试最多3次
result = chain.invoke({"question": "什么是 LCEL？"})
```

---

### 配置参数

```python
llm = ChatOpenAI().with_retry(
    stop_after_attempt=3,           # 最多重试3次
    wait_exponential_jitter=True,   # 指数退避 + 随机抖动
    retry_if_exception_type=(       # 只重试特定异常
        TimeoutError,
        ConnectionError
    )
)
```

**重试策略**：
- 第1次失败：等待 1 秒后重试
- 第2次失败：等待 2 秒后重试
- 第3次失败：等待 4 秒后重试
- 第4次失败：放弃

---

### 手写实现：with_retry

```python
import time
import random
from typing import Any, Type, Tuple


class RunnableWithRetry:
    """带重试机制的 Runnable"""

    def __init__(
        self,
        runnable: Any,
        max_attempts: int = 3,
        retry_exceptions: Tuple[Type[Exception], ...] = (Exception,)
    ):
        self.runnable = runnable
        self.max_attempts = max_attempts
        self.retry_exceptions = retry_exceptions

    def invoke(self, input: Any) -> Any:
        """带重试的调用"""
        for attempt in range(self.max_attempts):
            try:
                return self.runnable.invoke(input)
            except self.retry_exceptions as e:
                if attempt == self.max_attempts - 1:
                    # 最后一次尝试，抛出异常
                    raise e

                # 指数退避 + 随机抖动
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"第 {attempt + 1} 次失败，{wait_time:.1f}秒后重试...")
                time.sleep(wait_time)

    async def ainvoke(self, input: Any) -> Any:
        """异步版本"""
        import asyncio

        for attempt in range(self.max_attempts):
            try:
                return await self.runnable.ainvoke(input)
            except self.retry_exceptions as e:
                if attempt == self.max_attempts - 1:
                    raise e

                wait_time = (2 ** attempt) + random.uniform(0, 1)
                print(f"第 {attempt + 1} 次失败，{wait_time:.1f}秒后重试...")
                await asyncio.sleep(wait_time)


# 测试
class UnstableRunnable:
    def __init__(self, fail_count: int = 2):
        self.fail_count = fail_count
        self.attempts = 0

    def invoke(self, input: Any) -> str:
        self.attempts += 1
        if self.attempts <= self.fail_count:
            raise Exception(f"第 {self.attempts} 次调用失败")
        return f"第 {self.attempts} 次调用成功"


runnable = RunnableWithRetry(
    runnable=UnstableRunnable(fail_count=2),
    max_attempts=3
)

result = runnable.invoke("测试")
print(result)  # "第 3 次调用成功"
```

---

### 在实际应用中

**场景1：LLM API 重试**

```python
# LLM API 可能因为限流失败，需要重试
llm = ChatOpenAI().with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
)

chain = prompt | llm | parser
```

**场景2：数据库查询重试**

```python
# 数据库查询可能因为连接问题失败
db_query = database_runnable.with_retry(
    stop_after_attempt=3,
    retry_if_exception_type=(ConnectionError, TimeoutError)
)
```

**场景3：外部 API 重试**

```python
# 外部 API 可能因为网络波动失败
api_call = external_api.with_retry(
    stop_after_attempt=5,
    wait_exponential_jitter=True
)
```

---

## 组合使用：降级 + 重试

### 最佳实践

```python
# 先重试，再降级
llm = (
    ChatOpenAI(model="gpt-4")
    .with_retry(stop_after_attempt=3)  # 先重试3次
    .with_fallbacks([                   # 重试失败后降级
        ChatOpenAI(model="gpt-3.5-turbo").with_retry(stop_after_attempt=2),
        ChatOpenAI(model="gpt-3.5-turbo-16k")
    ])
)

chain = prompt | llm | parser

# 执行流程：
# 1. 尝试 gpt-4（最多3次）
# 2. 失败后降级到 gpt-3.5-turbo（最多2次）
# 3. 失败后降级到 gpt-3.5-turbo-16k
```

---

### 完整示例：生产级链

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch

# 1. 定义不同复杂度的链
simple_chain = (
    ChatPromptTemplate.from_template("简单回答：{question}")
    | ChatOpenAI(model="gpt-3.5-turbo")
    .with_retry(stop_after_attempt=3)
    .with_fallbacks([ChatOpenAI(model="gpt-3.5-turbo-16k")])
    | StrOutputParser()
)

complex_chain = (
    ChatPromptTemplate.from_template("深入分析：{question}")
    | ChatOpenAI(model="gpt-4")
    .with_retry(stop_after_attempt=3)
    .with_fallbacks([
        ChatOpenAI(model="gpt-3.5-turbo"),
        ChatOpenAI(model="gpt-3.5-turbo-16k")
    ])
    | StrOutputParser()
)

# 2. 根据问题复杂度路由
router = RunnableBranch(
    (lambda x: len(x["question"]) < 100, simple_chain),
    complex_chain
)

# 3. 添加并行分析（可选）
analysis_chain = {
    "answer": router,
    "keywords": keyword_extractor.with_retry(stop_after_attempt=2),
    "sentiment": sentiment_analyzer.with_retry(stop_after_attempt=2)
}

# 4. 使用
result = analysis_chain.invoke({"question": "什么是 LCEL？"})
print(result)
# {
#     "answer": "...",
#     "keywords": ["LCEL", "LangChain", "表达式语言"],
#     "sentiment": "neutral"
# }
```

---

## 高级特性总结表

| 特性 | 作用 | 触发方式 | 适用场景 |
|------|------|---------|---------|
| **RunnableParallel** | 并行执行 | 字典语法 `{}` | 多个独立任务 |
| **RunnableBranch** | 条件分支 | `RunnableBranch(...)` | 根据条件选择路径 |
| **with_fallbacks** | 降级策略 | `.with_fallbacks([...])` | 主链失败时切换备用 |
| **with_retry** | 重试机制 | `.with_retry(...)` | 临时性错误自动重试 |

---

## 学习检查清单

掌握高级特性后，你应该能够：

- [ ] 使用字典语法 `{}` 实现并行执行
- [ ] 理解并行执行的性能优势
- [ ] 使用 RunnableBranch 实现条件路由
- [ ] 根据不同条件选择不同的执行路径
- [ ] 使用 with_fallbacks 添加降级策略
- [ ] 使用 with_retry 添加重试机制
- [ ] 组合使用降级和重试（先重试，再降级）
- [ ] 构建生产级的 LCEL 链（路由 + 重试 + 降级 + 并行）
- [ ] 理解何时使用并行、何时使用串行
- [ ] 理解何时使用重试、何时使用降级

---

## 下一步

掌握了高级特性后，继续学习：

- **07_实战代码_01_基础链.md** - 实战代码：基础链的完整实现
- **07_实战代码_02_RAG链.md** - 实战代码：RAG 链的完整实现
- **07_实战代码_03_多链组合.md** - 实战代码：复杂链的组合

---

**版本**: v1.0
**最后更新**: 2026-02-12
