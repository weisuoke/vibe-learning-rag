# 核心概念 - 生成质量评估指标

## 概述

生成质量评估是RAG系统评估的第二步，确保LLM生成的答案忠实、相关、正确。本文档介绍2025-2026年生产环境中使用的核心生成评估指标。

---

## 一、Faithfulness (忠实度)

### 定义

Faithfulness衡量生成内容是否忠实于检索到的上下文，防止LLM"编造"信息。

**核心问题**: 答案中的每个陈述是否都能在上下文中找到依据？

### 评估方法

**方法1: NLI-based (Natural Language Inference)**

```python
from transformers import pipeline

# 使用NLI模型判断蕴含关系
nli_model = pipeline("text-classification",
                     model="microsoft/deberta-large-mnli")

def evaluate_faithfulness_nli(answer, context):
    """
    使用NLI模型评估忠实度

    Args:
        answer: 生成的答案
        context: 检索到的上下文

    Returns:
        float: 忠实度分数 (0-1)
    """
    # 将答案拆分为多个陈述
    statements = split_into_statements(answer)

    faithful_count = 0
    for statement in statements:
        # 判断context是否蕴含statement
        result = nli_model(f"{context} [SEP] {statement}")

        # entailment表示蕴含，即statement可以从context推导出
        if result[0]['label'] == 'ENTAILMENT':
            faithful_count += 1

    return faithful_count / len(statements) if statements else 0.0

def split_into_statements(text):
    """将文本拆分为独立陈述"""
    # 简单实现：按句号分割
    statements = [s.strip() for s in text.split('.') if s.strip()]
    return statements
```

**方法2: LLM-as-judge**

```python
from openai import OpenAI

def evaluate_faithfulness_llm(answer, context):
    """
    使用LLM评估忠实度

    Args:
        answer: 生成的答案
        context: 检索到的上下文

    Returns:
        float: 忠实度分数 (0-1)
    """
    client = OpenAI()

    prompt = f"""
你是一个严格的评估者。判断答案是否完全基于给定的上下文。

上下文:
{context}

答案:
{answer}

评估步骤:
1. 将答案拆分为独立的陈述
2. 对每个陈述，判断是否能在上下文中找到依据
3. 计算有依据的陈述比例

返回格式:
{{
    "faithful_statements": <有依据的陈述数>,
    "total_statements": <总陈述数>,
    "faithfulness_score": <分数0-1>,
    "unfaithful_parts": [<没有依据的陈述列表>]
}}

只返回JSON，不要解释。
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )

    import json
    result = json.loads(response.choices[0].message.content)
    return result['faithfulness_score']
```

### RAG应用场景

```python
# 场景: 医疗问答系统
context = """
阿司匹林是一种非甾体抗炎药(NSAID)，用于缓解轻至中度疼痛，
如头痛、牙痛、肌肉痛。常见副作用包括胃部不适和出血风险增加。
"""

# 答案A: 忠实
answer_faithful = "阿司匹林可以缓解头痛和牙痛，但可能导致胃部不适。"
score_a = evaluate_faithfulness_llm(answer_faithful, context)
print(f"Answer A Faithfulness: {score_a:.2f}")  # ~1.0

# 答案B: 不忠实（编造信息）
answer_unfaithful = "阿司匹林可以缓解头痛、牙痛和关节炎，每天服用可以预防心脏病。"
score_b = evaluate_faithfulness_llm(answer_unfaithful, context)
print(f"Answer B Faithfulness: {score_b:.2f}")  # ~0.5
```

### 优化建议

| Faithfulness | 评级 | 建议 |
|--------------|------|------|
| < 0.7 | 差 | Prompt需要强调"仅基于上下文"，考虑使用更强模型 |
| 0.7-0.85 | 中 | 可接受，但需要优化Prompt |
| 0.85-0.95 | 良好 | 生成质量较好 |
| > 0.95 | 优秀 | 生成质量优秀 |

---

## 二、Answer Relevancy (答案相关性)

### 定义

Answer Relevancy衡量生成答案是否直接回答了用户问题。

**核心问题**: 答案是否切题？是否答非所问？

### 评估方法

**方法1: 语义相似度**

```python
from sentence_transformers import SentenceTransformer, util

def evaluate_relevancy_semantic(question, answer):
    """
    使用语义相似度评估相关性

    Args:
        question: 用户问题
        answer: 生成的答案

    Returns:
        float: 相关性分数 (0-1)
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # 计算问题和答案的Embedding
    question_emb = model.encode(question, convert_to_tensor=True)
    answer_emb = model.encode(answer, convert_to_tensor=True)

    # 计算余弦相似度
    similarity = util.cos_sim(question_emb, answer_emb).item()

    return similarity
```

**方法2: LLM-as-judge**

```python
def evaluate_relevancy_llm(question, answer):
    """
    使用LLM评估相关性

    Args:
        question: 用户问题
        answer: 生成的答案

    Returns:
        float: 相关性分数 (0-1)
    """
    client = OpenAI()

    prompt = f"""
你是一个严格的评估者。判断答案是否直接回答了问题。

问题:
{question}

答案:
{answer}

评估标准:
1. 答案是否直接回答了问题的核心？
2. 答案是否包含问题所需的关键信息？
3. 答案是否偏离主题或答非所问？

返回格式:
{{
    "is_relevant": <true/false>,
    "relevancy_score": <分数0-1>,
    "missing_aspects": [<问题中未被回答的方面>],
    "irrelevant_parts": [<答案中不相关的部分>]
}}

只返回JSON，不要解释。
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )

    import json
    result = json.loads(response.choices[0].message.content)
    return result['relevancy_score']
```

**方法3: 反向问题生成 (RAGAS方法)**

```python
def evaluate_relevancy_reverse_question(question, answer):
    """
    通过反向生成问题评估相关性

    原理: 如果答案相关，从答案生成的问题应该与原问题相似

    Args:
        question: 原始问题
        answer: 生成的答案

    Returns:
        float: 相关性分数 (0-1)
    """
    client = OpenAI()

    # 步骤1: 从答案生成问题
    prompt = f"""
基于以下答案，生成3个可能的问题。

答案:
{answer}

返回格式:
{{
    "questions": ["问题1", "问题2", "问题3"]
}}

只返回JSON，不要解释。
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        response_format={"type": "json_object"}
    )

    import json
    generated_questions = json.loads(response.choices[0].message.content)['questions']

    # 步骤2: 计算原问题与生成问题的相似度
    model = SentenceTransformer('all-MiniLM-L6-v2')
    question_emb = model.encode(question, convert_to_tensor=True)

    similarities = []
    for gen_q in generated_questions:
        gen_q_emb = model.encode(gen_q, convert_to_tensor=True)
        sim = util.cos_sim(question_emb, gen_q_emb).item()
        similarities.append(sim)

    # 返回平均相似度
    return sum(similarities) / len(similarities)
```

### RAG应用场景

```python
# 场景: 客服问答
question = "如何退货？"

# 答案A: 相关
answer_relevant = """
退货流程如下：
1. 登录您的账户
2. 进入"我的订单"页面
3. 选择需要退货的订单
4. 点击"申请退货"按钮
5. 填写退货原因并提交
"""
score_a = evaluate_relevancy_llm(question, answer_relevant)
print(f"Answer A Relevancy: {score_a:.2f}")  # ~1.0

# 答案B: 不相关（答非所问）
answer_irrelevant = """
我们提供7天无理由退货服务，退货政策详见用户协议。
我们的客服团队随时为您服务。
"""
score_b = evaluate_relevancy_llm(question, answer_irrelevant)
print(f"Answer B Relevancy: {score_b:.2f}")  # ~0.3
```

### 优化建议

| Relevancy | 评级 | 建议 |
|-----------|------|------|
| < 0.6 | 差 | Prompt需要强调"直接回答问题" |
| 0.6-0.8 | 中 | 可接受，但需要优化 |
| 0.8-0.9 | 良好 | 相关性较好 |
| > 0.9 | 优秀 | 相关性优秀 |

---

## 三、Answer Correctness (答案正确性)

### 定义

Answer Correctness衡量生成答案与标准答案(ground truth)的匹配度。

**前提**: 需要有标准答案作为参考

### 评估方法

**方法1: 精确匹配**

```python
def evaluate_correctness_exact_match(answer, ground_truth):
    """
    精确匹配评估

    Args:
        answer: 生成的答案
        ground_truth: 标准答案

    Returns:
        float: 正确性分数 (0或1)
    """
    # 标准化文本
    answer_norm = answer.lower().strip()
    gt_norm = ground_truth.lower().strip()

    return 1.0 if answer_norm == gt_norm else 0.0
```

**方法2: F1分数 (Token级别)**

```python
def evaluate_correctness_f1(answer, ground_truth):
    """
    计算Token级别的F1分数

    Args:
        answer: 生成的答案
        ground_truth: 标准答案

    Returns:
        float: F1分数 (0-1)
    """
    # Token化
    answer_tokens = set(answer.lower().split())
    gt_tokens = set(ground_truth.lower().split())

    # 计算交集
    common_tokens = answer_tokens & gt_tokens

    if len(common_tokens) == 0:
        return 0.0

    # 计算Precision和Recall
    precision = len(common_tokens) / len(answer_tokens)
    recall = len(common_tokens) / len(gt_tokens)

    # 计算F1
    f1 = 2 * (precision * recall) / (precision + recall)

    return f1
```

**方法3: 语义相似度**

```python
def evaluate_correctness_semantic(answer, ground_truth):
    """
    使用语义相似度评估正确性

    Args:
        answer: 生成的答案
        ground_truth: 标准答案

    Returns:
        float: 正确性分数 (0-1)
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')

    answer_emb = model.encode(answer, convert_to_tensor=True)
    gt_emb = model.encode(ground_truth, convert_to_tensor=True)

    similarity = util.cos_sim(answer_emb, gt_emb).item()

    return similarity
```

**方法4: LLM-as-judge**

```python
def evaluate_correctness_llm(answer, ground_truth):
    """
    使用LLM评估正确性

    Args:
        answer: 生成的答案
        ground_truth: 标准答案

    Returns:
        float: 正确性分数 (0-1)
    """
    client = OpenAI()

    prompt = f"""
你是一个严格的评估者。比较生成答案与标准答案的正确性。

标准答案:
{ground_truth}

生成答案:
{answer}

评估标准:
1. 核心事实是否一致？
2. 关键信息是否完整？
3. 是否有错误信息？

返回格式:
{{
    "correctness_score": <分数0-1>,
    "correct_facts": [<正确的事实>],
    "incorrect_facts": [<错误的事实>],
    "missing_facts": [<遗漏的事实>]
}}

只返回JSON，不要解释。
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )

    import json
    result = json.loads(response.choices[0].message.content)
    return result['correctness_score']
```

### RAG应用场景

```python
# 场景: FAQ系统
question = "公司年假多少天？"
ground_truth = "公司年假为10天，工作满5年增加到15天。"

# 答案A: 完全正确
answer_correct = "公司年假为10天，工作满5年增加到15天。"
score_a = evaluate_correctness_llm(answer_correct, ground_truth)
print(f"Answer A Correctness: {score_a:.2f}")  # ~1.0

# 答案B: 部分正确
answer_partial = "公司年假为10天。"
score_b = evaluate_correctness_llm(answer_partial, ground_truth)
print(f"Answer B Correctness: {score_b:.2f}")  # ~0.7

# 答案C: 错误
answer_wrong = "公司年假为15天。"
score_c = evaluate_correctness_llm(answer_wrong, ground_truth)
print(f"Answer C Correctness: {score_c:.2f}")  # ~0.3
```

---

## 四、Fluency (流畅度)

### 定义

Fluency衡量生成文本的自然度和可读性。

### 评估方法

**方法1: 困惑度 (Perplexity)**

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

def evaluate_fluency_perplexity(text):
    """
    使用困惑度评估流畅度

    Args:
        text: 生成的文本

    Returns:
        float: 困惑度 (越低越好)
    """
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    # Tokenize
    inputs = tokenizer(text, return_tensors='pt')

    # 计算困惑度
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs['input_ids'])
        loss = outputs.loss
        perplexity = torch.exp(loss).item()

    return perplexity
```

**方法2: LLM-as-judge**

```python
def evaluate_fluency_llm(text):
    """
    使用LLM评估流畅度

    Args:
        text: 生成的文本

    Returns:
        float: 流畅度分数 (0-1)
    """
    client = OpenAI()

    prompt = f"""
你是一个语言专家。评估以下文本的流畅度。

文本:
{text}

评估标准:
1. 语法是否正确？
2. 句子是否通顺？
3. 表达是否自然？
4. 是否有重复或冗余？

返回格式:
{{
    "fluency_score": <分数0-1>,
    "grammar_issues": [<语法问题>],
    "readability": <可读性评分0-1>
}}

只返回JSON，不要解释。
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        response_format={"type": "json_object"}
    )

    import json
    result = json.loads(response.choices[0].message.content)
    return result['fluency_score']
```

---

## 五、Groundedness (基于性)

### 定义

Groundedness衡量答案是否基于提供的上下文，与Faithfulness类似但更严格。

### 评估方法

```python
def evaluate_groundedness(answer, context):
    """
    评估答案的基于性

    使用NLI模型判断每个陈述是否被上下文蕴含

    Args:
        answer: 生成的答案
        context: 检索到的上下文

    Returns:
        float: 基于性分数 (0-1)
    """
    from transformers import pipeline

    nli_model = pipeline("text-classification",
                        model="microsoft/deberta-large-mnli")

    # 拆分答案为陈述
    statements = split_into_statements(answer)

    grounded_count = 0
    for statement in statements:
        # 判断蕴含关系
        result = nli_model(f"{context} [SEP] {statement}")

        if result[0]['label'] == 'ENTAILMENT' and result[0]['score'] > 0.8:
            grounded_count += 1

    return grounded_count / len(statements) if statements else 0.0
```

---

## 六、指标对比与选择

### 指标特点对比

| 指标 | 关注点 | 需要ground truth | 评估成本 | 适用场景 |
|------|--------|------------------|----------|----------|
| **Faithfulness** | 忠实于上下文 | 否 | 中 | 所有RAG场景 |
| **Answer Relevancy** | 回答问题 | 否 | 中 | 所有RAG场景 |
| **Answer Correctness** | 与标准答案匹配 | 是 | 低-中 | 有标准答案的场景 |
| **Fluency** | 文本流畅度 | 否 | 低 | 对文本质量要求高的场景 |
| **Groundedness** | 严格基于上下文 | 否 | 中-高 | 高风险场景(医疗、法律) |

### 选择建议

```python
# 场景1: 通用RAG系统
recommended_metrics = ['Faithfulness', 'Answer Relevancy']

# 场景2: 有标准答案的FAQ系统
recommended_metrics = ['Answer Correctness', 'Answer Relevancy']

# 场景3: 高风险领域(医疗、法律)
recommended_metrics = ['Groundedness', 'Faithfulness', 'Answer Correctness']

# 场景4: 内容生成系统
recommended_metrics = ['Fluency', 'Answer Relevancy', 'Faithfulness']
```

### 2025-2026年行业基准

```python
industry_benchmarks = {
    '通用RAG': {
        'Faithfulness': 0.90,
        'Answer Relevancy': 0.85
    },
    'FAQ系统': {
        'Answer Correctness': 0.85,
        'Answer Relevancy': 0.90
    },
    '医疗问答': {
        'Groundedness': 0.95,
        'Faithfulness': 0.95,
        'Answer Correctness': 0.90
    },
    '内容生成': {
        'Fluency': 0.90,
        'Answer Relevancy': 0.80,
        'Faithfulness': 0.85
    }
}
```

---

## 七、完整评估示例

```python
class GenerationEvaluator:
    """生成质量评估器"""

    def __init__(self):
        self.client = OpenAI()
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    def evaluate(self, question, answer, context, ground_truth=None):
        """
        完整评估生成质量

        Args:
            question: 用户问题
            answer: 生成的答案
            context: 检索到的上下文
            ground_truth: 标准答案(可选)

        Returns:
            dict: 评估结果
        """
        results = {}

        # 1. Faithfulness
        results['faithfulness'] = self.evaluate_faithfulness(answer, context)

        # 2. Answer Relevancy
        results['answer_relevancy'] = self.evaluate_relevancy(question, answer)

        # 3. Answer Correctness (如果有ground truth)
        if ground_truth:
            results['answer_correctness'] = self.evaluate_correctness(
                answer, ground_truth
            )

        # 4. Fluency
        results['fluency'] = self.evaluate_fluency(answer)

        return results

    def evaluate_faithfulness(self, answer, context):
        """评估忠实度"""
        return evaluate_faithfulness_llm(answer, context)

    def evaluate_relevancy(self, question, answer):
        """评估相关性"""
        return evaluate_relevancy_llm(question, answer)

    def evaluate_correctness(self, answer, ground_truth):
        """评估正确性"""
        return evaluate_correctness_semantic(answer, ground_truth)

    def evaluate_fluency(self, answer):
        """评估流畅度"""
        return evaluate_fluency_llm(answer)

    def report(self, results):
        """生成评估报告"""
        print("=" * 50)
        print("生成质量评估报告")
        print("=" * 50)

        for metric, score in results.items():
            status = "✓" if score >= 0.8 else "✗"
            print(f"{status} {metric:20s}: {score:.3f}")

        print("=" * 50)

# 使用示例
evaluator = GenerationEvaluator()

results = evaluator.evaluate(
    question="公司的年假政策是什么？",
    answer="公司年假为10天，工作满5年增加到15天。",
    context="公司年假为10天，工作满5年增加到15天。员工可在每年1月申请。",
    ground_truth="年假10天，满5年15天"
)

evaluator.report(results)
```

---

## 八、总结

### 核心要点

1. **Faithfulness**: 防止幻觉，确保答案基于上下文
2. **Answer Relevancy**: 确保答案切题，不答非所问
3. **Answer Correctness**: 与标准答案对比，需要ground truth
4. **Fluency**: 确保文本自然流畅
5. **Groundedness**: 严格版的Faithfulness，用于高风险场景

### 实践建议

1. **必选指标**: Faithfulness + Answer Relevancy
2. **可选指标**: 根据场景选择Correctness/Fluency/Groundedness
3. **评估方法**: LLM-as-judge最灵活，NLI最严格，语义相似度最快
4. **成本控制**: 使用gpt-4o-mini进行评估，降低成本
5. **持续监控**: 生产环境实时追踪核心指标

### 2025-2026年标准

```python
production_standards = {
    'minimum': {
        'Faithfulness': 0.85,
        'Answer Relevancy': 0.80
    },
    'good': {
        'Faithfulness': 0.90,
        'Answer Relevancy': 0.85
    },
    'excellent': {
        'Faithfulness': 0.95,
        'Answer Relevancy': 0.90
    }
}
```

---

**参考资料**:
- https://medium.com/@sanjeebmeister/rag-evaluation-metrics-explained-a-complete-guide-with-examples-dea8bf4467db
- https://www.deepchecks.com/rag-evaluation-metrics-answer-relevancy-faithfulness-accuracy
- https://www.meilisearch.com/blog/rag-evaluation
