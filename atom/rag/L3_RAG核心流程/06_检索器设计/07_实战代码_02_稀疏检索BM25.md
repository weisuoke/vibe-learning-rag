# 实战代码2：稀疏检索BM25

> 完整的BM25稀疏检索实现，包含中文分词、停用词过滤和参数调优

---

## 代码概述

本示例展示如何实现一个完整的BM25稀疏检索系统，包括：
1. 中文分词（jieba）
2. 停用词过滤
3. BM25算法实现
4. 参数调优
5. 与LangChain集成

---

## 完整代码

```python
"""
BM25稀疏检索完整实现
演示：从文档分词到检索的完整流程
"""

import os
from typing import List, Dict, Optional, Set
import jieba
import jieba.analyse
from rank_bm25 import BM25Okapi, BM25L, BM25Plus
import numpy as np
import time
import pickle

# ===== 1. 停用词管理 =====
class StopwordsManager:
    """停用词管理器"""

    def __init__(self, stopwords_file: Optional[str] = None):
        self.stopwords = self._load_default_stopwords()

        # 加载自定义停用词
        if stopwords_file and os.path.exists(stopwords_file):
            with open(stopwords_file, 'r', encoding='utf-8') as f:
                custom_stopwords = set(line.strip() for line in f)
                self.stopwords.update(custom_stopwords)

        print(f"✓ 加载{len(self.stopwords)}个停用词")

    def _load_default_stopwords(self) -> Set[str]:
        """加载默认停用词"""
        # 常见中文停用词
        default_stopwords = {
            '的', '了', '在', '是', '我', '有', '和', '就', '不', '人',
            '都', '一', '一个', '上', '也', '很', '到', '说', '要', '去',
            '你', '会', '着', '没有', '看', '好', '自己', '这', '那', '个',
            '们', '中', '来', '为', '能', '对', '与', '把', '从', '给',
            '但', '被', '让', '由', '使', '则', '将', '或', '及', '而'
        }
        return default_stopwords

    def filter(self, tokens: List[str]) -> List[str]:
        """
        过滤停用词

        Args:
            tokens: 分词列表

        Returns:
            过滤后的分词列表
        """
        return [t for t in tokens if t not in self.stopwords and len(t) > 1]


# ===== 2. 中文分词器 =====
class ChineseTokenizer:
    """中文分词器（基于jieba）"""

    def __init__(self, user_dict: Optional[str] = None):
        # 加载自定义词典
        if user_dict and os.path.exists(user_dict):
            jieba.load_userdict(user_dict)
            print(f"✓ 加载自定义词典: {user_dict}")

        # 停用词管理器
        self.stopwords_manager = StopwordsManager()

    def add_word(self, word: str, freq: Optional[int] = None):
        """
        添加自定义词

        Args:
            word: 词语
            freq: 词频（可选）
        """
        if freq:
            jieba.add_word(word, freq)
        else:
            jieba.add_word(word)

    def tokenize(self, text: str, use_stopwords: bool = True) -> List[str]:
        """
        分词

        Args:
            text: 输入文本
            use_stopwords: 是否过滤停用词

        Returns:
            分词列表
        """
        # jieba分词（精确模式）
        tokens = list(jieba.cut(text, cut_all=False))

        # 过滤停用词
        if use_stopwords:
            tokens = self.stopwords_manager.filter(tokens)

        return tokens

    def tokenize_batch(self, texts: List[str], use_stopwords: bool = True) -> List[List[str]]:
        """
        批量分词

        Args:
            texts: 文本列表
            use_stopwords: 是否过滤停用词

        Returns:
            分词列表的列表
        """
        return [self.tokenize(text, use_stopwords) for text in texts]


# ===== 3. BM25检索器 =====
class BM25Retriever:
    """BM25稀疏检索器"""

    def __init__(
        self,
        algorithm: str = "okapi",  # okapi, l, plus
        k1: float = 1.5,
        b: float = 0.75,
        delta: float = 0.5
    ):
        """
        初始化BM25检索器

        Args:
            algorithm: BM25算法类型（okapi/l/plus）
            k1: 词频饱和参数
            b: 长度归一化参数
            delta: BM25L/Plus的额外参数
        """
        self.algorithm = algorithm
        self.k1 = k1
        self.b = b
        self.delta = delta

        self.tokenizer = ChineseTokenizer()
        self.documents = []
        self.tokenized_docs = []
        self.bm25 = None

    def index_documents(self, documents: List[str]):
        """
        索引文档

        Args:
            documents: 文档列表
        """
        print(f"\n=== 索引{len(documents)}个文档 ===")

        # 保存原始文档
        self.documents = documents

        # 分词
        start = time.time()
        self.tokenized_docs = self.tokenizer.tokenize_batch(documents)
        tokenize_time = time.time() - start

        print(f"✓ 分词耗时: {tokenize_time:.2f}s")

        # 构建BM25索引
        start = time.time()
        if self.algorithm == "okapi":
            self.bm25 = BM25Okapi(self.tokenized_docs, k1=self.k1, b=self.b)
        elif self.algorithm == "l":
            self.bm25 = BM25L(self.tokenized_docs, k1=self.k1, b=self.b, delta=self.delta)
        elif self.algorithm == "plus":
            self.bm25 = BM25Plus(self.tokenized_docs, k1=self.k1, b=self.b, delta=self.delta)
        else:
            raise ValueError(f"Unknown algorithm: {self.algorithm}")

        index_time = time.time() - start

        print(f"✓ 索引构建耗时: {index_time:.2f}s")
        print(f"✓ 总耗时: {tokenize_time + index_time:.2f}s")

    def search(self, query: str, k: int = 5) -> List[Dict]:
        """
        检索相似文档

        Args:
            query: 查询文本
            k: 返回结果数量

        Returns:
            检索结果列表
        """
        if self.bm25 is None:
            raise ValueError("请先调用index_documents()索引文档")

        print(f"\n=== 检索: {query} ===")

        # 查询分词
        start = time.time()
        query_tokens = self.tokenizer.tokenize(query)
        tokenize_time = time.time() - start

        print(f"✓ 查询分词: {query_tokens}")
        print(f"✓ 分词耗时: {tokenize_time:.3f}s")

        # BM25检索
        start = time.time()
        scores = self.bm25.get_scores(query_tokens)
        search_time = time.time() - start

        print(f"✓ 检索耗时: {search_time:.3f}s")

        # 排序并返回Top-K
        top_k_indices = scores.argsort()[-k:][::-1]

        results = []
        for rank, idx in enumerate(top_k_indices, 1):
            results.append({
                "rank": rank,
                "document": self.documents[idx],
                "score": scores[idx],
                "tokens": self.tokenized_docs[idx],
                "id": f"doc_{idx}"
            })

        return results

    def save(self, filepath: str):
        """保存索引"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                "algorithm": self.algorithm,
                "k1": self.k1,
                "b": self.b,
                "delta": self.delta,
                "documents": self.documents,
                "tokenized_docs": self.tokenized_docs,
                "bm25": self.bm25
            }, f)
        print(f"✓ 索引已保存到: {filepath}")

    @classmethod
    def load(cls, filepath: str):
        """加载索引"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)

        retriever = cls(
            algorithm=data["algorithm"],
            k1=data["k1"],
            b=data["b"],
            delta=data["delta"]
        )
        retriever.documents = data["documents"]
        retriever.tokenized_docs = data["tokenized_docs"]
        retriever.bm25 = data["bm25"]

        print(f"✓ 索引已加载: {filepath}")
        return retriever


# ===== 4. 参数调优 =====
class BM25Tuner:
    """BM25参数调优器"""

    def __init__(self, documents: List[str], test_queries: List[tuple]):
        """
        初始化调优器

        Args:
            documents: 文档列表
            test_queries: 测试查询列表 [(query, relevant_docs), ...]
        """
        self.documents = documents
        self.test_queries = test_queries

    def tune_k1(self, k1_values: List[float] = [1.2, 1.5, 2.0]) -> Dict:
        """
        调优k1参数

        Args:
            k1_values: k1候选值列表

        Returns:
            调优结果
        """
        print("\n=== 调优k1参数 ===")

        results = []

        for k1 in k1_values:
            retriever = BM25Retriever(algorithm="okapi", k1=k1, b=0.75)
            retriever.index_documents(self.documents)

            # 评估
            recall = self._evaluate(retriever)

            results.append({
                "k1": k1,
                "recall": recall
            })

            print(f"k1={k1}: Recall@5={recall:.3f}")

        # 找到最优k1
        best = max(results, key=lambda x: x["recall"])
        print(f"\n✓ 最优k1: {best['k1']} (Recall@5={best['recall']:.3f})")

        return best

    def tune_b(self, b_values: List[float] = [0.0, 0.5, 0.75, 1.0]) -> Dict:
        """
        调优b参数

        Args:
            b_values: b候选值列表

        Returns:
            调优结果
        """
        print("\n=== 调优b参数 ===")

        results = []

        for b in b_values:
            retriever = BM25Retriever(algorithm="okapi", k1=1.5, b=b)
            retriever.index_documents(self.documents)

            # 评估
            recall = self._evaluate(retriever)

            results.append({
                "b": b,
                "recall": recall
            })

            print(f"b={b}: Recall@5={recall:.3f}")

        # 找到最优b
        best = max(results, key=lambda x: x["recall"])
        print(f"\n✓ 最优b: {best['b']} (Recall@5={best['recall']:.3f})")

        return best

    def _evaluate(self, retriever: BM25Retriever) -> float:
        """
        评估检索器

        Args:
            retriever: BM25检索器

        Returns:
            Recall@5
        """
        recalls = []

        for query, relevant_docs in self.test_queries:
            results = retriever.search(query, k=5)
            retrieved_ids = [r["id"] for r in results]

            # 计算Recall@5
            hits = len(set(retrieved_ids) & set(relevant_docs))
            recall = hits / len(relevant_docs) if relevant_docs else 0
            recalls.append(recall)

        return np.mean(recalls)


# ===== 5. 使用示例 =====
def main():
    """主函数"""

    # 创建检索器
    retriever = BM25Retriever(algorithm="okapi", k1=1.5, b=0.75)

    # 添加自定义词
    retriever.tokenizer.add_word("RAG系统")
    retriever.tokenizer.add_word("混合检索")
    retriever.tokenizer.add_word("BM25算法")

    # 准备文档
    documents = [
        "Python是一种高级编程语言，由Guido van Rossum于1991年创建。",
        "JavaScript是一种用于Web开发的编程语言，主要用于前端开发。",
        "机器学习是人工智能的一个子领域，专注于让计算机从数据中学习。",
        "深度学习使用神经网络进行特征学习，是机器学习的一个分支。",
        "RAG系统结合检索和生成技术，用于构建智能问答系统。",
        "FastAPI是一个现代、快速的Python Web框架，用于构建API。",
        "Docker是一个容器化平台，用于打包和部署应用程序。",
        "Kubernetes是一个容器编排系统，用于管理容器化应用。",
        "PostgreSQL是一个强大的开源关系型数据库系统。",
        "Redis是一个内存数据库，常用于缓存和消息队列。"
    ]

    # 索引文档
    retriever.index_documents(documents)

    # 测试查询
    test_queries = [
        "如何学习编程",
        "人工智能技术",
        "容器化部署",
        "数据库选择"
    ]

    for query in test_queries:
        results = retriever.search(query, k=3)

        print(f"\n=== Top-3结果 ===")
        for result in results:
            print(f"\n{result['rank']}. [得分: {result['score']:.3f}]")
            print(f"   文档: {result['document']}")
            print(f"   分词: {result['tokens'][:10]}...")  # 只显示前10个词

    # 保存索引
    retriever.save("bm25_index.pkl")

    # 加载索引
    loaded_retriever = BM25Retriever.load("bm25_index.pkl")
    print("\n✓ 索引加载成功")


# ===== 6. 参数调优示例 =====
def tune_parameters():
    """参数调优示例"""

    print("\n" + "="*50)
    print("BM25参数调优")
    print("="*50)

    # 准备文档
    documents = [
        "Python是一种高级编程语言",
        "JavaScript用于Web开发",
        "机器学习是AI的核心",
        "深度学习使用神经网络",
        "RAG系统结合检索和生成",
        "FastAPI是Python Web框架",
        "Docker用于容器化部署",
        "Kubernetes管理容器应用",
        "PostgreSQL是关系型数据库",
        "Redis是内存数据库"
    ]

    # 测试查询（带标注）
    test_queries = [
        ("编程语言", ["doc_0", "doc_1"]),
        ("人工智能", ["doc_2", "doc_3"]),
        ("容器", ["doc_6", "doc_7"]),
        ("数据库", ["doc_8", "doc_9"])
    ]

    # 创建调优器
    tuner = BM25Tuner(documents, test_queries)

    # 调优k1
    best_k1 = tuner.tune_k1([1.2, 1.5, 2.0])

    # 调优b
    best_b = tuner.tune_b([0.0, 0.5, 0.75, 1.0])

    print(f"\n=== 最优参数 ===")
    print(f"k1: {best_k1['k1']}")
    print(f"b: {best_b['b']}")


# ===== 7. 性能测试 =====
def benchmark():
    """性能基准测试"""

    print("\n" + "="*50)
    print("性能基准测试")
    print("="*50)

    retriever = BM25Retriever()

    # 生成测试文档
    n_docs = 10000
    documents = [f"这是第{i}个测试文档，内容关于主题{i%10}" for i in range(n_docs)]

    # 测试索引性能
    print(f"\n测试索引{n_docs}个文档...")
    start = time.time()
    retriever.index_documents(documents)
    index_time = time.time() - start

    print(f"\n索引性能:")
    print(f"  总耗时: {index_time:.2f}s")
    print(f"  平均每文档: {index_time/n_docs*1000:.2f}ms")

    # 测试检索性能
    print(f"\n测试检索性能...")
    query = "测试查询"
    n_queries = 1000

    start = time.time()
    for _ in range(n_queries):
        retriever.search(query, k=5)
    search_time = time.time() - start

    print(f"\n检索性能:")
    print(f"  总耗时: {search_time:.2f}s")
    print(f"  平均每查询: {search_time/n_queries*1000:.2f}ms")
    print(f"  QPS: {n_queries/search_time:.1f}")


if __name__ == "__main__":
    # 运行主示例
    main()

    # 运行参数调优（可选）
    # tune_parameters()

    # 运行性能测试（可选）
    # benchmark()
```

---

## 运行输出示例

```
=== 索引10个文档 ===
✓ 分词耗时: 0.15s
✓ 索引构建耗时: 0.01s
✓ 总耗时: 0.16s

=== 检索: 如何学习编程 ===
✓ 查询分词: ['学习', '编程']
✓ 分词耗时: 0.003s
✓ 检索耗时: 0.001s

=== Top-3结果 ===

1. [得分: 3.245]
   文档: Python是一种高级编程语言，由Guido van Rossum于1991年创建。
   分词: ['Python', '高级', '编程', '语言', 'Guido', 'van', 'Rossum', '1991', '创建']...

2. [得分: 2.876]
   文档: JavaScript是一种用于Web开发的编程语言，主要用于前端开发。
   分词: ['JavaScript', '用于', 'Web', '开发', '编程', '语言', '主要', '前端', '开发']...

3. [得分: 1.234]
   文档: FastAPI是一个现代、快速的Python Web框架，用于构建API。
   分词: ['FastAPI', '现代', '快速', 'Python', 'Web', '框架', '用于', '构建', 'API']...
```

---

## 代码说明

### 1. StopwordsManager类

**功能**：
- 管理停用词列表
- 过滤无意义的词（如"的"、"了"等）
- 支持自定义停用词

**关键方法**：
- `filter()`: 过滤停用词

### 2. ChineseTokenizer类

**功能**：
- 中文分词（基于jieba）
- 支持自定义词典
- 停用词过滤

**关键方法**：
- `tokenize()`: 单个文本分词
- `tokenize_batch()`: 批量分词
- `add_word()`: 添加自定义词

### 3. BM25Retriever类

**功能**：
- BM25算法实现
- 支持三种变体（Okapi/L/Plus）
- 索引保存和加载

**关键方法**：
- `index_documents()`: 索引文档
- `search()`: 检索文档
- `save()`/`load()`: 保存/加载索引

### 4. BM25Tuner类

**功能**：
- 参数调优
- 评估检索质量
- 找到最优参数

**关键方法**：
- `tune_k1()`: 调优k1参数
- `tune_b()`: 调优b参数

---

## 优化要点

### 1. 中文分词优化

```python
# 添加自定义词典
jieba.load_userdict("custom_dict.txt")

# 格式：词语 词频 词性
# RAG系统 100 n
# 混合检索 50 n

# 动态添加词
jieba.add_word("检索器设计", freq=100)
```

### 2. 停用词优化

```python
# 自定义停用词文件
stopwords = """
的
了
在
是
我
有
和
"""

with open("stopwords.txt", "w", encoding="utf-8") as f:
    f.write(stopwords)

# 加载
tokenizer = ChineseTokenizer()
tokenizer.stopwords_manager = StopwordsManager("stopwords.txt")
```

### 3. BM25参数选择

```python
# 推荐参数
params = {
    "通用场景": {"k1": 1.5, "b": 0.75},
    "短文档": {"k1": 1.2, "b": 0.5},
    "长文档": {"k1": 2.0, "b": 0.9}
}
```

---

## 扩展功能

### 1. 多语言支持

```python
def detect_language(text: str) -> str:
    """检测语言"""
    # 简单判断：包含中文用jieba，否则用split
    if any('\u4e00' <= char <= '\u9fff' for char in text):
        return "zh"
    else:
        return "en"

def tokenize_multilingual(text: str) -> List[str]:
    """多语言分词"""
    lang = detect_language(text)

    if lang == "zh":
        return list(jieba.cut(text))
    else:
        return text.lower().split()
```

### 2. TF-IDF权重

```python
from sklearn.feature_extraction.text import TfidfVectorizer

def compute_tfidf_weights(documents: List[List[str]]):
    """计算TF-IDF权重"""
    # 将分词列表转换为字符串
    docs_str = [" ".join(doc) for doc in documents]

    # 计算TF-IDF
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(docs_str)

    return tfidf_matrix, vectorizer
```

### 3. 查询扩展

```python
def expand_query(query: str, synonyms: Dict[str, List[str]]) -> str:
    """查询扩展（添加同义词）"""
    tokens = jieba.cut(query)
    expanded_tokens = []

    for token in tokens:
        expanded_tokens.append(token)
        # 添加同义词
        if token in synonyms:
            expanded_tokens.extend(synonyms[token])

    return " ".join(expanded_tokens)

# 使用
synonyms = {
    "编程": ["程序设计", "代码"],
    "学习": ["掌握", "了解"]
}

expanded = expand_query("如何学习编程", synonyms)
# 输出: "如何 学习 掌握 了解 编程 程序设计 代码"
```

---

## 常见问题

### Q1: 中文分词不准确怎么办？

**A**:
1. 添加自定义词典
2. 使用jieba.add_word()动态添加
3. 尝试其他分词工具（pkuseg、thulac）

### Q2: BM25得分如何归一化？

**A**:
```python
def normalize_scores(scores: np.ndarray) -> np.ndarray:
    """归一化BM25得分到[0, 1]"""
    min_score = scores.min()
    max_score = scores.max()

    if max_score == min_score:
        return np.ones_like(scores)

    return (scores - min_score) / (max_score - min_score)
```

### Q3: 如何处理英文文档？

**A**:
```python
# 英文分词
def tokenize_english(text: str) -> List[str]:
    # 转小写 + 分词
    return text.lower().split()

# 或使用nltk
import nltk
from nltk.tokenize import word_tokenize

tokens = word_tokenize(text.lower())
```

---

## 总结

本示例展示了完整的BM25稀疏检索实现，包括：
- ✅ 中文分词（jieba）
- ✅ 停用词过滤
- ✅ BM25算法（三种变体）
- ✅ 参数调优
- ✅ 索引保存/加载
- ✅ 性能优化

**下一步**：学习【实战代码3：混合检索融合】，实现Dense+Sparse的混合检索。
