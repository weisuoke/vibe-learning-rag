# 双重类比

用前端开发和日常生活的类比，帮助理解上下文注入与生成。

---

## 类比1：Prompt 组装 ≈ API 请求组装

### 前端类比：构建 HTTP 请求

```
RAG Prompt 组装              前端 API 请求组装
─────────────────────────    ─────────────────────────
System Prompt (角色指令)  ≈   Headers (请求头)
Context (检索结果)        ≈   Body (请求体数据)
User Query (用户问题)     ≈   Query Params (查询参数)
LLM Response (生成答案)   ≈   API Response (响应数据)
```

```python
# RAG Prompt 组装
prompt = f"""
{system_prompt}      # 类似 Headers：定义行为规则

【参考资料】
{context}            # 类似 Body：携带数据

【问题】
{query}              # 类似 Query Params：具体请求
"""
```

```javascript
// 前端 API 请求组装
const response = await fetch('/api/answer', {
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer xxx'  // 类似 System Prompt
  },
  body: JSON.stringify({
    context: retrievedDocs,        // 类似 RAG Context
    question: userQuery            // 类似 User Query
  })
});
```

### 日常生活类比：给厨师下单

```
你去餐厅点菜：

System Prompt = 告诉厨师你的饮食要求
  "我对花生过敏，请不要放花生"
  "我喜欢清淡口味"

Context = 给厨师看菜单和食材
  "今天有这些新鲜食材：..."
  "这是我们的招牌菜做法：..."

User Query = 你的具体点单
  "我想吃一道鱼"

LLM Response = 厨师做出的菜
  基于你的要求 + 可用食材 + 具体点单
```

---

## 类比2：Token 预算 ≈ 请求体大小限制

### 前端类比：HTTP 请求大小限制

```
Token 预算管理                前端请求限制
─────────────────────────    ─────────────────────────
Context Window (128K)     ≈   Max Request Size (10MB)
Token 计数                ≈   Content-Length
预留输出空间              ≈   预留响应缓冲区
动态截断                  ≈   分页/懒加载
```

```python
# RAG：Token 预算管理
max_tokens = 12000
system_tokens = 500
query_tokens = 100
context_budget = max_tokens - system_tokens - query_tokens - 2000  # 预留输出
```

```javascript
// 前端：请求大小管理
const MAX_BODY_SIZE = 10 * 1024 * 1024; // 10MB
const headerSize = JSON.stringify(headers).length;
const querySize = query.length;
const bodyBudget = MAX_BODY_SIZE - headerSize - querySize - BUFFER;
```

### 日常生活类比：行李箱容量

```
你要打包行李去旅行：

Context Window = 行李箱总容量 (20kg)

Token 分配：
- System Prompt = 必带物品 (护照、钱包) → 1kg
- User Query = 旅行目的相关 (泳衣/滑雪服) → 1kg
- Context = 其他衣物和用品 → 15kg
- 预留空间 = 回程买的纪念品 → 3kg

如果东西太多：
- 优先级截断 = 只带最重要的
- 压缩 = 用真空袋压缩衣物
```

---

## 类比3：Lost in the Middle ≈ 长列表滚动

### 前端类比：长列表的用户注意力

```
Lost in the Middle           前端长列表
─────────────────────────    ─────────────────────────
开头内容注意力高          ≈   首屏内容点击率高
中间内容容易被忽略        ≈   中间内容滚动跳过
结尾内容注意力回升        ≈   底部"加载更多"被注意到
```

```
用户浏览电商列表：

位置        注意力      类比 LLM
────────    ────────    ────────
第1-3个     ████████    开头内容，重点关注
第4-7个     ████        中间内容，快速滑过
第8-10个    ██████      结尾内容，停下来看看

解决方案：
- 前端：把重要商品放首屏
- RAG：把最相关文档放开头和结尾
```

### 日常生活类比：开会时的注意力

```
一个1小时的会议：

时间段          注意力        类比
────────────    ────────      ────────
开头 10 分钟    ████████      精神集中，记住要点
中间 40 分钟    ████          走神，刷手机
结尾 10 分钟    ██████        快结束了，重新集中

所以好的会议主持人会：
- 开头：说最重要的事
- 结尾：总结和行动项
- 中间：讨论细节（可以会后看纪要）

RAG 也一样：
- 开头：放最相关的文档
- 结尾：放次相关的文档
- 中间：放补充信息
```

---

## 类比4：生成参数 ≈ 随机数种子

### 前端类比：Math.random() 的控制

```
Temperature                  随机数控制
─────────────────────────    ─────────────────────────
temperature=0             ≈   固定种子，结果确定
temperature=0.7           ≈   部分随机
temperature=1.0           ≈   完全随机
```

```python
# RAG：temperature 控制
llm = ChatOpenAI(temperature=0)    # 每次回答一样
llm = ChatOpenAI(temperature=1)    # 每次回答不同
```

```javascript
// 前端：随机数控制
function seededRandom(seed) {
  // 固定种子 = 确定性输出
  return Math.sin(seed) * 10000 % 1;
}

Math.random();  // 每次不同 = temperature=1
seededRandom(42);  // 每次相同 = temperature=0
```

### 日常生活类比：厨师的创意程度

```
temperature = 厨师做菜的创意程度

temperature=0（严格按菜谱）：
- "请按照标准做法做一份宫保鸡丁"
- 每次做出来都一样
- 适合：需要稳定口味的连锁店

temperature=0.7（适度创新）：
- "做一份宫保鸡丁，可以有点个人风格"
- 基本一样，但有小变化
- 适合：大多数餐厅

temperature=1.0（自由发挥）：
- "用这些食材做一道创意菜"
- 每次都不一样
- 适合：创意料理、私房菜
```

---

## 类比5：RAG 生成链 ≈ 前端数据流

### 前端类比：React 数据流

```
RAG 生成链                   React 数据流
─────────────────────────    ─────────────────────────
检索结果                  ≈   API 响应数据
format_docs()             ≈   数据转换/格式化
Prompt 模板               ≈   组件模板
LLM 生成                  ≈   渲染输出
StrOutputParser           ≈   结果解析
```

```python
# RAG 链
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
```

```javascript
// React 数据流（概念类比）
const Answer = ({ question }) => {
  const docs = useRetriever(question);           // 检索
  const context = formatDocs(docs);              // 格式化
  const prompt = buildPrompt(context, question); // 组装
  const answer = useLLM(prompt);                 // 生成
  return <div>{parseOutput(answer)}</div>;       // 解析输出
};
```

### 日常生活类比：做一道菜的流程

```
RAG 生成链 = 做菜流程

1. 检索 (Retriever)
   = 从冰箱里找食材
   "我要做番茄炒蛋，需要番茄和鸡蛋"

2. 格式化 (format_docs)
   = 食材预处理
   "番茄切块，鸡蛋打散"

3. Prompt 组装
   = 按菜谱准备
   "先炒鸡蛋，再炒番茄，最后混合"

4. LLM 生成
   = 开火烹饪
   "实际操作，产出成品"

5. 输出解析
   = 装盘上桌
   "把菜从锅里盛出来"
```

---

## 类比总结表

| RAG 概念 | 前端类比 | 日常生活类比 |
|----------|----------|--------------|
| Prompt 组装 | API 请求组装 | 给厨师下单 |
| System Prompt | HTTP Headers | 饮食要求/过敏信息 |
| Context | Request Body | 可用食材清单 |
| User Query | Query Params | 具体点什么菜 |
| Token 预算 | 请求大小限制 | 行李箱容量 |
| Lost in the Middle | 长列表注意力 | 开会时的注意力 |
| Temperature | 随机数种子 | 厨师创意程度 |
| RAG Chain | React 数据流 | 做菜流程 |

---

## 一句话记忆

**上下文注入与生成 = 组装 API 请求 + 调用接口 + 解析响应**

就像前端调接口一样：
- 准备好请求头（System Prompt）
- 准备好请求体（Context）
- 发送请求（调用 LLM）
- 处理响应（解析答案）

---

**下一步：** [06_反直觉点](./06_反直觉点.md) - 避开最常见的三个误区
