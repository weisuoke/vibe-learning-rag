# 实战代码5：上下文感知分块实现 ⭐ NEW 2024-2025

完整可运行的上下文感知分块代码示例（Anthropic 2024-2025 方法）。

---

## 基础实现

```python
"""
上下文感知分块 - 基础实现
研究来源: Anthropic 2024-2025 Contextual Retrieval

效果：减少 49-67% 检索失败
成本：$0.02/10万字
"""

from anthropic import Anthropic
from langchain.text_splitter import RecursiveCharacterTextSplitter

client = Anthropic()

def add_context_to_chunk(
    chunk: str,
    document: str,
    chunk_index: int = 0,
    total_chunks: int = 0
) -> str:
    """
    为 chunk 添加上下文（Anthropic 方法）
    
    效果：减少 49% 检索失败
    """
    prompt = f"""
为以下文档片段生成 50-100 token 的上下文说明。

完整文档（前2000字符）：
{document[:2000]}

文档片段（第 {chunk_index+1}/{total_chunks} 块）：
{chunk}

要求：
1. 说明这个片段在文档中的位置（章节、主题）
2. 概括片段的核心内容
3. 保持简洁（50-100 tokens）

格式：
这是关于[主题]的[文档类型]，本片段位于[章节]，讨论[核心内容]。
"""

    response = client.messages.create(
        model="claude-3-haiku-20240307",  # 使用便宜的模型
        max_tokens=150,
        messages=[{"role": "user", "content": prompt}]
    )
    
    context = response.content[0].text.strip()
    return f"{context}\n\n{chunk}"

# 使用示例
document = """
Python 编程基础教程

第一章：变量与数据类型
Python 是动态类型语言。变量不需要声明类型。
"""

# 1. 基础分块
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77
)
base_chunks = splitter.split_text(document)

# 2. 添加上下文
contextual_chunks = []
for i, chunk in enumerate(base_chunks):
    contextual_chunk = add_context_to_chunk(
        chunk, document, i, len(base_chunks)
    )
    contextual_chunks.append(contextual_chunk)

print("原始 chunk:")
print(base_chunks[0][:200])
print("\n上下文感知 chunk:")
print(contextual_chunks[0][:300])
```

---

## 生产级实现（Prompt Caching 优化）

```python
"""
生产级上下文感知分块
使用 Prompt Caching 降低 90% 成本
"""

from anthropic import Anthropic
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict

class ContextualChunker:
    """上下文感知分块器（生产级）"""
    
    def __init__(
        self,
        model: str = "claude-3-haiku-20240307",
        context_length: int = 100
    ):
        self.client = Anthropic()
        self.model = model
        self.context_length = context_length
    
    def chunk_with_context(
        self,
        document: str,
        base_chunks: List[str]
    ) -> List[Dict]:
        """为基础分块添加上下文"""
        contextual_chunks = []
        
        for i, chunk in enumerate(base_chunks):
            context = self._generate_context_cached(
                chunk, document, i, len(base_chunks)
            )
            contextual_chunks.append({
                "original_chunk": chunk,
                "context": context,
                "final_text": f"{context}\n\n{chunk}",
                "index": i
            })
        
        return contextual_chunks
    
    def _generate_context_cached(
        self,
        chunk: str,
        document: str,
        index: int,
        total: int
    ) -> str:
        """生成上下文（使用 Prompt Caching）"""
        response = self.client.messages.create(
            model=self.model,
            max_tokens=self.context_length + 50,
            system=[
                {
                    "type": "text",
                    "text": "你是文档上下文生成专家。",
                },
                {
                    "type": "text",
                    "text": f"完整文档：\n{document}",
                    "cache_control": {"type": "ephemeral"}  # 缓存文档
                }
            ],
            messages=[{
                "role": "user",
                "content": f"""
为文档片段生成 {self.context_length} token 的上下文说明。

片段位置：第 {index+1}/{total} 块
片段内容：
{chunk[:500]}

要求：
1. 说明片段在文档中的位置和主题
2. 保持简洁（约 {self.context_length} tokens）
3. 使用陈述句，不要使用"这是..."开头
"""
            }]
        )
        
        return response.content[0].text.strip()

# 使用示例
from langchain.text_splitter import RecursiveCharacterTextSplitter

document = "你的长文本..." * 100

# 1. 基础分块（NVIDIA 2025 推荐）
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77
)
base_chunks = splitter.split_text(document)

# 2. 添加上下文
chunker = ContextualChunker()
contextual_chunks = chunker.chunk_with_context(document, base_chunks)

# 3. 查看结果
for chunk_data in contextual_chunks[:2]:
    print(f"\n原始: {chunk_data['original_chunk'][:100]}...")
    print(f"上下文: {chunk_data['context']}")
    print(f"最终: {chunk_data['final_text'][:150]}...")
```

---

## 与 Reranking 结合（减少 67% 失败）

```python
"""
上下文感知 + Reranking
效果：减少 67% 检索失败
"""

from anthropic import Anthropic
import numpy as np
from typing import List, Dict

def contextual_retrieval_with_rerank(
    query: str,
    contextual_chunks: List[Dict],
    top_k: int = 5
) -> List[Dict]:
    """
    上下文感知检索 + Reranking
    
    效果：减少 67% 检索失败
    """
    from openai import OpenAI
    client = OpenAI()
    
    # 1. 向量检索（使用上下文感知的 chunk）
    query_embedding = client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    ).data[0].embedding
    
    chunk_embeddings = [
        client.embeddings.create(
            model="text-embedding-3-small",
            input=chunk["final_text"]
        ).data[0].embedding
        for chunk in contextual_chunks
    ]
    
    # 计算相似度
    similarities = [
        np.dot(query_embedding, chunk_emb)
        for chunk_emb in chunk_embeddings
    ]
    
    # 获取 top-20 候选
    top_20_indices = np.argsort(similarities)[-20:][::-1]
    candidates = [contextual_chunks[i] for i in top_20_indices]
    
    # 2. Reranking
    reranked = rerank_chunks(query, candidates, top_k)
    
    return reranked

def rerank_chunks(query: str, candidates: List[Dict], top_k: int) -> List[Dict]:
    """使用 LLM 进行 reranking"""
    from anthropic import Anthropic
    client = Anthropic()
    
    candidates_text = "\n\n".join([
        f"[{i+1}] {c['final_text'][:200]}..."
        for i, c in enumerate(candidates)
    ])
    
    prompt = f"""
查询：{query}

候选片段：
{candidates_text}

请按相关性排序，返回最相关的 {top_k} 个片段的编号（用逗号分隔）。
"""
    
    response = client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=50,
        messages=[{"role": "user", "content": prompt}]
    )
    
    # 解析排序结果
    rankings = [int(x.strip()) - 1 for x in response.content[0].text.split(",")]
    return [candidates[i] for i in rankings[:top_k]]
```

---

## 核心研究来源

**Anthropic 2024-2025**: [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)
- 减少 49% 检索失败（单独使用）
- 减少 67% 检索失败（结合 reranking）
- 成本可控（$0.02/10万字）

---

**下一步：** [07_实战代码_06_NVIDIA基准测试](./07_实战代码_06_NVIDIA基准测试.md)
