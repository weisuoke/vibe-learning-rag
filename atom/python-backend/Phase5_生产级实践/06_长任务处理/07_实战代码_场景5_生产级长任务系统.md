# 实战代码 - 场景5：生产级长任务系统

> 完整的生产级长任务处理系统，整合Celery、SSE、数据库、错误处理

---

## 场景描述

**需求**：构建生产级AI Agent文档处理系统，支持批量上传、实时进度、错误重试、任务管理

**技术栈**：
- FastAPI：Web框架
- Celery：任务队列
- Redis：消息代理
- PostgreSQL：数据库
- SSE：进度推送
- OpenAI：Embedding生成

---

## 系统架构

```
┌─────────────────────────────────────────────────────────┐
│              生产级长任务系统架构                        │
│                                                          │
│  ┌──────────┐                                           │
│  │ 客户端   │                                           │
│  └────┬─────┘                                           │
│       │ 1. 上传文件                                     │
│       ↓                                                  │
│  ┌──────────────────────────────────────────┐          │
│  │         FastAPI服务器                     │          │
│  │  - 接收文件                               │          │
│  │  - 创建任务记录                           │          │
│  │  - 提交Celery任务                         │          │
│  │  - SSE进度推送                            │          │
│  └──────────────────────────────────────────┘          │
│       │                  │                               │
│       ↓                  ↓                               │
│  ┌─────────┐      ┌──────────┐                         │
│  │ Redis   │      │PostgreSQL│                         │
│  │ 消息队列│      │ 任务状态 │                         │
│  └─────────┘      └──────────┘                         │
│       │                  ↑                               │
│       ↓                  │                               │
│  ┌──────────────────────────────────────────┐          │
│  │         Celery Worker                     │          │
│  │  - 解析PDF                                │          │
│  │  - 生成Embedding                          │          │
│  │  - 保存到向量库                           │          │
│  │  - 更新任务状态                           │          │
│  │  - 错误重试                               │          │
│  └──────────────────────────────────────────┘          │
└─────────────────────────────────────────────────────────┘
```

---

## 完整代码实现

### 1. 项目结构

```
production-task-system/
├── app/
│   ├── __init__.py
│   ├── main.py              # FastAPI应用
│   ├── celery_app.py        # Celery配置
│   ├── tasks.py             # Celery任务
│   ├── models/
│   │   ├── __init__.py
│   │   └── task.py          # 任务模型
│   ├── schemas/
│   │   ├── __init__.py
│   │   └── task.py          # Pydantic模型
│   ├── services/
│   │   ├── __init__.py
│   │   ├── task_service.py  # 任务服务
│   │   └── document.py      # 文档处理
│   ├── api/
│   │   ├── __init__.py
│   │   └── tasks.py         # API路由
│   └── core/
│       ├── __init__.py
│       ├── config.py        # 配置
│       └── database.py      # 数据库
├── .env
├── pyproject.toml
└── README.md
```

---

### 2. 配置文件

```python
# app/core/config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # 数据库
    DATABASE_URL: str
    # Redis
    REDIS_URL: str
    # OpenAI
    OPENAI_API_KEY: str
    # 任务配置
    MAX_RETRIES: int = 3
    TASK_TIMEOUT: int = 300
    # 清理配置
    CLEANUP_DAYS: int = 30

    class Config:
        env_file = ".env"

settings = Settings()
```

---

### 3. 任务模型（完整版）

```python
# app/models/task.py
from sqlalchemy import Column, Integer, String, Float, DateTime, Text, Enum, JSON
from sqlalchemy.sql import func
from app.core.database import Base
import enum

class TaskStatus(str, enum.Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELED = "canceled"

class Task(Base):
    __tablename__ = "tasks"

    id = Column(Integer, primary_key=True, index=True)
    task_id = Column(String, unique=True, index=True, nullable=False)
    task_type = Column(String, index=True, nullable=False)
    status = Column(Enum(TaskStatus), default=TaskStatus.PENDING, index=True)
    progress = Column(Float, default=0.0)
    params = Column(JSON, nullable=True)
    result = Column(JSON, nullable=True)
    error = Column(Text, nullable=True)
    retry_count = Column(Integer, default=0)
    metadata = Column(JSON, nullable=True)
    created_at = Column(DateTime, server_default=func.now(), index=True)
    updated_at = Column(DateTime, onupdate=func.now())
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
```

---

### 4. Celery任务（生产级）

```python
# app/tasks.py
from app.celery_app import app
from app.models.task import Task, TaskStatus
from app.core.database import SessionLocal
from app.services.document import parse_pdf, generate_embedding, save_to_vectordb
from celery.exceptions import SoftTimeLimitExceeded
import structlog
import json

logger = structlog.get_logger()

@app.task(
    bind=True,
    max_retries=3,
    time_limit=300,
    soft_time_limit=270
)
def process_document(self, file_path: str, db_task_id: int):
    """
    生产级文档处理任务

    特性：
    - 错误重试（指数退避）
    - 超时控制
    - 进度追踪
    - 结构化日志
    - 资源清理
    """
    db = SessionLocal()

    try:
        # 获取任务记录
        task = db.query(Task).filter(Task.id == db_task_id).first()

        # 更新状态
        task.status = TaskStatus.RUNNING
        task.started_at = func.now()
        db.commit()

        logger.info("task_started", task_id=self.request.id, file_path=file_path)

        # 步骤1：解析PDF
        logger.info("parsing_pdf", file_path=file_path)
        content = parse_pdf(file_path)
        task.progress = 33.0
        task.metadata = {"message": "解析完成"}
        db.commit()

        # 步骤2：生成Embedding
        logger.info("generating_embedding")
        embedding = generate_embedding(content)
        task.progress = 66.0
        task.metadata = {"message": "Embedding完成"}
        db.commit()

        # 步骤3：保存到向量库
        logger.info("saving_to_vectordb")
        save_to_vectordb(file_path, embedding)
        task.progress = 100.0
        task.status = TaskStatus.COMPLETED
        task.completed_at = func.now()
        task.result = json.dumps({
            "file": file_path,
            "embedding_dim": len(embedding),
            "content_length": len(content)
        })
        db.commit()

        logger.info("task_completed", task_id=self.request.id)
        return {"status": "success"}

    except SoftTimeLimitExceeded:
        # 软超时
        logger.warning("task_timeout", task_id=self.request.id)
        task.status = TaskStatus.FAILED
        task.error = "Task timeout"
        db.commit()
        raise

    except Exception as e:
        # 错误处理
        logger.error("task_failed", task_id=self.request.id, error=str(e))

        task.retry_count += 1
        task.error = str(e)

        # 检查是否达到最大重试次数
        if self.request.retries >= self.max_retries:
            task.status = TaskStatus.FAILED
            task.completed_at = func.now()
            db.commit()

            # 发送告警
            send_alert(f"Task {self.request.id} failed after {self.max_retries} retries")
        else:
            db.commit()

        # 重试（指数退避）
        countdown = 60 * (2 ** self.request.retries)
        raise self.retry(exc=e, countdown=countdown)

    finally:
        db.close()

def send_alert(message: str):
    """发送告警"""
    logger.critical("alert", message=message)
    # 实际实现：发送到Slack/邮件
```

---

### 5. FastAPI应用（完整版）

```python
# app/main.py
from fastapi import FastAPI, UploadFile, File, Depends
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from app.core.database import get_db, engine, Base
from app.models.task import Task, TaskStatus
from app.tasks import process_document
from app.services.task_service import TaskService
import asyncio
import json
import os

# 创建数据库表
Base.metadata.create_all(bind=engine)

app = FastAPI(
    title="生产级长任务系统",
    version="1.0.0",
    description="AI Agent文档处理系统"
)

@app.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """上传文件并异步处理"""
    # 1. 保存文件
    file_path = f"/tmp/{file.filename}"
    with open(file_path, "wb") as f:
        f.write(await file.read())

    # 2. 创建任务记录
    task = TaskService.create_task(
        db,
        task_type="document_process",
        params={"file_path": file_path, "filename": file.filename}
    )

    # 3. 异步调用Celery任务
    celery_result = process_document.delay(file_path, task.id)

    # 4. 更新Celery任务ID
    task.task_id = celery_result.id
    db.commit()

    return {
        "task_id": celery_result.id,
        "status": "processing",
        "message": "文件已上传，正在后台处理"
    }

@app.get("/tasks/{task_id}/stream")
async def stream_task_progress(task_id: str):
    """SSE推送任务进度"""
    async def generate():
        db = SessionLocal()

        try:
            while True:
                task = db.query(Task).filter(Task.task_id == task_id).first()

                if not task:
                    yield f"data: {json.dumps({'error': 'Task not found'})}\n\n"
                    break

                # 推送进度
                data = {
                    "task_id": task_id,
                    "status": task.status.value,
                    "progress": task.progress,
                    "message": task.metadata.get('message') if task.metadata else None
                }
                yield f"data: {json.dumps(data)}\n\n"

                # 任务完成
                if task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELED]:
                    yield f"event: complete\n"
                    yield f"data: {json.dumps({'result': task.result})}\n\n"
                    break

                await asyncio.sleep(1)

        finally:
            db.close()

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"
        }
    )

@app.get("/tasks/{task_id}")
async def get_task(task_id: str, db: Session = Depends(get_db)):
    """查询任务状态"""
    task = TaskService.get_task(db, task_id)
    if not task:
        return {"error": "Task not found"}
    return task

@app.get("/tasks")
async def list_tasks(
    status: str = None,
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(get_db)
):
    """查询任务列表"""
    tasks = TaskService.get_tasks(db, status=status, skip=skip, limit=limit)
    return {"total": len(tasks), "tasks": tasks}

@app.get("/stats")
async def get_stats(db: Session = Depends(get_db)):
    """获取统计信息"""
    return TaskService.get_task_stats(db)

@app.post("/cleanup")
async def cleanup_old_tasks(days: int = 30, db: Session = Depends(get_db)):
    """清理旧任务"""
    deleted = TaskService.cleanup_old_tasks(db, days)
    return {"deleted": deleted}

@app.get("/health")
async def health_check():
    """健康检查"""
    return {"status": "ok"}
```

---

## 部署配置

### 1. Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:14-alpine
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: task_system
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  api:
    build: .
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/task_system
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
      - postgres

  worker:
    build: .
    command: celery -A app.celery_app worker --loglevel=info
    environment:
      - DATABASE_URL=postgresql://user:password@postgres:5432/task_system
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
      - postgres

volumes:
  postgres_data:
```

---

### 2. Dockerfile

```dockerfile
FROM python:3.13-slim

WORKDIR /app

# 安装依赖
COPY pyproject.toml .
RUN pip install uv && uv pip install --system -r pyproject.toml

# 复制代码
COPY app/ ./app/

# 暴露端口
EXPOSE 8000

# 默认命令
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## 监控与告警

### 1. Prometheus指标

```python
# app/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# 任务计数器
task_total = Counter(
    'task_total',
    'Total tasks',
    ['task_type', 'status']
)

# 任务执行时间
task_duration = Histogram(
    'task_duration_seconds',
    'Task duration',
    ['task_type']
)

# 当前运行任务数
task_running = Gauge(
    'task_running',
    'Currently running tasks'
)
```

---

### 2. 健康检查

```python
@app.get("/health")
async def health_check(db: Session = Depends(get_db)):
    """健康检查"""
    try:
        # 检查数据库
        db.execute("SELECT 1")

        # 检查Redis
        from redis import Redis
        r = Redis.from_url(settings.REDIS_URL)
        r.ping()

        # 检查Celery
        from app.celery_app import app as celery_app
        celery_app.control.inspect().active()

        return {"status": "healthy"}

    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}
```

---

## 运行步骤

```bash
# 1. 使用Docker Compose启动所有服务
docker-compose up -d

# 2. 查看日志
docker-compose logs -f

# 3. 测试API
curl -X POST "http://localhost:8000/upload" \
  -F "file=@test.pdf"

# 4. 连接SSE
curl -N "http://localhost:8000/tasks/{task_id}/stream"

# 5. 查看统计
curl "http://localhost:8000/stats"
```

---

## 性能优化

### 1. 并发控制

```python
# Celery Worker配置
celery -A app.celery_app worker \
  --concurrency=4 \
  --max-tasks-per-child=100
```

### 2. 连接池优化

```python
# app/core/database.py
engine = create_engine(
    settings.DATABASE_URL,
    pool_size=20,
    max_overflow=40,
    pool_pre_ping=True
)
```

### 3. Redis缓存

```python
# 缓存任务状态
import redis

r = redis.Redis.from_url(settings.REDIS_URL)

def get_task_cached(task_id: str):
    # 先从Redis读取
    cached = r.get(f"task:{task_id}")
    if cached:
        return json.loads(cached)

    # 从数据库读取
    task = get_task_from_db(task_id)

    # 缓存5分钟
    r.setex(f"task:{task_id}", 300, json.dumps(task))

    return task
```

---

## 总结

本示例演示了完整的生产级长任务系统：

### 核心特性
- ✅ Celery任务队列
- ✅ SSE实时进度推送
- ✅ 数据库状态管理
- ✅ 错误重试（指数退避）
- ✅ 超时控制
- ✅ 结构化日志
- ✅ Docker部署
- ✅ 健康检查
- ✅ 监控指标

### 生产级要点
1. **可靠性**：错误重试、超时控制、资源清理
2. **可观测性**：结构化日志、监控指标、健康检查
3. **可扩展性**：Docker部署、连接池优化、并发控制
4. **用户体验**：实时进度推送、任务管理、统计信息

---

**恭喜！** 你已经掌握了生产级长任务处理系统的完整实现。
