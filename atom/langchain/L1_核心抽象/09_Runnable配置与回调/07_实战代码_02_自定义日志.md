# 实战代码 02：自定义日志

> **实现结构化日志、性能监控和调试追踪的完整方案**

---

## 场景 1：基础日志记录

```python
import logging
from langchain_core.callbacks import BaseCallbackHandler
from langchain_openai import ChatOpenAI

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleLogger(BaseCallbackHandler):
    """简单日志回调"""

    def on_llm_start(self, serialized, prompts, **kwargs):
        logger.info(f"LLM 开始调用: {prompts[0][:50]}...")

    def on_llm_end(self, response, **kwargs):
        text = response.generations[0][0].text
        logger.info(f"LLM 调用完成: {text[:50]}...")

    def on_llm_error(self, error, **kwargs):
        logger.error(f"LLM 调用失败: {error}")

# 使用
llm = ChatOpenAI()
llm.invoke("你好", config={"callbacks": [SimpleLogger()]})
```

**输出**：
```
2025-01-15 10:30:00 - __main__ - INFO - LLM 开始调用: 你好
2025-01-15 10:30:02 - __main__ - INFO - LLM 调用完成: 你好！有什么我可以帮助你的吗？
```

---

## 场景 2：结构化日志（JSON）

```python
import json
import time
from langchain_core.callbacks import BaseCallbackHandler

class StructuredLogger(BaseCallbackHandler):
    """结构化日志回调"""

    def __init__(self):
        self.start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()
        metadata = kwargs.get("metadata", {})

        log = {
            "event": "llm_start",
            "timestamp": time.time(),
            "model": serialized.get("name", "unknown"),
            "prompts": prompts,
            "metadata": metadata
        }
        print(json.dumps(log, ensure_ascii=False))

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time
        metadata = kwargs.get("metadata", {})

        usage = response.llm_output.get("token_usage", {})
        text = response.generations[0][0].text

        log = {
            "event": "llm_end",
            "timestamp": time.time(),
            "duration_ms": int(duration * 1000),
            "output": text[:100],  # 截取前100字符
            "tokens": usage,
            "metadata": metadata
        }
        print(json.dumps(log, ensure_ascii=False))

    def on_llm_error(self, error, **kwargs):
        metadata = kwargs.get("metadata", {})

        log = {
            "event": "llm_error",
            "timestamp": time.time(),
            "error": str(error),
            "error_type": type(error).__name__,
            "metadata": metadata
        }
        print(json.dumps(log, ensure_ascii=False))

# 使用
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
llm.invoke(
    "什么是机器学习？",
    config={
        "callbacks": [StructuredLogger()],
        "metadata": {"user_id": "user_123", "session": "abc"}
    }
)
```

**输出**：
```json
{"event": "llm_start", "timestamp": 1708329600.123, "model": "gpt-3.5-turbo", "prompts": ["什么是机器学习？"], "metadata": {"user_id": "user_123", "session": "abc"}}
{"event": "llm_end", "timestamp": 1708329602.456, "duration_ms": 2333, "output": "机器学习是人工智能的一个分支...", "tokens": {"prompt_tokens": 15, "completion_tokens": 120, "total_tokens": 135}, "metadata": {"user_id": "user_123", "session": "abc"}}
```

---

## 场景 3：性能监控日志

```python
import time
from collections import defaultdict
from langchain_core.callbacks import BaseCallbackHandler

class PerformanceLogger(BaseCallbackHandler):
    """性能监控回调"""

    def __init__(self):
        self.start_times = {}
        self.durations = defaultdict(list)

    def on_llm_start(self, serialized, prompts, **kwargs):
        run_id = kwargs.get("run_id")
        self.start_times[run_id] = time.time()

    def on_llm_end(self, response, **kwargs):
        run_id = kwargs.get("run_id")
        duration = time.time() - self.start_times.pop(run_id, time.time())

        model = response.llm_output.get("model_name", "unknown")
        self.durations[model].append(duration)

        logger.info(f"LLM 调用耗时: {duration:.2f}s (模型: {model})")

    def get_stats(self):
        """获取性能统计"""
        stats = {}
        for model, durations in self.durations.items():
            stats[model] = {
                "count": len(durations),
                "avg": sum(durations) / len(durations),
                "min": min(durations),
                "max": max(durations)
            }
        return stats

# 使用
from langchain_openai import ChatOpenAI

perf_logger = PerformanceLogger()
llm = ChatOpenAI()

# 多次调用
for i in range(5):
    llm.invoke(f"问题{i}", config={"callbacks": [perf_logger]})

# 查看统计
stats = perf_logger.get_stats()
print(json.dumps(stats, indent=2))
```

**输出**：
```json
{
  "gpt-3.5-turbo": {
    "count": 5,
    "avg": 1.234,
    "min": 0.987,
    "max": 1.567
  }
}
```

---

## 场景 4：调试追踪日志

```python
from langchain_core.callbacks import BaseCallbackHandler

class DebugLogger(BaseCallbackHandler):
    """调试追踪回调"""

    def __init__(self):
        self.depth = 0

    def on_chain_start(self, serialized, inputs, **kwargs):
        indent = "  " * self.depth
        logger.debug(f"{indent}→ Chain 开始: {serialized.get('name', 'unknown')}")
        logger.debug(f"{indent}  输入: {inputs}")
        self.depth += 1

    def on_chain_end(self, outputs, **kwargs):
        self.depth -= 1
        indent = "  " * self.depth
        logger.debug(f"{indent}← Chain 结束")
        logger.debug(f"{indent}  输出: {outputs}")

    def on_llm_start(self, serialized, prompts, **kwargs):
        indent = "  " * self.depth
        logger.debug(f"{indent}→ LLM 开始: {serialized.get('name', 'unknown')}")
        logger.debug(f"{indent}  Prompts: {prompts}")

    def on_llm_end(self, response, **kwargs):
        indent = "  " * self.depth
        text = response.generations[0][0].text
        logger.debug(f"{indent}← LLM 结束: {text[:50]}...")

# 使用
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# 配置 DEBUG 级别日志
logging.basicConfig(level=logging.DEBUG)

chain = ChatOpenAI() | StrOutputParser()
chain.invoke("你好", config={"callbacks": [DebugLogger()]})
```

**输出**：
```
→ Chain 开始: RunnableSequence
  输入: {'input': '你好'}
  → LLM 开始: ChatOpenAI
    Prompts: ['你好']
  ← LLM 结束: 你好！有什么我可以帮助你的吗？
← Chain 结束
  输出: 你好！有什么我可以帮助你的吗？
```

---

## 场景 5：文件日志

```python
import logging
from logging.handlers import RotatingFileHandler
from langchain_core.callbacks import BaseCallbackHandler

# 配置文件日志
file_handler = RotatingFileHandler(
    'langchain.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)
file_handler.setFormatter(logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
))

logger = logging.getLogger('langchain_app')
logger.addHandler(file_handler)
logger.setLevel(logging.INFO)

class FileLogger(BaseCallbackHandler):
    """文件日志回调"""

    def on_llm_start(self, serialized, prompts, **kwargs):
        metadata = kwargs.get("metadata", {})
        logger.info(f"LLM 开始", extra={
            "user_id": metadata.get("user_id"),
            "prompts": prompts
        })

    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})
        logger.info(f"LLM 完成", extra={
            "tokens": usage.get("total_tokens", 0)
        })

# 使用
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
llm.invoke("你好", config={"callbacks": [FileLogger()]})

# 日志会写入 langchain.log 文件
```

---

## 场景 6：多级日志

```python
import logging
from langchain_core.callbacks import BaseCallbackHandler

class MultiLevelLogger(BaseCallbackHandler):
    """多级日志回调"""

    def on_llm_start(self, serialized, prompts, **kwargs):
        # INFO 级别：记录基本信息
        logger.info(f"LLM 调用开始")

        # DEBUG 级别：记录详细信息
        logger.debug(f"模型: {serialized.get('name')}")
        logger.debug(f"Prompts: {prompts}")

    def on_llm_end(self, response, **kwargs):
        usage = response.llm_output.get("token_usage", {})

        # INFO 级别：记录成功
        logger.info(f"LLM 调用完成: {usage.get('total_tokens', 0)} tokens")

        # DEBUG 级别：记录详细 token 使用
        logger.debug(f"Prompt tokens: {usage.get('prompt_tokens', 0)}")
        logger.debug(f"Completion tokens: {usage.get('completion_tokens', 0)}")

    def on_llm_error(self, error, **kwargs):
        # ERROR 级别：记录错误
        logger.error(f"LLM 调用失败: {error}")

        # DEBUG 级别：记录错误堆栈
        logger.debug(f"错误类型: {type(error).__name__}", exc_info=True)

# 使用
# 生产环境：只记录 INFO 及以上
logging.basicConfig(level=logging.INFO)

# 开发环境：记录 DEBUG 及以上
logging.basicConfig(level=logging.DEBUG)
```

---

## 场景 7：按用户分组日志

```python
from langchain_core.callbacks import BaseCallbackHandler

class UserGroupedLogger(BaseCallbackHandler):
    """按用户分组的日志回调"""

    def on_llm_start(self, serialized, prompts, **kwargs):
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        # 使用 extra 字段传递用户信息
        logger.info(
            f"LLM 调用开始",
            extra={
                "user_id": user_id,
                "prompts": prompts[0][:50]
            }
        )

    def on_llm_end(self, response, **kwargs):
        metadata = kwargs.get("metadata", {})
        user_id = metadata.get("user_id", "unknown")

        usage = response.llm_output.get("token_usage", {})

        logger.info(
            f"LLM 调用完成",
            extra={
                "user_id": user_id,
                "tokens": usage.get("total_tokens", 0)
            }
        )

# 使用
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

# 用户 A 的调用
llm.invoke(
    "你好",
    config={
        "callbacks": [UserGroupedLogger()],
        "metadata": {"user_id": "user_001"}
    }
)

# 用户 B 的调用
llm.invoke(
    "你好",
    config={
        "callbacks": [UserGroupedLogger()],
        "metadata": {"user_id": "user_002"}
    }
)

# 可以在日志系统中按 user_id 筛选
```

---

## 场景 8：完整生产日志系统

```python
import logging
import json
import time
from logging.handlers import RotatingFileHandler
from langchain_core.callbacks import BaseCallbackHandler

# 配置日志
def setup_logging():
    """配置生产环境日志"""
    # 创建 logger
    logger = logging.getLogger('langchain_prod')
    logger.setLevel(logging.INFO)

    # 文件处理器（JSON 格式）
    file_handler = RotatingFileHandler(
        'logs/langchain.log',
        maxBytes=50*1024*1024,  # 50MB
        backupCount=10
    )
    file_handler.setFormatter(logging.Formatter('%(message)s'))

    # 控制台处理器（人类可读）
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    ))

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger

logger = setup_logging()

class ProductionLogger(BaseCallbackHandler):
    """生产环境日志回调"""

    def __init__(self):
        self.start_time = None

    def on_llm_start(self, serialized, prompts, **kwargs):
        self.start_time = time.time()
        metadata = kwargs.get("metadata", {})
        run_id = kwargs.get("run_id")

        log = {
            "event": "llm_start",
            "timestamp": time.time(),
            "run_id": str(run_id),
            "model": serialized.get("name", "unknown"),
            "user_id": metadata.get("user_id"),
            "session": metadata.get("session"),
            "env": metadata.get("env", "unknown")
        }

        logger.info(json.dumps(log))

    def on_llm_end(self, response, **kwargs):
        duration = time.time() - self.start_time
        metadata = kwargs.get("metadata", {})
        run_id = kwargs.get("run_id")

        usage = response.llm_output.get("token_usage", {})
        text = response.generations[0][0].text

        log = {
            "event": "llm_end",
            "timestamp": time.time(),
            "run_id": str(run_id),
            "duration_ms": int(duration * 1000),
            "tokens": usage,
            "output_length": len(text),
            "user_id": metadata.get("user_id"),
            "session": metadata.get("session")
        }

        logger.info(json.dumps(log))

    def on_llm_error(self, error, **kwargs):
        metadata = kwargs.get("metadata", {})
        run_id = kwargs.get("run_id")

        log = {
            "event": "llm_error",
            "timestamp": time.time(),
            "run_id": str(run_id),
            "error": str(error),
            "error_type": type(error).__name__,
            "user_id": metadata.get("user_id"),
            "session": metadata.get("session")
        }

        logger.error(json.dumps(log))

# 使用
from langchain_openai import ChatOpenAI

llm = ChatOpenAI()

def query_with_logging(query: str, user_id: str, session: str):
    """带日志的查询函数"""
    config = {
        "callbacks": [ProductionLogger()],
        "metadata": {
            "user_id": user_id,
            "session": session,
            "env": "production"
        }
    }

    return llm.invoke(query, config=config)

# 执行查询
result = query_with_logging(
    query="什么是量子纠缠？",
    user_id="user_123",
    session="session_abc"
)
```

**日志输出**（logs/langchain.log）：
```json
{"event": "llm_start", "timestamp": 1708329600.123, "run_id": "a1b2c3d4", "model": "gpt-3.5-turbo", "user_id": "user_123", "session": "session_abc", "env": "production"}
{"event": "llm_end", "timestamp": 1708329602.456, "run_id": "a1b2c3d4", "duration_ms": 2333, "tokens": {"prompt_tokens": 15, "completion_tokens": 120, "total_tokens": 135}, "output_length": 245, "user_id": "user_123", "session": "session_abc"}
```

---

## 场景 9：日志聚合（ELK Stack）

```python
import logging
from pythonjsonlogger import jsonlogger
from langchain_core.callbacks import BaseCallbackHandler

# 配置 JSON 日志（用于 ELK Stack）
logger = logging.getLogger('langchain_elk')
logHandler = logging.StreamHandler()
formatter = jsonlogger.JsonFormatter()
logHandler.setFormatter(formatter)
logger.addHandler(logHandler)
logger.setLevel(logging.INFO)

class ELKLogger(BaseCallbackHandler):
    """ELK Stack 日志回调"""

    def on_llm_end(self, response, **kwargs):
        metadata = kwargs.get("metadata", {})
        usage = response.llm_output.get("token_usage", {})

        logger.info(
            "LLM 调用完成",
            extra={
                "event_type": "llm_call",
                "user_id": metadata.get("user_id"),
                "tokens": usage.get("total_tokens", 0),
                "model": response.llm_output.get("model_name"),
                "env": metadata.get("env")
            }
        )

# 使用
# 日志会以 JSON 格式输出，可以直接发送到 Logstash
```

---

## 总结

### 日志类型对比

| 类型 | 用途 | 格式 | 适用场景 |
|------|------|------|----------|
| **简单日志** | 基础记录 | 文本 | 开发调试 |
| **结构化日志** | 机器解析 | JSON | 生产监控 |
| **性能日志** | 性能分析 | 统计数据 | 性能优化 |
| **调试日志** | 问题定位 | 详细追踪 | 故障排查 |
| **文件日志** | 持久化 | 文本/JSON | 审计合规 |

### 最佳实践

1. **使用结构化日志**：JSON 格式便于机器解析
2. **记录关键信息**：user_id、session、timestamp、tokens
3. **分级记录**：INFO（生产）、DEBUG（开发）、ERROR（错误）
4. **日志轮转**：使用 RotatingFileHandler 避免文件过大
5. **异常捕获**：在回调中捕获异常，避免影响主流程

---

## 参考资料

- [Python Logging Documentation](https://docs.python.org/3/library/logging.html)
- [LangChain Custom Callbacks](https://python.langchain.com/docs/how_to/custom_callbacks/)
