# 语义相似度

> L1_NLP基础 | 第3个知识点 | RAG 检索的核心基础

---

## 1. 【30字核心】

**语义相似度是衡量两段文本含义接近程度的数值指标，是 RAG 系统实现精准检索和排序的核心依据。**

---

## 2. 【第一性原理】

### 什么是第一性原理？

**第一性原理**：回到事物最基本的真理，从源头思考问题

### 语义相似度的第一性原理

#### 1. 最基础的定义

**语义相似度 = 两段文本在“意思”上的接近程度**

仅此而已！没有更基础的了。

- 不是字面上的相同（“汽车” vs “automobile”）
- 不是字符的重叠（“我爱你” vs “你爱我”）
- 而是**含义**的接近程度

#### 2. 为什么需要语义相似度？

**核心问题：计算机如何理解“意思相近”？**

人类可以轻松判断：
- “今天天气真好” 和 “今天阳光明媚” 意思相近
- “苹果手机” 和 “iPhone” 指的是同一个东西
- “如何学习编程” 和 “怎么入门写代码” 是同一个问题

但计算机只能看到字符序列，无法直接理解“意思”。

**解决方案：**
1. 先把文本转换成向量（Embedding）
2. 再计算向量之间的“距离”或“角度”
3. 这个距离/角度就代表了语义相似度

#### 3. 语义相似度的三层价值

##### 价值1：实现语义检索

不再依赖关键词完全匹配，而是找到“意思相近”的内容。

```
用户问：“如何提高代码质量？”
能找到：“编写高质量代码的最佳实践”
即使没有完全相同的词！
```

##### 价值2：结果排序

多个候选结果时，按相似度高低排序，最相关的排在前面。

##### 价值3：相关性过滤

设置阈值，过滤掉相似度太低的结果，避免返回无关内容。

#### 4. 从第一性原理推导 RAG 检索

**推理链：**
```
1. 用户提出问题（自然语言）
   ↓
2. 问题被转换为向量（Query Embedding）
   ↓
3. 知识库中的文档也是向量（Document Embeddings）
   ↓
4. 计算问题向量与所有文档向量的相似度
   ↓
5. 按相似度排序，取 Top-K 个最相关的文档
   ↓
6. 将这些文档作为上下文，让 LLM 生成答案
```

#### 5. 一句话总结第一性原理

**语义相似度是将“意思接近”这个人类直觉量化为数值的方法，让计算机能够理解和比较文本的含义。**

---

## 3. 【核心概念（全面覆盖）】

### 核心概念1：余弦相似度（Cosine Similarity）

**衡量两个向量方向的一致程度，是最常用的语义相似度计算方法。**

```python
import numpy as np

def cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
    """计算两个向量的余弦相似度"""
    dot_product = np.dot(vec_a, vec_b)
    norm_a = np.linalg.norm(vec_a)
    norm_b = np.linalg.norm(vec_b)
    return dot_product / (norm_a * norm_b)

# 示例
vec1 = np.array([1, 2, 3])
vec2 = np.array([1, 2, 3.1])  # 方向几乎相同
vec3 = np.array([-1, -2, -3]) # 方向完全相反

print(f"相同方向: {cosine_similarity(vec1, vec2):.4f}")  # ≈ 1.0
print(f"相反方向: {cosine_similarity(vec1, vec3):.4f}")  # ≈ -1.0
```

**核心特点：**
- 取值范围：[-1, 1]
- 1 表示方向完全相同（最相似）
- 0 表示正交（无关）
- -1 表示方向完全相反（最不相似）
- **只关注方向，不关注长度**（这很重要！）

**可视化理解：**
```
        ↑ vec2
       /
      /  θ = 小角度 → 高相似度
     /
    ●----→ vec1

        ↑ vec3
        |
        |  θ = 90° → 相似度 = 0
        |
    ●----→ vec1
```

**在 RAG 开发中的应用：**
- 向量数据库常用余弦相似度进行检索
- 适合比较 Embedding 向量（常见场景是已归一化）
- 计算效率高，适合大规模检索

---

### 核心概念2：欧氏距离（Euclidean Distance）

**衡量两个向量在空间中的直线距离。**

```python
import numpy as np

def euclidean_distance(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
    """计算两个向量的欧氏距离"""
    return np.linalg.norm(vec_a - vec_b)

def euclidean_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
    """将欧氏距离转换为相似度（0-1范围）"""
    distance = euclidean_distance(vec_a, vec_b)
    return 1 / (1 + distance)

# 示例
vec1 = np.array([1, 2, 3])
vec2 = np.array([1.1, 2.1, 3.1])  # 距离很近
vec3 = np.array([10, 20, 30])     # 距离很远

print(f"近距离: {euclidean_distance(vec1, vec2):.4f}")
print(f"远距离: {euclidean_distance(vec1, vec3):.4f}")
```

**核心特点：**
- 取值范围：[0, +∞)
- 0 表示完全相同
- 值越大，距离越远，越不相似
- **同时考虑方向和长度**

**与余弦相似度的区别：**
```
场景：vec1 = [1, 2], vec2 = [2, 4], vec3 = [1, 2.1]

余弦相似度：
- vec1 vs vec2 = 1.0（方向完全相同）
- vec1 vs vec3 ≈ 0.999（方向几乎相同）

欧氏距离：
- vec1 vs vec2 ≈ 2.24（距离较远）
- vec1 vs vec3 = 0.1（距离很近）
```

**在 RAG 开发中的应用：**
- 某些向量数据库支持欧氏距离检索
- 适合未归一化的向量
- 对向量的“强度”敏感

---

### 核心概念3：点积（Dot Product / Inner Product）

**两个向量对应元素相乘后求和，是最基础的向量运算。**

```python
import numpy as np

def dot_product(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
    """计算两个向量的点积"""
    return np.dot(vec_a, vec_b)

# 示例
vec1 = np.array([1, 2, 3])
vec2 = np.array([4, 5, 6])

# 手动计算：1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32
print(f"点积: {dot_product(vec1, vec2)}")  # 32
```

**核心特点：**
- 取值范围：(-∞, +∞)
- 值越大，越相似（方向一致且长度大）
- 值为负，方向相反
- **同时反映方向和长度**

**与余弦相似度的关系：**
```
余弦相似度 = 点积 / (向量A的长度 × 向量B的长度)

如果向量已归一化（长度=1）：
余弦相似度 = 点积
```

**在 RAG 开发中的应用：**
- 归一化向量时，点积等价于余弦相似度
- 计算速度最快（无需计算长度）
- 很多向量数据库内部用点积优化检索

---

### 扩展覆盖：曼哈顿距离（L1）与杰卡德相似度（Jaccard）

#### 扩展概念4：曼哈顿距离（Manhattan Distance）

**沿坐标轴方向的距离之和，也叫 L1 距离。**

```python
import numpy as np

def manhattan_distance(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
    """计算曼哈顿距离"""
    return np.sum(np.abs(vec_a - vec_b))

vec1 = np.array([1, 2, 3])
vec2 = np.array([4, 6, 8])

# |1-4| + |2-6| + |3-8| = 3 + 4 + 5 = 12
print(f"曼哈顿距离: {manhattan_distance(vec1, vec2)}")  # 12
```

**在 RAG 开发中的应用：**
- 较少使用，但在某些稀疏向量/异常值更敏感的场景可作为备选度量

#### 扩展概念5：杰卡德相似度（Jaccard Similarity）

**衡量两个集合的重叠程度，常见于词袋/集合式特征。**

```python
def jaccard_similarity(set_a: set, set_b: set) -> float:
    """计算杰卡德相似度"""
    intersection = len(set_a & set_b)
    union = len(set_a | set_b)
    return intersection / union if union > 0 else 0

# 示例：比较两个句子的“字符集合”（仅演示；真实场景可用分词后的词集合）
text1 = "我喜欢学习编程"
text2 = "我喜欢写代码"

words1 = set(text1)
words2 = set(text2)

print(f"杰卡德相似度: {jaccard_similarity(words1, words2):.4f}")
```

**在 RAG 开发中的应用：**
- 作为关键词检索/稀疏特征的一种“重叠度”指标，常用于混合检索中的稀疏侧

---

### 三种主要方法对比

| 方法 | 取值范围 | 是否考虑长度 | 计算复杂度 | 适用场景 |
|------|----------|--------------|------------|----------|
| 余弦相似度 | [-1, 1] | 否 | O(n) | 归一化向量、语义检索 |
| 欧氏距离 | [0, +∞) | 是 | O(n) | 未归一化向量 |
| 点积 | (-∞, +∞) | 是 | O(n) | 归一化向量（等价余弦） |

---

## 4. 【最小可用】

掌握以下内容，就能开始进行 RAG 开发：

### 4.1 用余弦相似度计算相似度（最常用）

```python
import numpy as np
from numpy.linalg import norm

def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b) / (norm(a) * norm(b)))
```

### 4.2 理解相似度阈值：过滤“明显不相关”的结果

```python
# 常用阈值参考（余弦相似度；不同模型会不同）
THRESHOLD_HIGH = 0.85    # 高度相关
THRESHOLD_MEDIUM = 0.70  # 中度相关
THRESHOLD_LOW = 0.50     # 低度相关

def filter_by_similarity(results, threshold=0.70):
    """过滤低相似度结果"""
    return [r for r in results if r["similarity"] >= threshold]
```

### 4.3 用现成库快速批量计算（评估/分析很常用）

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

embeddings = np.array([
    [0.1, 0.2, 0.3],
    [0.1, 0.2, 0.31],
    [0.9, 0.8, 0.7]
])

sim_matrix = cosine_similarity(embeddings)
print(sim_matrix)
```

### 4.4 在向量数据库中检索（RAG 真正落地）

```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("docs")

collection.add(
    documents=["Python是编程语言", "JavaScript用于前端开发"],
    ids=["doc1", "doc2"]
)

results = collection.query(
    query_texts=["什么是Python"],
    n_results=2
)

print(results["documents"])
```

**这些知识足以：**
- 理解 RAG 检索的核心机制（向量 + 相似度）
- 用向量数据库做语义检索并排序
- 用阈值做基础过滤，减少无关上下文
- 为后续学习 ReRank、混合检索打基础

---

## 5. 【双重类比】

### 类比1：余弦相似度 = 比较方向（不看走多远）

**前端类比：CSS 渐变方向**
```css
/* 两个渐变方向接近 → 视觉效果接近 */
background: linear-gradient(45deg, red, blue);
background: linear-gradient(47deg, red, blue);

/* 方向差很大 → 效果差很大 */
background: linear-gradient(45deg, red, blue);
background: linear-gradient(135deg, red, blue);
```

**日常生活类比：指南针方向**
- 两个人都朝东北走：相似度高
- 一个朝东一个朝西：相似度低（甚至为负）
- 不管谁走得更远，只看方向：这就是余弦相似度

```python
import numpy as np

a = np.array([1, 1])     # 东北
b = np.array([2, 2])     # 也是东北（更长）
c = np.array([1, -1])    # 东南

def cos(a, b):
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

print(cos(a, b))  # 1.0
print(cos(a, c))  # 0.0
```

---

### 类比2：欧氏距离 = 地图上两点直线距离

**前端类比：两个 DOM 元素的坐标距离**
```javascript
function distance(p1, p2) {
  const dx = p1.x - p2.x;
  const dy = p1.y - p2.y;
  return Math.sqrt(dx * dx + dy * dy); // 欧氏距离
}
```

**日常生活类比：两家店的直线距离**
- 距离越近，越“相似”（在位置上）

```python
import numpy as np

p1 = np.array([0, 0])
p2 = np.array([3, 4])
print(np.linalg.norm(p1 - p2))  # 5.0
```

---

### 类比3：点积 = “同向用力”的有效程度

**前端类比：拖拽方向是否朝着目标方向**
```javascript
function aligned(drag, target) {
  const dot = drag.x * target.x + drag.y * target.y;
  return dot > 0; // 同向为正
}
```

**日常生活类比：推箱子**
- 同向推：点积大，推得动
- 垂直推：点积接近 0，白费力
- 反向推：点积为负，越推越回去

```python
import numpy as np
a = np.array([1, 0])   # 向右
b = np.array([2, 0])   # 也向右
c = np.array([-1, 0])  # 向左
print(np.dot(a, b))  # 2
print(np.dot(a, c))  # -1
```

---

### 类比4：相似度阈值 = “及格线”

**前端类比：表单校验分数线**
- 分数越高越严格，能通过的人越少

**日常生活类比：考试及格线**
- 60 分及格：更多人过，但质量一般
- 90 分优秀：更少人过，但质量更高

```python
def pass_or_fail(sim, threshold):
    return sim >= threshold

print(pass_or_fail(0.72, 0.70))  # True
print(pass_or_fail(0.72, 0.85))  # False
```

---

### 类比5：向量检索 = 搜索框“模糊匹配”的升级版

**前端类比：includes() 是字面模糊；语义检索是“意思模糊”**
- includes() 只能找包含某个字的
- 语义检索能找“含义相近”的

**日常生活类比：图书馆找书**
- 关键词：只能找书名含“Python”
- 语义：能找到“编程入门”“代码学习”等相关书

---

### 类比总结表

| 语义相似度概念 | 前端类比 | 日常生活类比 |
|----------------|----------|--------------|
| 余弦相似度 | CSS 渐变角度比较 | 指南针方向比较 |
| 欧氏距离 | DOM 元素位置距离 | 地图直线距离 |
| 点积 | 拖拽方向判断 | 推箱子的力方向 |
| 阈值 | 表单校验分数线 | 考试及格线 |
| 向量检索 | 模糊搜索组件 | 图书馆找相关书 |

---

## 6. 【反直觉点】

### 误区1：向量相似度高 = 语义一定相关 ❌

**为什么错？**
- 相似度高只表示“模型认为”向量方向接近
- Embedding 模型也会误判：同形异义、领域差异、训练偏差都可能导致“假相似”

**为什么人们容易这样错？**
- 看到数值就容易把它当成“真理”
- 忽略了：相似度的前提是“模型表达得对”

**正确理解：**
```python
# 相似度高不一定业务相关
query = "苹果公司的股票"
doc1 = "苹果手机的最新功能"  # 可能相似度不低，但业务不相关
doc2 = "Apple Inc. 财报与股价" # 才是业务相关

# 解决思路：
# 1) 召回：向量相似度 Top-K
# 2) 精排：ReRank / 规则 / 关键词过滤
```

---

### 误区2：余弦相似度和欧氏距离效果一样 ❌

**为什么错？**
- 余弦只看方向，不看长度
- 欧氏看方向也看长度
- 未归一化向量时，两者可能给出完全不同的排序

**为什么人们容易这样错？**
- 许多教程只说“都能衡量相似度”，但没强调“归一化”是关键前提
- 在“归一化后”的常见设置下，两者确实更接近，导致误以为永远一样

**正确理解：**
```python
import numpy as np

v1 = np.array([1, 2, 3])
v2 = np.array([10, 20, 30])  # 方向相同但长度差很多

cos = float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
dist = float(np.linalg.norm(v1 - v2))

print("余弦相似度:", cos)   # 1.0
print("欧氏距离:", dist)    # 很大
```

---

### 误区3：相似度阈值越高越好 ❌

**为什么错？**
- 阈值太高：漏掉相关内容（召回率下降）
- 阈值太低：引入噪音（精确率下降）
- 阈值要跟“业务风险”和“数据分布”一起看

**为什么人们容易这样错？**
- 人倾向于追求“更精确”，忽略“漏检”的代价
- 没做评估就拍脑袋定阈值

**正确理解：**
```python
# 策略：用评估数据调阈值，而不是凭感觉
# 1) 收集 (query, 正例doc, 负例doc)
# 2) 扫描阈值
# 3) 看 precision/recall/F1 或结合业务指标选点

thresholds = [0.55, 0.65, 0.75, 0.85]
# best = argmax(score(threshold))
```

---

## 7. 【实战代码】

```python
"""
语义相似度 实战示例
演示：相似度计算 + 一个简化版 RAG 检索（不依赖外部 API）
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Tuple

import numpy as np

# ===== 1. 基础：相似度函数 =====
print("=== 1. 相似度函数 ===\n")

def cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
    """计算余弦相似度（更常用于 Embedding）"""
    norm_a = np.linalg.norm(vec_a)
    norm_b = np.linalg.norm(vec_b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return float(np.dot(vec_a, vec_b) / (norm_a * norm_b))

def euclidean_distance(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
    """计算欧氏距离（距离越小越相似）"""
    return float(np.linalg.norm(vec_a - vec_b))

def normalize(vec: np.ndarray) -> np.ndarray:
    """将向量归一化为单位长度，便于使用余弦/点积"""
    n = np.linalg.norm(vec)
    return vec / n if n > 0 else vec

# ===== 2. 模拟 Embedding（演示用：真实项目会用 embedding 模型）=====
print("=== 2. 模拟 Embedding（演示用）===\n")

# 注意：这里是“模拟向量”，只用于展示流程
embedding_python = np.array([0.8, 0.6, 0.1, 0.2])
embedding_coding = np.array([0.75, 0.65, 0.15, 0.18])
embedding_cooking = np.array([0.1, 0.2, 0.9, 0.8])

print("余弦相似度：")
print("  Python编程 vs 写代码:", round(cosine_similarity(embedding_python, embedding_coding), 4))
print("  Python编程 vs 做饭:  ", round(cosine_similarity(embedding_python, embedding_cooking), 4))
print()

# ===== 3. 一个最小可用的“向量库”检索（RAG 的检索核心）=====
print("=== 3. 简化版向量检索（RAG 召回）===\n")

@dataclass
class Doc:
    doc_id: str
    text: str
    embedding: np.ndarray

class SimpleVectorStore:
    def __init__(self) -> None:
        self.docs: List[Doc] = []

    def add(self, doc_id: str, text: str, embedding: np.ndarray) -> None:
        self.docs.append(Doc(doc_id=doc_id, text=text, embedding=embedding))

    def search(
        self,
        query_embedding: np.ndarray,
        top_k: int = 3,
        threshold: float = 0.0,
        normalize_embeddings: bool = False,
    ) -> List[Tuple[Doc, float]]:
        """
        返回 (文档, 相似度)
        - normalize_embeddings=True：对 query 和 doc 向量统一归一化（常见于余弦/点积场景）
        """
        q = normalize(query_embedding) if normalize_embeddings else query_embedding

        scored: List[Tuple[Doc, float]] = []
        for d in self.docs:
            v = normalize(d.embedding) if normalize_embeddings else d.embedding
            sim = cosine_similarity(q, v)
            if sim >= threshold:
                scored.append((d, sim))

        scored.sort(key=lambda x: x[1], reverse=True)
        return scored[:top_k]

store = SimpleVectorStore()
store.add("doc1", "Python是一种流行的编程语言", np.array([0.80, 0.60, 0.10, 0.20]))
store.add("doc2", "JavaScript用于前端开发",     np.array([0.70, 0.50, 0.20, 0.30]))
store.add("doc3", "机器学习需要大量数据",       np.array([0.60, 0.70, 0.30, 0.10]))
store.add("doc4", "今天天气很好",               np.array([0.10, 0.10, 0.80, 0.90]))
store.add("doc5", "如何学习编程入门",           np.array([0.75, 0.65, 0.15, 0.20]))

query = "怎么学Python"
query_embedding = np.array([0.78, 0.62, 0.12, 0.22])

print("用户查询:", query)
print("检索 Top-3（阈值=0.5）：")
results = store.search(query_embedding, top_k=3, threshold=0.5, normalize_embeddings=True)
for i, (doc, sim) in enumerate(results, 1):
    print(f"  {i}. [{sim:.4f}] {doc.text}")
print()

# ===== 4. 阈值对结果的影响 =====
print("=== 4. 阈值对结果的影响 ===\n")

all_results = store.search(query_embedding, top_k=10, threshold=-1.0, normalize_embeddings=True)

for threshold in [0.5, 0.7, 0.9]:
    filtered = [(d, s) for d, s in all_results if s >= threshold]
    print(f"阈值 {threshold}: 返回 {len(filtered)} 条")
    for d, s in filtered:
        print(f"  [{s:.4f}] {d.text}")
    print()

# ===== 5. 归一化的直觉 =====
print("=== 5. 归一化的直觉 ===\n")

vec_a = np.array([3.0, 4.0])
vec_b = np.array([6.0, 8.0])  # 方向相同，长度更长

print("原始：")
print("  余弦相似度:", round(cosine_similarity(vec_a, vec_b), 4))
print("  欧氏距离:  ", round(euclidean_distance(vec_a, vec_b), 4))

vec_a_n = normalize(vec_a)
vec_b_n = normalize(vec_b)

print("归一化后：")
print("  余弦相似度:", round(cosine_similarity(vec_a_n, vec_b_n), 4))
print("  欧氏距离:  ", round(euclidean_distance(vec_a_n, vec_b_n), 4))
```

**运行输出示例：**
```
=== 1. 相似度函数 ===

=== 2. 模拟 Embedding（演示用）===

余弦相似度：
  Python编程 vs 写代码: 0.9973
  Python编程 vs 做饭:   0.4142

=== 3. 简化版向量检索（RAG 召回）===

用户查询: 怎么学Python
检索 Top-3（阈值=0.5）：
  1. [0.9987] Python是一种流行的编程语言
  2. [0.9962] 如何学习编程入门
  3. [0.9876] JavaScript用于前端开发

=== 4. 阈值对结果的影响 ===

阈值 0.5: 返回 4 条
阈值 0.7: 返回 4 条
阈值 0.9: 返回 3 条

=== 5. 归一化的直觉 ===

原始：
  余弦相似度: 1.0
  欧氏距离:   5.0
归一化后：
  余弦相似度: 1.0
  欧氏距离:   0.0
```

---

## 8. 【面试必问】

### 问题1："请解释余弦相似度和欧氏距离的区别，在 RAG 系统中应该如何选择？"

**普通回答（❌ 不出彩）：**
“余弦相似度计算角度，欧氏距离计算直线距离。一般用余弦相似度就行。”

**出彩回答（✅ 推荐）：**

> **余弦相似度和欧氏距离有三个关键区别：**
>
> 1. **关注点不同**：余弦相似度只关注方向（角度），忽略长度；欧氏距离同时考虑方向和长度。
>
> 2. **取值范围不同**：余弦相似度在 [-1, 1] 之间，便于设置阈值；欧氏距离在 [0, +∞)，常需要转换成“越大越相似”的形式。
>
> 3. **归一化决定相近程度**：在归一化向量上，两者行为会更接近；未归一化时差异可能非常大。
>
> **在 RAG 系统里的选择策略：**
> - 使用常见 Embedding（很多会做归一化或可视为近似归一化）时，**优先余弦相似度/点积**，稳定直观、效率高。
> - 当向量长度本身有业务含义（少见）或你确认向量未归一化且长度差异重要时，可考虑欧氏距离或先归一化再做余弦。
>
> **工程实践**：我会先用余弦相似度做 baseline，然后用评估集（或线上点击/采纳）验证是否需要改度量或加 ReRank。

**为什么这个回答出彩？**
1. ✅ 对比维度全面（关注点/范围/归一化）
2. ✅ 结合 RAG 的工程选型
3. ✅ 给出可落地的验证方法

---

### 问题2："如何确定语义检索的相似度阈值？"

**普通回答（❌ 不出彩）：**
“一般设置 0.7 或 0.8 就可以了。”

**出彩回答（✅ 推荐）：**

> **确定阈值要看三件事：**
>
> 1. **业务容错度**：法律/医疗更严格（阈值更高），推荐/创意更宽松（阈值更低）。
>
> 2. **模型分布**：不同 embedding 模型的相似度分布不同，不能套一个固定数字。
>
> 3. **数据驱动调参**：用标注或反馈数据，扫描阈值，计算 Precision/Recall/F1 或结合业务指标选点。
>
> **实践建议**：先从 0.7 起步，再通过“误召回/漏召回”案例持续调整，并在关键场景加 ReRank 提升鲁棒性。

**为什么这个回答出彩？**
1. ✅ 给出思考框架而不是死数字
2. ✅ 强调“分布”和“评估”
3. ✅ 说明线上可迭代方法

---

## 9. 【化骨绵掌】

### 卡片1：什么是语义相似度？

**一句话：** 语义相似度是衡量两段文本“意思接近程度”的数值。

**举例：**
- “我喜欢苹果” vs “我爱吃苹果” → 高相似度
- “我喜欢苹果” vs “今天天气很好” → 低相似度

**应用：** RAG 用它来判断哪些文档值得作为上下文喂给 LLM。

---

### 卡片2：为什么不能用字符串匹配？

**一句话：** 字符串匹配看的是“字”，语义相似度看的是“意思”。

**举例：**
```python
query = "如何学编程"
doc = "Python入门教程"  # 字面不匹配，但语义相关
```

**应用：** 语义检索能覆盖同义词、改写、不同表达方式。

---

### 卡片3：余弦相似度的直觉

**一句话：** 余弦相似度只看方向一致不一致，不看长度。

**举例：**
```python
import numpy as np
v1 = np.array([1, 2])
v2 = np.array([2, 4])  # 同方向
print(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))  # 1.0
```

**应用：** Embedding 常用余弦做检索排序。

---

### 卡片4：欧氏距离的直觉

**一句话：** 欧氏距离看“位置差多远”，越近越相似。

**举例：**
```python
import numpy as np
a = np.array([0, 0])
b = np.array([3, 4])
print(np.linalg.norm(a - b))  # 5.0
```

**应用：** 当你确实想把“长度差异”也当成差异时才更适合。

---

### 卡片5：点积为何常被用来加速

**一句话：** 点积是最基础也最快的，相当于“方向×长度”的综合结果。

**举例：**
```python
import numpy as np
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
print(np.dot(a, b))
```

**应用：** 如果向量归一化，点积≈余弦，可用点积做高效检索。

---

### 卡片6：相似度 vs 距离

**一句话：** 相似度越大越好；距离越小越好；它们可相互转换。

**举例：**
```python
distance = 2.0
similarity = 1 / (1 + distance)  # 简单转换
print(similarity)
```

**应用：** 读向量数据库返回结果时要看清是“距离”还是“相似度”。

---

### 卡片7：阈值不是越高越好

**一句话：** 阈值控制精确率/召回率的平衡。

**举例：**
- 阈值高：更干净，但可能漏掉答案
- 阈值低：更全，但更容易塞噪音进上下文

**应用：** 客服问答常取中间值；高风险业务更严格。

---

### 卡片8：归一化的关键作用

**一句话：** 归一化把“长度”统一，突出“方向”，让余弦更稳定。

**举例：**
```python
import numpy as np
v = np.array([3.0, 4.0])
v_norm = v / np.linalg.norm(v)
print(np.linalg.norm(v_norm))  # 1.0
```

**应用：** 很多 embedding 流程默认归一化（或在向量库内部处理）。

---

### 卡片9：RAG 中相似度检索的最小流程

**一句话：** Query 向量 vs 文档向量 → 计算相似度 → 取 Top-K → 塞给 LLM。

**举例：**
```python
# 伪代码
query_vec = embed(query)
scores = [cos(query_vec, doc_vec) for doc_vec in doc_vecs]
top_docs = topk(scores)
```

**应用：** 这是 RAG 的“检索”部分，决定上下文质量。

---

### 卡片10：相似度的局限与补救

**一句话：** 相似度只是召回工具，最终质量靠 ReRank/规则/评估闭环兜底。

**举例：**
- 同一个词多义（银行：金融/河岸）
- 不同领域词义漂移

**应用：**
- 召回用向量（快）
- 精排用 ReRank（准）
- 线上用反馈做持续迭代

---

## 10. 【一句话总结】

**语义相似度是将文本含义的接近程度量化为数值的技术，通过余弦相似度、欧氏距离等方法计算向量间关系，在 RAG 开发中用于语义检索召回、排序与阈值过滤，从而把更相关的上下文送入大模型生成更可靠答案。**

---

## 附录

### 质量检查清单（自检用）

- [ ] 10个维度完整
- [ ] 30字核心包含定义 + 价值（RAG相关）
- [ ] 第一性原理含：定义/为什么/价值/推导/总结
- [ ] 核心概念3个（并扩展覆盖更多子概念）
- [ ] 最小可用 3-5 要点，含代码与RAG场景
- [ ] 双重类比（前端 + 日常）3-5个，并有总结表
- [ ] 3个常见误区（为什么错/为什么易错/正确理解）
- [ ] 实战代码可直接运行，输出清晰，包含RAG相关示例
- [ ] 面试必问 1-2 个，普通 vs 出彩 + 解释
- [ ] 化骨绵掌 10 张卡片递进完整
- [ ] 一句话总结 50-80 字且与 RAG 强绑定

### 下一步学习建议

- 学完本节后，进入 `L3_RAG核心流程` 学“检索器设计”“Chunking”“上下文注入与生成”
- 然后在 `L4_RAG进阶优化` 学“ReRank重排序”“混合检索”“评估与调优”
