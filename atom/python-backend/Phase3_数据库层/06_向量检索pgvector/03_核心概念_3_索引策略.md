# 核心概念3：索引策略

## 为什么需要索引？

**无索引的向量检索是 O(N) 复杂度：需要计算查询向量与所有向量的距离。**

```python
# 暴力搜索（无索引）
def brute_force_search(query_embedding, all_embeddings):
    distances = []
    for embedding in all_embeddings:  # O(N)
        distance = cosine_distance(query_embedding, embedding)
        distances.append(distance)
    return sorted(distances)[:10]  # Top-10

# 50 万向量 × 1536 维 × 浮点运算 = 数秒延迟
```

**索引的作用：**
- 将 O(N) 复杂度降到 O(log N)
- 50 万向量：从 5 秒降到 30 毫秒（提升 166 倍）
- 代价：近似搜索，召回率 95-99%（而非 100%）

## pgvector 支持的索引类型

pgvector 支持两种向量索引：

| 索引类型 | 原理 | 查询速度 | 构建速度 | 召回率 | 内存占用 |
|---------|------|---------|---------|--------|---------|
| **HNSW** | 多层图结构 | 快 | 慢 | 高（95-99%） | 高 |
| **IVFFlat** | 聚类+倒排索引 | 中等 | 快 | 中等（80-95%） | 低 |

## HNSW 索引

### 原理

**HNSW（Hierarchical Navigable Small World）= 分层的可导航小世界图**

**核心思想：**
1. **多层图结构**：类似高速公路网络
   - 顶层：稀疏图，快速跳跃（高速公路）
   - 中层：中等密度（国道）
   - 底层：稠密图，精确搜索（小路）

2. **查询过程：**
   ```
   1. 从顶层的入口节点开始
   2. 在当前层找到最近的邻居
   3. 下降到下一层
   4. 重复步骤 2-3，直到底层
   5. 在底层精确搜索 Top-K
   ```

**可视化：**
```
顶层（稀疏）：  A -------- B -------- C
                |          |          |
中层（中等）：  A -- D -- B -- E -- C -- F
                |    |    |    |    |    |
底层（稠密）：  A-D-G-B-E-H-C-F-I-J-K-L
```

### 创建 HNSW 索引

```sql
-- 基本语法
CREATE INDEX ON documents
USING hnsw (embedding vector_cosine_ops);

-- 带参数
CREATE INDEX ON documents
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

**参数说明：**

#### 1. m（邻居数）

- **定义**：每个节点在图中的最大邻居数
- **默认值**：16
- **范围**：2-100
- **影响**：
  - 越大：召回率越高，但内存占用越大
  - 越小：内存占用越小，但召回率越低

```sql
-- 高召回率（适合精度要求高的场景）
CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops)
WITH (m = 32);

-- 低内存占用（适合内存受限的场景）
CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops)
WITH (m = 8);
```

**推荐值：**
- 通用场景：`m = 16`（默认）
- 高精度场景：`m = 32`
- 内存受限：`m = 8`

#### 2. ef_construction（构建时搜索范围）

- **定义**：构建索引时的搜索范围
- **默认值**：64
- **范围**：4-1000
- **影响**：
  - 越大：索引质量越高，但构建时间越长
  - 越小：构建时间越短，但索引质量越低

```sql
-- 高质量索引（适合离线构建）
CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops)
WITH (ef_construction = 128);

-- 快速构建（适合频繁重建索引）
CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops)
WITH (ef_construction = 32);
```

**推荐值：**
- 通用场景：`ef_construction = 64`（默认）
- 高质量索引：`ef_construction = 128`
- 快速构建：`ef_construction = 32`

### 查询时调整搜索范围

```python
# 调整查询时的搜索范围（ef_search）
cursor.execute("SET hnsw.ef_search = 100")

# 执行查询
cursor.execute(
    "SELECT * FROM documents ORDER BY embedding <=> %s LIMIT 10",
    (query_embedding,)
)
```

**ef_search 参数：**
- **定义**：查询时的搜索范围
- **默认值**：40
- **范围**：1-1000
- **影响**：
  - 越大：召回率越高，但查询延迟越长
  - 越小：查询延迟越短，但召回率越低

**推荐值：**
- 快速查询：`ef_search = 40`（默认）
- 平衡：`ef_search = 100`
- 高召回率：`ef_search = 200`

### HNSW 性能测试

```python
import time
import numpy as np

# 测试不同 ef_search 的性能
def benchmark_hnsw(query_embedding, ef_search_values):
    results = []

    for ef_search in ef_search_values:
        # 设置搜索范围
        cursor.execute(f"SET hnsw.ef_search = {ef_search}")

        # 测试查询延迟
        start = time.time()
        cursor.execute(
            "SELECT id FROM documents ORDER BY embedding <=> %s LIMIT 10",
            (query_embedding,)
        )
        result_ids = [row[0] for row in cursor.fetchall()]
        latency = (time.time() - start) * 1000  # 毫秒

        results.append({
            "ef_search": ef_search,
            "latency_ms": latency,
            "result_ids": result_ids
        })

    return results

# 运行测试
ef_search_values = [40, 100, 200, 400]
results = benchmark_hnsw(query_embedding, ef_search_values)

# 输出结果
for r in results:
    print(f"ef_search={r['ef_search']}: {r['latency_ms']:.2f}ms")

# 输出示例：
# ef_search=40: 15.23ms
# ef_search=100: 28.45ms
# ef_search=200: 52.67ms
# ef_search=400: 98.12ms
```

### HNSW 适用场景

**✅ 适合：**
- 查询频繁，插入不频繁（如文档库）
- 对召回率要求高（> 95%）
- 内存充足
- 数据量：10 万 - 1000 万

**❌ 不适合：**
- 数据频繁插入/更新（如实时聊天记录）
- 内存受限
- 对召回率要求不严格（80-90% 可接受）

---

## IVFFlat 索引

### 原理

**IVFFlat（Inverted File with Flat Compression）= 聚类 + 倒排索引**

**核心思想：**
1. **聚类**：将所有向量聚类成 N 个簇（如 100 个）
2. **倒排索引**：每个簇存储属于该簇的向量 ID
3. **查询过程**：
   ```
   1. 找到查询向量最近的 K 个聚类中心（如 5 个）
   2. 只在这 K 个簇内暴力搜索
   3. 返回 Top-K 结果
   ```

**可视化：**
```
聚类中心：  C1    C2    C3    C4    C5
            |     |     |     |     |
向量分布：  v1    v4    v7    v10   v13
            v2    v5    v8    v11   v14
            v3    v6    v9    v12   v15

查询：找到最近的 2 个聚类中心（C2, C3）
      只在 C2 和 C3 的簇内搜索（v4-v9）
      而不是搜索所有向量（v1-v15）
```

### 创建 IVFFlat 索引

```sql
-- 基本语法
CREATE INDEX ON documents
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

**参数说明：**

#### lists（聚类数）

- **定义**：将向量聚类成多少个簇
- **默认值**：100
- **推荐值**：`lists = sqrt(N)`，其中 N 是向量总数
  - 10 万向量：`lists = 316`
  - 50 万向量：`lists = 707`
  - 100 万向量：`lists = 1000`

```sql
-- 10 万向量
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 316);

-- 50 万向量
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 707);

-- 100 万向量
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 1000);
```

**影响：**
- lists 越大：查询越快，但召回率越低
- lists 越小：召回率越高，但查询越慢

### 查询时调整探测簇数

```python
# 调整查询时探测的簇数（probes）
cursor.execute("SET ivfflat.probes = 10")

# 执行查询
cursor.execute(
    "SELECT * FROM documents ORDER BY embedding <=> %s LIMIT 10",
    (query_embedding,)
)
```

**probes 参数：**
- **定义**：查询时探测多少个簇
- **默认值**：1
- **推荐值**：`probes = lists / 10`
  - lists=100：`probes = 10`
  - lists=316：`probes = 32`
  - lists=707：`probes = 71`

**影响：**
- probes 越大：召回率越高，但查询越慢
- probes 越小：查询越快，但召回率越低

### IVFFlat 性能测试

```python
# 测试不同 probes 的性能
def benchmark_ivfflat(query_embedding, probes_values):
    results = []

    for probes in probes_values:
        # 设置探测簇数
        cursor.execute(f"SET ivfflat.probes = {probes}")

        # 测试查询延迟
        start = time.time()
        cursor.execute(
            "SELECT id FROM documents ORDER BY embedding <=> %s LIMIT 10",
            (query_embedding,)
        )
        result_ids = [row[0] for row in cursor.fetchall()]
        latency = (time.time() - start) * 1000  # 毫秒

        results.append({
            "probes": probes,
            "latency_ms": latency,
            "result_ids": result_ids
        })

    return results

# 运行测试（lists=100）
probes_values = [1, 5, 10, 20]
results = benchmark_ivfflat(query_embedding, probes_values)

# 输出结果
for r in results:
    print(f"probes={r['probes']}: {r['latency_ms']:.2f}ms")

# 输出示例：
# probes=1: 8.12ms
# probes=5: 22.34ms
# probes=10: 38.56ms
# probes=20: 65.78ms
```

### IVFFlat 适用场景

**✅ 适合：**
- 数据频繁插入/更新（如实时聊天记录）
- 内存受限
- 对召回率要求不严格（80-95% 可接受）
- 数据量：10 万 - 500 万

**❌ 不适合：**
- 对召回率要求极高（> 95%）
- 查询频繁，插入不频繁（HNSW 更好）

---

## HNSW vs IVFFlat 对比

### 性能对比（50 万向量，1536 维）

| 维度 | HNSW | IVFFlat |
|------|------|---------|
| **构建时间** | 15 分钟 | 3 分钟 |
| **索引大小** | 2.5 GB | 1.2 GB |
| **查询延迟（P50）** | 15ms | 25ms |
| **查询延迟（P95）** | 30ms | 50ms |
| **召回率（默认参数）** | 96% | 85% |
| **召回率（调优后）** | 98% | 92% |
| **插入性能** | 中等 | 快 |

### 选择策略

```python
def choose_index_type(
    data_size: int,
    insert_frequency: str,  # "low", "medium", "high"
    recall_requirement: float,  # 0.8 - 1.0
    memory_limit: str  # "low", "medium", "high"
):
    """根据场景选择索引类型"""

    # 1. 高召回率要求 (> 95%)
    if recall_requirement > 0.95:
        return "HNSW"

    # 2. 频繁插入
    if insert_frequency == "high":
        return "IVFFlat"

    # 3. 内存受限
    if memory_limit == "low":
        return "IVFFlat"

    # 4. 数据量小 (< 10 万)
    if data_size < 100000:
        return "无需索引（暴力搜索更快）"

    # 5. 默认推荐
    return "HNSW"

# 使用
index_type = choose_index_type(
    data_size=500000,
    insert_frequency="low",
    recall_requirement=0.96,
    memory_limit="high"
)
print(f"推荐索引类型: {index_type}")  # HNSW
```

### 实际案例

#### 案例1：文档知识库（推荐 HNSW）

```python
# 场景：50 万文档，每天新增 < 1000 条，查询 QPS ~ 500
# 要求：召回率 > 95%

# 创建 HNSW 索引
cursor.execute("""
    CREATE INDEX ON documents
    USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 64)
""")

# 查询时设置
cursor.execute("SET hnsw.ef_search = 100")

# 结果：
# - 查询延迟 P95: 30ms
# - 召回率: 96%
# - 内存占用: 2.5GB
```

#### 案例2：实时聊天记录（推荐 IVFFlat）

```python
# 场景：100 万条聊天记录，每秒新增 100 条，查询 QPS ~ 200
# 要求：召回率 > 85%

# 创建 IVFFlat 索引
cursor.execute("""
    CREATE INDEX ON conversations
    USING ivfflat (embedding vector_cosine_ops)
    WITH (lists = 1000)
""")

# 查询时设置
cursor.execute("SET ivfflat.probes = 100")

# 结果：
# - 查询延迟 P95: 50ms
# - 召回率: 88%
# - 内存占用: 1.2GB
# - 插入性能: 快
```

---

## 索引维护

### 1. 重建索引

```sql
-- 删除旧索引
DROP INDEX IF EXISTS documents_embedding_idx;

-- 创建新索引
CREATE INDEX documents_embedding_idx ON documents
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

### 2. 并发构建索引

```sql
-- 并发构建（不阻塞查询）
CREATE INDEX CONCURRENTLY documents_embedding_idx ON documents
USING hnsw (embedding vector_cosine_ops);
```

### 3. 监控索引大小

```sql
-- 查看索引大小
SELECT
    indexname,
    pg_size_pretty(pg_relation_size(indexname::regclass)) AS index_size
FROM pg_indexes
WHERE tablename = 'documents';
```

### 4. 索引统计信息

```sql
-- 查看索引使用情况
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,  -- 索引扫描次数
    idx_tup_read,  -- 读取的元组数
    idx_tup_fetch  -- 获取的元组数
FROM pg_stat_user_indexes
WHERE tablename = 'documents';
```

---

## 在 AI Agent 开发中的应用

### 场景1：RAG 文档问答（HNSW）

```python
# 1. 创建索引
cursor.execute("""
    CREATE INDEX ON documents
    USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 64)
""")

# 2. 查询时设置
cursor.execute("SET hnsw.ef_search = 100")

# 3. 检索文档
def retrieve_documents(query: str, top_k: int = 5):
    query_embedding = client.embeddings.create(
        input=query,
        model="text-embedding-3-small"
    ).data[0].embedding

    cursor.execute(
        """
        SELECT id, content, embedding <=> %s AS distance
        FROM documents
        ORDER BY distance
        LIMIT %s
        """,
        (query_embedding, top_k)
    )

    return cursor.fetchall()
```

### 场景2：对话历史检索（IVFFlat）

```python
# 1. 创建索引
cursor.execute("""
    CREATE INDEX ON conversations
    USING ivfflat (embedding vector_cosine_ops)
    WITH (lists = 316)
""")

# 2. 查询时设置
cursor.execute("SET ivfflat.probes = 32")

# 3. 检索对话历史
def retrieve_conversation_history(user_id: int, query: str, top_k: int = 5):
    query_embedding = client.embeddings.create(
        input=query,
        model="text-embedding-3-small"
    ).data[0].embedding

    cursor.execute(
        """
        SELECT id, message, created_at, embedding <=> %s AS distance
        FROM conversations
        WHERE user_id = %s
        ORDER BY distance
        LIMIT %s
        """,
        (query_embedding, user_id, top_k)
    )

    return cursor.fetchall()
```

---

## 常见问题

### Q1: 索引创建失败怎么办？

```python
# 错误：insufficient memory
# 原因：索引构建需要大量内存

# 解决方案1：减小 ef_construction
CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops)
WITH (ef_construction = 32);  # 从 64 降到 32

# 解决方案2：使用 IVFFlat
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

# 解决方案3：增加 PostgreSQL 内存配置
# postgresql.conf
shared_buffers = 4GB
work_mem = 256MB
```

### Q2: 如何验证索引是否生效？

```python
# 使用 EXPLAIN ANALYZE
cursor.execute("""
    EXPLAIN ANALYZE
    SELECT * FROM documents
    ORDER BY embedding <=> %s
    LIMIT 10
""", (query_embedding,))

plan = cursor.fetchall()
for line in plan:
    print(line[0])

# 输出示例（索引生效）：
# Index Scan using documents_embedding_idx on documents
# ...

# 输出示例（索引未生效）：
# Seq Scan on documents
# ...
```

### Q3: 索引占用空间太大怎么办？

```python
# 1. 使用 halfvec 减少存储
ALTER TABLE documents ALTER COLUMN embedding TYPE halfvec(1536);

# 2. 使用 IVFFlat（内存占用更小）
CREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);

# 3. 降维
# 将 1536 维降到 768 维
```

---

## 总结

**索引策略的核心要点：**
1. **HNSW**：查询快、召回率高、内存占用大，适合查询频繁的场景
2. **IVFFlat**：构建快、内存占用小、召回率中等，适合插入频繁的场景
3. **参数调优**：`ef_search`（HNSW）和 `probes`（IVFFlat）影响召回率和延迟
4. **选择策略**：根据数据量、插入频率、召回率要求、内存限制选择

**在 AI Agent 开发中：**
- RAG 文档问答：HNSW（高召回率）
- 对话历史检索：IVFFlat（频繁插入）
- 性能监控：使用 EXPLAIN ANALYZE 验证索引效果
- 定期维护：重建索引、监控索引大小
