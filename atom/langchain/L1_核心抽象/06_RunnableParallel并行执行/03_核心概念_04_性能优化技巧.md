# RunnableParallel并行执行 - 核心概念4：性能优化技巧

> 深入理解并行执行的性能优化策略和技巧

---

## 概述

本文档深入讲解 RunnableParallel 的性能优化技巧：
1. **批处理优化**：使用 langasync 降低成本 50%
2. **连接池管理**：复用 HTTP 连接
3. **缓存策略**：避免重复调用
4. **并发控制**：优化并发数量

**学习目标**：
- 掌握批处理 API 的使用
- 理解连接池的原理
- 实现多层缓存策略
- 能够手写性能监控工具

---

## 第一部分：批处理优化

### 1.1 批处理的原理

**批处理 API 提供 50% 折扣**

```python
# 标准 API 调用
# OpenAI: $0.01/1K tokens
# Anthropic: $0.015/1K tokens

# 批处理 API 调用
# OpenAI Batch API: $0.005/1K tokens（50% 折扣）
# Anthropic Batch API: 50% 折扣
```

**适用场景**：
- ✅ 批量评估和测试
- ✅ 数据标注任务
- ✅ 离线分析和报告
- ❌ 实时对话（需要即时响应）

---

### 1.2 langasync 批处理

**2026 年推荐的批处理工具**

```python
from langasync import wrap_chain
from langchain_core.runnables import RunnableParallel

# 原始并行链
parallel = RunnableParallel(
    sentiment=sentiment_chain,
    topic=topic_chain,
    entities=entity_chain
)

# 包装为批处理模式
async_parallel = wrap_chain(parallel, batch_size=10)

# 批量执行
inputs = [{"text": f"text{i}"} for i in range(100)]
results = await async_parallel.abatch(inputs)

# 成本对比
# 标准模式：100 × $0.01 = $1.00
# 批处理模式：100 × $0.005 = $0.50
# 节省：50%
```

---

### 1.3 手动批处理实现

**理解批处理的底层机制**

```python
import asyncio
from typing import List, Any

class BatchProcessor:
    """批处理器"""

    def __init__(self, batch_size: int = 10):
        self.batch_size = batch_size

    async def process_batch(
        self,
        parallel: RunnableParallel,
        inputs: List[Any]
    ) -> List[Any]:
        """批量处理输入"""
        results = []

        # 分批处理
        for i in range(0, len(inputs), self.batch_size):
            batch = inputs[i:i + self.batch_size]

            # 并行处理当前批次
            batch_tasks = [
                parallel.ainvoke(input)
                for input in batch
            ]
            batch_results = await asyncio.gather(*batch_tasks)

            results.extend(batch_results)

            # 可选：批次间延迟（避免限流）
            if i + self.batch_size < len(inputs):
                await asyncio.sleep(0.1)

        return results

# 使用示例
processor = BatchProcessor(batch_size=10)
results = await processor.process_batch(parallel, inputs)
```

---

### 1.4 批处理 + 进度追踪

**显示批处理进度**

```python
from tqdm.asyncio import tqdm

async def process_with_progress(
    parallel: RunnableParallel,
    inputs: List[Any],
    batch_size: int = 10
) -> List[Any]:
    """带进度条的批处理"""
    results = []

    # 使用 tqdm 显示进度
    for i in tqdm(range(0, len(inputs), batch_size), desc="Processing"):
        batch = inputs[i:i + batch_size]

        batch_tasks = [parallel.ainvoke(input) for input in batch]
        batch_results = await asyncio.gather(*batch_tasks)

        results.extend(batch_results)

    return results

# 使用
results = await process_with_progress(parallel, inputs)
# 输出: Processing: 100%|██████████| 10/10 [00:15<00:00,  1.50s/it]
```

---

## 第二部分：连接池管理

### 2.1 连接池的原理

**复用 HTTP 连接，减少握手开销**

```python
# 无连接池（每次请求都建立新连接）
import requests

for i in range(100):
    response = requests.get("https://api.openai.com/v1/models")
    # 每次请求：
    # 1. TCP 握手（50-100ms）
    # 2. TLS 握手（50-100ms）
    # 3. HTTP 请求（100-200ms）
    # 总耗时：200-400ms

# 有连接池（复用连接）
session = requests.Session()
for i in range(100):
    response = session.get("https://api.openai.com/v1/models")
    # 第一次请求：200-400ms
    # 后续请求：100-200ms（节省握手时间）
```

**性能提升**：
- 减少 TCP/TLS 握手时间
- 高并发场景提升 20-30%

---

### 2.2 httpx 连接池

**推荐使用 httpx（异步 HTTP 客户端）**

```python
import httpx
import asyncio

# 配置连接池
client = httpx.AsyncClient(
    limits=httpx.Limits(
        max_connections=100,        # 最大连接数
        max_keepalive_connections=20  # 保持活跃的连接数
    ),
    timeout=httpx.Timeout(30.0)  # 超时设置
)

# 使用连接池
async def fetch_with_pool(url):
    response = await client.get(url)
    return response.json()

# 并行请求（复用连接）
tasks = [fetch_with_pool(f"https://api.example.com/{i}") for i in range(100)]
results = await asyncio.gather(*tasks)

# 关闭连接池
await client.aclose()
```

---

### 2.3 LangChain 中的连接池

**配置 LangChain 的 HTTP 客户端**

```python
from langchain_openai import ChatOpenAI
import httpx

# 创建自定义 HTTP 客户端
http_client = httpx.AsyncClient(
    limits=httpx.Limits(
        max_connections=100,
        max_keepalive_connections=20
    )
)

# 使用自定义客户端
llm = ChatOpenAI(
    model="gpt-4o-mini",
    http_client=http_client
)

# 并行调用（复用连接）
parallel = RunnableParallel(
    task1=llm,
    task2=llm,
    task3=llm
)
```

---

## 第三部分：缓存策略

### 3.1 内存缓存

**使用 LangChain 内置缓存**

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# 启用内存缓存
set_llm_cache(InMemoryCache())

# 使用
llm = ChatOpenAI(model="gpt-4o-mini")

# 第一次调用（API 请求）
result1 = llm.invoke("什么是量子计算？")  # 1.5秒

# 第二次调用（从缓存读取）
result2 = llm.invoke("什么是量子计算？")  # <0.01秒

# 节省：99% 时间，100% 成本
```

---

### 3.2 Redis 缓存

**持久化缓存，跨进程共享**

```python
from langchain.cache import RedisCache
from langchain.globals import set_llm_cache
import redis

# 连接 Redis
redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=0
)

# 启用 Redis 缓存
set_llm_cache(RedisCache(redis_client))

# 使用（与内存缓存相同）
llm = ChatOpenAI(model="gpt-4o-mini")
result = llm.invoke("什么是量子计算？")

# 优势：
# - 持久化（重启后仍有效）
# - 跨进程共享（多个服务共享缓存）
# - 可设置过期时间
```

---

### 3.3 语义缓存

**基于语义相似度的缓存**

```python
from langchain.cache import GPTCache
from langchain.globals import set_llm_cache
import gptcache

# 配置语义缓存
gptcache.init(
    similarity_threshold=0.9  # 相似度阈值
)

set_llm_cache(GPTCache())

# 使用
llm = ChatOpenAI(model="gpt-4o-mini")

# 第一次调用
result1 = llm.invoke("什么是量子计算？")  # API 请求

# 语义相似的查询（从缓存读取）
result2 = llm.invoke("请解释量子计算")  # 缓存命中
result3 = llm.invoke("量子计算是什么意思？")  # 缓存命中

# 节省：65% 成本（根据 2025 年数据）
```

---

### 3.4 多层缓存

**组合多种缓存策略**

```python
class MultiLevelCache:
    """多层缓存"""

    def __init__(self):
        self.memory_cache = {}  # L1: 内存缓存
        self.redis_client = redis.Redis()  # L2: Redis 缓存

    async def get(self, key: str):
        """获取缓存"""
        # 1. 尝试内存缓存
        if key in self.memory_cache:
            return self.memory_cache[key]

        # 2. 尝试 Redis 缓存
        value = self.redis_client.get(key)
        if value:
            # 回填内存缓存
            self.memory_cache[key] = value
            return value

        # 3. 缓存未命中
        return None

    async def set(self, key: str, value: Any, ttl: int = 3600):
        """设置缓存"""
        # 同时写入两层缓存
        self.memory_cache[key] = value
        self.redis_client.setex(key, ttl, value)

# 使用
cache = MultiLevelCache()

# 查询
cached_value = await cache.get("query_key")
if cached_value:
    return cached_value
else:
    result = await llm.ainvoke(query)
    await cache.set("query_key", result)
    return result
```

---

## 第四部分：并发控制

### 4.1 动态并发调整

**根据系统负载动态调整并发数**

```python
import asyncio
import psutil

class AdaptiveConcurrency:
    """自适应并发控制"""

    def __init__(self, min_concurrent=5, max_concurrent=50):
        self.min_concurrent = min_concurrent
        self.max_concurrent = max_concurrent
        self.current_concurrent = min_concurrent

    def get_optimal_concurrency(self):
        """根据系统负载计算最优并发数"""
        # 获取 CPU 使用率
        cpu_percent = psutil.cpu_percent(interval=1)

        # 获取内存使用率
        memory_percent = psutil.virtual_memory().percent

        # 动态调整
        if cpu_percent < 50 and memory_percent < 70:
            # 系统空闲，增加并发
            self.current_concurrent = min(
                self.current_concurrent + 5,
                self.max_concurrent
            )
        elif cpu_percent > 80 or memory_percent > 85:
            # 系统繁忙，减少并发
            self.current_concurrent = max(
                self.current_concurrent - 5,
                self.min_concurrent
            )

        return self.current_concurrent

# 使用
controller = AdaptiveConcurrency()

async def process_with_adaptive_concurrency(inputs):
    results = []
    semaphore = asyncio.Semaphore(controller.get_optimal_concurrency())

    async def limited_process(input):
        async with semaphore:
            return await parallel.ainvoke(input)

    tasks = [limited_process(input) for input in inputs]
    results = await asyncio.gather(*tasks)
    return results
```

---

### 4.2 令牌桶算法

**平滑限流，避免突发流量**

```python
import time
import asyncio

class TokenBucket:
    """令牌桶限流器"""

    def __init__(self, rate: float, capacity: int):
        """
        Args:
            rate: 令牌生成速率（个/秒）
            capacity: 桶容量
        """
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """获取令牌"""
        async with self.lock:
            now = time.time()
            # 生成新令牌
            elapsed = now - self.last_update
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now

            # 等待令牌
            while self.tokens < 1:
                await asyncio.sleep(0.1)
                now = time.time()
                elapsed = now - self.last_update
                self.tokens = min(
                    self.capacity,
                    self.tokens + elapsed * self.rate
                )
                self.last_update = now

            # 消耗令牌
            self.tokens -= 1

# 使用
bucket = TokenBucket(rate=10, capacity=50)  # 10 请求/秒

async def rate_limited_invoke(input):
    await bucket.acquire()
    return await parallel.ainvoke(input)

# 并行调用（受限流控制）
tasks = [rate_limited_invoke(input) for input in inputs]
results = await asyncio.gather(*tasks)
```

---

## 第五部分：性能监控

### 5.1 基础性能监控

**手写性能监控工具**

```python
import time
from typing import Dict, List
from dataclasses import dataclass

@dataclass
class PerformanceMetrics:
    """性能指标"""
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    success_count: int
    failure_count: int
    throughput: float  # 请求/秒

class PerformanceMonitor:
    """性能监控器"""

    def __init__(self):
        self.metrics: List[float] = []
        self.errors: List[Exception] = []

    async def monitor(self, func, input):
        """监控函数执行"""
        start = time.time()
        try:
            result = await func(input)
            elapsed = time.time() - start
            self.metrics.append(elapsed)
            return result
        except Exception as e:
            self.errors.append(e)
            raise

    def get_metrics(self) -> PerformanceMetrics:
        """获取性能指标"""
        if not self.metrics:
            return None

        total_time = sum(self.metrics)
        return PerformanceMetrics(
            total_time=total_time,
            avg_time=total_time / len(self.metrics),
            min_time=min(self.metrics),
            max_time=max(self.metrics),
            success_count=len(self.metrics),
            failure_count=len(self.errors),
            throughput=len(self.metrics) / total_time
        )

# 使用
monitor = PerformanceMonitor()

tasks = [monitor.monitor(parallel.ainvoke, input) for input in inputs]
results = await asyncio.gather(*tasks, return_exceptions=True)

metrics = monitor.get_metrics()
print(f"平均耗时: {metrics.avg_time:.2f}秒")
print(f"吞吐量: {metrics.throughput:.2f} 请求/秒")
print(f"成功率: {metrics.success_count / (metrics.success_count + metrics.failure_count) * 100:.1f}%")
```

---

### 5.2 LangSmith 集成

**使用 LangSmith 追踪性能**

```python
from langsmith import trace

@trace
async def monitored_parallel(input):
    """带 LangSmith 追踪的并行执行"""
    result = await parallel.ainvoke(input)
    return result

# LangSmith 自动记录：
# - 每个任务的耗时
# - Token 使用量
# - 成本估算
# - 错误和异常
# - 并发数量
```

---

### 5.3 实时性能仪表板

**构建实时监控仪表板**

```python
from collections import deque
import time

class RealtimeMonitor:
    """实时性能监控"""

    def __init__(self, window_size=100):
        self.window = deque(maxlen=window_size)
        self.start_time = time.time()

    def record(self, duration: float, success: bool):
        """记录一次执行"""
        self.window.append({
            "duration": duration,
            "success": success,
            "timestamp": time.time()
        })

    def get_stats(self) -> Dict:
        """获取实时统计"""
        if not self.window:
            return {}

        recent = list(self.window)
        durations = [r["duration"] for r in recent]
        successes = [r["success"] for r in recent]

        return {
            "avg_duration": sum(durations) / len(durations),
            "p95_duration": sorted(durations)[int(len(durations) * 0.95)],
            "success_rate": sum(successes) / len(successes),
            "throughput": len(recent) / (time.time() - recent[0]["timestamp"])
        }

# 使用
monitor = RealtimeMonitor()

async def monitored_invoke(input):
    start = time.time()
    try:
        result = await parallel.ainvoke(input)
        monitor.record(time.time() - start, True)
        return result
    except Exception as e:
        monitor.record(time.time() - start, False)
        raise

# 定期打印统计
import asyncio

async def print_stats():
    while True:
        stats = monitor.get_stats()
        print(f"平均耗时: {stats.get('avg_duration', 0):.2f}秒")
        print(f"P95 耗时: {stats.get('p95_duration', 0):.2f}秒")
        print(f"成功率: {stats.get('success_rate', 0) * 100:.1f}%")
        print(f"吞吐量: {stats.get('throughput', 0):.2f} 请求/秒")
        await asyncio.sleep(10)
```

---

## 第六部分：综合优化案例

### 6.1 生产级优化配置

**2025-2026 推荐配置**

```python
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableParallel
from langchain.cache import RedisCache
from langchain.globals import set_llm_cache
import httpx
import redis

# 1. 配置缓存
redis_client = redis.Redis(host='localhost', port=6379)
set_llm_cache(RedisCache(redis_client))

# 2. 配置连接池
http_client = httpx.AsyncClient(
    limits=httpx.Limits(
        max_connections=100,
        max_keepalive_connections=20
    ),
    timeout=httpx.Timeout(30.0)
)

# 3. 配置 LLM（带重试和 fallback）
llm = (
    ChatOpenAI(
        model="gpt-4o-mini",
        http_client=http_client
    )
    .with_retry(stop_after_attempt=3)
    .with_fallback([ChatOpenAI(model="gpt-3.5-turbo")])
)

# 4. 配置并行执行
parallel = RunnableParallel(
    sentiment=sentiment_chain,
    topic=topic_chain,
    entities=entity_chain
)

# 5. 批处理执行
from langasync import wrap_chain

async_parallel = wrap_chain(parallel, batch_size=10)

# 6. 使用
results = await async_parallel.abatch(inputs)

# 性能提升：
# - 缓存：节省 30-50% 成本
# - 连接池：提升 20-30% 速度
# - 批处理：节省 50% 成本
# - 重试+fallback：提升 99% 可靠性
# 综合：10倍速度，5倍成本优化
```

---

### 6.2 成本优化对比

**优化前 vs 优化后**

```python
# 优化前
# - 无缓存
# - 无连接池
# - 无批处理
# - 无重试/fallback

# 性能指标：
# - 平均耗时：4.5秒/请求
# - 成本：$0.01/请求
# - 成功率：95%
# - 吞吐量：800 请求/小时

# 月成本（100万请求）：$10,000

# 优化后
# - Redis 缓存（30% 命中率）
# - httpx 连接池
# - langasync 批处理
# - 重试+fallback

# 性能指标：
# - 平均耗时：0.5秒/请求（9倍提升）
# - 成本：$0.002/请求（5倍降低）
# - 成功率：99.9%（提升 4.9%）
# - 吞吐量：7200 请求/小时（9倍提升）

# 月成本（100万请求）：$2,000（节省 80%）
```

---

## 学习检查清单

- [ ] 理解批处理 API 的原理和使用
- [ ] 掌握连接池的配置和优化
- [ ] 实现多层缓存策略
- [ ] 掌握并发控制技巧
- [ ] 能够手写性能监控工具
- [ ] 理解综合优化的配置

---

## 参考资料

[来源: How to Make LangChain Apps 10x Faster and 5x Cheaper - https://medium.com/@vinodkrane/langchain-in-production-performance-security-and-cost-optimization-d5e0b44a26fd, 访问日期: 2026-02-18]

[来源: LangChain Production Scaling: Complete 2026 Deployment Guide - https://www.tannox.ai/ai/40382350-0825-4795-8a34-cbc28f4ae042, 访问日期: 2026-02-18]

[来源: LangChain Best Practices - https://www.swarnendu.de/blog/langchain-best-practices, 访问日期: 2026-02-18]

---

**版本**: v1.0
**最后更新**: 2026-02-18
**行数**: 约490行
