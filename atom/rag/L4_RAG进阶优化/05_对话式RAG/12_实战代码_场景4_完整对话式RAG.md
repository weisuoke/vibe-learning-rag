# 实战代码：场景4 - 完整对话式RAG系统

> 整合所有模块，实现端到端的对话式RAG系统

---

## 场景说明

本场景整合前3个场景的所有模块，实现一个完整的对话式RAG系统：
- **对话管理**：Session管理、历史存储
- **历史压缩**：自动压缩长对话
- **指代消解**：Query重写
- **RAG检索生成**：向量检索 + LLM生成

**完整Pipeline：**
```
用户输入 → 获取历史 → 指代消解 → 历史压缩（如需）→
向量检索 → 上下文注入 → LLM生成 → 保存历史 → 返回回答
```

---

## 完整代码实现

```python
"""
完整对话式RAG系统
整合：ConversationManager + HistoryCompressor + CoreferenceResolver + RAG Pipeline
"""

from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime
from openai import OpenAI
import chromadb


# ===== 数据结构 =====

@dataclass
class Message:
    """对话消息"""
    role: str
    content: str
    timestamp: datetime


# ===== 对话管理器 =====

class ConversationManager:
    """对话管理器"""

    def __init__(self, max_history: int = 20):
        self.sessions: Dict[str, List[Message]] = {}
        self.max_history = max_history

    def add_message(self, session_id: str, role: str, content: str) -> Message:
        """添加消息"""
        message = Message(role=role, content=content, timestamp=datetime.now())

        if session_id not in self.sessions:
            self.sessions[session_id] = []

        self.sessions[session_id].append(message)

        # 滑动窗口
        if len(self.sessions[session_id]) > self.max_history:
            self.sessions[session_id] = self.sessions[session_id][-self.max_history:]

        return message

    def get_context(self, session_id: str, last_n: Optional[int] = None) -> List[Message]:
        """获取上下文"""
        if session_id not in self.sessions:
            return []

        messages = self.sessions[session_id]

        if last_n is None:
            return messages

        keep_count = last_n * 2
        return messages[-keep_count:] if len(messages) > keep_count else messages

    def clear_session(self, session_id: str):
        """清空会话"""
        if session_id in self.sessions:
            del self.sessions[session_id]


# ===== 历史压缩器 =====

class HistoryCompressor:
    """历史压缩器"""

    def __init__(self):
        self.client = OpenAI()

    def should_compress(self, messages: List[Message]) -> bool:
        """判断是否需要压缩"""
        return len(messages) > 20  # 超过10轮

    def compress(self, messages: List[Message]) -> List[Message]:
        """压缩历史（使用LLM摘要）"""
        if not self.should_compress(messages):
            return messages

        # 分割：需要摘要的部分 + 保留的最近部分
        to_summarize = messages[:-6]  # 前面的消息
        to_keep = messages[-6:]       # 保留最近3轮

        # 生成摘要
        summary_text = self._generate_summary(to_summarize)

        # 构建摘要消息
        summary_message = Message(
            role="system",
            content=f"[历史摘要] {summary_text}",
            timestamp=datetime.now()
        )

        compressed = [summary_message] + to_keep

        print(f"[历史压缩] {len(messages)}条 → {len(compressed)}条")
        return compressed

    def _generate_summary(self, messages: List[Message]) -> str:
        """生成摘要"""
        history_text = "\n".join([f"{m.role}: {m.content}" for m in messages])

        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[{
                    "role": "system",
                    "content": "总结以下对话的关键信息（200字以内）"
                }, {
                    "role": "user",
                    "content": f"对话历史:\n{history_text}"
                }],
                temperature=0.3,
                max_tokens=300
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"[警告] 摘要生成失败: {e}")
            return "对话历史摘要"


# ===== 指代消解器 =====

class CoreferenceResolver:
    """指代消解器"""

    def __init__(self):
        self.client = OpenAI()
        self.pronouns = ["它", "这个", "那个", "这", "那", "它们", "这些", "那些"]

    def resolve(self, query: str, history: List[Message]) -> str:
        """解析指代"""
        if not self._needs_resolution(query, history):
            return query

        resolved = self._rewrite_with_llm(query, history)

        if resolved != query:
            print(f"[指代消解] {query} → {resolved}")

        return resolved

    def _needs_resolution(self, query: str, history: List[Message]) -> bool:
        """判断是否需要消解"""
        if not history:
            return False

        has_pronoun = any(p in query for p in self.pronouns)
        is_short = len(query) < 10

        return has_pronoun or is_short

    def _rewrite_with_llm(self, query: str, history: List[Message]) -> str:
        """使用LLM重写"""
        context = "\n".join([f"{m.role}: {m.content}" for m in history[-6:]])

        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[{
                    "role": "system",
                    "content": """根据对话历史，将问题重写为独立完整的问题。
要求：解决所有指代，保持原意图，只输出重写后的问题。"""
                }, {
                    "role": "user",
                    "content": f"对话历史:\n{context}\n\n当前问题: {query}\n\n重写后:"
                }],
                temperature=0.3,
                max_tokens=100
            )
            return response.choices[0].message.content.strip().strip('"\'')
        except Exception as e:
            print(f"[警告] 指代消解失败: {e}")
            return query


# ===== 完整对话式RAG系统 =====

class ConversationalRAG:
    """完整对话式RAG系统"""

    def __init__(self, collection_name: str = "documents"):
        """
        初始化对话式RAG系统

        Args:
            collection_name: 向量数据库集合名称
        """
        # 初始化各个组件
        self.conversation_manager = ConversationManager(max_history=30)
        self.compressor = HistoryCompressor()
        self.resolver = CoreferenceResolver()
        self.client = OpenAI()

        # 初始化向量数据库
        self.chroma_client = chromadb.Client()
        try:
            self.collection = self.chroma_client.get_collection(collection_name)
        except:
            self.collection = self.chroma_client.create_collection(collection_name)

        print(f"[初始化] 对话式RAG系统已启动")

    def add_documents(self, documents: List[str], ids: Optional[List[str]] = None):
        """
        添加文档到向量库

        Args:
            documents: 文档列表
            ids: 文档ID列表（可选）
        """
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(documents))]

        self.collection.add(
            documents=documents,
            ids=ids
        )

        print(f"[向量库] 已添加{len(documents)}个文档")

    def chat(self, session_id: str, user_query: str) -> str:
        """
        处理用户输入，返回回答

        完整流程：
        1. 获取对话历史
        2. 指代消解（重写Query）
        3. 历史压缩（如果超过阈值）
        4. 向量检索
        5. 构建Prompt（注入历史和文档）
        6. LLM生成回答
        7. 保存到历史

        Args:
            session_id: 会话ID
            user_query: 用户问题

        Returns:
            系统回答
        """
        print(f"\n{'='*60}")
        print(f"[会话] {session_id}")
        print(f"[用户] {user_query}")
        print(f"{'='*60}")

        # 1. 获取对话历史
        history = self.conversation_manager.get_context(session_id, last_n=10)
        print(f"[历史] 当前历史: {len(history)}条消息")

        # 2. 指代消解
        resolved_query = self.resolver.resolve(user_query, history)

        # 3. 历史压缩（如果需要）
        if self.compressor.should_compress(history):
            history = self.compressor.compress(history)
            # 更新会话中的历史
            self.conversation_manager.sessions[session_id] = history

        # 4. 向量检索
        print(f"[检索] 使用Query: {resolved_query}")
        search_results = self.collection.query(
            query_texts=[resolved_query],
            n_results=3
        )

        # 提取文档
        if search_results['documents'] and search_results['documents'][0]:
            context_docs = "\n\n".join(search_results['documents'][0])
            print(f"[检索] 找到{len(search_results['documents'][0])}个相关文档")
        else:
            context_docs = "未找到相关文档"
            print(f"[检索] 未找到相关文档")

        # 5. 构建Prompt
        messages = self._build_prompt(history, context_docs, user_query, resolved_query)

        # 6. LLM生成
        print(f"[生成] 调用LLM生成回答...")
        try:
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=messages,
                temperature=0.7
            )

            answer = response.choices[0].message.content

        except Exception as e:
            print(f"[错误] LLM生成失败: {e}")
            answer = "抱歉，生成回答时出现错误。"

        # 7. 保存到历史
        self.conversation_manager.add_message(session_id, "user", user_query)
        self.conversation_manager.add_message(session_id, "assistant", answer)

        print(f"[助手] {answer[:100]}...")
        print(f"{'='*60}\n")

        return answer

    def _build_prompt(self, history: List[Message], context_docs: str,
                     user_query: str, resolved_query: str) -> List[Dict]:
        """
        构建Prompt消息列表
        """
        messages = []

        # 系统消息
        messages.append({
            "role": "system",
            "content": """你是一个智能助手，基于提供的参考文档和对话历史回答用户问题。

要求：
1. 优先使用参考文档中的信息
2. 结合对话历史理解用户意图
3. 如果文档中没有相关信息，基于你的知识回答
4. 回答要准确、简洁、有条理
"""
        })

        # 添加历史摘要（如果有）
        if history and history[0].role == "system":
            messages.append({
                "role": "system",
                "content": history[0].content
            })
            history = history[1:]  # 移除摘要

        # 添加最近的对话历史（最多3轮）
        recent_history = history[-6:] if len(history) > 6 else history
        if recent_history:
            history_text = "\n".join([
                f"{msg.role}: {msg.content}"
                for msg in recent_history
            ])
            messages.append({
                "role": "system",
                "content": f"对话历史:\n{history_text}"
            })

        # 添加检索到的文档
        messages.append({
            "role": "system",
            "content": f"参考文档:\n{context_docs}"
        })

        # 添加用户问题
        if resolved_query != user_query:
            messages.append({
                "role": "user",
                "content": f"{user_query}\n（完整问题：{resolved_query}）"
            })
        else:
            messages.append({
                "role": "user",
                "content": user_query
            })

        return messages

    def get_session_info(self, session_id: str) -> Dict:
        """获取会话信息"""
        history = self.conversation_manager.get_context(session_id)
        return {
            "session_id": session_id,
            "message_count": len(history),
            "turn_count": len(history) // 2
        }

    def clear_session(self, session_id: str):
        """清空会话"""
        self.conversation_manager.clear_session(session_id)
        print(f"[会话] {session_id} 已清空")


# ===== 使用示例 =====

def demo_basic_usage():
    """演示基础使用"""
    print("=" * 60)
    print("演示1：基础对话式RAG")
    print("=" * 60)

    # 1. 初始化系统
    rag = ConversationalRAG(collection_name="demo_docs")

    # 2. 添加文档到向量库
    documents = [
        "RAG（检索增强生成）是一种结合检索和生成的技术。它通过检索外部知识库来增强LLM的回答质量。",
        "RAG的主要优势包括：1. 知识更新及时，无需重新训练模型 2. 减少幻觉，提供可溯源的回答 3. 成本较低，不需要微调大模型。",
        "RAG的核心组件包括：检索器（Retriever）、向量数据库（Vector Store）和生成器（Generator）。",
        "实现RAG需要：1. 文档预处理和分块 2. 使用Embedding模型生成向量 3. 存储到向量数据库 4. 检索相关文档 5. 将文档注入Prompt 6. LLM生成回答。",
    ]

    rag.add_documents(documents)

    # 3. 开始对话
    session_id = "user_demo"

    # 第1轮：基础问答
    answer1 = rag.chat(session_id, "什么是RAG？")

    # 第2轮：含指代的追问
    answer2 = rag.chat(session_id, "它有什么优势？")

    # 第3轮：继续追问
    answer3 = rag.chat(session_id, "如何实现它？")

    # 4. 查看会话信息
    info = rag.get_session_info(session_id)
    print(f"\n会话信息: {info}")


def demo_long_conversation():
    """演示长对话场景（触发历史压缩）"""
    print("\n" + "=" * 60)
    print("演示2：长对话场景（历史压缩）")
    print("=" * 60)

    rag = ConversationalRAG(collection_name="demo_docs2")

    # 添加文档
    documents = [
        f"关于RAG的知识点{i}：这是一些详细的技术说明..."
        for i in range(10)
    ]
    rag.add_documents(documents)

    session_id = "user_long_chat"

    # 模拟15轮对话（会触发压缩）
    for i in range(15):
        query = f"请介绍RAG的第{i+1}个方面"
        rag.chat(session_id, query)

    # 查看最终历史长度
    info = rag.get_session_info(session_id)
    print(f"\n最终会话信息: {info}")


def demo_multi_user():
    """演示多用户场景"""
    print("\n" + "=" * 60)
    print("演示3：多用户场景")
    print("=" * 60)

    rag = ConversationalRAG(collection_name="demo_docs3")

    # 添加文档
    documents = [
        "RAG是检索增强生成技术",
        "Transformer是一种神经网络架构",
    ]
    rag.add_documents(documents)

    # 用户A的对话
    print("\n--- 用户A ---")
    rag.chat("user_a", "什么是RAG？")
    rag.chat("user_a", "它的优势是什么？")

    # 用户B的对话
    print("\n--- 用户B ---")
    rag.chat("user_b", "什么是Transformer？")
    rag.chat("user_b", "它有什么特点？")

    # 用户A继续对话（不会混淆）
    print("\n--- 用户A继续 ---")
    rag.chat("user_a", "如何实现它？")  # "它"指RAG，不是Transformer


# ===== 主函数 =====

def main():
    """主函数"""
    # 演示1：基础使用
    demo_basic_usage()

    # 演示2：长对话
    # demo_long_conversation()

    # 演示3：多用户
    # demo_multi_user()


if __name__ == "__main__":
    main()
```

---

## 运行输出示例

```
============================================================
演示1：基础对话式RAG
============================================================
[初始化] 对话式RAG系统已启动
[向量库] 已添加4个文档

============================================================
[会话] user_demo
[用户] 什么是RAG？
============================================================
[历史] 当前历史: 0条消息
[检索] 使用Query: 什么是RAG？
[检索] 找到3个相关文档
[生成] 调用LLM生成回答...
[助手] RAG（检索增强生成）是一种结合检索和生成的技术。它通过检索外部知识库来增强LLM的回答质量。RAG的核心思想是...
============================================================

============================================================
[会话] user_demo
[用户] 它有什么优势？
============================================================
[历史] 当前历史: 2条消息
[指代消解] 它有什么优势？ → RAG有什么优势？
[检索] 使用Query: RAG有什么优势？
[检索] 找到3个相关文档
[生成] 调用LLM生成回答...
[助手] RAG的主要优势包括：1. 知识更新及时，无需重新训练模型 2. 减少幻觉，提供可溯源的回答 3. 成本较低，不需要微调...
============================================================

============================================================
[会话] user_demo
[用户] 如何实现它？
============================================================
[历史] 当前历史: 4条消息
[指代消解] 如何实现它？ → 如何实现RAG？
[检索] 使用Query: 如何实现RAG？
[检索] 找到3个相关文档
[生成] 调用LLM生成回答...
[助手] 实现RAG需要以下步骤：1. 文档预处理和分块 2. 使用Embedding模型生成向量 3. 存储到向量数据库 4. 检索相关文档...
============================================================

会话信息: {'session_id': 'user_demo', 'message_count': 6, 'turn_count': 3}
```

---

## 生产环境部署

### 1. FastAPI服务封装

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()

# 全局RAG实例
rag_system = ConversationalRAG(collection_name="production_docs")

class ChatRequest(BaseModel):
    session_id: str
    query: str

class ChatResponse(BaseModel):
    answer: str
    session_info: Dict

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """对话接口"""
    try:
        answer = rag_system.chat(request.session_id, request.query)
        info = rag_system.get_session_info(request.session_id)

        return ChatResponse(answer=answer, session_info=info)

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/documents")
async def add_documents(documents: List[str]):
    """添加文档接口"""
    rag_system.add_documents(documents)
    return {"status": "success", "count": len(documents)}

@app.delete("/session/{session_id}")
async def clear_session(session_id: str):
    """清空会话接口"""
    rag_system.clear_session(session_id)
    return {"status": "success"}

# 启动：uvicorn main:app --reload
```

### 2. 数据库持久化

```python
import psycopg2

class PersistentConversationalRAG(ConversationalRAG):
    """带数据库持久化的对话式RAG"""

    def __init__(self, db_config: Dict, **kwargs):
        super().__init__(**kwargs)
        self.conn = psycopg2.connect(**db_config)
        self._create_tables()

    def _create_tables(self):
        """创建数据库表"""
        with self.conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS conversations (
                    id SERIAL PRIMARY KEY,
                    session_id VARCHAR(255) NOT NULL,
                    role VARCHAR(20) NOT NULL,
                    content TEXT NOT NULL,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_session (session_id, timestamp)
                )
            """)
        self.conn.commit()

    def chat(self, session_id: str, user_query: str) -> str:
        """重写chat方法，添加数据库持久化"""
        # 调用父类方法
        answer = super().chat(session_id, user_query)

        # 持久化到数据库
        self._save_to_db(session_id, "user", user_query)
        self._save_to_db(session_id, "assistant", answer)

        return answer

    def _save_to_db(self, session_id: str, role: str, content: str):
        """保存到数据库"""
        with self.conn.cursor() as cur:
            cur.execute(
                "INSERT INTO conversations (session_id, role, content) VALUES (%s, %s, %s)",
                (session_id, role, content)
            )
        self.conn.commit()
```

### 3. 监控和日志

```python
import logging
from datetime import datetime

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rag_system.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class MonitoredConversationalRAG(ConversationalRAG):
    """带监控的对话式RAG"""

    def chat(self, session_id: str, user_query: str) -> str:
        """添加监控和日志"""
        start_time = datetime.now()

        try:
            # 记录请求
            logger.info(f"[Request] session={session_id}, query={user_query[:50]}")

            # 调用父类方法
            answer = super().chat(session_id, user_query)

            # 记录响应时间
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"[Response] session={session_id}, elapsed={elapsed:.2f}s")

            return answer

        except Exception as e:
            logger.error(f"[Error] session={session_id}, error={str(e)}")
            raise
```

---

## 性能优化

### 1. 缓存优化

```python
from functools import lru_cache
import hashlib

class CachedConversationalRAG(ConversationalRAG):
    """带缓存的对话式RAG"""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.query_cache: Dict[str, str] = {}

    def chat(self, session_id: str, user_query: str) -> str:
        """带缓存的对话"""
        # 生成缓存键
        cache_key = self._generate_cache_key(session_id, user_query)

        # 检查缓存
        if cache_key in self.query_cache:
            logger.info(f"[Cache Hit] {user_query}")
            return self.query_cache[cache_key]

        # 调用父类方法
        answer = super().chat(session_id, user_query)

        # 存入缓存
        self.query_cache[cache_key] = answer

        return answer

    def _generate_cache_key(self, session_id: str, query: str) -> str:
        """生成缓存键"""
        return hashlib.md5(f"{session_id}:{query}".encode()).hexdigest()
```

### 2. 异步处理

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncConversationalRAG(ConversationalRAG):
    """异步对话式RAG"""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.executor = ThreadPoolExecutor(max_workers=10)

    async def chat_async(self, session_id: str, user_query: str) -> str:
        """异步对话"""
        loop = asyncio.get_event_loop()
        answer = await loop.run_in_executor(
            self.executor,
            self.chat,
            session_id,
            user_query
        )
        return answer
```

---

## 成本分析

### Token消耗估算

```
单次对话Token消耗：
- 历史上下文: 500-1000 tokens
- 检索文档: 500-1000 tokens
- 用户问题: 50-100 tokens
- 系统Prompt: 200-300 tokens
- 总输入: 1250-2400 tokens

- LLM输出: 200-500 tokens

成本（GPT-4）：
- 输入: 1800 tokens × $0.03/1K = $0.054
- 输出: 350 tokens × $0.06/1K = $0.021
- 单次对话: $0.075

优化后（使用压缩和缓存）：
- 输入: 1000 tokens × $0.03/1K = $0.03
- 输出: 300 tokens × $0.06/1K = $0.018
- 单次对话: $0.048（节省36%）
```

---

## 总结

**本场景实现了：**
- ✅ 完整的对话式RAG系统
- ✅ 整合所有核心模块
- ✅ 支持多用户并发
- ✅ 生产环境部署方案

**核心特性：**
- Session管理：隔离不同用户
- 历史压缩：自动控制Context Window
- 指代消解：支持自然对话
- RAG检索生成：准确回答问题

**生产环境建议：**
- 使用FastAPI封装REST API
- 数据库持久化对话历史
- 添加监控和日志
- 实现缓存和异步处理

**下一步学习：**
- 学习 `06_最小可用.md` - 快速上手的核心知识
- 学习 `13_面试必问.md` - 面试准备
