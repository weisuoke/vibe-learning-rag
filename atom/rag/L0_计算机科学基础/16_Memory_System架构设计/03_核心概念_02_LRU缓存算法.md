# 核心概念:LRU缓存算法

> **理解最经典的缓存淘汰策略**

---

## 概念定义

**LRU (Least Recently Used) = 淘汰最久未使用的数据，保留最近使用的数据**

### 核心思想

基于**时间局部性原理**：
- 最近被访问的数据，很可能马上再次被访问
- 最久未被访问的数据，未来被访问的概率最低

---

## 算法原理

### 数据结构选择

**需求分析**：
1. O(1) 查找数据
2. O(1) 更新访问顺序
3. O(1) 淘汰最久未使用的数据

**解决方案**：
- **哈希表**：实现 O(1) 查找
- **双向链表**：实现 O(1) 插入/删除/移动

```
哈希表: key → Node
双向链表: Head ← → Node1 ← → Node2 ← → ... ← → Tail

最近使用的在头部 ←→ 最久未使用的在尾部
```

---

## 实现方式

### 方式1：使用 OrderedDict（简单）

```python
from collections import OrderedDict

class LRUCache:
    """LRU 缓存（OrderedDict 实现）"""
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity

    def get(self, key: str) -> any:
        """获取数据"""
        if key not in self.cache:
            return None

        # 移到末尾（标记为最近使用）
        self.cache.move_to_end(key)
        return self.cache[key]

    def put(self, key: str, value: any) -> None:
        """存入数据"""
        if key in self.cache:
            # 更新已有数据
            self.cache.move_to_end(key)
        else:
            # 新增数据
            if len(self.cache) >= self.capacity:
                # 缓存已满，删除最久未使用的（第一个）
                self.cache.popitem(last=False)

        self.cache[key] = value

# 使用示例
cache = LRUCache(capacity=3)

cache.put("a", 1)
cache.put("b", 2)
cache.put("c", 3)
print(list(cache.cache.keys()))  # ['a', 'b', 'c']

cache.get("a")  # 访问 a
print(list(cache.cache.keys()))  # ['b', 'c', 'a']

cache.put("d", 4)  # 缓存满了，淘汰 b
print(list(cache.cache.keys()))  # ['c', 'a', 'd']
```

**优点**：
- 代码简洁
- Python 内置，无需额外依赖

**缺点**：
- 面试时可能被要求手写底层实现
- 无法展示对数据结构的理解

---

### 方式2：手写双向链表（面试推荐）

```python
class Node:
    """双向链表节点"""
    def __init__(self, key: str = "", value: any = None):
        self.key = key
        self.value = value
        self.prev: Node | None = None
        self.next: Node | None = None

class LRUCache:
    """LRU 缓存（手写双向链表实现）"""
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache: dict[str, Node] = {}  # key → Node

        # 虚拟头尾节点（简化边界处理）
        self.head = Node()
        self.tail = Node()
        self.head.next = self.tail
        self.tail.prev = self.head

    def _add_to_head(self, node: Node) -> None:
        """将节点添加到头部（标记为最近使用）"""
        node.prev = self.head
        node.next = self.head.next
        self.head.next.prev = node
        self.head.next = node

    def _remove_node(self, node: Node) -> None:
        """从链表中移除节点"""
        node.prev.next = node.next
        node.next.prev = node.prev

    def _move_to_head(self, node: Node) -> None:
        """将节点移到头部"""
        self._remove_node(node)
        self._add_to_head(node)

    def _remove_tail(self) -> Node:
        """移除尾部节点（最久未使用）"""
        node = self.tail.prev
        self._remove_node(node)
        return node

    def get(self, key: str) -> any:
        """O(1) 获取数据"""
        if key not in self.cache:
            return None

        node = self.cache[key]
        self._move_to_head(node)  # 标记为最近使用
        return node.value

    def put(self, key: str, value: any) -> None:
        """O(1) 写入数据"""
        if key in self.cache:
            # 更新已有节点
            node = self.cache[key]
            node.value = value
            self._move_to_head(node)
        else:
            # 新增节点
            node = Node(key, value)
            self.cache[key] = node
            self._add_to_head(node)

            if len(self.cache) > self.capacity:
                # 缓存已满，删除尾部节点
                removed = self._remove_tail()
                del self.cache[removed.key]

# 测试
cache = LRUCache(capacity=2)

cache.put("a", 1)
cache.put("b", 2)
print(cache.get("a"))  # 1

cache.put("c", 3)  # 淘汰 b
print(cache.get("b"))  # None

cache.put("d", 4)  # 淘汰 a
print(cache.get("a"))  # None
print(cache.get("c"))  # 3
print(cache.get("d"))  # 4
```

**优点**：
- 展示对数据结构的深入理解
- 面试官认可度高
- 时间复杂度 O(1)

**关键点**：
1. **虚拟头尾节点**：避免空指针判断
2. **哈希表 + 双向链表**：结合两者优势
3. **移动操作**：先删除，再添加到头部

---

## 复杂度分析

### 时间复杂度

| 操作 | OrderedDict | 手写链表 | 说明 |
|------|-------------|----------|------|
| `get(key)` | O(1) | O(1) | 哈希表查找 + 链表移动 |
| `put(key, value)` | O(1) | O(1) | 哈希表插入 + 链表操作 |
| `淘汰` | O(1) | O(1) | 删除尾部节点 |

### 空间复杂度

**O(capacity)**：
- 哈希表：存储 capacity 个键值对
- 双向链表：存储 capacity 个节点

---

## AI Agent 应用

### 1. 对话历史管理

```python
class ConversationMemory:
    """对话历史记忆（LRU 策略）"""
    def __init__(self, max_turns: int = 10):
        self.memory = LRUCache(capacity=max_turns)
        self.turn = 0

    def add_message(self, role: str, content: str):
        """添加对话消息"""
        self.turn += 1
        key = f"turn_{self.turn}"
        self.memory.put(key, {
            "role": role,
            "content": content,
            "timestamp": time.time()
        })

    def get_recent_messages(self, n: int = 5) -> list:
        """获取最近 n 条消息"""
        all_keys = list(self.memory.cache.keys())
        recent_keys = all_keys[-n:]

        messages = []
        for key in recent_keys:
            msg = self.memory.get(key)
            if msg:
                messages.append(msg)

        return messages

    def format_for_llm(self) -> list:
        """格式化为 LLM 输入"""
        messages = self.get_recent_messages(n=10)
        return [
            {"role": msg["role"], "content": msg["content"]}
            for msg in messages
        ]

# 使用示例
memory = ConversationMemory(max_turns=5)

memory.add_message("user", "你好")
memory.add_message("assistant", "你好！有什么可以帮你的？")
memory.add_message("user", "什么是 RAG？")
memory.add_message("assistant", "RAG 是检索增强生成...")
memory.add_message("user", "能举个例子吗？")
memory.add_message("assistant", "当然可以...")

# 获取最近3条消息作为上下文
context = memory.format_for_llm()
print(f"上下文: {len(context)} 条消息")
```

### 2. Token 窗口管理

```python
class TokenWindowManager:
    """Token 窗口管理（LRU 策略）"""
    def __init__(self, max_tokens: int = 4000):
        self.max_tokens = max_tokens
        self.messages = LRUCache(capacity=100)
        self.current_tokens = 0

    def _count_tokens(self, text: str) -> int:
        """估算 token 数量（简化版）"""
        return len(text) // 4

    def add_message(self, role: str, content: str):
        """添加消息（自动管理 token 窗口）"""
        tokens = self._count_tokens(content)

        # 如果超过窗口大小，删除最久的消息
        while self.current_tokens + tokens > self.max_tokens:
            # 获取最久的消息
            oldest_key = next(iter(self.messages.cache))
            oldest_msg = self.messages.cache[oldest_key]
            oldest_tokens = self._count_tokens(oldest_msg["content"])

            # 删除最久的消息
            del self.messages.cache[oldest_key]
            self.current_tokens -= oldest_tokens

        # 添加新消息
        key = f"msg_{time.time()}"
        self.messages.put(key, {"role": role, "content": content})
        self.current_tokens += tokens

    def get_context(self) -> list:
        """获取当前上下文"""
        return [
            {"role": msg["role"], "content": msg["content"]}
            for msg in self.messages.cache.values()
        ]

# 使用示例
manager = TokenWindowManager(max_tokens=1000)

manager.add_message("user", "很长的问题..." * 100)
manager.add_message("assistant", "很长的回答..." * 100)
manager.add_message("user", "继续提问..." * 100)

context = manager.get_context()
print(f"当前上下文: {len(context)} 条消息")
```

### 3. 函数调用缓存

```python
import functools
import time

class LRUCacheDecorator:
    """LRU 缓存装饰器"""
    def __init__(self, capacity: int = 128):
        self.cache = LRUCache(capacity=capacity)

    def __call__(self, func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # 生成缓存键
            key = f"{func.__name__}:{args}:{kwargs}"

            # 查询缓存
            result = self.cache.get(key)
            if result is not None:
                print(f"✅ 缓存命中: {func.__name__}")
                return result

            # 缓存未命中，执行函数
            print(f"❌ 缓存未命中: {func.__name__}")
            result = func(*args, **kwargs)

            # 存入缓存
            self.cache.put(key, result)
            return result

        return wrapper

# 使用示例
@LRUCacheDecorator(capacity=10)
def expensive_computation(n: int) -> int:
    """耗时计算"""
    time.sleep(1)  # 模拟耗时操作
    return n * n

# 第一次调用：缓存未命中
start = time.time()
result1 = expensive_computation(5)
print(f"第一次: {time.time() - start:.2f}秒")

# 第二次调用：缓存命中
start = time.time()
result2 = expensive_computation(5)
print(f"第二次: {time.time() - start:.2f}秒")
```

---

## LRU 的优缺点

### 优点

1. **实现简单**：数据结构清晰，代码易懂
2. **性能优秀**：O(1) 时间复杂度
3. **符合直觉**：最近使用的数据保留
4. **适用广泛**：大多数场景都适用

### 缺点

1. **无法识别热点数据**：
   - 高频访问的数据可能被淘汰
   - 例如：A 访问 100 次，B 访问 1 次，如果 B 更新，A 可能被淘汰

2. **顺序扫描性能差**：
   - 扫描大文件时，每个数据块只访问一次
   - LRU 无法利用这种访问模式

3. **缓存污染**：
   - 偶尔访问的大量数据会污染缓存
   - 例如：批量导入数据时，会淘汰所有热数据

---

## LRU 变种

### 1. LRU-K

**思想**：记录最近 K 次访问的时间，淘汰第 K 次访问时间最早的数据

```python
class LRUKCache:
    """LRU-K 缓存（K=2）"""
    def __init__(self, capacity: int, k: int = 2):
        self.capacity = capacity
        self.k = k
        self.cache = {}
        self.access_history = {}  # key → [timestamp1, timestamp2, ...]

    def get(self, key: str) -> any:
        if key not in self.cache:
            return None

        # 记录访问时间
        if key not in self.access_history:
            self.access_history[key] = []
        self.access_history[key].append(time.time())

        # 只保留最近 K 次
        self.access_history[key] = self.access_history[key][-self.k:]

        return self.cache[key]

    def put(self, key: str, value: any):
        if len(self.cache) >= self.capacity and key not in self.cache:
            # 淘汰第 K 次访问时间最早的
            victim = min(
                self.access_history.items(),
                key=lambda x: x[1][0] if len(x[1]) >= self.k else float('inf')
            )[0]
            del self.cache[victim]
            del self.access_history[victim]

        self.cache[key] = value
```

### 2. 2Q（Two Queues）

**思想**：维护两个队列，新数据先进入 FIFO 队列，再次访问后进入 LRU 队列

```python
class TwoQueueCache:
    """2Q 缓存"""
    def __init__(self, capacity: int):
        self.fifo_capacity = capacity // 4
        self.lru_capacity = capacity - self.fifo_capacity

        self.fifo_queue = []  # FIFO 队列
        self.lru_cache = LRUCache(self.lru_capacity)

    def get(self, key: str) -> any:
        # 先查 LRU 缓存
        value = self.lru_cache.get(key)
        if value is not None:
            return value

        # 再查 FIFO 队列
        if key in self.fifo_queue:
            # 提升到 LRU 缓存
            self.fifo_queue.remove(key)
            self.lru_cache.put(key, value)
            return value

        return None

    def put(self, key: str, value: any):
        # 新数据先进入 FIFO 队列
        if len(self.fifo_queue) >= self.fifo_capacity:
            self.fifo_queue.pop(0)
        self.fifo_queue.append(key)
```

---

## 最佳实践

### 1. 选择合适的容量

```python
def benchmark_cache_size(access_pattern: list):
    """测试不同缓存大小的性能"""
    for size in [10, 50, 100, 200, 500]:
        cache = LRUCache(capacity=size)
        hits = 0
        misses = 0

        for key in access_pattern:
            if cache.get(key) is not None:
                hits += 1
            else:
                misses += 1
                cache.put(key, f"data_{key}")

        hit_rate = hits / (hits + misses)
        print(f"容量: {size:3d} | 命中率: {hit_rate:.2%}")
```

### 2. 监控缓存性能

```python
class MonitoredLRUCache(LRUCache):
    """带监控的 LRU 缓存"""
    def __init__(self, capacity: int):
        super().__init__(capacity)
        self.hits = 0
        self.misses = 0

    def get(self, key: str) -> any:
        result = super().get(key)
        if result is not None:
            self.hits += 1
        else:
            self.misses += 1
        return result

    def hit_rate(self) -> float:
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    def stats(self) -> dict:
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": self.hit_rate(),
            "size": len(self.cache)
        }
```

### 3. 线程安全

```python
import threading

class ThreadSafeLRUCache(LRUCache):
    """线程安全的 LRU 缓存"""
    def __init__(self, capacity: int):
        super().__init__(capacity)
        self.lock = threading.Lock()

    def get(self, key: str) -> any:
        with self.lock:
            return super().get(key)

    def put(self, key: str, value: any):
        with self.lock:
            super().put(key, value)
```

---

## 参考资源

### LeetCode 题目
- [146. LRU Cache](https://leetcode.com/problems/lru-cache/) - 中等难度，高频面试题

### 2025-2026 最新研究
- [Redis LRU Implementation](https://redis.io/docs/reference/eviction/) - Redis 的 LRU 实现
- [LangGraph Memory](https://docs.langchain.com/oss/python/langgraph/add-memory) - AI Agent 记忆管理

### 经典资源
- [Python OrderedDict Documentation](https://docs.python.org/3/library/collections.html#collections.OrderedDict)
- [Cache Replacement Policies](https://en.wikipedia.org/wiki/Cache_replacement_policies)
