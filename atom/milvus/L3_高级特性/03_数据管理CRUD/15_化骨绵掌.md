# 化骨绵掌 - Milvus 数据管理 CRUD

> 10 个知识卡片，快速掌握 Milvus CRUD 的核心要点

---

## 什么是化骨绵掌？

**化骨绵掌**是一种武功，看似柔和，实则威力巨大。在学习中，化骨绵掌代表：
- **看似简单**：每个知识点都很简洁
- **威力巨大**：掌握后能解决大部分问题
- **融会贯通**：将零散知识点串联成体系

---

## 卡片 1: CRUD 四大操作

### 核心要点

```
CRUD = Create + Read + Update + Delete
       ↓       ↓      ↓       ↓
     Insert  Query  Upsert  Delete
            Search
```

### 关键特性

| 操作 | Milvus 方法 | 特点 | 适用场景 |
|------|------------|------|---------|
| **Create** | `insert()` | 批量插入，列式存储 | 数据导入 |
| **Read** | `query()` / `search()` | 精确查询 / 相似度检索 | 数据查询 |
| **Update** | `upsert()` | 存在则更新，不存在则插入 | 数据更新 |
| **Delete** | `delete()` | 软删除，需要 Compaction | 数据清理 |

### 一句话记忆

**Insert 批量写，Query 精确查，Search 相似找，Delete 软删除，Upsert 自动判。**

---

## 卡片 2: Insert 的三步曲

### 核心要点

```
Insert 三步曲 = insert() → flush() → load()
```

### 详细步骤

```python
# 第一步：插入数据（写入内存缓冲区）
collection.insert(data)

# 第二步：持久化（写入磁盘）
collection.flush()

# 第三步：加载到内存（才能检索）
collection.load()
```

### 为什么需要三步？

1. **insert()**: 写入内存缓冲区（MemTable），快速
2. **flush()**: 批量写入磁盘（Segment），持久化
3. **load()**: 加载索引到内存，才能检索

### 常见错误

- ❌ 忘记 `flush()`：数据可能丢失
- ❌ 忘记 `load()`：无法检索
- ❌ 每次插入都 `flush()`：性能低下

### 一句话记忆

**Insert 写缓冲，flush 存磁盘，load 进内存，三步不能少。**

---

## 卡片 3: Query vs Search

### 核心要点

```
Query  = 精确查询（标量匹配）
Search = 相似度检索（向量匹配）
```

### 对比表

| 特性 | Query | Search |
|------|-------|--------|
| **查询方式** | 表达式（expr） | 向量（data） |
| **返回结果** | 所有匹配 | Top-K 最相似 |
| **使用索引** | 不使用向量索引 | 使用向量索引 |
| **性能** | O(n) 全表扫描 | O(log n) 索引加速 |
| **适用场景** | 按 ID 查询 | 相似度检索 |

### 代码示例

```python
# Query: 精确查询
results = collection.query(
    expr="id in [1, 2, 3]",
    output_fields=["id", "text"]
)

# Search: 相似度检索
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    limit=5
)
```

### 一句话记忆

**Query 找精确，Search 找相似，各有所长，不可混淆。**

---

## 卡片 4: Delete 的软删除机制

### 核心要点

```
Delete = 标记删除 → flush() → Compaction → 空间释放
```

### 三个阶段

```python
# 阶段 1: 标记删除（软删除）
collection.delete(expr="id in [1, 2, 3]")

# 阶段 2: 持久化删除标记
collection.flush()

# 阶段 3: Compaction 清理数据
collection.compact()
collection.wait_for_compaction_completed()
```

### 为什么是软删除？

1. **Segment 不可变**：数据存储在不可变的 Segment 中
2. **性能考虑**：立即删除需要重写 Segment，代价高
3. **一致性保证**：软删除保证原子性

### 空间释放时机

- **自动 Compaction**：删除比例 > 10% 或 Segment > 512MB
- **手动 Compaction**：调用 `compact()`
- **定期 Compaction**：每天凌晨自动执行

### 一句话记忆

**Delete 只标记，Compaction 才清理，空间释放有延迟。**

---

## 卡片 5: Upsert 的原子操作

### 核心要点

```
Upsert = 存在则更新 + 不存在则插入（原子操作）
```

### 对比传统方式

```python
# 传统方式：手动判断
existing = collection.query(expr=f"id == {id}")
if existing:
    collection.delete(expr=f"id == {id}")
    collection.insert(data)
else:
    collection.insert(data)
collection.flush()

# Upsert：原子操作
collection.upsert(data)
collection.flush()
```

### 优势

1. **原子性**：一次操作完成，不会出现中间状态
2. **简化逻辑**：不需要手动判断是否存在
3. **性能更好**：内部优化，比手动 Delete + Insert 快

### 适用场景

- 数据更新（如更新 Embedding）
- 数据同步（如从其他系统同步）
- 增量导入（如定期更新知识库）

### 一句话记忆

**Upsert 自动判，存在就更新，不存在就插入，原子又高效。**

---

## 卡片 6: 列式存储的数据格式

### 核心要点

```
行式存储 = [{id: 1, text: "A"}, {id: 2, text: "B"}]
列式存储 = [[1, 2], ["A", "B"]]
```

### 转换方法

```python
# 行式数据
rows = [
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]}
]

# 转换为列式
ids = [row["id"] for row in rows]
texts = [row["text"] for row in rows]
embeddings = [row["embedding"] for row in rows]

# 插入
collection.insert([ids, texts, embeddings])
```

### 为什么用列式存储？

1. **压缩效率高**：同类型数据连续存储，压缩率更高
2. **向量计算快**：SIMD 加速效果更好
3. **内存占用少**：只加载需要的列

### 注意事项

- ✅ 列的顺序必须与 Schema 一致
- ✅ 每列的长度必须相同
- ❌ 不能使用行式数据直接插入

### 一句话记忆

**列式存储压缩好，向量计算效率高，转换格式不能少。**

---

## 卡片 7: flush() 的批量持久化

### 核心要点

```
flush() = 批量写入磁盘 = 性能优化的关键
```

### 性能对比

```python
# 错误：每次插入都 flush（性能差）
for data in batches:
    collection.insert(data)
    collection.flush()  # 每次都写磁盘，慢

# 正确：批量 flush（性能好）
for data in batches:
    collection.insert(data)
collection.flush()  # 批量写磁盘，快
```

### 性能提升

| 方式 | 性能 | 说明 |
|------|------|------|
| 每次 flush | 1x | 基准性能 |
| 批量 flush | 10-100x | 减少磁盘 I/O |

### flush() 的作用

1. **持久化**：将内存缓冲区的数据写入磁盘
2. **构建索引**：为新数据构建索引
3. **保证一致性**：确保数据不丢失

### 何时调用 flush()？

- ✅ 批量插入后
- ✅ 删除数据后
- ✅ Upsert 数据后
- ❌ 每次插入后（性能差）

### 一句话记忆

**flush 批量写，性能提升高，频繁调用要避免。**

---

## 卡片 8: load() 的显式加载

### 核心要点

```
load() = 加载索引到内存 = 检索的前提条件
```

### 为什么需要 load()？

```python
# 不 load：无法检索
collection.insert(data)
collection.flush()
results = collection.search(...)  # 报错：collection not loaded

# load 后：可以检索
collection.insert(data)
collection.flush()
collection.load()  # 加载到内存
results = collection.search(...)  # 成功
```

### load() 的作用

1. **加载索引**：将向量索引加载到内存
2. **加载数据**：将数据加载到内存
3. **准备检索**：为检索做准备

### 内存管理

```python
# 加载 Collection
collection.load()  # 占用内存

# 释放 Collection
collection.release()  # 释放内存
```

### 何时调用 load()？

- ✅ 创建 Collection 后
- ✅ 插入数据并 flush 后
- ✅ 重启 Milvus 后
- ❌ 每次检索前（已加载则不需要）

### 一句话记忆

**load 进内存，检索才能行，释放用 release，内存要管理。**

---

## 卡片 9: 混合检索（Search + expr）

### 核心要点

```
混合检索 = 向量检索 + 标量过滤
```

### 代码示例

```python
# 纯向量检索
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    limit=10
)

# 混合检索（向量 + 标量过滤）
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    limit=10,
    expr="category == 'tech' and price < 100"  # 标量过滤
)
```

### 执行顺序

```
1. 向量检索（找到候选集）
   ↓
2. 标量过滤（过滤候选集）
   ↓
3. 返回 Top-K
```

### 性能优化

| 策略 | 性能 | 说明 |
|------|------|------|
| 先向量后标量 | 快 | Milvus 默认策略 |
| 先标量后向量 | 慢 | 需要全表扫描 |

### 适用场景

- 电商搜索：相似商品 + 价格过滤
- 文档检索：相似文档 + 类别过滤
- 推荐系统：相似用户 + 年龄过滤

### 一句话记忆

**混合检索先向量，再用标量来过滤，性能高效又精准。**

---

## 卡片 10: 一致性级别的选择

### 核心要点

```
一致性级别 = 性能 vs 一致性的权衡
```

### 四种一致性级别

| 级别 | 说明 | 性能 | 适用场景 |
|------|------|------|---------|
| **Strong** | 强一致性，读取最新数据 | 低 | 金融、交易 |
| **Bounded** | 有界一致性，读取 N 秒前的数据 | 中 | 实时推荐 |
| **Session** | 会话一致性，同一会话读取最新数据 | 中 | 用户会话 |
| **Eventually** | 最终一致性，读取可能延迟 | 高 | 日志分析 |

### 代码示例

```python
# 强一致性（性能低）
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    limit=5,
    consistency_level="Strong"
)

# 最终一致性（性能高）
results = collection.search(
    data=[query_vector],
    anns_field="embedding",
    limit=5,
    consistency_level="Eventually"
)
```

### 选择建议

- **金融系统**：Strong（强一致性）
- **实时推荐**：Bounded（有界一致性）
- **用户会话**：Session（会话一致性）
- **日志分析**：Eventually（最终一致性）

### 性能影响

```
Strong > Bounded > Session > Eventually
  ↑                              ↑
一致性强                      性能高
性能低                        一致性弱
```

### 一句话记忆

**一致性强性能低，最终一致性能高，根据场景来选择。**

---

## 知识卡片总结

### 10 个核心要点

1. **CRUD 四大操作**：Insert, Query/Search, Upsert, Delete
2. **Insert 三步曲**：insert() → flush() → load()
3. **Query vs Search**：精确查询 vs 相似度检索
4. **Delete 软删除**：标记删除 → Compaction → 空间释放
5. **Upsert 原子操作**：存在则更新，不存在则插入
6. **列式存储**：行式转列式，压缩效率高
7. **flush() 批量持久化**：批量写入，性能优化
8. **load() 显式加载**：加载到内存，才能检索
9. **混合检索**：向量检索 + 标量过滤
10. **一致性级别**：性能 vs 一致性的权衡

### 记忆口诀

```
Insert 三步不能少，flush load 要记牢
Query Search 要分清，精确相似各不同
Delete 软删有延迟，Compaction 才清理
Upsert 原子又高效，存在更新不存在插
列式存储压缩好，向量计算效率高
批量 flush 性能优，频繁调用要避免
load 进内存才能查，release 释放内存佳
混合检索先向量，标量过滤再筛选
一致性强性能低，根据场景来选择
```

### 实践检查清单

在使用 Milvus CRUD 前，确保你已经：

- [ ] 理解 CRUD 四大操作的区别
- [ ] 掌握 Insert 的三步曲（insert → flush → load）
- [ ] 区分 Query 和 Search 的使用场景
- [ ] 理解 Delete 的软删除机制
- [ ] 会使用 Upsert 简化更新逻辑
- [ ] 掌握列式存储的数据格式转换
- [ ] 知道何时调用 flush() 和 load()
- [ ] 会使用混合检索（Search + expr）
- [ ] 根据场景选择合适的一致性级别
- [ ] 定期执行 Compaction 释放空间

---

## 从知识卡片到实践

掌握了这 10 个知识卡片后，你可以：

1. **快速上手**：理解 Milvus CRUD 的核心概念
2. **避免踩坑**：避免常见的错误和误区
3. **优化性能**：掌握性能优化的关键技巧
4. **设计系统**：根据场景选择合适的方案

---

**下一步**: 学习 [03_核心概念_1_数据插入策略.md](./03_核心概念_1_数据插入策略.md) 深入理解插入操作
