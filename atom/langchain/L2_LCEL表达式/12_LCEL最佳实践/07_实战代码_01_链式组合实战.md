# 实战代码：链式组合实战

> **本节目标**：通过完整的RAG项目，掌握RunnableSequence、RunnableParallel、RunnableBranch和RunnablePassthrough的实战应用。

---

## 一、项目场景

**智能文档问答系统**：
- 支持多种检索策略（向量检索、关键词检索）
- 并行执行多个分析任务
- 根据查询复杂度智能路由
- 完整的数据流管理

**技术栈**：
- LangChain + OpenAI
- ChromaDB（向量存储）
- Python 3.13+

---

## 二、环境准备

### 2.1 安装依赖

```bash
# 安装依赖
uv add langchain langchain-openai langchain-community chromadb

# 配置环境变量
cat > .env << EOF
OPENAI_API_KEY=your_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1  # 可选
EOF
```

### 2.2 项目结构

```
rag_chain_demo/
├── data/
│   └── documents.txt          # 示例文档
├── chains/
│   ├── __init__.py
│   ├── retrieval.py          # 检索链
│   ├── generation.py         # 生成链
│   └── router.py             # 路由链
├── main.py                   # 主程序
└── .env                      # 环境变量
```

---

## 三、基础组件实现

### 3.1 检索链（RunnableSequence）

```python
# chains/retrieval.py
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.runnables import RunnableLambda
from typing import List
from langchain.schema import Document

def create_retrieval_chain(vectorstore: Chroma, k: int = 5):
    """创建检索链（顺序组合）

    流程：Query → Retriever → Format Docs → Context String
    """
    # 步骤1：检索器
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})

    # 步骤2：格式化文档
    def format_docs(docs: List[Document]) -> str:
        """格式化检索到的文档"""
        if not docs:
            return "未找到相关文档"

        formatted = []
        for i, doc in enumerate(docs, 1):
            formatted.append(f"文档{i}:\n{doc.page_content}\n")

        return "\n".join(formatted)

    # 顺序组合：retriever | format
    chain = retriever | RunnableLambda(format_docs)

    return chain


# 示例使用
if __name__ == "__main__":
    from langchain.text_splitter import CharacterTextSplitter

    # 准备数据
    documents = [
        "LCEL是LangChain表达式语言，用于组合链。",
        "RunnableSequence实现顺序执行，使用管道操作符|。",
        "RunnableParallel实现并发执行，提升性能40-50%。",
    ]

    # 创建向量存储
    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
    texts = text_splitter.create_documents(documents)

    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(texts, embeddings)

    # 创建检索链
    retrieval_chain = create_retrieval_chain(vectorstore, k=2)

    # 测试
    result = retrieval_chain.invoke("什么是LCEL？")
    print(result)
    # 输出：
    # 文档1:
    # LCEL是LangChain表达式语言，用于组合链。
    #
    # 文档2:
    # RunnableSequence实现顺序执行，使用管道操作符|。
```

### 3.2 生成链（RunnableSequence）

```python
# chains/generation.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def create_generation_chain(model: str = "gpt-4o-mini", temperature: float = 0):
    """创建生成链（顺序组合）

    流程：Input → Prompt → LLM → Parser → Output
    """
    # 步骤1：Prompt模板
    prompt = ChatPromptTemplate.from_template(
        """基于以下上下文回答问题。如果上下文中没有相关信息，请说"根据提供的信息无法回答"。

上下文：
{context}

问题：{question}

回答："""
    )

    # 步骤2：LLM
    llm = ChatOpenAI(model=model, temperature=temperature)

    # 步骤3：输出解析器
    parser = StrOutputParser()

    # 顺序组合：prompt | llm | parser
    chain = prompt | llm | parser

    return chain


# 示例使用
if __name__ == "__main__":
    generation_chain = create_generation_chain()

    # 测试
    result = generation_chain.invoke({
        "context": "LCEL是LangChain表达式语言，用于组合链。",
        "question": "什么是LCEL？"
    })
    print(result)
    # 输出：LCEL是LangChain表达式语言，用于组合链。
```

---

## 四、并行执行（RunnableParallel）

### 4.1 多维度分析链

```python
# chains/analysis.py
from langchain_core.runnables import RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def create_analysis_chain():
    """创建多维度分析链（并行执行）

    并行执行：摘要 + 关键词 + 情感分析
    """
    llm = ChatOpenAI(model="gpt-4o-mini")
    parser = StrOutputParser()

    # 分支1：摘要
    summary_prompt = ChatPromptTemplate.from_template(
        "用一句话总结以下文本：\n\n{text}"
    )
    summary_chain = summary_prompt | llm | parser

    # 分支2：关键词提取
    keywords_prompt = ChatPromptTemplate.from_template(
        "提取以下文本的3个关键词（用逗号分隔）：\n\n{text}"
    )
    keywords_chain = keywords_prompt | llm | parser

    # 分支3：情感分析
    sentiment_prompt = ChatPromptTemplate.from_template(
        "分析以下文本的情感（积极/中性/消极）：\n\n{text}"
    )
    sentiment_chain = sentiment_prompt | llm | parser

    # 并行组合
    parallel_chain = RunnableParallel({
        "summary": summary_chain,
        "keywords": keywords_chain,
        "sentiment": sentiment_chain
    })

    return parallel_chain


# 示例使用
if __name__ == "__main__":
    import time

    analysis_chain = create_analysis_chain()

    text = """
    LCEL是LangChain表达式语言，它提供了一种声明式的方式来组合链。
    通过使用管道操作符，开发者可以轻松构建复杂的AI应用。
    LCEL的设计理念是简单、直观、高效。
    """

    # 测试并行执行
    start = time.time()
    result = analysis_chain.invoke({"text": text})
    elapsed = time.time() - start

    print(f"执行时间: {elapsed:.2f}秒")
    print(f"摘要: {result['summary']}")
    print(f"关键词: {result['keywords']}")
    print(f"情感: {result['sentiment']}")

    # 输出示例：
    # 执行时间: 2.1秒（并行执行，而非6秒串行）
    # 摘要: LCEL是一种声明式的链组合语言，简单高效。
    # 关键词: LCEL, 管道操作符, AI应用
    # 情感: 积极
```

### 4.2 性能对比

```python
# 性能对比：串行 vs 并行
import time

def serial_execution(text: str):
    """串行执行"""
    summary = summary_chain.invoke({"text": text})
    keywords = keywords_chain.invoke({"text": text})
    sentiment = sentiment_chain.invoke({"text": text})
    return {"summary": summary, "keywords": keywords, "sentiment": sentiment}

def parallel_execution(text: str):
    """并行执行"""
    return analysis_chain.invoke({"text": text})

# 测试
text = "测试文本..."

# 串行
start = time.time()
result_serial = serial_execution(text)
time_serial = time.time() - start

# 并行
start = time.time()
result_parallel = parallel_execution(text)
time_parallel = time.time() - start

print(f"串行耗时: {time_serial:.2f}秒")
print(f"并行耗时: {time_parallel:.2f}秒")
print(f"性能提升: {(time_serial / time_parallel - 1) * 100:.1f}%")

# 输出示例：
# 串行耗时: 6.3秒
# 并行耗时: 2.1秒
# 性能提升: 200.0%（3倍提升）
```

---

## 五、条件路由（RunnableBranch）

### 5.1 智能路由链

```python
# chains/router.py
from langchain_core.runnables import RunnableBranch
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def create_smart_router():
    """创建智能路由链（条件分支）

    路由逻辑：
    - 简单查询 → 快速模型（gpt-4o-mini）
    - 复杂查询 → 强大模型（gpt-4o）
    - 默认 → 标准模型（gpt-4o-mini）
    """
    # 条件1：简单查询
    def is_simple_query(input_dict: dict) -> bool:
        query = input_dict.get("question", "")
        # 简单查询：少于5个词
        return len(query.split()) < 5

    # 条件2：复杂查询
    def is_complex_query(input_dict: dict) -> bool:
        query = input_dict.get("question", "")
        # 复杂查询：包含"分析"、"比较"、"解释"等关键词
        complex_keywords = ["分析", "比较", "解释", "为什么", "如何"]
        return any(keyword in query for keyword in complex_keywords)

    # 分支1：快速模型（简单查询）
    fast_prompt = ChatPromptTemplate.from_template(
        "简洁回答：{question}"
    )
    fast_chain = (
        fast_prompt
        | ChatOpenAI(model="gpt-4o-mini", temperature=0)
        | StrOutputParser()
    )

    # 分支2：强大模型（复杂查询）
    powerful_prompt = ChatPromptTemplate.from_template(
        """请详细回答以下问题：

问题：{question}

要求：
1. 提供深入的分析
2. 包含具体示例
3. 结构清晰"""
    )
    powerful_chain = (
        powerful_prompt
        | ChatOpenAI(model="gpt-4o", temperature=0.3)
        | StrOutputParser()
    )

    # 分支3：标准模型（默认）
    standard_prompt = ChatPromptTemplate.from_template(
        "回答问题：{question}"
    )
    standard_chain = (
        standard_prompt
        | ChatOpenAI(model="gpt-4o-mini", temperature=0)
        | StrOutputParser()
    )

    # 条件路由
    router = RunnableBranch(
        (is_simple_query, fast_chain),      # 简单 → 快速
        (is_complex_query, powerful_chain),  # 复杂 → 强大
        standard_chain                       # 默认 → 标准
    )

    return router


# 示例使用
if __name__ == "__main__":
    router = create_smart_router()

    # 测试1：简单查询
    result1 = router.invoke({"question": "什么是LCEL？"})
    print(f"简单查询: {result1}")
    # 路由到：fast_chain（gpt-4o-mini）

    # 测试2：复杂查询
    result2 = router.invoke({"question": "请分析LCEL的设计理念和优势"})
    print(f"复杂查询: {result2}")
    # 路由到：powerful_chain（gpt-4o）

    # 测试3：标准查询
    result3 = router.invoke({"question": "LCEL支持哪些功能"})
    print(f"标准查询: {result3}")
    # 路由到：standard_chain（gpt-4o-mini）
```

---

## 六、数据透传（RunnablePassthrough）

### 6.1 完整RAG链

```python
# main.py
from langchain_core.runnables import RunnablePassthrough
from chains.retrieval import create_retrieval_chain
from chains.generation import create_generation_chain
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

def create_rag_chain(vectorstore: Chroma):
    """创建完整RAG链

    流程：
    1. 输入问题
    2. 并行执行：
       - 检索上下文（retrieval_chain）
       - 保留原始问题（RunnablePassthrough）
    3. 生成答案（generation_chain）
    """
    # 组件1：检索链
    retrieval_chain = create_retrieval_chain(vectorstore, k=3)

    # 组件2：生成链
    generation_chain = create_generation_chain()

    # 完整RAG链
    rag_chain = (
        {
            "context": retrieval_chain,           # 检索上下文
            "question": RunnablePassthrough()     # 保留原始问题
        }
        | generation_chain                        # 生成答案
    )

    return rag_chain


def main():
    """主程序"""
    # 1. 准备数据
    documents = [
        "LCEL是LangChain表达式语言，用于声明式地组合链。",
        "RunnableSequence通过管道操作符|实现顺序执行。",
        "RunnableParallel实现并发执行，性能提升40-50%。",
        "RunnableBranch根据条件选择执行路径。",
        "RunnablePassthrough保留原始输入数据。",
        "LCEL支持流式输出、批处理、异步执行等功能。",
    ]

    # 2. 创建向量存储
    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)
    texts = text_splitter.create_documents(documents)

    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(texts, embeddings)

    # 3. 创建RAG链
    rag_chain = create_rag_chain(vectorstore)

    # 4. 测试查询
    questions = [
        "什么是LCEL？",
        "LCEL有哪些核心组件？",
        "RunnableParallel的性能提升是多少？",
    ]

    print("=" * 60)
    print("智能文档问答系统")
    print("=" * 60)

    for question in questions:
        print(f"\n问题: {question}")
        answer = rag_chain.invoke(question)
        print(f"答案: {answer}")
        print("-" * 60)


if __name__ == "__main__":
    main()
```

---

## 七、高级组合：嵌套链

### 7.1 多层嵌套

```python
# 高级示例：嵌套组合
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

def create_advanced_rag_chain(vectorstore: Chroma):
    """创建高级RAG链（嵌套组合）

    结构：
    - 并行执行：
      - 向量检索
      - 关键词检索
    - 合并结果
    - 智能路由（根据查询复杂度）
    - 生成答案
    """
    # 检索层：并行检索
    vector_retrieval = create_retrieval_chain(vectorstore, k=3)
    keyword_retrieval = create_retrieval_chain(vectorstore, k=2)

    parallel_retrieval = RunnableParallel({
        "vector_context": vector_retrieval,
        "keyword_context": keyword_retrieval,
        "question": RunnablePassthrough()
    })

    # 合并层：合并检索结果
    def merge_contexts(input_dict: dict) -> dict:
        """合并多个检索结果"""
        vector_ctx = input_dict["vector_context"]
        keyword_ctx = input_dict["keyword_context"]
        question = input_dict["question"]

        merged_context = f"向量检索结果:\n{vector_ctx}\n\n关键词检索结果:\n{keyword_ctx}"

        return {
            "context": merged_context,
            "question": question
        }

    merge_chain = RunnableLambda(merge_contexts)

    # 路由层：智能路由
    router = create_smart_router()

    # 完整链：检索 → 合并 → 路由 → 生成
    advanced_chain = (
        parallel_retrieval
        | merge_chain
        | router
    )

    return advanced_chain


# 使用示例
if __name__ == "__main__":
    vectorstore = ...  # 初始化向量存储

    advanced_chain = create_advanced_rag_chain(vectorstore)

    # 测试
    result = advanced_chain.invoke("请详细分析LCEL的设计理念")
    print(result)
```

---

## 八、完整项目代码

### 8.1 项目入口

```python
# main.py（完整版）
import os
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from chains.retrieval import create_retrieval_chain
from chains.generation import create_generation_chain
from chains.analysis import create_analysis_chain
from chains.router import create_smart_router
from langchain_core.runnables import RunnablePassthrough

# 加载环境变量
load_dotenv()

class RAGSystem:
    """RAG问答系统"""

    def __init__(self, documents: list[str]):
        """初始化系统"""
        self.vectorstore = self._create_vectorstore(documents)
        self.rag_chain = self._create_rag_chain()
        self.analysis_chain = create_analysis_chain()

    def _create_vectorstore(self, documents: list[str]) -> Chroma:
        """创建向量存储"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        texts = text_splitter.create_documents(documents)

        embeddings = OpenAIEmbeddings()
        vectorstore = Chroma.from_documents(texts, embeddings)

        return vectorstore

    def _create_rag_chain(self):
        """创建RAG链"""
        retrieval = create_retrieval_chain(self.vectorstore, k=3)
        generation = create_generation_chain()

        chain = (
            {
                "context": retrieval,
                "question": RunnablePassthrough()
            }
            | generation
        )

        return chain

    def ask(self, question: str) -> str:
        """问答"""
        return self.rag_chain.invoke(question)

    def analyze(self, text: str) -> dict:
        """分析文本"""
        return self.analysis_chain.invoke({"text": text})


def main():
    """主程序"""
    # 示例文档
    documents = [
        "LCEL是LangChain表达式语言，提供声明式的链组合方式。",
        "RunnableSequence实现顺序执行，使用管道操作符|连接组件。",
        "RunnableParallel实现并发执行，可提升性能40-50%。",
        "RunnableBranch根据条件选择执行路径，实现智能路由。",
        "RunnablePassthrough保留原始输入，常用于并行分支中。",
        "LCEL支持流式输出、批处理、异步执行、重试、降级等功能。",
    ]

    # 创建系统
    system = RAGSystem(documents)

    # 交互式问答
    print("=" * 60)
    print("智能文档问答系统（输入'quit'退出）")
    print("=" * 60)

    while True:
        question = input("\n问题: ").strip()

        if question.lower() in ["quit", "exit", "q"]:
            print("再见！")
            break

        if not question:
            continue

        try:
            answer = system.ask(question)
            print(f"答案: {answer}")
        except Exception as e:
            print(f"错误: {e}")


if __name__ == "__main__":
    main()
```

---

## 九、运行与测试

### 9.1 运行程序

```bash
# 运行主程序
python main.py

# 输出示例：
# ============================================================
# 智能文档问答系统（输入'quit'退出）
# ============================================================
#
# 问题: 什么是LCEL？
# 答案: LCEL是LangChain表达式语言，提供声明式的链组合方式。
#
# 问题: RunnableParallel的性能提升是多少？
# 答案: RunnableParallel可提升性能40-50%。
```

### 9.2 测试脚本

```python
# test_chains.py
import pytest
from chains.retrieval import create_retrieval_chain
from chains.generation import create_generation_chain
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

def test_retrieval_chain():
    """测试检索链"""
    # 准备数据
    documents = ["测试文档1", "测试文档2"]
    vectorstore = Chroma.from_texts(documents, OpenAIEmbeddings())

    # 创建链
    chain = create_retrieval_chain(vectorstore, k=1)

    # 测试
    result = chain.invoke("测试")
    assert "文档" in result

def test_generation_chain():
    """测试生成链"""
    chain = create_generation_chain()

    result = chain.invoke({
        "context": "LCEL是表达式语言",
        "question": "什么是LCEL？"
    })

    assert "LCEL" in result
    assert len(result) > 0

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

---

## 十、总结

### 核心要点

1. **RunnableSequence**：顺序组合，清晰的数据流
2. **RunnableParallel**：并发执行，性能提升40-50%
3. **RunnableBranch**：条件路由，智能决策
4. **RunnablePassthrough**：数据透传，保留原始信息

### 实战收获

- ✅ 完整的RAG问答系统
- ✅ 多维度并行分析
- ✅ 智能路由机制
- ✅ 嵌套链组合
- ✅ 可测试的模块化设计

### 性能优化

| 优化点 | 提升 |
|--------|------|
| 并行检索 | 2x |
| 并行分析 | 3x |
| 智能路由 | 成本降低30% |

---

**下一步**：学习【错误处理与重试实战】，构建弹性系统。
