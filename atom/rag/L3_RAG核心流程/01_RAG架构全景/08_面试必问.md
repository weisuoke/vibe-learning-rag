# 面试必问

> RAG 相关的高频面试问题与出彩回答

---

## 问题1："什么是 RAG？为什么需要它？"

### 普通回答（❌ 不出彩）

> "RAG 就是检索增强生成，先检索相关文档，再让 LLM 生成答案。"

**问题**：太简单，没有展示深度理解。

### 出彩回答（✅ 推荐）

> **RAG 是解决 LLM 知识局限性的核心方案，它有三层含义：**
>
> **1. 字面含义**：Retrieval-Augmented Generation，检索增强生成。通过检索外部知识库，为 LLM 提供上下文，再生成答案。
>
> **2. 本质含义**：让 LLM 从"闭卷考试"变成"开卷考试"。LLM 的知识是静态的（训练时固定），而 RAG 让它能动态获取最新、最相关的知识。
>
> **3. 架构含义**：RAG 是一个双阶段系统——离线索引阶段将文档转为向量存储，在线查询阶段检索相关内容并生成答案。
>
> **为什么需要 RAG？** 因为 LLM 有三个致命问题：
> - 知识截止：训练数据有时间限制
> - 知识缺失：没有私有/专业领域数据
> - 幻觉问题：不知道的事情会编造
>
> RAG 通过"检索真实知识"解决这些问题，而且相比 Fine-tuning，成本更低、更新更快、答案可追溯。

### 为什么这个回答出彩？

1. ✅ 分层次回答（字面、本质、架构）
2. ✅ 用类比解释（开卷考试）
3. ✅ 说明了为什么需要（解决什么问题）
4. ✅ 对比了替代方案（vs Fine-tuning）

---

## 问题2："RAG 和 Fine-tuning 有什么区别？什么时候用哪个？"

### 普通回答（❌ 不出彩）

> "RAG 是检索知识，Fine-tuning 是训练模型。RAG 更便宜，Fine-tuning 效果更好。"

**问题**：过于笼统，没有具体场景分析。

### 出彩回答（✅ 推荐）

> **RAG 和 Fine-tuning 解决的是不同类型的问题：**
>
> | 维度 | RAG | Fine-tuning |
> |-----|-----|-------------|
> | **解决的问题** | 知识获取 | 行为调整 |
> | **知识更新** | 实时（改文档即可） | 需要重新训练 |
> | **成本** | 低（向量库 + API） | 高（GPU 训练） |
> | **可解释性** | 高（可追溯来源） | 低（黑盒） |
> | **适用场景** | 知识密集型问答 | 风格/格式调整 |
>
> **选择建议：**
>
> **用 RAG 的场景：**
> - 企业知识库问答（FAQ、文档查询）
> - 需要引用来源的场景（法律、医疗）
> - 知识频繁更新的场景（产品手册、政策）
>
> **用 Fine-tuning 的场景：**
> - 特定风格/语气（品牌调性）
> - 特定格式输出（JSON、代码）
> - 领域专业术语（医学、法律术语）
>
> **实际项目中，两者经常结合使用**：Fine-tuning 调整模型的输出风格，RAG 提供最新知识。

### 为什么这个回答出彩？

1. ✅ 用表格清晰对比
2. ✅ 给出具体场景建议
3. ✅ 提到两者可以结合使用
4. ✅ 展示了实际项目经验

---

## 问题3："RAG 系统的核心组件有哪些？"

### 普通回答（❌ 不出彩）

> "有文档加载、分块、向量化、检索、生成这些组件。"

### 出彩回答（✅ 推荐）

> **RAG 系统有 6 个核心组件，分布在两个阶段：**
>
> **离线索引阶段（4个组件）：**
> 1. **Loader**：加载各种格式的文档（PDF、Word、HTML）
> 2. **Chunker**：将长文本切分成适合检索的小块
> 3. **Embedder**：将文本转换为向量表示
> 4. **VectorStore**：存储向量并支持相似度检索
>
> **在线查询阶段（2个组件）：**
> 5. **Retriever**：根据用户问题检索最相关的文档块
> 6. **Generator**：基于检索结果生成最终答案
>
> **数据流：**
> ```
> 文档 → Loader → Chunker → Embedder → VectorStore
>                                          ↓
> 问题 → Embedder → Retriever ←────────────┘
>                      ↓
>                  Generator → 答案
> ```
>
> **每个组件的关键决策：**
> - Chunker：块大小（通常 500-1000 字符）
> - Embedder：模型选择（OpenAI vs 开源）
> - Retriever：Top-K 数量（通常 3-5 个）

### 为什么这个回答出彩？

1. ✅ 按阶段组织，结构清晰
2. ✅ 画出数据流图
3. ✅ 提到关键决策点
4. ✅ 给出具体参数建议

---

## 问题4："RAG 系统效果不好，你会怎么排查？"

### 普通回答（❌ 不出彩）

> "检查检索结果对不对，Prompt 写得好不好。"

### 出彩回答（✅ 推荐）

> **RAG 效果不好，我会按数据流逐层排查：**
>
> **Step 1：先定位问题在检索还是生成**
> ```python
> # 打印检索结果，人工判断
> results = retriever.search(question)
> print(results)  # 检索结果相关吗？
> ```
> - 如果检索结果不相关 → 检索问题
> - 如果检索结果相关但答案错误 → 生成问题
>
> **Step 2：如果是检索问题，逐层排查**
>
> | 层级 | 检查点 | 常见问题 |
> |-----|-------|---------|
> | 文档解析 | 原始文本是否完整 | PDF 解析乱码、表格丢失 |
> | 分块策略 | 块大小是否合适 | 块太大检索不精准，块太小丢失上下文 |
> | Embedding | 向量质量 | 模型不适合当前领域 |
> | 检索策略 | Top-K、阈值 | K 太小漏掉相关内容，K 太大引入噪声 |
>
> **Step 3：如果是生成问题**
> - 检查 Prompt 设计（指令是否清晰）
> - 检查上下文组织（重要信息是否在开头/结尾）
> - 检查 LLM 参数（temperature 是否太高）
>
> **实际经验：80% 的问题出在检索环节，特别是分块策略。**

### 为什么这个回答出彩？

1. ✅ 有系统的排查方法论
2. ✅ 先定位大方向，再细化
3. ✅ 给出具体检查点和常见问题
4. ✅ 分享实际经验（80% 在检索）

---

## 面试技巧总结

| 技巧 | 说明 |
|-----|------|
| **分层回答** | 从多个角度/层次回答，展示全面理解 |
| **用类比** | 用简单类比解释复杂概念 |
| **画图/表格** | 结构化展示，清晰易懂 |
| **给具体数字** | 如 "Top-K 通常 3-5 个"，展示实战经验 |
| **提到权衡** | 说明不同选择的 trade-off |
| **分享经验** | "实际项目中..." 展示真实经验 |

---

**下一步：** [09_化骨绵掌](./09_化骨绵掌.md) - 10个2分钟知识卡片
