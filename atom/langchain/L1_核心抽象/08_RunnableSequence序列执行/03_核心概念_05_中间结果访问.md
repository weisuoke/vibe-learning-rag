# æ ¸å¿ƒæ¦‚å¿µ5ï¼šä¸­é—´ç»“æœè®¿é—®

> ç†è§£å¦‚ä½•æŸ¥çœ‹å’Œè°ƒè¯• RunnableSequence çš„ä¸­é—´æ­¥éª¤

---

## ä¸ºä»€ä¹ˆéœ€è¦è®¿é—®ä¸­é—´ç»“æœ

**é—®é¢˜**ï¼šRunnableSequence é»˜è®¤åªè¿”å›æœ€åä¸€æ­¥çš„è¾“å‡ºï¼Œä¸­é—´æ­¥éª¤çš„ç»“æœè¢«éšè—ã€‚

```python
chain = step1 | step2 | step3
result = chain.invoke(input)
# åªèƒ½çœ‹åˆ° step3 çš„è¾“å‡º
# step1 å’Œ step2 çš„è¾“å‡ºè¢«éšè—
```

**éœ€æ±‚**ï¼š
- è°ƒè¯•ï¼šæ‰¾å‡ºå“ªä¸€æ­¥å‡ºé”™
- æ€§èƒ½åˆ†æï¼šæŸ¥çœ‹æ¯ä¸€æ­¥çš„è€—æ—¶
- æ•°æ®éªŒè¯ï¼šæ£€æŸ¥ä¸­é—´æ•°æ®æ˜¯å¦æ­£ç¡®
- å¯è§‚æµ‹æ€§ï¼šç›‘æ§ç”Ÿäº§ç¯å¢ƒçš„æ‰§è¡Œè¿‡ç¨‹

---

## æ–¹æ³•1ï¼šastream_events - æœ€å¼ºå¤§çš„è°ƒè¯•å·¥å…·

### åŸºç¡€ç”¨æ³•

**æ¥æº**ï¼šLangChain å®˜æ–¹æ–‡æ¡£

```python
import asyncio
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

async def debug_chain():
    prompt = ChatPromptTemplate.from_template("è§£é‡Šï¼š{topic}")
    llm = ChatOpenAI(model="gpt-4o-mini")
    parser = StrOutputParser()

    chain = prompt | llm | parser

    # ä½¿ç”¨ astream_events æŸ¥çœ‹æ‰€æœ‰äº‹ä»¶
    async for event in chain.astream_events(
        {"topic": "é‡å­è®¡ç®—"},
        version="v2"  # ä½¿ç”¨ v2 ç‰ˆæœ¬
    ):
        kind = event["event"]
        name = event.get("name", "")

        if kind == "on_chain_start":
            print(f"â–¶ å¼€å§‹: {name}")
            print(f"  è¾“å…¥: {event.get('data', {}).get('input')}")

        elif kind == "on_chain_end":
            print(f"â—€ ç»“æŸ: {name}")
            output = event.get('data', {}).get('output')
            if isinstance(output, str):
                print(f"  è¾“å‡º: {output[:100]}...")
            else:
                print(f"  è¾“å‡ºç±»å‹: {type(output).__name__}")
            print()

asyncio.run(debug_chain())
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```
â–¶ å¼€å§‹: RunnableSequence
  è¾“å…¥: {'topic': 'é‡å­è®¡ç®—'}
â–¶ å¼€å§‹: ChatPromptTemplate
  è¾“å…¥: {'topic': 'é‡å­è®¡ç®—'}
â—€ ç»“æŸ: ChatPromptTemplate
  è¾“å‡ºç±»å‹: ChatPromptValue

â–¶ å¼€å§‹: ChatOpenAI
â—€ ç»“æŸ: ChatOpenAI
  è¾“å‡ºç±»å‹: AIMessage

â–¶ å¼€å§‹: StrOutputParser
â—€ ç»“æŸ: StrOutputParser
  è¾“å‡º: é‡å­è®¡ç®—æ˜¯åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œä¿¡æ¯å¤„ç†çš„è®¡ç®—æ–¹å¼...

â—€ ç»“æŸ: RunnableSequence
  è¾“å‡º: é‡å­è®¡ç®—æ˜¯åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œä¿¡æ¯å¤„ç†çš„è®¡ç®—æ–¹å¼...
```

### äº‹ä»¶ç±»å‹

| äº‹ä»¶ç±»å‹ | è¯´æ˜ | ä½•æ—¶è§¦å‘ |
|---------|------|----------|
| `on_chain_start` | é“¾å¼€å§‹æ‰§è¡Œ | æ¯ä¸ªç»„ä»¶å¼€å§‹æ—¶ |
| `on_chain_end` | é“¾æ‰§è¡Œç»“æŸ | æ¯ä¸ªç»„ä»¶ç»“æŸæ—¶ |
| `on_chain_error` | é“¾æ‰§è¡Œå¤±è´¥ | ç»„ä»¶æŠ›å‡ºå¼‚å¸¸æ—¶ |
| `on_llm_start` | LLM å¼€å§‹è°ƒç”¨ | LLM ç»„ä»¶å¼€å§‹æ—¶ |
| `on_llm_end` | LLM è°ƒç”¨ç»“æŸ | LLM ç»„ä»¶ç»“æŸæ—¶ |
| `on_llm_new_token` | LLM ç”Ÿæˆæ–° token | æµå¼è¾“å‡ºæ—¶ |
| `on_tool_start` | å·¥å…·å¼€å§‹è°ƒç”¨ | å·¥å…·ç»„ä»¶å¼€å§‹æ—¶ |
| `on_tool_end` | å·¥å…·è°ƒç”¨ç»“æŸ | å·¥å…·ç»„ä»¶ç»“æŸæ—¶ |

### è¿‡æ»¤ç‰¹å®šäº‹ä»¶

```python
async def filter_events():
    chain = prompt | llm | parser

    async for event in chain.astream_events(
        {"topic": "AI"},
        version="v2"
    ):
        kind = event["event"]

        # åªå…³æ³¨ LLM ç›¸å…³äº‹ä»¶
        if kind in ["on_llm_start", "on_llm_end", "on_llm_new_token"]:
            print(f"{kind}: {event.get('name')}")
            if kind == "on_llm_new_token":
                print(f"  Token: {event['data']['chunk'].content}")

asyncio.run(filter_events())
```

---

## æ–¹æ³•2ï¼šè‡ªå®šä¹‰å›è°ƒ - çµæ´»çš„ç›‘æ§

### åŸºç¡€å›è°ƒ

```python
from langchain.callbacks.base import BaseCallbackHandler
from typing import Any, Dict

class DebugCallback(BaseCallbackHandler):
    """è°ƒè¯•å›è°ƒï¼Œæ‰“å°æ‰€æœ‰ä¸­é—´ç»“æœ"""

    def on_chain_start(
        self,
        serialized: Dict[str, Any],
        inputs: Dict[str, Any],
        **kwargs
    ) -> None:
        """é“¾å¼€å§‹æ—¶è°ƒç”¨"""
        class_name = serialized.get("name", serialized.get("id", ["unknown"])[-1])
        print(f"\nâ–¶ å¼€å§‹: {class_name}")
        print(f"  è¾“å…¥: {inputs}")

    def on_chain_end(
        self,
        outputs: Dict[str, Any],
        **kwargs
    ) -> None:
        """é“¾ç»“æŸæ—¶è°ƒç”¨"""
        print(f"â—€ ç»“æŸ")
        print(f"  è¾“å‡º: {outputs}")

    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: list[str],
        **kwargs
    ) -> None:
        """LLM å¼€å§‹æ—¶è°ƒç”¨"""
        print(f"\nğŸ¤– LLM å¼€å§‹")
        print(f"  Prompts: {prompts[:100]}...")

    def on_llm_end(
        self,
        response,
        **kwargs
    ) -> None:
        """LLM ç»“æŸæ—¶è°ƒç”¨"""
        print(f"ğŸ¤– LLM ç»“æŸ")
        print(f"  å“åº”: {str(response)[:100]}...")


# ä½¿ç”¨
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("è§£é‡Šï¼š{topic}")
llm = ChatOpenAI(model="gpt-4o-mini")

chain = prompt | llm

result = chain.invoke(
    {"topic": "AI"},
    config={"callbacks": [DebugCallback()]}
)
```

### æ€§èƒ½åˆ†æå›è°ƒ

```python
import time
from typing import Dict, Any

class PerformanceCallback(BaseCallbackHandler):
    """æ€§èƒ½åˆ†æå›è°ƒï¼Œè®°å½•æ¯ä¸€æ­¥çš„è€—æ—¶"""

    def __init__(self):
        self.start_times: Dict[str, float] = {}
        self.durations: Dict[str, float] = {}

    def on_chain_start(
        self,
        serialized: Dict[str, Any],
        inputs: Dict[str, Any],
        **kwargs
    ) -> None:
        """è®°å½•å¼€å§‹æ—¶é—´"""
        run_id = kwargs.get("run_id")
        self.start_times[str(run_id)] = time.time()

    def on_chain_end(
        self,
        outputs: Dict[str, Any],
        **kwargs
    ) -> None:
        """è®¡ç®—è€—æ—¶"""
        run_id = kwargs.get("run_id")
        start_time = self.start_times.get(str(run_id))
        if start_time:
            duration = time.time() - start_time
            class_name = kwargs.get("name", "unknown")
            self.durations[class_name] = duration
            print(f"â±ï¸  {class_name}: {duration:.2f}s")

    def print_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        print("\n=== æ€§èƒ½æ‘˜è¦ ===")
        total = sum(self.durations.values())
        for name, duration in sorted(
            self.durations.items(),
            key=lambda x: x[1],
            reverse=True
        ):
            percentage = (duration / total) * 100
            print(f"{name}: {duration:.2f}s ({percentage:.1f}%)")
        print(f"æ€»è®¡: {total:.2f}s")


# ä½¿ç”¨
perf_callback = PerformanceCallback()

result = chain.invoke(
    {"topic": "AI"},
    config={"callbacks": [perf_callback]}
)

perf_callback.print_summary()
```

---

## æ–¹æ³•3ï¼šæ‰‹å†™ä¸­é—´ç»“æœè¿½è¸ªå™¨

### ç®€å•è¿½è¸ªå™¨

```python
from typing import Any, List, Dict
from datetime import datetime

class IntermediateTracker:
    """ä¸­é—´ç»“æœè¿½è¸ªå™¨"""

    def __init__(self):
        self.results: List[Dict] = []

    def track(self, step_name: str, input_data: Any, output_data: Any):
        """è®°å½•ä¸€æ¬¡æ‰§è¡Œ"""
        self.results.append({
            "timestamp": datetime.now().isoformat(),
            "step": step_name,
            "input": input_data,
            "output": output_data,
            "input_type": type(input_data).__name__,
            "output_type": type(output_data).__name__
        })

    def print_trace(self):
        """æ‰“å°è¿½è¸ªä¿¡æ¯"""
        print("=== ä¸­é—´ç»“æœè¿½è¸ª ===\n")
        for i, result in enumerate(self.results, 1):
            print(f"æ­¥éª¤{i}: {result['step']}")
            print(f"  æ—¶é—´: {result['timestamp']}")
            print(f"  è¾“å…¥ç±»å‹: {result['input_type']}")
            print(f"  è¾“å…¥: {str(result['input'])[:100]}")
            print(f"  è¾“å‡ºç±»å‹: {result['output_type']}")
            print(f"  è¾“å‡º: {str(result['output'])[:100]}")
            print()

    def get_step_output(self, step_name: str) -> Any:
        """è·å–ç‰¹å®šæ­¥éª¤çš„è¾“å‡º"""
        for result in self.results:
            if result['step'] == step_name:
                return result['output']
        return None


# åŒ…è£… Runnable ä»¥æ”¯æŒè¿½è¸ª
from langchain_core.runnables import RunnableLambda

def make_trackable(runnable, tracker: IntermediateTracker, step_name: str):
    """å°† Runnable åŒ…è£…ä¸ºå¯è¿½è¸ªçš„"""
    def tracked_invoke(input_data):
        output_data = runnable.invoke(input_data)
        tracker.track(step_name, input_data, output_data)
        return output_data

    return RunnableLambda(tracked_invoke)


# ä½¿ç”¨
tracker = IntermediateTracker()

prompt_trackable = make_trackable(prompt, tracker, "PromptTemplate")
llm_trackable = make_trackable(llm, tracker, "ChatOpenAI")
parser_trackable = make_trackable(parser, tracker, "StrOutputParser")

chain = prompt_trackable | llm_trackable | parser_trackable

result = chain.invoke({"topic": "AI"})

tracker.print_trace()

# è·å–ç‰¹å®šæ­¥éª¤çš„è¾“å‡º
llm_output = tracker.get_step_output("ChatOpenAI")
print(f"LLM è¾“å‡º: {llm_output}")
```

### é«˜çº§è¿½è¸ªå™¨ï¼šæ”¯æŒå¯è§†åŒ–

```python
import json
from typing import Any, List, Dict

class VisualTracker:
    """å¯è§†åŒ–è¿½è¸ªå™¨"""

    def __init__(self):
        self.results: List[Dict] = []

    def track(self, step_name: str, input_data: Any, output_data: Any):
        """è®°å½•ä¸€æ¬¡æ‰§è¡Œ"""
        self.results.append({
            "step": step_name,
            "input": self._serialize(input_data),
            "output": self._serialize(output_data)
        })

    def _serialize(self, data: Any) -> str:
        """åºåˆ—åŒ–æ•°æ®"""
        if isinstance(data, str):
            return data[:200]
        elif isinstance(data, dict):
            return json.dumps(data, ensure_ascii=False)[:200]
        else:
            return str(data)[:200]

    def print_flow_diagram(self):
        """æ‰“å°æ•°æ®æµå›¾"""
        print("=== æ•°æ®æµå›¾ ===\n")
        print("è¾“å…¥")
        for i, result in enumerate(self.results):
            print(f"  â†“")
            print(f"[{result['step']}]")
            print(f"  è¾“å…¥: {result['input']}")
            print(f"  è¾“å‡º: {result['output']}")
        print(f"  â†“")
        print("æœ€ç»ˆè¾“å‡º")

    def export_json(self, filename: str):
        """å¯¼å‡ºä¸º JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"è¿½è¸ªç»“æœå·²å¯¼å‡ºåˆ°: {filename}")


# ä½¿ç”¨
tracker = VisualTracker()

# ... åŒ…è£… Runnable ...

result = chain.invoke({"topic": "AI"})

tracker.print_flow_diagram()
tracker.export_json("trace.json")
```

---

## æ–¹æ³•4ï¼šLangSmith - ç”Ÿäº§çº§å¯è§‚æµ‹æ€§

### é›†æˆ LangSmith

**æ¥æº**ï¼šLangSmith å®˜æ–¹æ–‡æ¡£

```python
import os

# é…ç½® LangSmith
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "your_api_key"
os.environ["LANGSMITH_PROJECT"] = "my-project"

# æ­£å¸¸ä½¿ç”¨é“¾
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("è§£é‡Šï¼š{topic}")
llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | llm | parser

# è‡ªåŠ¨è¿½è¸ªåˆ° LangSmith
result = chain.invoke({"topic": "AI"})

# åœ¨ LangSmith æ§åˆ¶å°æŸ¥çœ‹ï¼š
# - æ¯ä¸€æ­¥çš„è¾“å…¥è¾“å‡º
# - æ¯ä¸€æ­¥çš„è€—æ—¶
# - é”™è¯¯å †æ ˆ
# - Token ä½¿ç”¨é‡
# - æˆæœ¬
```

### æ·»åŠ æ ‡è®°å’Œå…ƒæ•°æ®

```python
# æ·»åŠ è¿è¡Œåç§°å’Œæ ‡è®°
result = chain.invoke(
    {"topic": "AI"},
    config={
        "run_name": "explain_ai",
        "tags": ["production", "explain"],
        "metadata": {
            "user_id": "user123",
            "session_id": "session456"
        }
    }
)

# åœ¨ LangSmith ä¸­å¯ä»¥æŒ‰æ ‡è®°å’Œå…ƒæ•°æ®è¿‡æ»¤
```

### LangSmith çš„ä¼˜åŠ¿

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **è‡ªåŠ¨è¿½è¸ª** | æ— éœ€ä¿®æ”¹ä»£ç ï¼Œè‡ªåŠ¨è®°å½•æ‰€æœ‰æ‰§è¡Œ |
| **å¯è§†åŒ–** | å›¾å½¢åŒ–å±•ç¤ºæ•°æ®æµå’Œæ‰§è¡Œè¿‡ç¨‹ |
| **æ€§èƒ½åˆ†æ** | è‡ªåŠ¨è®¡ç®—æ¯ä¸€æ­¥çš„è€—æ—¶ |
| **æˆæœ¬è¿½è¸ª** | è‡ªåŠ¨è®¡ç®— Token ä½¿ç”¨å’Œæˆæœ¬ |
| **é”™è¯¯åˆ†æ** | è¯¦ç»†çš„é”™è¯¯å †æ ˆå’Œä¸Šä¸‹æ–‡ |
| **å¯¹æ¯”åˆ†æ** | å¯¹æ¯”ä¸åŒè¿è¡Œçš„ç»“æœ |
| **å›¢é˜Ÿåä½œ** | å›¢é˜Ÿæˆå‘˜å¯ä»¥æŸ¥çœ‹å’Œåˆ†æ |

---

## å®æˆ˜åœºæ™¯

### åœºæ™¯1ï¼šè°ƒè¯• RAG é“¾

```python
import asyncio
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

async def debug_rag_chain():
    """è°ƒè¯• RAG é“¾ï¼ŒæŸ¥çœ‹æ¯ä¸€æ­¥çš„è¾“å‡º"""

    # æ„å»º RAG é“¾
    vectorstore = Chroma(embedding_function=OpenAIEmbeddings())
    retriever = vectorstore.as_retriever()

    prompt = ChatPromptTemplate.from_template("""
    ä¸Šä¸‹æ–‡ï¼š{context}
    é—®é¢˜ï¼š{question}
    ç­”æ¡ˆï¼š
    """)

    llm = ChatOpenAI(model="gpt-4o-mini")
    parser = StrOutputParser()

    rag_chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | parser
    )

    # ä½¿ç”¨ astream_events è°ƒè¯•
    print("=== RAG é“¾è°ƒè¯• ===\n")

    async for event in rag_chain.astream_events(
        "ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ",
        version="v2"
    ):
        kind = event["event"]
        name = event.get("name", "")

        if kind == "on_chain_end":
            output = event.get('data', {}).get('output')

            # æ£€ç´¢å™¨è¾“å‡º
            if "retriever" in name.lower():
                print(f"ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£:")
                if isinstance(output, list):
                    for i, doc in enumerate(output[:3], 1):
                        print(f"  {i}. {doc.page_content[:100]}...")
                print()

            # Prompt è¾“å‡º
            elif "prompt" in name.lower():
                print(f"ğŸ“ ç”Ÿæˆçš„ Prompt:")
                print(f"  {str(output)[:200]}...")
                print()

            # LLM è¾“å‡º
            elif "llm" in name.lower() or "chat" in name.lower():
                print(f"ğŸ¤– LLM å“åº”:")
                if hasattr(output, 'content'):
                    print(f"  {output.content[:200]}...")
                print()

            # Parser è¾“å‡º
            elif "parser" in name.lower():
                print(f"âœ… æœ€ç»ˆç­”æ¡ˆ:")
                print(f"  {output[:200]}...")
                print()

asyncio.run(debug_rag_chain())
```

### åœºæ™¯2ï¼šæ€§èƒ½ç“¶é¢ˆåˆ†æ

```python
import time
from typing import Dict

class BottleneckAnalyzer(BaseCallbackHandler):
    """æ€§èƒ½ç“¶é¢ˆåˆ†æå™¨"""

    def __init__(self):
        self.start_times: Dict[str, float] = {}
        self.durations: Dict[str, float] = {}

    def on_chain_start(self, serialized, inputs, **kwargs):
        run_id = str(kwargs.get("run_id"))
        self.start_times[run_id] = time.time()

    def on_chain_end(self, outputs, **kwargs):
        run_id = str(kwargs.get("run_id"))
        if run_id in self.start_times:
            duration = time.time() - self.start_times[run_id]
            name = kwargs.get("name", "unknown")
            self.durations[name] = duration

    def analyze(self):
        """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
        print("\n=== æ€§èƒ½ç“¶é¢ˆåˆ†æ ===\n")

        total = sum(self.durations.values())
        sorted_durations = sorted(
            self.durations.items(),
            key=lambda x: x[1],
            reverse=True
        )

        print("è€—æ—¶æ’å:")
        for i, (name, duration) in enumerate(sorted_durations, 1):
            percentage = (duration / total) * 100
            bar = "â–ˆ" * int(percentage / 2)
            print(f"{i}. {name}")
            print(f"   {duration:.2f}s ({percentage:.1f}%) {bar}")
            print()

        # è¯†åˆ«ç“¶é¢ˆ
        if sorted_durations:
            bottleneck_name, bottleneck_time = sorted_durations[0]
            bottleneck_percentage = (bottleneck_time / total) * 100

            if bottleneck_percentage > 50:
                print(f"âš ï¸  ç“¶é¢ˆ: {bottleneck_name} å ç”¨äº† {bottleneck_percentage:.1f}% çš„æ—¶é—´")
                print(f"   å»ºè®®: ä¼˜åŒ– {bottleneck_name} çš„æ€§èƒ½")


# ä½¿ç”¨
analyzer = BottleneckAnalyzer()

result = chain.invoke(
    {"topic": "AI"},
    config={"callbacks": [analyzer]}
)

analyzer.analyze()
```

### åœºæ™¯3ï¼šæ•°æ®éªŒè¯

```python
class DataValidator(BaseCallbackHandler):
    """æ•°æ®éªŒè¯å™¨ï¼Œæ£€æŸ¥ä¸­é—´æ•°æ®æ˜¯å¦ç¬¦åˆé¢„æœŸ"""

    def __init__(self):
        self.issues: List[str] = []

    def on_chain_end(self, outputs, **kwargs):
        """éªŒè¯è¾“å‡ºæ•°æ®"""
        name = kwargs.get("name", "")

        # éªŒè¯ Prompt è¾“å‡º
        if "prompt" in name.lower():
            if not outputs:
                self.issues.append(f"{name}: è¾“å‡ºä¸ºç©º")

        # éªŒè¯ LLM è¾“å‡º
        elif "llm" in name.lower() or "chat" in name.lower():
            if hasattr(outputs, 'content'):
                content = outputs.content
                if len(content) < 10:
                    self.issues.append(f"{name}: è¾“å‡ºè¿‡çŸ­ ({len(content)} å­—ç¬¦)")
                if "error" in content.lower():
                    self.issues.append(f"{name}: è¾“å‡ºåŒ…å«é”™è¯¯ä¿¡æ¯")

        # éªŒè¯ Parser è¾“å‡º
        elif "parser" in name.lower():
            if not isinstance(outputs, str):
                self.issues.append(f"{name}: è¾“å‡ºç±»å‹é”™è¯¯ (æœŸæœ› strï¼Œå®é™… {type(outputs).__name__})")

    def report(self):
        """æŠ¥å‘ŠéªŒè¯ç»“æœ"""
        print("\n=== æ•°æ®éªŒè¯æŠ¥å‘Š ===\n")
        if self.issues:
            print(f"å‘ç° {len(self.issues)} ä¸ªé—®é¢˜:")
            for i, issue in enumerate(self.issues, 1):
                print(f"{i}. {issue}")
        else:
            print("âœ… æ‰€æœ‰æ•°æ®éªŒè¯é€šè¿‡")


# ä½¿ç”¨
validator = DataValidator()

result = chain.invoke(
    {"topic": "AI"},
    config={"callbacks": [validator]}
)

validator.report()
```

---

## 2025-2026 æœ€ä½³å®è·µ

### 1. å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨ astream_events

**æ¥æº**ï¼šLangChain å®˜æ–¹æ–‡æ¡£

```python
# å¼€å‘ç¯å¢ƒï¼šè¯¦ç»†è°ƒè¯•
async def dev_debug():
    async for event in chain.astream_events(input, version="v2"):
        print(event)

asyncio.run(dev_debug())
```

### 2. ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨ LangSmith

```python
# ç”Ÿäº§ç¯å¢ƒï¼šè‡ªåŠ¨è¿½è¸ª
os.environ["LANGSMITH_TRACING"] = "true"
result = chain.invoke(input)
# åœ¨ LangSmith æ§åˆ¶å°æŸ¥çœ‹
```

### 3. æ€§èƒ½åˆ†æï¼šä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ

```python
# æ€§èƒ½åˆ†æï¼šè®°å½•è€—æ—¶
perf_callback = PerformanceCallback()
result = chain.invoke(input, config={"callbacks": [perf_callback]})
perf_callback.print_summary()
```

### 4. æ•°æ®éªŒè¯ï¼šä½¿ç”¨éªŒè¯å›è°ƒ

```python
# æ•°æ®éªŒè¯ï¼šæ£€æŸ¥ä¸­é—´æ•°æ®
validator = DataValidator()
result = chain.invoke(input, config={"callbacks": [validator]})
validator.report()
```

---

## å¸¸è§é—®é¢˜

### Q1: astream_events å’Œå›è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**:
- **astream_events**: å¼‚æ­¥è¿­ä»£å™¨ï¼Œé€‚åˆè°ƒè¯•å’Œåˆ†æ
- **å›è°ƒ**: åŒæ­¥é’©å­ï¼Œé€‚åˆç›‘æ§å’Œæ—¥å¿—

### Q2: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒæŸ¥çœ‹ä¸­é—´ç»“æœï¼Ÿ

**A**: ä½¿ç”¨ LangSmithï¼Œæ— éœ€ä¿®æ”¹ä»£ç ï¼Œè‡ªåŠ¨è¿½è¸ªæ‰€æœ‰æ‰§è¡Œã€‚

### Q3: å¦‚ä½•å¯¼å‡ºä¸­é—´ç»“æœï¼Ÿ

**A**: ä½¿ç”¨è‡ªå®šä¹‰å›è°ƒæˆ–è¿½è¸ªå™¨ï¼Œå°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“ã€‚

---

## å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ä¸ºä»€ä¹ˆéœ€è¦è®¿é—®ä¸­é—´ç»“æœ
- [ ] æŒæ¡ astream_events çš„ä½¿ç”¨
- [ ] æŒæ¡è‡ªå®šä¹‰å›è°ƒçš„ä½¿ç”¨
- [ ] èƒ½æ‰‹å†™ä¸­é—´ç»“æœè¿½è¸ªå™¨
- [ ] äº†è§£ LangSmith çš„é›†æˆ
- [ ] èƒ½è¿›è¡Œæ€§èƒ½ç“¶é¢ˆåˆ†æ
- [ ] èƒ½è¿›è¡Œæ•°æ®éªŒè¯

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-18
**å‚è€ƒæ¥æº**:
- LangChain å®˜æ–¹æ–‡æ¡£
- LangSmith å®˜æ–¹æ–‡æ¡£
- LangChain å›è°ƒç³»ç»Ÿæ–‡æ¡£
