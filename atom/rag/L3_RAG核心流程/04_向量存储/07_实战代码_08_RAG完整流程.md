# 实战代码08：RAG完整流程

## 代码说明

本示例展示从文档加载到答案生成的完整RAG流程，整合所有向量存储知识点。

**环境要求**：
```bash
pip install chromadb sentence-transformers openai pypdf python-dotenv
```

---

## 完整代码

```python
"""
RAG完整流程示例
从文档加载到答案生成的端到端实现
"""

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import os
from dotenv import load_dotenv
from pathlib import Path
from typing import List, Dict, Any
import time

# 加载环境变量
load_dotenv()

# ============================================
# 1. 文档加载
# ============================================

def load_documents(file_paths: List[str]) -> List[Dict[str, Any]]:
    """加载文档"""
    print("=" * 50)
    print("步骤1：文档加载")
    print("=" * 50)

    documents = []

    # 模拟文档加载（实际应使用pypdf等库）
    sample_docs = [
        {
            "text": "RAG（Retrieval-Augmented Generation）是一种结合检索和生成的技术。它首先从知识库中检索相关文档，然后将检索结果作为上下文输入给大语言模型生成答案。",
            "metadata": {"source": "rag_intro.txt", "category": "RAG", "page": 1}
        },
        {
            "text": "向量数据库是RAG系统的核心组件，用于存储和检索文档的embedding向量。常见的向量数据库包括ChromaDB、FAISS、Milvus等。",
            "metadata": {"source": "vector_db.txt", "category": "VectorDB", "page": 1}
        },
        {
            "text": "HNSW（Hierarchical Navigable Small World）是一种高效的近似最近邻搜索算法，在向量检索中表现优异。它通过构建分层图结构实现快速检索。",
            "metadata": {"source": "hnsw.txt", "category": "Algorithm", "page": 1}
        },
        {
            "text": "Embedding是将文本转换为向量表示的过程。常用的embedding模型包括OpenAI的text-embedding-ada-002和开源的sentence-transformers。",
            "metadata": {"source": "embedding.txt", "category": "Embedding", "page": 1}
        },
        {
            "text": "Prompt Engineering是优化LLM输出的关键技术。好的prompt应该包括清晰的角色设定、具体的指令和必要的上下文信息。",
            "metadata": {"source": "prompt.txt", "category": "PromptEng", "page": 1}
        }
    ]

    documents.extend(sample_docs)

    print(f"✓ 已加载{len(documents)}个文档")
    for i, doc in enumerate(documents, 1):
        print(f"  {i}. {doc['metadata']['source']} ({len(doc['text'])}字符)")

    return documents


# ============================================
# 2. 文本分块
# ============================================

def chunk_documents(documents: List[Dict[str, Any]], chunk_size: int = 200) -> List[Dict[str, Any]]:
    """文本分块"""
    print("\n" + "=" * 50)
    print("步骤2：文本分块")
    print("=" * 50)

    chunks = []

    for doc in documents:
        text = doc["text"]
        metadata = doc["metadata"]

        # 简单分块（实际应使用更智能的分块策略）
        for i in range(0, len(text), chunk_size):
            chunk_text = text[i:i + chunk_size]
            chunk = {
                "text": chunk_text,
                "metadata": {
                    **metadata,
                    "chunk_id": len(chunks),
                    "chunk_size": len(chunk_text)
                }
            }
            chunks.append(chunk)

    print(f"✓ 已分块为{len(chunks)}个片段")
    print(f"  平均大小: {sum(len(c['text']) for c in chunks) / len(chunks):.0f}字符")

    return chunks


# ============================================
# 3. 生成Embeddings
# ============================================

def generate_embeddings(chunks: List[Dict[str, Any]], model: SentenceTransformer) -> List[Dict[str, Any]]:
    """生成embeddings"""
    print("\n" + "=" * 50)
    print("步骤3：生成Embeddings")
    print("=" * 50)

    texts = [chunk["text"] for chunk in chunks]

    print(f"生成{len(texts)}个文本的embeddings...")
    start_time = time.time()

    embeddings = model.encode(texts, show_progress_bar=False)

    elapsed = time.time() - start_time

    print(f"✓ Embeddings生成完成")
    print(f"  耗时: {elapsed:.2f}秒")
    print(f"  向量维度: {embeddings.shape[1]}")
    print(f"  平均速度: {len(texts)/elapsed:.1f}个/秒")

    # 将embeddings添加到chunks
    for chunk, embedding in zip(chunks, embeddings):
        chunk["embedding"] = embedding.tolist()

    return chunks


# ============================================
# 4. 存储到向量数据库
# ============================================

def store_in_vector_db(chunks: List[Dict[str, Any]], persist_directory: str = "./rag_chroma_db"):
    """存储到向量数据库"""
    print("\n" + "=" * 50)
    print("步骤4：存储到向量数据库")
    print("=" * 50)

    # 创建ChromaDB客户端
    client = chromadb.PersistentClient(path=persist_directory)

    # 创建或获取collection
    collection = client.get_or_create_collection(
        name="rag_knowledge_base",
        metadata={"hnsw:space": "cosine"}
    )

    print(f"Collection: {collection.name}")
    print(f"  现有文档数: {collection.count()}")

    # 如果collection已有数据，清空
    if collection.count() > 0:
        client.delete_collection("rag_knowledge_base")
        collection = client.create_collection(
            name="rag_knowledge_base",
            metadata={"hnsw:space": "cosine"}
        )
        print(f"  已清空旧数据")

    # 准备数据
    ids = [f"chunk_{i}" for i in range(len(chunks))]
    embeddings = [chunk["embedding"] for chunk in chunks]
    documents = [chunk["text"] for chunk in chunks]
    metadatas = [chunk["metadata"] for chunk in chunks]

    # 批量添加
    print(f"\n添加{len(chunks)}个文档片段...")
    collection.add(
        ids=ids,
        embeddings=embeddings,
        documents=documents,
        metadatas=metadatas
    )

    print(f"✓ 数据已存储")
    print(f"  总文档数: {collection.count()}")

    return collection


# ============================================
# 5. 检索相关文档
# ============================================

def retrieve_documents(collection, query: str, model: SentenceTransformer, top_k: int = 3) -> Dict[str, Any]:
    """检索相关文档"""
    print("\n" + "=" * 50)
    print("步骤5：检索相关文档")
    print("=" * 50)

    print(f"\n查询: {query}")

    # 生成查询embedding
    start_time = time.time()
    query_embedding = model.encode(query).tolist()
    embedding_time = time.time() - start_time

    # 检索
    start_time = time.time()
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    search_time = time.time() - start_time

    print(f"\n✓ 检索完成")
    print(f"  Embedding耗时: {embedding_time*1000:.2f}ms")
    print(f"  检索耗时: {search_time*1000:.2f}ms")
    print(f"  总耗时: {(embedding_time+search_time)*1000:.2f}ms")

    print(f"\nTop {top_k}相关文档:")
    for i, (doc, metadata, distance) in enumerate(zip(
        results['documents'][0],
        results['metadatas'][0],
        results['distances'][0]
    ), 1):
        similarity = 1 - distance
        print(f"\n{i}. [相似度: {similarity:.3f}]")
        print(f"   来源: {metadata.get('source', 'unknown')}")
        print(f"   分类: {metadata.get('category', 'unknown')}")
        print(f"   内容: {doc[:100]}...")

    return {
        "query": query,
        "documents": results['documents'][0],
        "metadatas": results['metadatas'][0],
        "distances": results['distances'][0]
    }


# ============================================
# 6. 生成答案
# ============================================

def generate_answer(retrieval_results: Dict[str, Any], llm_client: OpenAI) -> str:
    """生成答案"""
    print("\n" + "=" * 50)
    print("步骤6：生成答案")
    print("=" * 50)

    query = retrieval_results["query"]
    documents = retrieval_results["documents"]

    # 构建上下文
    context = "\n\n".join([
        f"文档{i+1}:\n{doc}"
        for i, doc in enumerate(documents)
    ])

    print(f"\n上下文长度: {len(context)}字符")

    # 构建prompt
    system_prompt = """你是一个专业的AI助手，擅长基于提供的文档回答问题。

请遵循以下规则：
1. 只基于提供的文档内容回答
2. 如果文档中没有相关信息，明确说明
3. 回答要准确、简洁、有条理
4. 可以引用文档中的关键信息"""

    user_prompt = f"""参考文档：
{context}

问题：{query}

请基于上述文档回答问题。"""

    print(f"\n调用LLM生成答案...")

    try:
        start_time = time.time()

        response = llm_client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=500
        )

        answer = response.choices[0].message.content
        elapsed = time.time() - start_time

        print(f"✓ 答案生成完成")
        print(f"  耗时: {elapsed:.2f}秒")
        print(f"  Token使用: {response.usage.total_tokens}")

        return answer

    except Exception as e:
        print(f"✗ LLM调用失败: {e}")
        return f"[错误] 无法生成答案: {e}"


# ============================================
# 7. 完整RAG流程
# ============================================

def rag_pipeline(query: str, persist_directory: str = "./rag_chroma_db"):
    """完整RAG流程"""
    print("=" * 50)
    print("RAG完整流程")
    print("=" * 50)

    # 初始化模型
    print("\n初始化模型...")
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    llm_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    print("✓ 模型初始化完成")

    # 检查是否已有向量数据库
    client = chromadb.PersistentClient(path=persist_directory)

    try:
        collection = client.get_collection("rag_knowledge_base")
        print(f"\n✓ 使用现有向量数据库（{collection.count()}个文档）")
    except:
        print(f"\n构建向量数据库...")

        # 1. 加载文档
        documents = load_documents([])

        # 2. 文本分块
        chunks = chunk_documents(documents)

        # 3. 生成embeddings
        chunks = generate_embeddings(chunks, embedding_model)

        # 4. 存储到向量数据库
        collection = store_in_vector_db(chunks, persist_directory)

    # 5. 检索相关文档
    retrieval_results = retrieve_documents(collection, query, embedding_model, top_k=3)

    # 6. 生成答案
    answer = generate_answer(retrieval_results, llm_client)

    return answer


# ============================================
# 8. 批量查询
# ============================================

def batch_rag_queries(queries: List[str], persist_directory: str = "./rag_chroma_db"):
    """批量RAG查询"""
    print("\n" + "=" * 50)
    print("批量RAG查询")
    print("=" * 50)

    # 初始化
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    llm_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    client = chromadb.PersistentClient(path=persist_directory)
    collection = client.get_collection("rag_knowledge_base")

    results = []

    for i, query in enumerate(queries, 1):
        print(f"\n--- 查询 {i}/{len(queries)} ---")

        # 检索
        retrieval_results = retrieve_documents(collection, query, embedding_model, top_k=3)

        # 生成答案
        answer = generate_answer(retrieval_results, llm_client)

        results.append({
            "query": query,
            "answer": answer,
            "sources": [m.get('source') for m in retrieval_results['metadatas']]
        })

    return results


# ============================================
# 9. 性能统计
# ============================================

def performance_statistics():
    """RAG系统性能统计"""
    print("\n" + "=" * 50)
    print("性能统计")
    print("=" * 50)

    stats = """
    典型RAG查询性能（基于本示例）：

    **文档加载**: 一次性操作
      - 5个文档: <1秒

    **文本分块**: 一次性操作
      - 5个文档 → 5个片段: <0.1秒

    **Embedding生成**: 一次性操作
      - 5个片段: ~0.5秒
      - 速度: ~10个/秒

    **向量存储**: 一次性操作
      - 5个片段: <0.1秒

    **检索**: 每次查询
      - Query embedding: ~50ms
      - 向量检索: ~10ms
      - 总计: ~60ms

    **答案生成**: 每次查询
      - LLM调用: 2-5秒
      - Token使用: 200-500

    **端到端延迟**: 2-5秒
      - 检索: <100ms (2%)
      - 生成: 2-5秒 (98%)

    **优化建议**:
      1. 使用缓存减少重复查询
      2. 批量处理embedding生成
      3. 使用更快的LLM模型（如gpt-3.5-turbo）
      4. 预加载向量索引到内存
    """

    print(stats)


# ============================================
# 主函数
# ============================================

def main():
    """主函数"""
    print("RAG完整流程示例")
    print("=" * 50)

    # 单个查询示例
    query = "什么是RAG技术？它的核心组件有哪些？"

    print(f"\n查询: {query}\n")

    answer = rag_pipeline(query)

    print("\n" + "=" * 50)
    print("最终答案")
    print("=" * 50)
    print(answer)

    # 批量查询示例
    queries = [
        "向量数据库有哪些常见选择？",
        "HNSW算法的优势是什么？",
        "如何优化Prompt？"
    ]

    print("\n\n" + "=" * 50)
    print("批量查询示例")
    print("=" * 50)

    batch_results = batch_rag_queries(queries)

    for i, result in enumerate(batch_results, 1):
        print(f"\n--- 结果 {i} ---")
        print(f"问题: {result['query']}")
        print(f"答案: {result['answer']}")
        print(f"来源: {', '.join(result['sources'])}")

    # 性能统计
    performance_statistics()

    print("\n" + "=" * 50)
    print("所有示例执行完成！")
    print("=" * 50)


if __name__ == "__main__":
    main()
```

---

## 关键要点

### 1. RAG流程架构

```
┌─────────────────────────────────────┐
│  1. 文档加载                         │
│     - PDF/Word/HTML/Markdown        │
│     - 元数据提取                     │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│  2. 文本分块                         │
│     - 固定大小/递归/语义分块         │
│     - Chunk size: 200-500 tokens    │
│     - Overlap: 10-20%               │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│  3. Embedding生成                    │
│     - sentence-transformers         │
│     - OpenAI text-embedding-ada-002 │
│     - 向量维度: 384-1536            │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│  4. 向量存储                         │
│     - ChromaDB/FAISS/Milvus         │
│     - HNSW索引                       │
│     - 持久化存储                     │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│  5. 检索（Query时）                  │
│     - Query embedding               │
│     - 向量检索 (Top-K)              │
│     - 相似度排序                     │
└─────────────────────────────────────┘
              ↓
┌─────────────────────────────────────┐
│  6. 答案生成                         │
│     - 上下文构建                     │
│     - Prompt工程                     │
│     - LLM生成答案                    │
└─────────────────────────────────────┘
```

### 2. 性能优化策略

**索引构建阶段**（一次性）：
- 批量生成embeddings
- 使用GPU加速
- 并行处理文档

**查询阶段**（每次）：
- 缓存热门查询
- 批量查询
- 异步处理

### 3. 参数配置建议

**文本分块**：
```python
chunk_size = 500  # tokens
overlap = 50      # tokens (10%)
```

**检索参数**：
```python
top_k = 3-5       # 检索文档数
similarity_threshold = 0.7  # 相似度阈值
```

**LLM参数**：
```python
temperature = 0.3  # 降低随机性
max_tokens = 500   # 限制输出长度
```

### 4. 常见问题与解决方案

**问题1：检索结果不相关**
- 原因：embedding模型不匹配
- 解决：使用领域特定的embedding模型

**问题2：答案不准确**
- 原因：上下文不足或过多
- 解决：调整top_k和chunk_size

**问题3：延迟过高**
- 原因：LLM调用慢
- 解决：使用更快的模型或流式输出

**问题4：成本过高**
- 原因：频繁调用LLM
- 解决：添加缓存层

### 5. 生产环境检查清单

**功能完整性**：
- [ ] 支持多种文档格式
- [ ] 增量更新机制
- [ ] 错误处理和重试
- [ ] 日志和监控

**性能优化**：
- [ ] 缓存机制
- [ ] 批量处理
- [ ] 异步查询
- [ ] 连接池

**数据安全**：
- [ ] 数据加密
- [ ] 访问控制
- [ ] 审计日志
- [ ] 备份恢复

**可观测性**：
- [ ] 查询延迟监控
- [ ] 召回率监控
- [ ] 成本监控
- [ ] 错误率监控

---

## 引用来源

1. **RAG端到端实现**：
   - https://ajita-gupta.medium.com/step-by-step-how-to-build-a-rag-pipeline-e3e8d0d661a2
   - https://www.bauplanlabs.com/post/full-stack-rag-system-in-300-lines-of-python

2. **RAG最佳实践**：
   - https://www.askpython.com/python/examples/building-rag-applications-with-python
   - https://pub.towardsai.net/the-complete-rag-playbook-part-1-building-your-first-rag-pipeline

3. **生产级RAG**：
   - https://www.dhiwise.com/post/build-rag-pipeline-guide
   - https://github.com/FareedKhan-dev/complex-RAG-guide

4. **RAG工具包**：
   - https://github.com/RUC-NLPIR/FlashRAG

---

**最后更新**：2026-02-15
**基于资料**：2025-2026最新RAG实践
