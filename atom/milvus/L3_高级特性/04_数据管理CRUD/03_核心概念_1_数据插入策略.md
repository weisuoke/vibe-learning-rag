# 核心概念 1: 数据插入策略

> 深入理解 Milvus 的数据插入机制，掌握单条、批量、流式插入的最佳实践

---

## 概述

数据插入是 Milvus CRUD 操作的起点。理解插入策略对于构建高性能的向量数据库应用至关重要。

**本章内容**：
1. 单条插入 vs 批量插入
2. 列式存储的数据格式
3. 插入性能优化
4. 流式插入策略
5. 错误处理与重试
6. 在 RAG 系统中的应用

---

## 1. 单条插入 vs 批量插入

### 1.1 单条插入

**定义**：每次插入一条数据。

```python
from pymilvus import Collection
import numpy as np

collection = Collection("my_collection")

# 单条插入
for i in range(100):
    data = [
        [i],  # id
        [f"text_{i}"],  # text
        [np.random.rand(128).tolist()]  # embedding
    ]
    collection.insert(data)
    collection.flush()  # 每次都 flush
```

**性能特征**：
- **写入速度**：慢（每次都需要 I/O）
- **内存占用**：低（只缓存一条数据）
- **适用场景**：实时性要求极高的场景

**性能数据**：
- 单条插入：~100 条/秒
- 批量插入：~10,000 条/秒
- **性能差距**：100 倍

### 1.2 批量插入

**定义**：每次插入多条数据。

```python
# 批量插入
batch_size = 1000
ids = list(range(batch_size))
texts = [f"text_{i}" for i in range(batch_size)]
embeddings = [np.random.rand(128).tolist() for _ in range(batch_size)]

data = [ids, texts, embeddings]
collection.insert(data)
collection.flush()  # 批量 flush
```

**性能特征**：
- **写入速度**：快（减少 I/O 次数）
- **内存占用**：中（缓存一批数据）
- **适用场景**：大规模数据导入

**批量大小选择**：

| 批量大小 | 性能 | 内存占用 | 推荐场景 |
|---------|------|---------|---------|
| 100 | 低 | 低 | 实时插入 |
| 1,000 | 中 | 中 | 常规插入 |
| 10,000 | 高 | 高 | 批量导入 |
| 100,000 | 最高 | 很高 | 离线导入 |

**推荐**：
- **实时插入**：100-1,000 条/批
- **批量导入**：10,000-100,000 条/批

### 1.3 性能对比

```python
import time

def benchmark_insert(collection, num_records, batch_size):
    """性能测试"""
    start = time.time()

    for i in range(0, num_records, batch_size):
        batch_ids = list(range(i, min(i + batch_size, num_records)))
        batch_texts = [f"text_{j}" for j in batch_ids]
        batch_embeddings = [np.random.rand(128).tolist() for _ in batch_ids]

        collection.insert([batch_ids, batch_texts, batch_embeddings])

    collection.flush()
    elapsed = time.time() - start

    print(f"批量大小: {batch_size}")
    print(f"总记录数: {num_records}")
    print(f"耗时: {elapsed:.2f} 秒")
    print(f"吞吐量: {num_records / elapsed:.2f} 条/秒")

# 测试不同批量大小
benchmark_insert(collection, 10000, 1)      # 单条插入
benchmark_insert(collection, 10000, 100)    # 小批量
benchmark_insert(collection, 10000, 1000)   # 中批量
benchmark_insert(collection, 10000, 10000)  # 大批量
```

**预期结果**：
```
批量大小: 1      → 吞吐量: ~100 条/秒
批量大小: 100    → 吞吐量: ~5,000 条/秒
批量大小: 1000   → 吞吐量: ~10,000 条/秒
批量大小: 10000  → 吞吐量: ~15,000 条/秒
```

---

## 2. 列式存储的数据格式

### 2.1 行式 vs 列式

**行式存储**（传统数据库）：
```python
# 行式数据：每行是一个对象
rows = [
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]},
    {"id": 3, "text": "C", "embedding": [0.5, 0.6]}
]
```

**列式存储**（Milvus）：
```python
# 列式数据：每列是一个数组
columns = [
    [1, 2, 3],  # id 列
    ["A", "B", "C"],  # text 列
    [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]  # embedding 列
]
```

### 2.2 数据格式转换

**方法 1: 手动转换**

```python
def rows_to_columns(rows):
    """行式转列式"""
    if not rows:
        return []

    # 获取所有字段名
    fields = list(rows[0].keys())

    # 转换为列式
    columns = []
    for field in fields:
        column = [row[field] for row in rows]
        columns.append(column)

    return columns

# 使用
rows = [
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]}
]
columns = rows_to_columns(rows)
collection.insert(columns)
```

**方法 2: 使用 Pandas**

```python
import pandas as pd

# 创建 DataFrame
df = pd.DataFrame([
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]}
])

# 转换为列式
columns = [
    df["id"].tolist(),
    df["text"].tolist(),
    df["embedding"].tolist()
]
collection.insert(columns)
```

**方法 3: 使用列表推导式**

```python
rows = [
    {"id": 1, "text": "A", "embedding": [0.1, 0.2]},
    {"id": 2, "text": "B", "embedding": [0.3, 0.4]}
]

# 转换为列式
ids = [row["id"] for row in rows]
texts = [row["text"] for row in rows]
embeddings = [row["embedding"] for row in rows]

collection.insert([ids, texts, embeddings])
```

### 2.3 字段顺序

**重要**：列的顺序必须与 Schema 定义的顺序一致。

```python
from pymilvus import FieldSchema, CollectionSchema, DataType

# 定义 Schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=500),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)
]
schema = CollectionSchema(fields=fields)

# 插入数据：顺序必须与 Schema 一致
data = [
    [1, 2, 3],  # id（第一个字段）
    ["A", "B", "C"],  # text（第二个字段）
    [[0.1]*128, [0.2]*128, [0.3]*128]  # embedding（第三个字段）
]
collection.insert(data)
```

**错误示例**：
```python
# 错误：顺序不一致
data = [
    ["A", "B", "C"],  # text
    [1, 2, 3],  # id
    [[0.1]*128, [0.2]*128, [0.3]*128]  # embedding
]
collection.insert(data)  # 报错：类型不匹配
```

---

## 3. 插入性能优化

### 3.1 批量大小优化

**原则**：批量大小越大，性能越好，但内存占用也越大。

```python
def optimal_batch_size(vector_dim, num_fields, available_memory_mb):
    """计算最优批量大小"""
    # 每条记录的大小（字节）
    record_size = (
        8 +  # id (INT64)
        500 +  # text (VARCHAR, max 500)
        vector_dim * 4  # embedding (FLOAT_VECTOR)
    )

    # 可用内存（字节）
    available_memory = available_memory_mb * 1024 * 1024

    # 最优批量大小
    batch_size = int(available_memory * 0.5 / record_size)

    return batch_size

# 示例
batch_size = optimal_batch_size(
    vector_dim=128,
    num_fields=3,
    available_memory_mb=1024  # 1GB
)
print(f"推荐批量大小: {batch_size}")  # ~10,000
```

### 3.2 并行插入

**策略**：使用多线程或多进程并行插入。

```python
from concurrent.futures import ThreadPoolExecutor
import numpy as np

def insert_batch(collection, start_id, batch_size):
    """插入一批数据"""
    ids = list(range(start_id, start_id + batch_size))
    texts = [f"text_{i}" for i in ids]
    embeddings = [np.random.rand(128).tolist() for _ in ids]

    collection.insert([ids, texts, embeddings])

def parallel_insert(collection, total_records, batch_size, num_workers=4):
    """并行插入"""
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = []
        for start_id in range(0, total_records, batch_size):
            future = executor.submit(insert_batch, collection, start_id, batch_size)
            futures.append(future)

        # 等待所有任务完成
        for future in futures:
            future.result()

    # 批量 flush
    collection.flush()

# 使用
parallel_insert(collection, total_records=100000, batch_size=10000, num_workers=4)
```

**性能提升**：
- 单线程：~10,000 条/秒
- 4 线程：~30,000 条/秒
- 8 线程：~50,000 条/秒

**注意**：
- 线程数不宜过多（推荐 4-8 个）
- 需要考虑 Milvus 服务器的并发能力

### 3.3 减少 flush() 调用

**原则**：批量 flush，不要每次插入都 flush。

```python
# 错误：频繁 flush
for batch in batches:
    collection.insert(batch)
    collection.flush()  # 每次都 flush，性能差

# 正确：批量 flush
for batch in batches:
    collection.insert(batch)
collection.flush()  # 批量 flush，性能好
```

**性能对比**：
- 频繁 flush：~1,000 条/秒
- 批量 flush：~10,000 条/秒
- **性能提升**：10 倍

### 3.4 使用 auto_id

**策略**：让 Milvus 自动生成 ID，减少数据传输。

```python
# 定义 Schema（auto_id=True）
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=500),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)
]
schema = CollectionSchema(fields=fields)

# 插入数据（不需要提供 id）
data = [
    ["A", "B", "C"],  # text
    [[0.1]*128, [0.2]*128, [0.3]*128]  # embedding
]
result = collection.insert(data)

# 获取自动生成的 ID
print(result.primary_keys)  # [1, 2, 3]
```

**优势**：
- 减少数据传输量
- 避免 ID 冲突
- 简化代码逻辑

---

## 4. 流式插入策略

### 4.1 什么是流式插入？

**定义**：数据源源不断地产生，需要持续插入到 Milvus。

**场景**：
- 实时日志收集
- 实时数据同步
- 流式 ETL

### 4.2 流式插入实现

```python
import queue
import threading
import time

class StreamInserter:
    """流式插入器"""

    def __init__(self, collection, batch_size=1000, flush_interval=5):
        self.collection = collection
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.buffer = []
        self.lock = threading.Lock()
        self.running = False
        self.thread = None

    def start(self):
        """启动流式插入"""
        self.running = True
        self.thread = threading.Thread(target=self._flush_loop)
        self.thread.start()

    def stop(self):
        """停止流式插入"""
        self.running = False
        if self.thread:
            self.thread.join()
        self._flush_buffer()  # 刷新剩余数据

    def insert(self, record):
        """插入一条记录"""
        with self.lock:
            self.buffer.append(record)

            # 达到批量大小，立即刷新
            if len(self.buffer) >= self.batch_size:
                self._flush_buffer()

    def _flush_buffer(self):
        """刷新缓冲区"""
        if not self.buffer:
            return

        with self.lock:
            # 转换为列式数据
            ids = [r["id"] for r in self.buffer]
            texts = [r["text"] for r in self.buffer]
            embeddings = [r["embedding"] for r in self.buffer]

            # 插入
            self.collection.insert([ids, texts, embeddings])
            self.collection.flush()

            print(f"已插入 {len(self.buffer)} 条记录")
            self.buffer.clear()

    def _flush_loop(self):
        """定时刷新循环"""
        while self.running:
            time.sleep(self.flush_interval)
            self._flush_buffer()

# 使用
inserter = StreamInserter(collection, batch_size=1000, flush_interval=5)
inserter.start()

# 模拟流式数据
for i in range(10000):
    record = {
        "id": i,
        "text": f"text_{i}",
        "embedding": np.random.rand(128).tolist()
    }
    inserter.insert(record)
    time.sleep(0.01)  # 模拟数据产生速度

inserter.stop()
```

### 4.3 流式插入优化

**策略 1: 动态批量大小**

```python
class AdaptiveStreamInserter(StreamInserter):
    """自适应流式插入器"""

    def __init__(self, collection, min_batch_size=100, max_batch_size=10000):
        super().__init__(collection, batch_size=min_batch_size)
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.insert_rate = 0  # 插入速率（条/秒）

    def insert(self, record):
        """插入一条记录"""
        with self.lock:
            self.buffer.append(record)

            # 动态调整批量大小
            if self.insert_rate > 1000:
                self.batch_size = self.max_batch_size
            elif self.insert_rate > 100:
                self.batch_size = 1000
            else:
                self.batch_size = self.min_batch_size

            # 达到批量大小，立即刷新
            if len(self.buffer) >= self.batch_size:
                self._flush_buffer()
```

**策略 2: 背压控制**

```python
class BackpressureStreamInserter(StreamInserter):
    """带背压控制的流式插入器"""

    def __init__(self, collection, batch_size=1000, max_buffer_size=100000):
        super().__init__(collection, batch_size=batch_size)
        self.max_buffer_size = max_buffer_size

    def insert(self, record):
        """插入一条记录"""
        # 背压控制：缓冲区满时阻塞
        while len(self.buffer) >= self.max_buffer_size:
            time.sleep(0.1)

        with self.lock:
            self.buffer.append(record)

            if len(self.buffer) >= self.batch_size:
                self._flush_buffer()
```

---

## 5. 错误处理与重试

### 5.1 常见错误

**错误 1: 主键冲突**

```python
# 错误：插入重复的主键
data = [[1, 1, 2], ["A", "B", "C"], [embeddings]]
collection.insert(data)  # 报错：主键冲突
```

**解决方案**：
```python
# 方案 1: 使用 auto_id
# 方案 2: 确保 ID 唯一
# 方案 3: 使用 Upsert
collection.upsert(data)
```

**错误 2: 数据类型不匹配**

```python
# 错误：类型不匹配
data = [["1", "2"], ["A", "B"], [embeddings]]  # id 应该是 INT64
collection.insert(data)  # 报错：类型不匹配
```

**解决方案**：
```python
# 转换为正确的类型
ids = [int(id) for id in ["1", "2"]]
data = [ids, ["A", "B"], [embeddings]]
collection.insert(data)
```

**错误 3: 向量维度不匹配**

```python
# 错误：向量维度不匹配
data = [[1, 2], ["A", "B"], [[0.1]*64, [0.2]*64]]  # 应该是 128 维
collection.insert(data)  # 报错：维度不匹配
```

### 5.2 重试机制

```python
import time
from pymilvus.exceptions import MilvusException

def insert_with_retry(collection, data, max_retries=3, retry_delay=1):
    """带重试的插入"""
    for attempt in range(max_retries):
        try:
            collection.insert(data)
            collection.flush()
            return True
        except MilvusException as e:
            print(f"插入失败（尝试 {attempt + 1}/{max_retries}）: {e}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (2 ** attempt))  # 指数退避
            else:
                raise
    return False
```

### 5.3 批量插入的错误处理

```python
def batch_insert_with_error_handling(collection, batches):
    """批量插入（带错误处理）"""
    success_count = 0
    failed_batches = []

    for i, batch in enumerate(batches):
        try:
            collection.insert(batch)
            success_count += len(batch[0])
        except Exception as e:
            print(f"批次 {i} 插入失败: {e}")
            failed_batches.append((i, batch))

    # 刷新成功的数据
    collection.flush()

    # 重试失败的批次
    for i, batch in failed_batches:
        try:
            collection.insert(batch)
            collection.flush()
            success_count += len(batch[0])
            print(f"批次 {i} 重试成功")
        except Exception as e:
            print(f"批次 {i} 重试失败: {e}")

    print(f"成功插入 {success_count} 条记录")
    print(f"失败 {len(failed_batches)} 个批次")
```

---

## 6. 在 RAG 系统中的应用

### 6.1 文档导入

```python
from sentence_transformers import SentenceTransformer

def import_documents_to_milvus(collection, documents, batch_size=1000):
    """导入文档到 Milvus"""
    # 加载 Embedding 模型
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # 批量处理
    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i + batch_size]

        # 提取文本
        texts = [doc["text"] for doc in batch_docs]

        # 生成 Embedding
        embeddings = model.encode(texts).tolist()

        # 准备数据
        ids = [doc["id"] for doc in batch_docs]
        data = [ids, texts, embeddings]

        # 插入
        collection.insert(data)
        print(f"已导入 {i + len(batch_docs)}/{len(documents)} 条文档")

    # 刷新
    collection.flush()
    collection.load()
```

### 6.2 增量更新

```python
def incremental_update(collection, new_documents):
    """增量更新文档"""
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # 生成 Embedding
    texts = [doc["text"] for doc in new_documents]
    embeddings = model.encode(texts).tolist()

    # 准备数据
    ids = [doc["id"] for doc in new_documents]
    data = [ids, texts, embeddings]

    # 使用 Upsert（存在则更新，不存在则插入）
    collection.upsert(data)
    collection.flush()
```

### 6.3 实时索引

```python
class RealtimeIndexer:
    """实时索引器"""

    def __init__(self, collection, model_name='all-MiniLM-L6-v2'):
        self.collection = collection
        self.model = SentenceTransformer(model_name)
        self.inserter = StreamInserter(collection, batch_size=100, flush_interval=5)
        self.inserter.start()

    def index_document(self, doc_id, text):
        """索引一条文档"""
        # 生成 Embedding
        embedding = self.model.encode([text])[0].tolist()

        # 插入
        record = {
            "id": doc_id,
            "text": text,
            "embedding": embedding
        }
        self.inserter.insert(record)

    def stop(self):
        """停止索引器"""
        self.inserter.stop()

# 使用
indexer = RealtimeIndexer(collection)

# 实时索引文档
for i, doc in enumerate(documents):
    indexer.index_document(i, doc["text"])

indexer.stop()
```

---

## 总结

### 核心要点

1. **批量插入优于单条插入**：性能提升 10-100 倍
2. **列式存储是必须的**：需要转换数据格式
3. **批量 flush 提升性能**：减少磁盘 I/O
4. **流式插入适合实时场景**：缓冲 + 定时刷新
5. **错误处理和重试很重要**：保证数据完整性

### 最佳实践

1. **选择合适的批量大小**：1,000-10,000 条/批
2. **使用并行插入**：4-8 个线程
3. **减少 flush() 调用**：批量 flush
4. **使用 auto_id**：简化逻辑，避免冲突
5. **实现重试机制**：处理临时错误

### 性能优化清单

- [ ] 使用批量插入（batch_size >= 1000）
- [ ] 批量 flush（不要每次插入都 flush）
- [ ] 使用并行插入（4-8 个线程）
- [ ] 使用 auto_id（减少数据传输）
- [ ] 实现错误处理和重试
- [ ] 监控插入性能（吞吐量、延迟）

---

**下一步**: 学习 [04_核心概念_2_数据查询方法.md](./04_核心概念_2_数据查询方法.md) 深入理解查询操作
