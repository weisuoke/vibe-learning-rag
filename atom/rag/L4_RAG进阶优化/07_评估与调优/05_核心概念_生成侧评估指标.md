# 核心概念：生成侧评估指标

> 衡量 RAG 系统生成环节质量的核心指标 — Faithfulness, Answer Relevancy, Answer Correctness, LLM-as-Judge

---

## 一句话定义

**生成侧评估指标是衡量 RAG 系统"写答案"能力的量化工具，核心回答三个问题：答得真不真（Faithfulness）、答得对不对题（Relevancy）、答得准不准（Correctness）。**

---

## 为什么需要单独评估生成质量？

RAG 系统的工作流程是"先检索、再生成"。检索侧评估（Precision、Recall 等）只能告诉你"找到的文档好不好"，但找到好文档不代表最终答案就好——LLM 在生成阶段仍然可能出问题。

这就好比考试时你带了一本完美的参考书（检索完美），但你的阅读理解能力不行（生成有问题），最终答案照样不及格。

### 三个核心原因

**1. 检索好不代表生成好**

```
即使检索完美（Context Recall = 1.0），生成仍可能失败：
✅ 检索到正确文档 → ❌ LLM 编造了不存在的信息（幻觉）
✅ 检索到正确文档 → ❌ LLM 回答了另一个问题（跑题）
✅ 检索到正确文档 → ❌ LLM 遗漏了关键信息（不完整）
```

**2. 生成问题的优化方向与检索完全不同**

- 检索不好 → 换 Embedding 模型、调整 Chunk 大小、加 ReRank
- 生成不好 → 优化 Prompt、调整 Temperature、换 LLM 模型

如果不单独评估生成质量，你无法判断问题出在哪个环节。

**3. 生成质量直接决定用户体验**

用户看到的是最终答案，不是检索到的文档。即使你的检索做得再好，如果生成的答案有幻觉、跑题或不完整，用户体验依然很差。

---

## 指标1：Faithfulness（忠实度）

### 定义

**生成的答案中，有多少内容可以从检索到的上下文中推导出来。**

### 直觉理解

"答案有没有编造？每句话都有据可查吗？"

就像记者写新闻：好记者只报道采访到的事实，不会添油加醋。Faithfulness 衡量的就是 LLM 是不是一个"好记者"——只说有依据的话，不自己编造。

### 计算方式

```
1. 将答案拆分为独立陈述（claims）
2. 对每个陈述，用 NLI 或 LLM 判断是否能从上下文推导
3. Faithfulness = 可推导的陈述数 / 总陈述数
```

### 手写实现（基于 LLM-as-Judge）

```python
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()


def extract_claims(answer: str) -> list[str]:
    """用 LLM 将答案拆分为独立陈述"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"""请将以下答案拆分为独立的事实陈述，每行一个：

答案：{answer}

要求：
- 每个陈述必须是独立的、可验证的事实
- 不要包含主观评价
- 每行一个陈述，不要编号"""
        }],
        temperature=0
    )
    claims = response.choices[0].message.content.strip().split("\n")
    return [c.strip() for c in claims if c.strip()]


def check_claim_support(claim: str, contexts: list[str]) -> bool:
    """检查单个陈述是否被上下文支持"""
    context_text = "\n".join(contexts)
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"""判断以下陈述是否可以从给定的上下文中推导出来。

上下文：
{context_text}

陈述：{claim}

只回答 "支持" 或 "不支持"。"""
        }],
        temperature=0
    )
    return "支持" in response.choices[0].message.content


def faithfulness_score(answer: str, contexts: list[str]) -> float:
    """计算忠实度分数"""
    claims = extract_claims(answer)
    if not claims:
        return 0.0

    supported = sum(1 for c in claims if check_claim_support(c, contexts))
    score = supported / len(claims)

    print(f"总陈述数: {len(claims)}")
    print(f"有支持的: {supported}")
    print(f"忠实度: {score:.2f}")
    return score


# ===== 使用示例 =====
answer = "RAG 是检索增强生成技术，由 Google 在 2020 年提出，可以减少幻觉。"
contexts = [
    "RAG（Retrieval-Augmented Generation）是一种检索增强生成技术，"
    "通过检索外部知识来增强 LLM 的回答。"
]

# 陈述1: "RAG 是检索增强生成技术" → 上下文支持 ✅
# 陈述2: "由 Google 在 2020 年提出" → 上下文不支持 ❌（编造了来源）
# 陈述3: "可以减少幻觉"           → 上下文不支持 ❌（上下文没提到）
# Faithfulness = 1/3 ≈ 0.33
```

### 分数解读

| 分数 | 含义 | 行动建议 |
|------|------|----------|
| 0.9-1.0 | 高度忠实，几乎没有编造 | 保持当前策略 |
| 0.7-0.9 | 基本忠实，有少量编造 | 在 Prompt 中加强"仅基于上下文回答"的约束 |
| 0.5-0.7 | 幻觉较多 | 降低 temperature + 改进 Prompt + 检查上下文质量 |
| < 0.5 | 严重幻觉 | 需要根本性改进：换模型、重新设计 Prompt、检查检索质量 |

### 提升 Faithfulness 的实用技巧

```python
# 技巧1：在 Prompt 中明确约束
system_prompt = """你是一个严谨的问答助手。
规则：
1. 只基于提供的上下文回答问题
2. 如果上下文中没有相关信息，请说"根据提供的信息无法回答"
3. 不要添加上下文中没有的信息
4. 如果不确定，请明确标注"不确定"
"""

# 技巧2：降低 temperature（减少随机性 = 减少编造）
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages,
    temperature=0.1  # 低 temperature → 更忠实
)

# 技巧3：要求引用来源
system_prompt_with_citation = """回答问题时，请在每个关键信息后标注来源。
格式：[信息内容]（来源：上下文第X段）
如果某个信息在上下文中找不到来源，不要包含在答案中。
"""
```

---

## 指标2：Answer Relevancy（答案相关性）

### 定义

**生成的答案与用户问题的相关程度。**

### 直觉理解

"答案有没有跑题？是不是在回答用户问的问题？"

就像考试答题：即使你写的内容都是对的，但如果答非所问，也拿不到分。Answer Relevancy 衡量的就是"答案是否切题"。

**注意：** 这个指标不关心答案是否"正确"，只关心答案是否"切题"。一个错误但切题的答案，Answer Relevancy 可能很高。

### 计算方式（RAGAS 方法）

```
1. 根据答案反向生成 N 个可能的问题
2. 计算这些生成问题与原始问题的 embedding 相似度
3. Answer Relevancy = 平均相似度
```

**为什么这样计算？** 核心思路是：如果答案真的在回答原始问题，那么从答案反推出的问题应该和原始问题很像。反过来，如果答案跑题了，反推出的问题就会和原始问题差别很大。

### 手写实现

```python
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()


def get_embedding(text: str) -> list[float]:
    """获取文本的 embedding 向量"""
    resp = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return resp.data[0].embedding


def cosine_similarity(vec_a: list[float], vec_b: list[float]) -> float:
    """计算余弦相似度"""
    a = np.array(vec_a)
    b = np.array(vec_b)
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


def answer_relevancy_score(question: str, answer: str, n: int = 3) -> float:
    """计算答案相关性分数"""
    # 第一步：根据答案反向生成问题
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"""根据以下答案，生成 {n} 个这个答案最可能在回答的问题。
每行一个问题，不要编号。

答案：{answer}"""
        }],
        temperature=0.3
    )
    generated_questions = response.choices[0].message.content.strip().split("\n")
    generated_questions = [q.strip() for q in generated_questions if q.strip()][:n]

    # 第二步：计算原始问题与每个生成问题的 embedding 相似度
    q_emb = get_embedding(question)

    similarities = []
    for gen_q in generated_questions:
        gen_emb = get_embedding(gen_q)
        sim = cosine_similarity(q_emb, gen_emb)
        similarities.append(sim)

    # 第三步：取平均值
    score = np.mean(similarities)

    print(f"原始问题: {question}")
    print(f"生成问题: {generated_questions}")
    print(f"各相似度: {[f'{s:.3f}' for s in similarities]}")
    print(f"答案相关性: {score:.3f}")
    return float(score)
```

### 低分案例分析

```python
# 案例1：完全跑题 → 低分
question = "Python 3.9 有什么新特性？"
answer = "Python 是一种广泛使用的编程语言，由 Guido van Rossum 创建。"
# 反向生成的问题: ["Python 是什么？", "谁创建了 Python？", ...]
# 这些问题和原始问题差别很大
# Answer Relevancy ≈ 0.4

# 案例2：部分相关 → 中等分
question = "Python 3.9 有什么新特性？"
answer = "Python 3.9 新增了字典合并运算符。此外 Python 的历史可以追溯到 1991 年。"
# 前半句切题，后半句跑题
# Answer Relevancy ≈ 0.7

# 案例3：高度相关 → 高分
question = "Python 3.9 有什么新特性？"
answer = "Python 3.9 的主要新特性包括：字典合并运算符(|)、字符串方法改进和类型提示增强。"
# 反向生成的问题和原始问题高度相似
# Answer Relevancy ≈ 0.95
```

### 提升 Answer Relevancy 的技巧

```python
# 技巧1：在 Prompt 中要求直接回答问题
system_prompt = """请直接回答用户的问题，不要添加无关的背景信息。
如果问题是"X 是什么？"，直接解释 X，不要扯到 Y 和 Z。
"""

# 技巧2：在 Prompt 中重复问题
user_prompt = f"""基于以下上下文回答问题。

上下文：{context}

问题：{question}

请针对上述问题给出简洁、直接的回答。不要包含与问题无关的信息。
"""
```

---

## 指标3：Answer Correctness（答案正确性）

### 定义

**生成的答案与标准答案（ground truth）的一致程度。**

### 直觉理解

"答案和标准答案比，对了多少？"

这就像老师批改试卷：拿学生的答案和标准答案对比，看内容是否一致、是否有错误、是否有遗漏。

**注意：** 这个指标**需要 ground truth**（人工标注的标准答案），因此使用成本比 Faithfulness 和 Answer Relevancy 更高。

### 计算方式

通常结合两种方法来综合评估：

1. **语义相似度**：答案与 ground truth 的 embedding 余弦相似度
2. **LLM-as-Judge 评分**：用 LLM 判断答案与标准答案的一致性

```python
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()


def get_embedding(text: str) -> list[float]:
    """获取文本的 embedding 向量"""
    resp = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return resp.data[0].embedding


def answer_correctness_score(answer: str, ground_truth: str) -> dict:
    """
    计算答案正确性（多维度）

    参数:
        answer: RAG 系统生成的答案
        ground_truth: 人工标注的标准答案

    返回:
        包含多个维度分数的字典
    """
    # ===== 方法1：语义相似度 =====
    ans_emb = np.array(get_embedding(answer))
    gt_emb = np.array(get_embedding(ground_truth))
    semantic_sim = float(
        np.dot(ans_emb, gt_emb) / (np.linalg.norm(ans_emb) * np.linalg.norm(gt_emb))
    )

    # ===== 方法2：LLM-as-Judge 评分 =====
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"""请评估生成答案与标准答案的一致性。

标准答案：{ground_truth}
生成答案：{answer}

评分标准（1-5分）：
5分：完全一致，信息完整准确
4分：基本一致，有少量遗漏
3分：部分一致，有明显遗漏或错误
2分：少量一致，大部分不对
1分：完全不一致

只回答一个数字（1-5）。"""
        }],
        temperature=0
    )
    llm_score = int(response.choices[0].message.content.strip()) / 5.0

    # ===== 综合分数（加权平均） =====
    combined = 0.5 * semantic_sim + 0.5 * llm_score

    result = {
        "semantic_similarity": round(semantic_sim, 3),
        "llm_judge_score": round(llm_score, 3),
        "combined_score": round(combined, 3)
    }

    print(f"语义相似度: {result['semantic_similarity']}")
    print(f"LLM 评分: {result['llm_judge_score']}")
    print(f"综合分数: {result['combined_score']}")
    return result


# ===== 使用示例 =====
ground_truth = "RAG 是检索增强生成技术，通过检索外部知识库来增强 LLM 的回答能力，不需要重新训练模型。"
answer_good = "RAG（Retrieval-Augmented Generation）通过检索外部知识来增强大模型的回答，无需微调模型参数。"
answer_bad = "RAG 是一种数据库技术，用于存储和查询结构化数据。"

# answer_good 的 combined_score ≈ 0.85（高度一致）
# answer_bad 的 combined_score ≈ 0.30（严重偏离）
```

---

## 指标4：Answer Completeness（答案完整性）

### 定义

**答案是否覆盖了所有应该回答的要点。**

### 直觉理解

"该说的都说了吗？有没有遗漏重要信息？"

这就像写报告：老板问你"这个项目的进展如何？"，你只说了"进度正常"，但没提到预算超支和人员变动——虽然你说的是对的，但不完整。

### 手写实现

```python
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()


def answer_completeness_score(answer: str, ground_truth: str) -> dict:
    """
    评估答案完整性：答案覆盖了标准答案中多少要点

    参数:
        answer: RAG 系统生成的答案
        ground_truth: 人工标注的标准答案

    返回:
        包含要点分析和覆盖率的字典
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"""请评估生成答案对标准答案要点的覆盖程度。

标准答案：{ground_truth}
生成答案：{answer}

请按以下步骤分析：
1. 列出标准答案中的所有关键要点
2. 逐一检查生成答案是否覆盖了每个要点
3. 给出覆盖率

请严格按以下 JSON 格式回答：
{{
  "key_points": ["要点1", "要点2", ...],
  "covered": ["已覆盖的要点1", ...],
  "missed": ["遗漏的要点1", ...],
  "coverage_rate": 0.XX
}}"""
        }],
        temperature=0
    )

    import json
    content = response.choices[0].message.content.strip()
    # 尝试解析 JSON
    try:
        result = json.loads(content)
    except json.JSONDecodeError:
        # 如果解析失败，返回默认值
        result = {"key_points": [], "covered": [], "missed": [], "coverage_rate": 0.0}

    print(f"关键要点: {result.get('key_points', [])}")
    print(f"已覆盖: {result.get('covered', [])}")
    print(f"遗漏: {result.get('missed', [])}")
    print(f"覆盖率: {result.get('coverage_rate', 0.0)}")
    return result


# ===== 使用示例 =====
ground_truth = "RAG 有三个核心优势：1) 不需要重新训练模型；2) 可以使用最新数据；3) 答案可追溯到来源文档。"
answer_partial = "RAG 的优势是不需要重新训练模型，而且可以使用最新的数据。"

# 标准答案有 3 个要点，答案只覆盖了 2 个
# 遗漏了"答案可追溯到来源文档"
# coverage_rate ≈ 0.67
```

---

## LLM-as-Judge 方法详解

### 什么是 LLM-as-Judge？

**用 LLM 来评估 LLM 的输出质量。**

传统的评估方法需要大量人工标注和评审，成本高、速度慢。LLM-as-Judge 的核心思路是：用一个强大的 LLM（如 GPT-4）来充当"评委"，自动评估另一个 LLM 的输出质量。

```
传统方法：人工评估（准确但昂贵、慢）
新方法：  LLM 评估（成本低、快、可扩展）
最佳实践：LLM 初筛 + 人工抽检（兼顾效率和准确性）
```

### 评估 Prompt 设计原则

一个好的评估 Prompt 需要包含：明确的评估维度、清晰的评分标准、结构化的输出格式。

```python
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI()


def llm_judge_evaluate(
    question: str,
    answer: str,
    contexts: list[str],
    ground_truth: str = ""
) -> dict:
    """
    使用 LLM-as-Judge 进行多维度评估

    参数:
        question: 用户问题
        answer: RAG 系统生成的答案
        contexts: 检索到的上下文
        ground_truth: 标准答案（可选）

    返回:
        包含各维度分数和理由的字典
    """
    context_text = "\n---\n".join(contexts)

    # 构建评估 Prompt
    gt_section = ""
    if ground_truth:
        gt_section = f"\n标准答案：{ground_truth}"

    evaluation_prompt = f"""你是一个专业的 RAG 系统评估专家。请评估以下答案的质量。

## 评估输入
问题：{question}
上下文：
{context_text}
生成答案：{answer}{gt_section}

## 评估维度和评分标准（每个维度 1-5 分）

### 忠实度（Faithfulness）
5 = 所有内容都有上下文支持
4 = 绝大部分有支持，极少量推断
3 = 大部分有支持，有少量编造
2 = 较多内容无法从上下文推导
1 = 大量编造，严重幻觉

### 相关性（Relevancy）
5 = 完全切题，直接回答问题
4 = 基本切题，有少量冗余
3 = 部分切题，有明显跑题内容
2 = 大部分跑题
1 = 完全跑题

### 完整性（Completeness）
5 = 覆盖所有关键要点
4 = 覆盖大部分要点，少量遗漏
3 = 覆盖约一半要点
2 = 遗漏大部分要点
1 = 几乎没有覆盖

## 输出格式（严格按此格式）
忠实度: X/5
相关性: X/5
完整性: X/5
总评: X/5
理由: [简要说明各维度的评分依据]"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": evaluation_prompt}],
        temperature=0
    )

    content = response.choices[0].message.content.strip()
    print(content)

    # 解析分数（简化版）
    scores = {}
    for line in content.split("\n"):
        for dim in ["忠实度", "相关性", "完整性", "总评"]:
            if dim in line and "/" in line:
                try:
                    score_str = line.split(":")[1].strip().split("/")[0].strip()
                    scores[dim] = int(score_str) / 5.0
                except (ValueError, IndexError):
                    pass

    return scores
```

### 多评委投票（减少偏差）

单次 LLM 评估可能有偏差。通过多次评估取平均，可以提高评估的稳定性和可靠性。

```python
def multi_judge_evaluation(
    question: str,
    answer: str,
    contexts: list[str],
    ground_truth: str = "",
    n_judges: int = 3
) -> dict:
    """
    多次评估取平均，减少单次评估的偏差

    参数:
        n_judges: 评估次数（相当于多个"评委"）
    """
    all_scores = []

    for i in range(n_judges):
        scores = llm_judge_evaluate(question, answer, contexts, ground_truth)
        all_scores.append(scores)
        print(f"第 {i+1} 次评估: {scores}")

    # 计算每个维度的平均分和方差
    result = {}
    for dim in ["忠实度", "相关性", "完整性", "总评"]:
        dim_scores = [s.get(dim, 0) for s in all_scores if dim in s]
        if dim_scores:
            avg = sum(dim_scores) / len(dim_scores)
            variance = sum((s - avg) ** 2 for s in dim_scores) / len(dim_scores)
            result[dim] = {
                "平均分": round(avg, 3),
                "方差": round(variance, 4)  # 方差大说明评估不稳定
            }

    print(f"\n综合结果: {result}")
    return result
```

---

## 自动评估 vs 人工评估

| 维度 | 自动评估（LLM-as-Judge） | 人工评估 |
|------|--------------------------|----------|
| 成本 | 低（API 调用费） | 高（人力成本） |
| 速度 | 快（秒级） | 慢（小时/天级） |
| 规模 | 大（可评估数千条） | 小（通常几十到几百条） |
| 一致性 | 高（同样输入同样输出） | 低（不同人评分不同） |
| 准确性 | 中（有系统偏差） | 高（人类判断更准确） |
| 创意评估 | 差（难以评估创意性回答） | 好（人类能感知创意） |
| 用户体验 | 无法评估 | 可以评估 |

**最佳实践：自动评估做初筛 + 人工评估做深度检查**

```
推荐工作流：
1. 自动评估（LLM-as-Judge）→ 快速筛出低分样本
2. 人工抽检（10-20%）→ 验证自动评估的准确性
3. 重点人工审查 → 对自动评估分数异常的样本深入分析
```

---

## 四个指标对比总结

| 指标 | 衡量什么 | 核心问题 | 需要 ground_truth | 需要 LLM |
|------|----------|----------|-------------------|----------|
| Faithfulness | 忠于上下文 | 答案有没有编造？ | 否 | 是 |
| Answer Relevancy | 回答相关性 | 答案有没有跑题？ | 否 | 是 |
| Answer Correctness | 答案准确性 | 答案和标准答案一致吗？ | 是 | 是 |
| Answer Completeness | 答案完整性 | 该说的都说了吗？ | 是 | 是 |

**如何根据指标定位问题并优化：**

```
Faithfulness 低      → 幻觉问题 → 加强 Prompt 约束 + 降低 temperature
Answer Relevancy 低  → 跑题问题 → 优化 Prompt 聚焦 + 检查检索结果相关性
Answer Correctness 低 → 准确性问题 → 换更强的 LLM + 改进上下文质量
Answer Completeness 低 → 遗漏问题 → 增加检索数量 + 在 Prompt 中要求全面回答
```

---

## 在 RAG 开发中的应用

### 应用1：Prompt 优化

当 Faithfulness 分数低时，说明 LLM 在"编造"信息。通过调整 Prompt 可以有效改善：

```python
# 对比不同 Prompt 策略对 Faithfulness 的影响
prompts = {
    "基础版": "根据上下文回答问题。\n上下文：{context}\n问题：{question}",
    "约束版": "严格根据以下上下文回答问题，不要添加任何上下文中没有的信息。"
              "如果无法回答，请说'信息不足'。\n上下文：{context}\n问题：{question}",
    "引用版": "根据上下文回答问题，并在每个关键信息后用[来源X]标注出处。"
              "只使用上下文中的信息。\n上下文：{context}\n问题：{question}",
}

# 用相同的测试集评估不同 Prompt
for name, prompt_template in prompts.items():
    answers = generate_answers(prompt_template, test_data)
    faith_score = evaluate_faithfulness(answers, test_data)
    print(f"{name}: Faithfulness = {faith_score:.3f}")

# 预期结果：
# 基础版: Faithfulness = 0.72
# 约束版: Faithfulness = 0.88
# 引用版: Faithfulness = 0.93
```

### 应用2：模型选择

不同 LLM 在生成质量上差异很大，用生成侧指标可以量化对比：

```python
# 对比不同模型的生成质量
models = ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"]

for model_name in models:
    answers = generate_answers_with_model(model_name, test_data)
    scores = {
        "Faithfulness": evaluate_faithfulness(answers, test_data),
        "Relevancy": evaluate_relevancy(answers, test_data),
        "Correctness": evaluate_correctness(answers, test_data),
    }
    print(f"{model_name}: {scores}")

# 预期结果（示意）：
# gpt-4o:       Faithfulness=0.95, Relevancy=0.93, Correctness=0.90
# gpt-4o-mini:  Faithfulness=0.88, Relevancy=0.90, Correctness=0.82
# gpt-3.5-turbo: Faithfulness=0.75, Relevancy=0.85, Correctness=0.70
```

### 应用3：Temperature 调优

Temperature 直接影响生成的"创造性"和"忠实度"之间的平衡：

```python
# 对比不同 temperature 对生成质量的影响
temperatures = [0.0, 0.3, 0.5, 0.7, 1.0]

for temp in temperatures:
    answers = generate_answers(test_data, temperature=temp)
    faith = evaluate_faithfulness(answers, test_data)
    relevancy = evaluate_relevancy(answers, test_data)
    print(f"temperature={temp}: Faithfulness={faith:.3f}, Relevancy={relevancy:.3f}")

# 预期趋势：
# temperature=0.0: Faithfulness=0.95, Relevancy=0.88  ← 最忠实，但可能过于死板
# temperature=0.3: Faithfulness=0.92, Relevancy=0.91  ← 推荐的平衡点
# temperature=0.5: Faithfulness=0.87, Relevancy=0.90
# temperature=0.7: Faithfulness=0.80, Relevancy=0.88
# temperature=1.0: Faithfulness=0.68, Relevancy=0.82  ← 创造性高，但幻觉多
```

**结论：** 对于 RAG 系统，推荐 temperature 设置在 0.1-0.3 之间，优先保证忠实度。

---

## 一句话记住

**生成评估的核心就三个问题：答得真不真（Faithfulness）、答得对不对题（Relevancy）、答得准不准（Correctness）——搞清楚这三个，生成优化就有方向了。**

---

## 学习检查清单

- [ ] 理解为什么需要单独评估生成质量（检索好不代表生成好）
- [ ] 掌握 Faithfulness 的定义和计算方式（陈述拆分 + 逐一验证）
- [ ] 掌握 Answer Relevancy 的定义和计算方式（反向生成问题 + 相似度）
- [ ] 掌握 Answer Correctness 的定义和计算方式（语义相似度 + LLM 评分）
- [ ] 理解 Answer Completeness 的含义（要点覆盖率）
- [ ] 理解 LLM-as-Judge 方法的优缺点
- [ ] 知道如何根据各指标的分数定位问题并优化
- [ ] 了解自动评估和人工评估的适用场景

---

## 下一步学习建议

1. **动手实践**：用本文的代码评估自己的 RAG 系统，看看各项指标的分数
2. **对比实验**：修改 Prompt 或 Temperature，观察指标变化
3. **学习 RAGAS 框架**：了解如何用 RAGAS 一键运行所有评估指标
4. **建立评估流水线**：将评估集成到 CI/CD 中，每次改动自动评估