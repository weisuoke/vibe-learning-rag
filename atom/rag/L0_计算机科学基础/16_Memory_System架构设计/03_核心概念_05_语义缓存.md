# 核心概念：语义缓存

> **理解基于向量相似度的智能缓存**

---

## 概念定义

**语义缓存（Semantic Cache）= 基于向量相似度匹配的缓存系统，而不是精确字符串匹配**

### 核心思想

传统缓存的问题：
```python
cache = {}
cache["什么是 RAG？"] = "RAG 是检索增强生成..."

# 问题：稍微改变问法就无法命中缓存
cache.get("什么是 RAG？")  # ✅ 命中
cache.get("RAG 是什么？")  # ❌ 未命中（但语义相同！）
cache.get("请解释 RAG")    # ❌ 未命中（但语义相同！）
```

语义缓存的解决方案：
```python
# 使用向量相似度匹配
semantic_cache.put("什么是 RAG？", "RAG 是检索增强生成...")

semantic_cache.get("RAG 是什么？")  # ✅ 命中（相似度 0.98）
semantic_cache.get("请解释 RAG")    # ✅ 命中（相似度 0.96）
```

---

## 工作原理

### 1. Embedding（向量化）

**将文本转换为高维向量**：

```python
from openai import OpenAI

client = OpenAI()

def get_embedding(text: str) -> list[float]:
    """获取文本的向量表示"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# 示例
emb1 = get_embedding("什么是 RAG？")
emb2 = get_embedding("RAG 是什么？")

print(f"向量维度: {len(emb1)}")  # 1536
print(f"向量类型: {type(emb1)}")  # list
```

### 2. 相似度计算

**使用余弦相似度衡量语义相似性**：

```python
import numpy as np

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """计算余弦相似度"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# 示例
emb1 = np.array(get_embedding("什么是 RAG？"))
emb2 = np.array(get_embedding("RAG 是什么？"))
emb3 = np.array(get_embedding("什么是向量数据库？"))

sim_12 = cosine_similarity(emb1, emb2)
sim_13 = cosine_similarity(emb1, emb3)

print(f"相似度（相同语义）: {sim_12:.3f}")  # 0.98
print(f"相似度（不同语义）: {sim_13:.3f}")  # 0.65
```

### 3. 缓存匹配

**查找最相似的缓存项**：

```python
class SemanticCache:
    """语义缓存"""
    def __init__(self, threshold: float = 0.95):
        self.cache = []  # [(embedding, question, answer), ...]
        self.threshold = threshold

    def get(self, question: str) -> str | None:
        """查询缓存"""
        query_emb = np.array(get_embedding(question))

        best_match = None
        best_similarity = 0.0

        for cached_emb, cached_q, cached_a in self.cache:
            similarity = cosine_similarity(query_emb, cached_emb)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = (cached_q, cached_a)

        if best_similarity >= self.threshold:
            print(f"✅ 缓存命中 (相似度: {best_similarity:.3f})")
            print(f"   原问题: {best_match[0]}")
            print(f"   新问题: {question}")
            return best_match[1]

        return None

    def put(self, question: str, answer: str):
        """存入缓存"""
        embedding = np.array(get_embedding(question))
        self.cache.append((embedding, question, answer))
```

---

## 完整实现

### 基础版本

```python
from openai import OpenAI
import numpy as np

client = OpenAI()

class BasicSemanticCache:
    """基础语义缓存"""
    def __init__(self, threshold: float = 0.95):
        self.cache = []
        self.threshold = threshold
        self.hits = 0
        self.misses = 0

    def _get_embedding(self, text: str) -> np.ndarray:
        """获取文本向量"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding)

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """计算余弦相似度"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    def get(self, question: str) -> str | None:
        """查询缓存"""
        if not self.cache:
            self.misses += 1
            return None

        query_emb = self._get_embedding(question)

        best_match = None
        best_similarity = 0.0

        for cached_emb, cached_q, cached_a in self.cache:
            similarity = self._cosine_similarity(query_emb, cached_emb)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = cached_a

        if best_similarity >= self.threshold:
            self.hits += 1
            print(f"✅ 语义缓存命中 (相似度: {best_similarity:.3f})")
            return best_match

        self.misses += 1
        return None

    def put(self, question: str, answer: str):
        """存入缓存"""
        embedding = self._get_embedding(question)
        self.cache.append((embedding, question, answer))

    def hit_rate(self) -> float:
        """计算命中率"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

# 使用示例
cache = BasicSemanticCache(threshold=0.95)

# 第一次提问
answer = cache.get("什么是 RAG？")
if answer is None:
    answer = "RAG 是检索增强生成..."
    cache.put("什么是 RAG？", answer)

# 不同问法，但语义相同
answer1 = cache.get("RAG 是什么？")  # ✅ 命中
answer2 = cache.get("请解释 RAG")    # ✅ 命中
answer3 = cache.get("什么是向量数据库？")  # ❌ 未命中

print(f"命中率: {cache.hit_rate():.2%}")
```

---

## Redis 集成版本

### 使用 Redis 存储

```python
import redis
import json
import numpy as np
from openai import OpenAI

client = OpenAI()

class RedisSemanticCache:
    """基于 Redis 的语义缓存"""
    def __init__(self, redis_client: redis.Redis, threshold: float = 0.95):
        self.redis = redis_client
        self.threshold = threshold

    def _get_embedding(self, text: str) -> np.ndarray:
        """获取文本向量"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding)

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """计算余弦相似度"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    def get(self, question: str) -> str | None:
        """查询缓存"""
        query_emb = self._get_embedding(question)

        # 获取所有缓存的键
        keys = self.redis.keys("semantic:*")
        if not keys:
            return None

        best_match = None
        best_similarity = 0.0

        for key in keys:
            # 获取缓存数据
            data = self.redis.hgetall(key)
            cached_emb = np.frombuffer(data[b"embedding"], dtype=np.float32)
            cached_answer = data[b"answer"].decode()

            # 计算相似度
            similarity = self._cosine_similarity(query_emb, cached_emb)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = cached_answer

        if best_similarity >= self.threshold:
            print(f"✅ Redis 语义缓存命中 (相似度: {best_similarity:.3f})")
            return best_match

        return None

    def put(self, question: str, answer: str, ttl: int = 86400):
        """存入缓存（带过期时间）"""
        embedding = self._get_embedding(question)
        key = f"semantic:{hash(question)}"

        # 存储到 Redis
        self.redis.hset(key, mapping={
            "question": question,
            "answer": answer,
            "embedding": embedding.astype(np.float32).tobytes()
        })

        # 设置过期时间（24小时）
        self.redis.expire(key, ttl)

# 使用示例
redis_client = redis.Redis(host='localhost', port=6379)
cache = RedisSemanticCache(redis_client, threshold=0.95)

# 缓存问答
cache.put("什么是 RAG？", "RAG 是检索增强生成...")

# 查询缓存
answer = cache.get("RAG 是什么？")  # ✅ 命中
print(answer)
```

---

## ChromaDB 集成版本

### 使用向量数据库

```python
import chromadb
from openai import OpenAI

client = OpenAI()

class ChromaSemanticCache:
    """基于 ChromaDB 的语义缓存"""
    def __init__(self, threshold: float = 0.95):
        self.chroma_client = chromadb.Client()
        self.collection = self.chroma_client.create_collection(
            name="semantic_cache",
            metadata={"hnsw:space": "cosine"}
        )
        self.threshold = threshold

    def _get_embedding(self, text: str) -> list[float]:
        """获取文本向量"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

    def get(self, question: str) -> str | None:
        """查询缓存"""
        query_embedding = self._get_embedding(question)

        # 查询最相似的结果
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=1
        )

        if not results['documents'][0]:
            return None

        # 检查相似度
        distance = results['distances'][0][0]
        similarity = 1 - distance  # ChromaDB 返回距离，需要转换为相似度

        if similarity >= self.threshold:
            print(f"✅ ChromaDB 语义缓存命中 (相似度: {similarity:.3f})")
            return results['documents'][0][0]

        return None

    def put(self, question: str, answer: str):
        """存入缓存"""
        embedding = self._get_embedding(question)

        self.collection.add(
            embeddings=[embedding],
            documents=[answer],
            metadatas=[{"question": question}],
            ids=[str(hash(question))]
        )

# 使用示例
cache = ChromaSemanticCache(threshold=0.95)

# 缓存问答
cache.put("什么是 RAG？", "RAG 是检索增强生成...")

# 查询缓存
answer = cache.get("RAG 是什么？")  # ✅ 命中
print(answer)
```

---

## AI Agent 应用

### 1. LLM 调用缓存

```python
from openai import OpenAI

client = OpenAI()

class LLMWithSemanticCache:
    """带语义缓存的 LLM 调用"""
    def __init__(self, threshold: float = 0.95):
        self.cache = BasicSemanticCache(threshold=threshold)
        self.api_calls = 0
        self.cache_hits = 0

    def ask(self, question: str) -> str:
        """提问（带缓存）"""
        # 先查缓存
        cached_answer = self.cache.get(question)
        if cached_answer:
            self.cache_hits += 1
            return cached_answer

        # 缓存未命中，调用 LLM
        self.api_calls += 1
        print(f"❌ 缓存未命中，调用 LLM (第 {self.api_calls} 次)")

        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": question}]
        )

        answer = response.choices[0].message.content

        # 存入缓存
        self.cache.put(question, answer)

        return answer

    def stats(self) -> dict:
        """统计信息"""
        total = self.api_calls + self.cache_hits
        return {
            "api_calls": self.api_calls,
            "cache_hits": self.cache_hits,
            "hit_rate": self.cache_hits / total if total > 0 else 0.0,
            "cost_saved": f"${self.cache_hits * 0.01:.2f}"  # 假设每次调用 $0.01
        }

# 使用示例
llm = LLMWithSemanticCache(threshold=0.95)

# 第一次提问
answer1 = llm.ask("什么是 RAG？")

# 不同问法，但语义相同
answer2 = llm.ask("RAG 是什么？")  # ✅ 缓存命中
answer3 = llm.ask("请解释 RAG")    # ✅ 缓存命中
answer4 = llm.ask("什么是向量数据库？")  # ❌ 缓存未命中

# 统计
stats = llm.stats()
print(f"API 调用: {stats['api_calls']} 次")
print(f"缓存命中: {stats['cache_hits']} 次")
print(f"命中率: {stats['hit_rate']:.2%}")
print(f"节省成本: {stats['cost_saved']}")
```

### 2. RAG 检索缓存

```python
class RAGWithSemanticCache:
    """带语义缓存的 RAG 系统"""
    def __init__(self, vector_db, threshold: float = 0.95):
        self.vector_db = vector_db
        self.cache = ChromaSemanticCache(threshold=threshold)

    def retrieve(self, query: str, top_k: int = 5) -> list:
        """检索文档（带缓存）"""
        # 先查缓存
        cached_docs = self.cache.get(query)
        if cached_docs:
            return json.loads(cached_docs)

        # 缓存未命中，执行检索
        print(f"❌ 缓存未命中，执行向量检索")
        docs = self.vector_db.similarity_search(query, k=top_k)

        # 存入缓存
        self.cache.put(query, json.dumps([doc.page_content for doc in docs]))

        return docs

    def answer(self, question: str) -> str:
        """回答问题（RAG）"""
        # 检索相关文档
        docs = self.retrieve(question)

        # 构建上下文
        context = "\n\n".join([doc.page_content for doc in docs])

        # 调用 LLM
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": f"基于以下上下文回答问题:\n{context}"},
                {"role": "user", "content": question}
            ]
        )

        return response.choices[0].message.content

# 使用示例
rag = RAGWithSemanticCache(vector_db, threshold=0.95)

# 第一次提问
answer1 = rag.answer("什么是 RAG？")

# 相似问题，检索结果可以复用
answer2 = rag.answer("RAG 是什么？")  # ✅ 检索缓存命中
```

---

## 性能优化

### 1. 批量 Embedding

```python
def batch_get_embeddings(texts: list[str]) -> list[np.ndarray]:
    """批量获取向量（提升性能）"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [np.array(emb.embedding) for emb in response.data]

# 使用示例
questions = ["什么是 RAG？", "什么是向量数据库？", "什么是 LLM？"]
embeddings = batch_get_embeddings(questions)

# 批量存入缓存
for question, embedding in zip(questions, embeddings):
    cache.cache.append((embedding, question, f"答案：{question}"))
```

### 2. 向量索引

```python
import faiss

class FAISSSemanticCache:
    """使用 FAISS 加速的语义缓存"""
    def __init__(self, dimension: int = 1536, threshold: float = 0.95):
        self.dimension = dimension
        self.threshold = threshold
        self.index = faiss.IndexFlatIP(dimension)  # 内积索引（余弦相似度）
        self.cache = []  # [(question, answer), ...]

    def put(self, question: str, answer: str):
        """存入缓存"""
        embedding = self._get_embedding(question)

        # 归一化（用于余弦相似度）
        embedding = embedding / np.linalg.norm(embedding)

        # 添加到 FAISS 索引
        self.index.add(embedding.reshape(1, -1).astype(np.float32))

        # 添加到缓存
        self.cache.append((question, answer))

    def get(self, question: str) -> str | None:
        """查询缓存（使用 FAISS 加速）"""
        if self.index.ntotal == 0:
            return None

        query_emb = self._get_embedding(question)
        query_emb = query_emb / np.linalg.norm(query_emb)

        # FAISS 搜索
        similarities, indices = self.index.search(
            query_emb.reshape(1, -1).astype(np.float32),
            k=1
        )

        similarity = similarities[0][0]
        if similarity >= self.threshold:
            idx = indices[0][0]
            print(f"✅ FAISS 缓存命中 (相似度: {similarity:.3f})")
            return self.cache[idx][1]

        return None

# 使用示例
cache = FAISSSemanticCache(dimension=1536, threshold=0.95)

# 批量添加
for i in range(1000):
    cache.put(f"问题 {i}", f"答案 {i}")

# 快速查询
answer = cache.get("问题 500")  # 毫秒级响应
```

### 3. 缓存预热

```python
def warm_up_cache(cache: SemanticCache, common_questions: list[str]):
    """缓存预热"""
    print(f"预热缓存: {len(common_questions)} 个常见问题")

    for question in common_questions:
        # 调用 LLM 生成答案
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": question}]
        )
        answer = response.choices[0].message.content

        # 存入缓存
        cache.put(question, answer)

    print("✅ 缓存预热完成")

# 使用示例
common_questions = [
    "什么是 RAG？",
    "什么是向量数据库？",
    "什么是 LLM？",
    "如何实现 LRU 缓存？",
    "Python 如何异步编程？"
]

warm_up_cache(cache, common_questions)
```

---

## 最佳实践

### 1. 阈值选择

```python
def find_optimal_threshold(cache: SemanticCache, test_data: list):
    """找到最优阈值"""
    thresholds = [0.90, 0.92, 0.94, 0.95, 0.96, 0.98]

    for threshold in thresholds:
        cache.threshold = threshold
        hits = 0
        false_positives = 0

        for question, expected_answer in test_data:
            answer = cache.get(question)
            if answer:
                if answer == expected_answer:
                    hits += 1
                else:
                    false_positives += 1

        print(f"阈值: {threshold:.2f} | 命中: {hits} | 误报: {false_positives}")

# 推荐阈值：
# - 0.95: 平衡性能和准确性
# - 0.98: 高准确性，低命中率
# - 0.90: 高命中率，可能误报
```

### 2. 缓存淘汰

```python
class SemanticCacheWithEviction:
    """带淘汰策略的语义缓存"""
    def __init__(self, max_size: int = 1000, threshold: float = 0.95):
        self.cache = []
        self.max_size = max_size
        self.threshold = threshold
        self.access_count = {}

    def put(self, question: str, answer: str):
        """存入缓存（带淘汰）"""
        if len(self.cache) >= self.max_size:
            # 淘汰访问次数最少的
            min_access_idx = min(
                range(len(self.cache)),
                key=lambda i: self.access_count.get(i, 0)
            )
            del self.cache[min_access_idx]
            del self.access_count[min_access_idx]

        embedding = self._get_embedding(question)
        self.cache.append((embedding, question, answer))
        self.access_count[len(self.cache) - 1] = 0

    def get(self, question: str) -> str | None:
        """查询缓存"""
        # ... 查询逻辑 ...
        if match_found:
            self.access_count[match_idx] += 1  # 增加访问次数
            return answer
```

### 3. 监控指标

```python
class MonitoredSemanticCache(BasicSemanticCache):
    """带监控的语义缓存"""
    def __init__(self, threshold: float = 0.95):
        super().__init__(threshold)
        self.similarity_scores = []

    def get(self, question: str) -> str | None:
        """查询缓存（记录相似度）"""
        # ... 查询逻辑 ...
        if best_similarity > 0:
            self.similarity_scores.append(best_similarity)

        return super().get(question)

    def stats(self) -> dict:
        """统计信息"""
        return {
            "hit_rate": self.hit_rate(),
            "avg_similarity": np.mean(self.similarity_scores) if self.similarity_scores else 0,
            "min_similarity": min(self.similarity_scores) if self.similarity_scores else 0,
            "max_similarity": max(self.similarity_scores) if self.similarity_scores else 0
        }
```

---

## 参考资源

### 2025-2026 最新研究
- [Redis Semantic Caching](https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis) - Redis 语义缓存实现
- [DeepLearning.AI: Building Applications with Vector Databases](https://www.deeplearning.ai/short-courses/building-applications-vector-databases/) - 向量数据库应用
- [LangChain Semantic Cache](https://python.langchain.com/docs/integrations/llm_caching) - LangChain 缓存集成

### 实现参考
- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings) - Embedding 生成
- [ChromaDB Documentation](https://docs.trychroma.com/) - 向量数据库
- [FAISS Documentation](https://github.com/facebookresearch/faiss) - 向量索引加速
