# 第一性原理

## 什么是第一性原理？

**第一性原理**：回到事物最基本的真理，从源头思考问题

不是问"别人怎么分块"，而是问"为什么需要分块"。

---

## 文本分块的第一性原理

### 1. 最基础的定义

**文本分块 = 将长文本切分成多个短文本片段**

仅此而已！没有更基础的了。

```python
# 最简单的分块
long_text = "这是一段很长的文本..."
chunks = [long_text[i:i+100] for i in range(0, len(long_text), 100)]
```

### 2. 为什么需要文本分块？

**核心问题：长文档无法直接用于精准检索**

让我们从三个约束条件推导：

#### 约束1：Embedding 模型有输入长度限制

```
┌─────────────────────────────────────────┐
│  Embedding 模型的 Token 限制            │
│                                         │
│  text-embedding-3-small: 8191 tokens    │
│  text-embedding-ada-002: 8191 tokens    │
│  bge-large-zh: 512 tokens               │
│                                         │
│  一本书可能有 10万+ tokens              │
│  → 必须切分才能向量化                    │
└─────────────────────────────────────────┘
```

#### 约束2：LLM 的 Context Window 有限

```
┌─────────────────────────────────────────┐
│  LLM Context Window 限制                │
│                                         │
│  GPT-4: 8K / 32K / 128K tokens          │
│  Claude: 100K / 200K tokens             │
│                                         │
│  即使能放下，太长的上下文：              │
│  - 成本高（按 token 计费）              │
│  - 效果差（Lost in the Middle 问题）    │
│  → 只能放入最相关的片段                  │
└─────────────────────────────────────────┘
```

#### 约束3：检索需要精准匹配

```
用户问题："Python 的 GIL 是什么？"

整本《Python 编程》作为一个向量？
→ 向量代表整本书的"平均语义"
→ 无法精准匹配到 GIL 那一章

分成小块后：
→ "GIL 章节"的向量专门代表 GIL 内容
→ 可以精准匹配
```

### 3. 文本分块的三层价值

#### 价值1：突破技术限制

```python
# 问题：文档太长，无法一次向量化
document = "..." * 100000  # 10万字的文档

# 解决：分块后逐个向量化
chunks = split_into_chunks(document, chunk_size=500)
vectors = [embed(chunk) for chunk in chunks]
```

#### 价值2：提升检索精度

```
┌─────────────────────────────────────────────────────┐
│  整本书作为一个向量 vs 分章节作为多个向量            │
│                                                     │
│  问题："如何处理 Python 异常？"                      │
│                                                     │
│  整本书向量：                                        │
│  [0.1, 0.2, 0.3, ...]  ← 代表整本书的"平均"语义     │
│  相似度：0.65（不够精准）                            │
│                                                     │
│  异常处理章节向量：                                  │
│  [0.8, 0.1, 0.9, ...]  ← 专门代表异常处理           │
│  相似度：0.92（精准匹配）                            │
└─────────────────────────────────────────────────────┘
```

#### 价值3：控制上下文成本

```python
# 不分块：把整本书塞给 LLM
cost = len(whole_book) * price_per_token  # 很贵！

# 分块后：只传入相关片段
relevant_chunks = retrieve_top_k(query, k=3)
cost = sum(len(c) for c in relevant_chunks) * price_per_token  # 便宜很多
```

### 4. 从第一性原理推导分块策略

**推理链：**

```
1. 文档太长，无法直接处理
   ↓
2. 需要切分成小块
   ↓
3. 切分点在哪里？
   ↓
4. 方案A：固定字符数切分（简单但可能切断语义）
   方案B：按语义边界切分（复杂但保持完整性）
   ↓
5. 块大小多少合适？
   ↓
6. 太小：上下文不完整，检索到也没用
   太大：噪音多，检索不精准
   ↓
7. 需要在"精度"和"完整性"之间找平衡
   ↓
8. 不同场景需要不同的平衡点
   ↓
9. 结论：没有万能的分块策略，需要根据场景选择
```

### 5. 分块的本质权衡

```
                    检索精度
                       ↑
                       │
        ┌──────────────┼──────────────┐
        │              │              │
        │   小块区域    │   理想区域    │
        │  精度高       │  精度+完整    │
        │  完整性差     │              │
        │              │              │
────────┼──────────────┼──────────────┼────→ 上下文完整性
        │              │              │
        │   最差区域    │   大块区域    │
        │  两者都差     │  完整性好     │
        │              │  精度差       │
        │              │              │
        └──────────────┼──────────────┘
                       │
```

**核心洞察：**
- 分块大小是精度和完整性的权衡
- 分块策略是简单性和效果的权衡
- 没有完美方案，只有适合场景的方案

### 6. 一句话总结第一性原理

**文本分块的本质是在"检索精度"和"上下文完整性"之间寻找平衡，通过控制信息粒度来优化 RAG 系统的整体效果。**

---

## 第一性原理的实践指导

| 原理 | 实践指导 |
|------|----------|
| 长文档无法直接检索 | 必须分块，没有例外 |
| 分块是精度与完整性的权衡 | 根据场景调整块大小 |
| 切分点影响语义完整性 | 优先在自然边界切分 |
| 没有万能策略 | 需要实验和评估 |

---

**下一步：** [03_核心概念](./03_核心概念.md) - 掌握三种核心分块策略
