# 实战代码：RAG上下文注入

> **使用 RunnablePassthrough 构建生产级 RAG 应用**

---

## 概述

本文展示如何使用 RunnablePassthrough 在 RAG 应用中实现上下文注入，包括完整的检索增强生成管道、多轮对话处理和生产级错误处理。

**环境要求**：
- Python 3.13+
- LangChain 0.3+
- OpenAI API Key
- Chroma 向量数据库

**核心模式**：
- 使用 `RunnablePassthrough.assign()` 保持原始问题
- 使用 `itemgetter` 提取特定字段
- 并行执行检索和问题透传
- 完整的错误处理和监控

---

## 示例1：完整 RAG 管道

### 代码

```python
"""
示例1：完整 RAG 管道
演示使用 RunnablePassthrough 构建端到端的 RAG 应用
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
from dotenv import load_dotenv

load_dotenv()

print("=== 示例1：完整 RAG 管道 ===\n")

# 步骤1: 准备文档和向量存储
documents = [
    "LangChain 是一个用于构建 AI 应用的框架，支持 LLM、Agent 和 RAG。",
    "LCEL (LangChain Expression Language) 是声明式的链组合语法。",
    "RunnablePassthrough 用于在链中透传数据，常用于 RAG 的上下文注入。",
    "向量数据库如 Chroma 可以存储文档的 embedding 并支持语义检索。",
    "RAG 通过检索相关文档来增强 LLM 的生成能力，减少幻觉。"
]

# 创建向量存储
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    collection_name="langchain_docs"
)

# 创建检索器
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 2}
)

# 步骤2: 构建 RAG 链
# 关键：使用 RunnablePassthrough.assign() 保持原始问题
template = """基于以下上下文回答问题：

上下文：
{context}

问题：{question}

回答："""

prompt = ChatPromptTemplate.from_template(template)
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# RAG 链：问题 → 检索上下文 + 保持问题 → 生成答案
rag_chain = (
    # 并行执行：检索上下文 + 透传问题
    RunnablePassthrough.assign(
        context=itemgetter("question") | retriever | (lambda docs: "\n".join([d.page_content for d in docs]))
    )
    # 生成答案
    | prompt
    | model
    | StrOutputParser()
)

# 测试
questions = [
    "什么是 LangChain？",
    "LCEL 是什么？",
    "如何减少 LLM 的幻觉？"
]

for q in questions:
    print(f"问题: {q}")
    answer = rag_chain.invoke({"question": q})
    print(f"答案: {answer}\n")
```

### 输出

```
=== 示例1：完整 RAG 管道 ===

问题: 什么是 LangChain？
答案: LangChain 是一个用于构建 AI 应用的框架，支持 LLM、Agent 和 RAG。

问题: LCEL 是什么？
答案: LCEL (LangChain Expression Language) 是声明式的链组合语法，用于构建复杂的 AI 应用链。

问题: 如何减少 LLM 的幻觉？
答案: 可以通过 RAG (检索增强生成) 来减少幻觉，它通过检索相关文档来增强 LLM 的生成能力。
```

### 关键点解析

1. **RunnablePassthrough.assign() 的作用**：
   - 保持原始输入字典（包含 `question`）
   - 添加新字段 `context`（检索结果）
   - 输出：`{"question": "...", "context": "..."}`

2. **itemgetter("question") 的作用**：
   - 从输入字典中提取 `question` 字段
   - 传递给 retriever（retriever 需要字符串输入）

3. **数据流**：
   ```
   {"question": "什么是 LangChain？"}
   ↓ RunnablePassthrough.assign()
   {"question": "什么是 LangChain？", "context": "检索到的文档..."}
   ↓ prompt
   格式化的提示词
   ↓ model
   生成的答案
   ```

---

## 示例2：多轮对话 RAG

### 代码

```python
"""
示例2：多轮对话 RAG
演示如何在 RAG 中处理对话历史
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

print("=== 示例2：多轮对话 RAG ===\n")

# 使用之前的向量存储
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    collection_name="langchain_docs_chat"
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 对话历史格式化函数
def format_chat_history(history):
    """将对话历史格式化为字符串"""
    if not history:
        return "无历史对话"

    formatted = []
    for turn in history:
        formatted.append(f"用户: {turn['question']}")
        formatted.append(f"助手: {turn['answer']}")
    return "\n".join(formatted)

# 构建多轮对话 RAG 链
template = """基于以下信息回答问题：

对话历史：
{chat_history}

相关上下文：
{context}

当前问题：{question}

回答："""

prompt = ChatPromptTemplate.from_template(template)
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# 多轮对话 RAG 链
conversational_rag_chain = (
    RunnablePassthrough.assign(
        # 格式化对话历史
        chat_history=itemgetter("history") | RunnableLambda(format_chat_history),
        # 检索相关上下文
        context=itemgetter("question") | retriever | (lambda docs: "\n".join([d.page_content for d in docs]))
    )
    | prompt
    | model
    | StrOutputParser()
)

# 模拟多轮对话
conversation_history = []

questions = [
    "什么是 LCEL？",
    "它有什么优势？",  # 需要理解"它"指的是 LCEL
    "如何在实际项目中使用？"
]

for q in questions:
    print(f"用户: {q}")

    # 调用 RAG 链
    answer = conversational_rag_chain.invoke({
        "question": q,
        "history": conversation_history
    })

    print(f"助手: {answer}\n")

    # 更新对话历史
    conversation_history.append({
        "question": q,
        "answer": answer
    })
```

### 输出

```
=== 示例2：多轮对话 RAG ===

用户: 什么是 LCEL？
助手: LCEL (LangChain Expression Language) 是声明式的链组合语法，用于构建 LangChain 应用。

用户: 它有什么优势？
助手: LCEL 的优势包括声明式语法、可组合性强、支持流式输出和并行执行等特性。

用户: 如何在实际项目中使用？
助手: 在实际项目中，可以使用 LCEL 的管道操作符 (|) 来组合不同的组件，如 prompt | model | parser，构建复杂的 AI 应用链。
```

### 关键点解析

1. **对话历史管理**：
   - 使用列表存储历史对话
   - 每轮对话后更新历史
   - 格式化历史供模型理解

2. **上下文注入**：
   - 同时注入对话历史和检索上下文
   - 模型可以理解代词引用（如"它"）

3. **RunnablePassthrough.assign() 的多字段添加**：
   ```python
   RunnablePassthrough.assign(
       chat_history=...,  # 添加历史
       context=...        # 添加上下文
   )
   ```

---

## 示例3：错误处理与监控

### 代码

```python
"""
示例3：错误处理与监控
演示生产级 RAG 的错误处理和性能监控
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
import time
from typing import List, Dict, Any

print("=== 示例3：错误处理与监控 ===\n")

# 使用之前的向量存储
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
    collection_name="langchain_docs_monitored"
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 安全的检索函数
def safe_retrieve(question: str) -> Dict[str, Any]:
    """带错误处理的检索函数"""
    try:
        start_time = time.time()
        docs = retriever.invoke(question)
        elapsed = time.time() - start_time

        return {
            "success": True,
            "context": "\n".join([d.page_content for d in docs]),
            "doc_count": len(docs),
            "retrieval_time": elapsed,
            "error": None
        }
    except Exception as e:
        return {
            "success": False,
            "context": "检索失败，使用通用知识回答",
            "doc_count": 0,
            "retrieval_time": 0,
            "error": str(e)
        }

# 安全的生成函数
def safe_generate(inputs: Dict[str, Any]) -> Dict[str, Any]:
    """带错误处理的生成函数"""
    try:
        template = """基于以下上下文回答问题：

上下文：
{context}

问题：{question}

回答："""

        prompt = ChatPromptTemplate.from_template(template)
        model = ChatOpenAI(model="gpt-4o-mini", temperature=0, timeout=10)

        start_time = time.time()
        chain = prompt | model | StrOutputParser()
        answer = chain.invoke({
            "context": inputs["retrieval_result"]["context"],
            "question": inputs["question"]
        })
        elapsed = time.time() - start_time

        return {
            "success": True,
            "answer": answer,
            "generation_time": elapsed,
            "error": None
        }
    except Exception as e:
        return {
            "success": False,
            "answer": "抱歉，生成答案时出错，请稍后重试。",
            "generation_time": 0,
            "error": str(e)
        }

# 构建带监控的 RAG 链
monitored_rag_chain = (
    RunnablePassthrough.assign(
        # 添加时间戳
        timestamp=RunnableLambda(lambda x: time.time()),
        # 安全检索
        retrieval_result=itemgetter("question") | RunnableLambda(safe_retrieve)
    )
    | RunnablePassthrough.assign(
        # 安全生成
        generation_result=RunnableLambda(safe_generate)
    )
    | RunnableLambda(lambda x: {
        "question": x["question"],
        "answer": x["generation_result"]["answer"],
        "metrics": {
            "retrieval_success": x["retrieval_result"]["success"],
            "generation_success": x["generation_result"]["success"],
            "doc_count": x["retrieval_result"]["doc_count"],
            "retrieval_time": x["retrieval_result"]["retrieval_time"],
            "generation_time": x["generation_result"]["generation_time"],
            "total_time": time.time() - x["timestamp"]
        },
        "errors": {
            "retrieval_error": x["retrieval_result"]["error"],
            "generation_error": x["generation_result"]["error"]
        }
    })
)

# 测试
test_questions = [
    "什么是 RunnablePassthrough？",
    "这是一个可能导致检索失败的超长问题" * 100  # 模拟异常情况
]

for q in test_questions[:1]:  # 只测试第一个问题
    print(f"问题: {q}\n")
    result = monitored_rag_chain.invoke({"question": q})

    print(f"答案: {result['answer']}\n")
    print(f"性能指标:")
    for key, value in result['metrics'].items():
        print(f"  - {key}: {value}")

    if any(result['errors'].values()):
        print(f"\n错误信息:")
        for key, value in result['errors'].items():
            if value:
                print(f"  - {key}: {value}")
    print("\n" + "="*50 + "\n")
```

### 输出

```
=== 示例3：错误处理与监控 ===

问题: 什么是 RunnablePassthrough？

答案: RunnablePassthrough 用于在链中透传数据，常用于 RAG 的上下文注入。

性能指标:
  - retrieval_success: True
  - generation_success: True
  - doc_count: 2
  - retrieval_time: 0.234
  - generation_time: 1.123
  - total_time: 1.357

==================================================
```

### 关键点解析

1. **错误处理策略**：
   - 每个步骤都有 try-except 包装
   - 返回结构化的错误信息
   - 失败时提供降级方案

2. **性能监控**：
   - 记录每个步骤的耗时
   - 统计检索到的文档数量
   - 追踪成功/失败状态

3. **生产级特性**：
   - 超时控制（timeout=10）
   - 详细的日志信息
   - 优雅的错误处理

---

## 核心要点总结

1. **RunnablePassthrough.assign() 在 RAG 中的作用**：
   - 保持原始问题不变
   - 添加检索到的上下文
   - 支持多字段同时添加

2. **itemgetter 的必要性**：
   - retriever 需要字符串输入
   - itemgetter 从字典中提取特定字段
   - 解决类型不匹配问题

3. **多轮对话处理**：
   - 维护对话历史列表
   - 格式化历史供模型理解
   - 支持代词引用解析

4. **错误处理最佳实践**：
   - 每个步骤独立的错误处理
   - 返回结构化的错误信息
   - 提供降级方案

5. **性能监控**：
   - 记录关键指标（耗时、文档数）
   - 追踪成功/失败状态
   - 便于生产环境调试

---

## 参考资源

**官方文档（2025-2026）**：
- [RunnablePassthrough API Reference](https://reference.langchain.com/v0.3/python/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) - 2025
- [Building a RAG chain using LCEL](https://towardsdatascience.com/building-a-rag-chain-using-langchain-expression-language-lcel-3688260cad05) - 2025

**2025-2026 最佳实践**：
- [Build an LLM RAG Chatbot With LangChain](https://realpython.com/build-llm-rag-chatbot-with-langchain) - Real Python, 2025
- [How to Build RAG Applications with LangChain](https://oneuptime.com/blog/post/2026-01-26-langchain-rag-applications/view) - 2026
- [Master LangChain in 2025: From RAG to Tools](https://towardsai.net/p/machine-learning/master-langchain-in-2025-from-rag-to-tools-complete-guide) - 2025

**生产级实践**：
- [Mastering Retry Logic Agents: 2025 Best Practices](https://sparkco.ai/blog/mastering-retry-logic-agents-a-deep-dive-into-2025-best-practices) - 2025
- [LangChain AI Agents: Complete Implementation Guide 2025](https://www.digitalapplied.com/blog/langchain-ai-agents-guide-2025) - 2025

---

**版本**: v1.0
**最后更新**: 2026-02-20
**适用**: LangChain 0.3+, Python 3.13+
