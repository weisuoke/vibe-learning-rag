# å®æˆ˜ä»£ç 3ï¼šç¾éš¾æ¢å¤å®Œæ•´æµç¨‹

æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•å®ç° Milvus é›†ç¾¤çš„å®Œæ•´ç¾éš¾æ¢å¤æµç¨‹ï¼ŒåŒ…æ‹¬å®šæœŸå¤‡ä»½ã€è·¨æ•°æ®ä¸­å¿ƒå¤åˆ¶ã€ç¾éš¾æ£€æµ‹å’Œè‡ªåŠ¨åˆ‡æ¢ã€‚

---

## åœºæ™¯è¯´æ˜

**ç›®æ ‡ï¼š** å®ç°ä¸€ä¸ªå®Œæ•´çš„ç¾éš¾æ¢å¤ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨ä¸»æ•°æ®ä¸­å¿ƒæ•…éšœæ—¶å¿«é€Ÿæ¢å¤æœåŠ¡

**åŒ…å«åŠŸèƒ½ï¼š**
1. è‡ªåŠ¨åŒ–æ¯æ—¥å¤‡ä»½åˆ°å¯¹è±¡å­˜å‚¨
2. è·¨æ•°æ®ä¸­å¿ƒæ•°æ®åŒæ­¥
3. ç¾éš¾æ£€æµ‹å’Œå‘Šè­¦
4. è‡ªåŠ¨ DNS åˆ‡æ¢åˆ°ç¾å¤‡é›†ç¾¤
5. æ•°æ®å®Œæ•´æ€§éªŒè¯

**RTO/RPO ç›®æ ‡ï¼š**
- RTO: < 1 å°æ—¶
- RPO: < 1 å°æ—¶ï¼ˆæ¯å°æ—¶åŒæ­¥ä¸€æ¬¡ï¼‰

---

## å®Œæ•´å®ç°ä»£ç 

### 1. ç¾éš¾æ¢å¤ç®¡ç†å™¨

```python
"""
Milvus ç¾éš¾æ¢å¤ç®¡ç†å™¨
"""
from pymilvus import connections, Collection, utility
import subprocess
import datetime
import time
import boto3
import requests
from typing import List, Dict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DisasterRecoveryManager:
    """ç¾éš¾æ¢å¤ç®¡ç†å™¨"""

    def __init__(self, config: Dict):
        self.primary_host = config['primary_host']
        self.dr_host = config['dr_host']
        self.backup_bucket = config['backup_bucket']
        self.dns_zone_id = config['dns_zone_id']
        self.domain_name = config['domain_name']
        self.port = config.get('port', '19530')

        # AWS å®¢æˆ·ç«¯
        self.s3 = boto3.client('s3')
        self.route53 = boto3.client('route53')

    def daily_backup(self) -> bool:
        """æ¯æ—¥å¤‡ä»½"""
        logger.info("=== å¼€å§‹æ¯æ—¥å¤‡ä»½ ===")

        try:
            # è¿æ¥åˆ°ä¸»é›†ç¾¤
            connections.connect(
                alias="primary",
                host=self.primary_host,
                port=self.port
            )

            # è·å–æ‰€æœ‰ Collection
            collections = utility.list_collections(using="primary")
            logger.info(f"å‘ç° {len(collections)} ä¸ª Collection")

            # å¤‡ä»½æ—¶é—´æˆ³
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"{self.backup_bucket}/daily/{timestamp}"

            # é€ä¸ªå¤‡ä»½
            for collection_name in collections:
                logger.info(f"å¤‡ä»½ {collection_name}...")

                # ä½¿ç”¨ Milvus Backup CLI
                cmd = [
                    "milvus-backup", "create",
                    "-n", f"{collection_name}_{timestamp}",
                    "-c", collection_name,
                    "-p", backup_path
                ]

                result = subprocess.run(cmd, capture_output=True, text=True)

                if result.returncode == 0:
                    logger.info(f"âœ… {collection_name} å¤‡ä»½æˆåŠŸ")
                else:
                    logger.error(f"âŒ {collection_name} å¤‡ä»½å¤±è´¥: {result.stderr}")
                    return False

            logger.info(f"âœ… æ¯æ—¥å¤‡ä»½å®Œæˆ: {backup_path}")
            return True

        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
            return False
        finally:
            connections.disconnect("primary")

    def hourly_sync(self) -> bool:
        """æ¯å°æ—¶åŒæ­¥åˆ°ç¾å¤‡é›†ç¾¤"""
        logger.info("=== å¼€å§‹æ¯å°æ—¶åŒæ­¥ ===")

        try:
            # è¿æ¥åˆ°ä¸»é›†ç¾¤å’Œç¾å¤‡é›†ç¾¤
            connections.connect("primary", host=self.primary_host, port=self.port)
            connections.connect("dr", host=self.dr_host, port=self.port)

            # è·å–ä¸»é›†ç¾¤çš„ Collection åˆ—è¡¨
            primary_collections = utility.list_collections(using="primary")

            for collection_name in primary_collections:
                logger.info(f"åŒæ­¥ {collection_name}...")

                # è·å–ä¸»é›†ç¾¤æ•°æ®
                primary_collection = Collection(collection_name, using="primary")
                primary_count = primary_collection.num_entities

                # æ£€æŸ¥ç¾å¤‡é›†ç¾¤æ˜¯å¦å­˜åœ¨è¯¥ Collection
                if not utility.has_collection(collection_name, using="dr"):
                    logger.info(f"  ç¾å¤‡é›†ç¾¤ä¸å­˜åœ¨ {collection_name}ï¼Œåˆ›å»ºä¸­...")
                    # å¤åˆ¶ Schema
                    self._replicate_collection_schema(collection_name)

                # å¢é‡åŒæ­¥æ•°æ®
                dr_collection = Collection(collection_name, using="dr")
                dr_count = dr_collection.num_entities

                if primary_count > dr_count:
                    logger.info(f"  åŒæ­¥å¢é‡æ•°æ®: {primary_count - dr_count} æ¡")
                    self._sync_incremental_data(collection_name, dr_count, primary_count)
                else:
                    logger.info(f"  æ•°æ®å·²åŒæ­¥ï¼Œæ— éœ€æ›´æ–°")

            logger.info("âœ… æ¯å°æ—¶åŒæ­¥å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"âŒ åŒæ­¥å¤±è´¥: {e}")
            return False
        finally:
            connections.disconnect("primary")
            connections.disconnect("dr")

    def _replicate_collection_schema(self, collection_name: str):
        """å¤åˆ¶ Collection Schema"""
        from pymilvus import FieldSchema, CollectionSchema, DataType

        # è·å–ä¸»é›†ç¾¤çš„ Schema
        primary_collection = Collection(collection_name, using="primary")
        schema = primary_collection.schema

        # åœ¨ç¾å¤‡é›†ç¾¤åˆ›å»ºç›¸åŒçš„ Collection
        Collection(name=collection_name, schema=schema, using="dr")

    def _sync_incremental_data(self, collection_name: str, start_offset: int, end_offset: int):
        """åŒæ­¥å¢é‡æ•°æ®"""
        batch_size = 1000
        offset = start_offset

        primary_collection = Collection(collection_name, using="primary")
        dr_collection = Collection(collection_name, using="dr")

        while offset < end_offset:
            # ä»ä¸»é›†ç¾¤æŸ¥è¯¢æ•°æ®
            results = primary_collection.query(
                expr="",
                output_fields=["*"],
                limit=batch_size,
                offset=offset
            )

            if not results:
                break

            # æ’å…¥åˆ°ç¾å¤‡é›†ç¾¤
            dr_collection.insert(results)

            offset += len(results)

    def check_primary_health(self) -> bool:
        """æ£€æŸ¥ä¸»é›†ç¾¤å¥åº·çŠ¶æ€"""
        try:
            response = requests.get(
                f"http://{self.primary_host}:9091/healthz",
                timeout=5
            )
            return response.status_code == 200
        except Exception as e:
            logger.error(f"ä¸»é›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def check_dr_health(self) -> bool:
        """æ£€æŸ¥ç¾å¤‡é›†ç¾¤å¥åº·çŠ¶æ€"""
        try:
            response = requests.get(
                f"http://{self.dr_host}:9091/healthz",
                timeout=5
            )
            return response.status_code == 200
        except Exception as e:
            logger.error(f"ç¾å¤‡é›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def switch_to_dr(self) -> bool:
        """åˆ‡æ¢åˆ°ç¾å¤‡é›†ç¾¤"""
        logger.info("=== å¼€å§‹åˆ‡æ¢åˆ°ç¾å¤‡é›†ç¾¤ ===")

        try:
            # 1. éªŒè¯ç¾å¤‡é›†ç¾¤å¥åº·
            logger.info("1. éªŒè¯ç¾å¤‡é›†ç¾¤å¥åº·...")
            if not self.check_dr_health():
                logger.error("âŒ ç¾å¤‡é›†ç¾¤ä¸å¥åº·ï¼Œæ— æ³•åˆ‡æ¢")
                return False

            # 2. éªŒè¯æ•°æ®å®Œæ•´æ€§
            logger.info("2. éªŒè¯æ•°æ®å®Œæ•´æ€§...")
            if not self._verify_data_integrity():
                logger.error("âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
                return False

            # 3. æ›´æ–° DNS è®°å½•
            logger.info("3. æ›´æ–° DNS è®°å½•...")
            self._update_dns_to_dr()

            # 4. ç­‰å¾… DNS ä¼ æ’­
            logger.info("4. ç­‰å¾… DNS ä¼ æ’­ï¼ˆ60 ç§’ï¼‰...")
            time.sleep(60)

            # 5. éªŒè¯æœåŠ¡å¯ç”¨æ€§
            logger.info("5. éªŒè¯æœåŠ¡å¯ç”¨æ€§...")
            if not self._verify_service():
                logger.error("âŒ æœåŠ¡éªŒè¯å¤±è´¥")
                return False

            logger.info("âœ… åˆ‡æ¢åˆ°ç¾å¤‡é›†ç¾¤å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"âŒ åˆ‡æ¢å¤±è´¥: {e}")
            return False

    def _verify_data_integrity(self) -> bool:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        try:
            connections.connect("dr", host=self.dr_host, port=self.port)

            collections = utility.list_collections(using="dr")
            logger.info(f"  ç¾å¤‡é›†ç¾¤æœ‰ {len(collections)} ä¸ª Collection")

            for collection_name in collections:
                collection = Collection(collection_name, using="dr")
                count = collection.num_entities
                logger.info(f"  {collection_name}: {count} æ¡æ•°æ®")

            connections.disconnect("dr")
            return True

        except Exception as e:
            logger.error(f"æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
            return False

    def _update_dns_to_dr(self):
        """æ›´æ–° DNS åˆ°ç¾å¤‡é›†ç¾¤"""
        response = self.route53.change_resource_record_sets(
            HostedZoneId=self.dns_zone_id,
            ChangeBatch={
                'Changes': [{
                    'Action': 'UPSERT',
                    'ResourceRecordSet': {
                        'Name': self.domain_name,
                        'Type': 'A',
                        'TTL': 60,
                        'ResourceRecords': [{'Value': self.dr_host}]
                    }
                }]
            }
        )
        logger.info(f"  DNS æ›´æ–°å®Œæˆ: {response['ChangeInfo']['Id']}")

    def _verify_service(self) -> bool:
        """éªŒè¯æœåŠ¡å¯ç”¨æ€§"""
        try:
            connections.connect("test", host=self.domain_name, port=self.port)
            collections = utility.list_collections(using="test")
            logger.info(f"  æœåŠ¡å¯ç”¨ï¼Œæœ‰ {len(collections)} ä¸ª Collection")
            connections.disconnect("test")
            return True
        except Exception as e:
            logger.error(f"æœåŠ¡éªŒè¯å¤±è´¥: {e}")
            return False

    def monitor_and_failover(self):
        """ç›‘æ§å¹¶è‡ªåŠ¨æ•…éšœè½¬ç§»"""
        logger.info("=== å¼€å§‹ç›‘æ§ä¸»é›†ç¾¤ ===")

        failure_count = 0
        max_failures = 3

        while True:
            is_healthy = self.check_primary_health()

            if is_healthy:
                failure_count = 0
                logger.info(f"ä¸»é›†ç¾¤å¥åº·: {self.primary_host}")
            else:
                failure_count += 1
                logger.warning(f"ä¸»é›†ç¾¤ä¸å¥åº·: {self.primary_host} ({failure_count}/{max_failures})")

                if failure_count >= max_failures:
                    logger.error("âš ï¸  ä¸»é›†ç¾¤è¿ç»­å¤±è´¥ï¼Œå¼€å§‹æ•…éšœè½¬ç§»...")

                    # å‘é€å‘Šè­¦
                    self._send_alert("ä¸»é›†ç¾¤æ•…éšœï¼Œå¼€å§‹ç¾éš¾æ¢å¤")

                    # åˆ‡æ¢åˆ°ç¾å¤‡é›†ç¾¤
                    if self.switch_to_dr():
                        logger.info("âœ… ç¾éš¾æ¢å¤å®Œæˆ")
                        self._send_alert("ç¾éš¾æ¢å¤å®Œæˆï¼ŒæœåŠ¡å·²åˆ‡æ¢åˆ°ç¾å¤‡é›†ç¾¤")
                        break
                    else:
                        logger.error("âŒ ç¾éš¾æ¢å¤å¤±è´¥")
                        self._send_alert("ç¾éš¾æ¢å¤å¤±è´¥ï¼Œéœ€è¦äººå·¥ä»‹å…¥")
                        break

            time.sleep(30)  # æ¯ 30 ç§’æ£€æŸ¥ä¸€æ¬¡

    def _send_alert(self, message: str):
        """å‘é€å‘Šè­¦"""
        logger.info(f"ğŸ“§ å‘é€å‘Šè­¦: {message}")
        # å®ç°å‘Šè­¦é€»è¾‘ï¼ˆé‚®ä»¶ã€çŸ­ä¿¡ã€Slack ç­‰ï¼‰
        # è¿™é‡Œä»…ä½œç¤ºä¾‹
        pass
```

---

### 2. é…ç½®æ–‡ä»¶

```yaml
# dr_config.yaml
primary:
  host: milvus-beijing.example.com
  port: 19530

dr:
  host: milvus-shanghai.example.com
  port: 19530

backup:
  bucket: s3://milvus-backup-bucket
  retention_days: 30

dns:
  zone_id: Z1234567890ABC
  domain: milvus.example.com

sync:
  interval_hours: 1

monitoring:
  health_check_interval: 30
  failure_threshold: 3
```

---

### 3. å®šæ—¶ä»»åŠ¡è„šæœ¬

```python
"""
å®šæ—¶ä»»åŠ¡ï¼šå¤‡ä»½å’ŒåŒæ­¥
"""
import yaml
import schedule
import time

def load_config(config_file: str) -> Dict:
    """åŠ è½½é…ç½®"""
    with open(config_file, 'r') as f:
        return yaml.safe_load(f)

def main():
    # åŠ è½½é…ç½®
    config = load_config('dr_config.yaml')

    dr_config = {
        'primary_host': config['primary']['host'],
        'dr_host': config['dr']['host'],
        'backup_bucket': config['backup']['bucket'],
        'dns_zone_id': config['dns']['zone_id'],
        'domain_name': config['dns']['domain'],
        'port': config['primary']['port']
    }

    dr_manager = DisasterRecoveryManager(dr_config)

    # å®šæ—¶ä»»åŠ¡
    schedule.every().day.at("02:00").do(dr_manager.daily_backup)
    schedule.every().hour.do(dr_manager.hourly_sync)

    logger.info("=== ç¾éš¾æ¢å¤å®šæ—¶ä»»åŠ¡å¯åŠ¨ ===")
    logger.info("æ¯æ—¥å¤‡ä»½: 02:00")
    logger.info("æ¯å°æ—¶åŒæ­¥: æ¯å°æ—¶")

    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == "__main__":
    main()
```

---

### 4. ç¾éš¾æ¢å¤æ¼”ç»ƒè„šæœ¬

```python
"""
ç¾éš¾æ¢å¤æ¼”ç»ƒ
"""
def run_dr_drill():
    """æ‰§è¡Œç¾éš¾æ¢å¤æ¼”ç»ƒ"""
    logger.info("="*60)
    logger.info("ç¾éš¾æ¢å¤æ¼”ç»ƒ")
    logger.info("="*60)

    config = load_config('dr_config.yaml')
    dr_config = {
        'primary_host': config['primary']['host'],
        'dr_host': config['dr']['host'],
        'backup_bucket': config['backup']['bucket'],
        'dns_zone_id': config['dns']['zone_id'],
        'domain_name': config['dns']['domain']
    }

    dr_manager = DisasterRecoveryManager(dr_config)

    start_time = time.time()

    # æ­¥éª¤1ï¼šéªŒè¯å¤‡ä»½
    logger.info("\næ­¥éª¤1: éªŒè¯æœ€æ–°å¤‡ä»½...")
    # å®ç°å¤‡ä»½éªŒè¯é€»è¾‘

    # æ­¥éª¤2ï¼šéªŒè¯ç¾å¤‡é›†ç¾¤
    logger.info("\næ­¥éª¤2: éªŒè¯ç¾å¤‡é›†ç¾¤...")
    if not dr_manager.check_dr_health():
        logger.error("âŒ ç¾å¤‡é›†ç¾¤ä¸å¥åº·")
        return

    # æ­¥éª¤3ï¼šæ¨¡æ‹Ÿä¸»é›†ç¾¤æ•…éšœ
    logger.info("\næ­¥éª¤3: æ¨¡æ‹Ÿä¸»é›†ç¾¤æ•…éšœ...")
    logger.info("  ï¼ˆåœ¨å®é™…æ¼”ç»ƒä¸­ï¼Œè¿™é‡Œä¼šå…³é—­ä¸»é›†ç¾¤ï¼‰")

    # æ­¥éª¤4: æ‰§è¡Œæ•…éšœè½¬ç§»
    logger.info("\næ­¥éª¤4: æ‰§è¡Œæ•…éšœè½¬ç§»...")
    if dr_manager.switch_to_dr():
        logger.info("âœ… æ•…éšœè½¬ç§»æˆåŠŸ")
    else:
        logger.error("âŒ æ•…éšœè½¬ç§»å¤±è´¥")
        return

    # æ­¥éª¤5: éªŒè¯æœåŠ¡
    logger.info("\næ­¥éª¤5: éªŒè¯æœåŠ¡...")
    if dr_manager._verify_service():
        logger.info("âœ… æœåŠ¡éªŒè¯æˆåŠŸ")
    else:
        logger.error("âŒ æœåŠ¡éªŒè¯å¤±è´¥")
        return

    # è®¡ç®— RTO
    rto = time.time() - start_time
    logger.info(f"\n=== æ¼”ç»ƒå®Œæˆ ===")
    logger.info(f"RTO: {rto:.2f} ç§’ ({rto/60:.2f} åˆ†é’Ÿ)")

    if rto < 3600:  # 1 å°æ—¶
        logger.info("âœ… RTO è¾¾æ ‡ (< 1 å°æ—¶)")
    else:
        logger.error("âŒ RTO ä¸è¾¾æ ‡ (>= 1 å°æ—¶)")

if __name__ == "__main__":
    run_dr_drill()
```

---

## éƒ¨ç½²å’Œä½¿ç”¨

### 1. å®‰è£…ä¾èµ–

```bash
pip install pymilvus boto3 pyyaml schedule requests
```

### 2. é…ç½®æ–‡ä»¶

åˆ›å»º `dr_config.yaml` å¹¶å¡«å†™é…ç½®ã€‚

### 3. å¯åŠ¨å®šæ—¶ä»»åŠ¡

```bash
# åå°è¿è¡Œ
nohup python dr_scheduler.py > dr.log 2>&1 &
```

### 4. å¯åŠ¨ç›‘æ§

```bash
# ç›‘æ§ä¸»é›†ç¾¤å¹¶è‡ªåŠ¨æ•…éšœè½¬ç§»
python dr_monitor.py
```

### 5. æ‰§è¡Œæ¼”ç»ƒ

```bash
# å®šæœŸæ‰§è¡Œç¾éš¾æ¢å¤æ¼”ç»ƒ
python dr_drill.py
```

---

## å…³é”®è¦ç‚¹

1. **è‡ªåŠ¨åŒ–**ï¼šå¤‡ä»½ã€åŒæ­¥ã€æ•…éšœè½¬ç§»å…¨éƒ¨è‡ªåŠ¨åŒ–
2. **ç›‘æ§**ï¼šæŒç»­ç›‘æ§ä¸»é›†ç¾¤å¥åº·çŠ¶æ€
3. **éªŒè¯**ï¼šæ•°æ®å®Œæ•´æ€§éªŒè¯ã€æœåŠ¡å¯ç”¨æ€§éªŒè¯
4. **æ¼”ç»ƒ**ï¼šå®šæœŸæ¼”ç»ƒï¼Œç¡®ä¿æµç¨‹å¯ç”¨
5. **å‘Šè­¦**ï¼šåŠæ—¶é€šçŸ¥è¿ç»´å›¢é˜Ÿ
