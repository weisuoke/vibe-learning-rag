# 最小可用

> 掌握这 20% 的核心知识，解决 80% 的问题

---

## 核心要点

作为 RAG 开发者，你不需要从头实现 Transformer。你只需要理解以下核心知识：

---

### 1. Transformer 的核心是"注意力"

**一句话：** 注意力机制让模型能"看到"所有输入，并决定关注哪些部分。

```
你需要知道的:
✅ 注意力让每个词都能直接关注其他所有词
✅ 这就是 LLM 能理解上下文的原因
✅ RAG 注入的内容会被模型"看到"

你不需要知道的:
❌ 注意力的数学公式怎么推导
❌ softmax 函数的具体实现
❌ 矩阵乘法的细节
```

**实际应用：** 当你设计 RAG Prompt 时，知道模型会"看到"你注入的所有内容，并根据相关性决定关注什么。

---

### 2. 位置很重要

**一句话：** 信息在 Prompt 中的位置会影响模型的关注程度。

```
关键发现 (Lost in the Middle):
┌────────────────────────────────────────┐
│  开头: 关注度高 ████████████           │
│  中间: 关注度低 ████                   │
│  结尾: 关注度高 ████████████           │
└────────────────────────────────────────┘
```

**实际应用：**

```python
# RAG Prompt 设计建议

# ❌ 不好的做法：重要信息在中间
prompt = f"""
一些背景介绍...
{最重要的检索结果}  # 容易被忽略！
更多背景...
用户问题: {question}
"""

# ✅ 好的做法：重要信息在开头或结尾
prompt = f"""
{最重要的检索结果}  # 开头，关注度高
其他检索结果...
用户问题: {question}  # 结尾，关注度高
"""
```

---

### 3. Context Window 是硬限制

**一句话：** Transformer 一次能处理的 Token 数量是有限的，这决定了 RAG 能注入多少内容。

```
常见模型的 Context Window:
┌─────────────────┬──────────────┐
│ 模型            │ Context 大小  │
├─────────────────┼──────────────┤
│ GPT-3.5         │ 4K / 16K     │
│ GPT-4           │ 8K / 128K    │
│ Claude 3        │ 200K         │
│ Llama 3         │ 8K / 128K    │
└─────────────────┴──────────────┘
```

**实际应用：**

```python
# Token 预算分配示例

total_tokens = 4096  # 假设使用 GPT-3.5

# 预留空间
system_prompt = 200      # 系统指令
user_question = 100      # 用户问题
output_buffer = 500      # 预留给输出

# 可用于检索内容的空间
available_for_context = total_tokens - system_prompt - user_question - output_buffer
# = 4096 - 200 - 100 - 500 = 3296 tokens

print(f"可注入的检索内容: 约 {available_for_context} tokens")
print(f"约等于: {available_for_context * 0.75:.0f} 个英文单词")
print(f"约等于: {available_for_context * 0.5:.0f} 个中文字符")
```

---

### 4. 并行处理 vs 顺序处理

**一句话：** Transformer 同时处理所有输入，不像人类按顺序阅读。

```
人类阅读:
"我" → "爱" → "北京" → "天安门"
 ↓      ↓      ↓        ↓
理解1 → 理解2 → 理解3 → 理解4

Transformer:
"我" "爱" "北京" "天安门"
  ↓    ↓    ↓      ↓
  同时处理，互相关注
  ↓
  综合理解
```

**实际应用：** 这意味着：
- 模型不会"忘记"前面的内容（不像 RNN）
- 但也意味着模型没有"阅读顺序"的概念
- 所以位置编码很重要

---

### 5. 多头 = 多角度理解

**一句话：** 模型从多个角度同时理解输入，每个"头"关注不同类型的关系。

```
实际意义:
- 头1 可能关注语法结构
- 头2 可能关注实体关系
- 头3 可能关注时间信息
- ...

你不需要知道:
❌ 具体有多少个头
❌ 每个头的维度是多少
❌ 头之间如何合并
```

**实际应用：** 这解释了为什么 LLM 能同时理解问题的多个方面（实体、时间、意图等）。

---

## 速查卡片

| 知识点 | 核心理解 | RAG 应用 |
|-------|---------|---------|
| 注意力机制 | 每个词看所有词 | 检索内容会被"看到" |
| 位置编码 | 模型知道词的顺序 | 重要信息放开头/结尾 |
| Context Window | Token 数量有限 | 合理分配 Token 预算 |
| 并行处理 | 同时处理所有输入 | 不会"忘记"前面内容 |
| 多头注意力 | 多角度理解 | 能同时理解多种关系 |

---

## 这些知识足以让你：

- ✅ 理解为什么 RAG 能工作
- ✅ 设计更好的 Prompt 结构
- ✅ 合理分配 Token 预算
- ✅ 理解 "Lost in the Middle" 问题
- ✅ 与团队讨论 LLM 相关话题
- ✅ 阅读 RAG 相关论文和博客

---

## 你暂时不需要深入的：

- ❌ Transformer 的数学推导
- ❌ 反向传播和梯度计算
- ❌ 模型训练和微调细节
- ❌ 不同 Transformer 变体的区别

这些可以在需要时再学习。

---

**下一步：** [05_双重类比](./05_双重类比.md) - 用熟悉的概念理解 Transformer
