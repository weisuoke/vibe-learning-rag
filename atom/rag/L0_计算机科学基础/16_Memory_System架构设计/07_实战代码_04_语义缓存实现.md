# 实战代码：语义缓存实现

> **构建基于向量相似度的智能缓存系统**

---

## 学习目标

- 掌握语义缓存的实现原理
- 理解 Embedding 和相似度计算
- 能够集成 OpenAI、ChromaDB、Redis
- 实现生产级的语义缓存系统

---

## 基础实现

### 简单版本

```python
from openai import OpenAI
import numpy as np
from typing import Optional

client = OpenAI()

class BasicSemanticCache:
    """基础语义缓存"""

    def __init__(self, threshold: float = 0.95):
        """
        初始化

        Args:
            threshold: 相似度阈值（0-1）
        """
        self.cache: list[tuple[np.ndarray, str, str]] = []
        self.threshold = threshold
        self.hits = 0
        self.misses = 0

    def _get_embedding(self, text: str) -> np.ndarray:
        """获取文本向量"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding)

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """计算余弦相似度"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    def get(self, question: str) -> Optional[str]:
        """
        查询缓存

        Args:
            question: 问题

        Returns:
            答案，如果未命中返回 None
        """
        if not self.cache:
            self.misses += 1
            return None

        query_emb = self._get_embedding(question)

        best_match = None
        best_similarity = 0.0

        for cached_emb, cached_q, cached_a in self.cache:
            similarity = self._cosine_similarity(query_emb, cached_emb)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = (cached_q, cached_a)

        if best_similarity >= self.threshold:
            self.hits += 1
            print(f"✅ 语义缓存命中 (相似度: {best_similarity:.3f})")
            print(f"   原问题: {best_match[0]}")
            print(f"   新问题: {question}")
            return best_match[1]

        self.misses += 1
        return None

    def put(self, question: str, answer: str) -> None:
        """
        存入缓存

        Args:
            question: 问题
            answer: 答案
        """
        embedding = self._get_embedding(question)
        self.cache.append((embedding, question, answer))

    def hit_rate(self) -> float:
        """计算命中率"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

# 测试
if __name__ == "__main__":
    cache = BasicSemanticCache(threshold=0.95)

    # 第一次提问
    answer = cache.get("什么是 RAG？")
    if answer is None:
        answer = "RAG 是检索增强生成..."
        cache.put("什么是 RAG？", answer)

    # 不同问法，但语义相同
    answer1 = cache.get("RAG 是什么？")  # ✅ 命中
    answer2 = cache.get("请解释 RAG")    # ✅ 命中
    answer3 = cache.get("什么是向量数据库？")  # ❌ 未命中

    print(f"\n命中率: {cache.hit_rate():.2%}")
```

---

## ChromaDB 集成

### 使用向量数据库

```python
import chromadb
from openai import OpenAI
from typing import Optional

client = OpenAI()

class ChromaSemanticCache:
    """基于 ChromaDB 的语义缓存"""

    def __init__(
        self,
        collection_name: str = "semantic_cache",
        threshold: float = 0.95
    ):
        """
        初始化

        Args:
            collection_name: 集合名称
            threshold: 相似度阈值
        """
        self.chroma_client = chromadb.Client()
        self.collection = self.chroma_client.create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        self.threshold = threshold
        self.hits = 0
        self.misses = 0

    def _get_embedding(self, text: str) -> list[float]:
        """获取文本向量"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

    def get(self, question: str) -> Optional[str]:
        """
        查询缓存

        Args:
            question: 问题

        Returns:
            答案，如果未命中返回 None
        """
        query_embedding = self._get_embedding(question)

        # 查询最相似的结果
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=1
        )

        if not results['documents'][0]:
            self.misses += 1
            return None

        # ChromaDB 返回距离，需要转换为相似度
        distance = results['distances'][0][0]
        similarity = 1 - distance

        if similarity >= self.threshold:
            self.hits += 1
            print(f"✅ ChromaDB 缓存命中 (相似度: {similarity:.3f})")
            return results['documents'][0][0]

        self.misses += 1
        return None

    def put(self, question: str, answer: str) -> None:
        """
        存入缓存

        Args:
            question: 问题
            answer: 答案
        """
        embedding = self._get_embedding(question)

        self.collection.add(
            embeddings=[embedding],
            documents=[answer],
            metadatas=[{"question": question}],
            ids=[str(hash(question))]
        )

    def clear(self) -> None:
        """清空缓存"""
        self.chroma_client.delete_collection(self.collection.name)
        self.collection = self.chroma_client.create_collection(
            name=self.collection.name,
            metadata={"hnsw:space": "cosine"}
        )

    def hit_rate(self) -> float:
        """计算命中率"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

# 测试
if __name__ == "__main__":
    cache = ChromaSemanticCache(threshold=0.95)

    # 缓存问答
    cache.put("什么是 RAG？", "RAG 是检索增强生成...")

    # 查询缓存
    answer = cache.get("RAG 是什么？")  # ✅ 命中
    print(answer)
```

---

## Redis 集成

### 使用 Redis 存储

```python
import redis
import json
import numpy as np
from openai import OpenAI
from typing import Optional

client = OpenAI()

class RedisSemanticCache:
    """基于 Redis 的语义缓存"""

    def __init__(
        self,
        redis_client: redis.Redis,
        threshold: float = 0.95,
        ttl: int = 86400
    ):
        """
        初始化

        Args:
            redis_client: Redis 客户端
            threshold: 相似度阈值
            ttl: 过期时间（秒）
        """
        self.redis = redis_client
        self.threshold = threshold
        self.ttl = ttl
        self.hits = 0
        self.misses = 0

    def _get_embedding(self, text: str) -> np.ndarray:
        """获取文本向量"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding)

    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """计算余弦相似度"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

    def get(self, question: str) -> Optional[str]:
        """
        查询缓存

        Args:
            question: 问题

        Returns:
            答案，如果未命中返回 None
        """
        query_emb = self._get_embedding(question)

        # 获取所有缓存的键
        keys = self.redis.keys("semantic:*")
        if not keys:
            self.misses += 1
            return None

        best_match = None
        best_similarity = 0.0

        for key in keys:
            # 获取缓存数据
            data = self.redis.hgetall(key)
            if not data:
                continue

            cached_emb = np.frombuffer(data[b"embedding"], dtype=np.float32)
            cached_answer = data[b"answer"].decode()

            # 计算相似度
            similarity = self._cosine_similarity(query_emb, cached_emb)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = cached_answer

        if best_similarity >= self.threshold:
            self.hits += 1
            print(f"✅ Redis 语义缓存命中 (相似度: {best_similarity:.3f})")
            return best_match

        self.misses += 1
        return None

    def put(self, question: str, answer: str) -> None:
        """
        存入缓存

        Args:
            question: 问题
            answer: 答案
        """
        embedding = self._get_embedding(question)
        key = f"semantic:{hash(question)}"

        # 存储到 Redis
        self.redis.hset(key, mapping={
            "question": question,
            "answer": answer,
            "embedding": embedding.astype(np.float32).tobytes()
        })

        # 设置过期时间
        self.redis.expire(key, self.ttl)

    def clear(self) -> None:
        """清空缓存"""
        keys = self.redis.keys("semantic:*")
        if keys:
            self.redis.delete(*keys)

    def hit_rate(self) -> float:
        """计算命中率"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

# 测试
if __name__ == "__main__":
    redis_client = redis.Redis(host='localhost', port=6379)
    cache = RedisSemanticCache(redis_client, threshold=0.95)

    # 缓存问答
    cache.put("什么是 RAG？", "RAG 是检索增强生成...")

    # 查询缓存
    answer = cache.get("RAG 是什么？")  # ✅ 命中
    print(answer)
```

---

## LLM 调用缓存

### 完整示例

```python
from openai import OpenAI
from typing import Optional
import time

client = OpenAI()

class LLMWithSemanticCache:
    """带语义缓存的 LLM 调用"""

    def __init__(
        self,
        cache: BasicSemanticCache,
        model: str = "gpt-4"
    ):
        """
        初始化

        Args:
            cache: 语义缓存实例
            model: LLM 模型
        """
        self.cache = cache
        self.model = model
        self.api_calls = 0
        self.total_cost = 0.0

    def ask(self, question: str) -> str:
        """
        提问（带缓存）

        Args:
            question: 问题

        Returns:
            答案
        """
        # 先查缓存
        cached_answer = self.cache.get(question)
        if cached_answer:
            return cached_answer

        # 缓存未命中，调用 LLM
        self.api_calls += 1
        print(f"❌ 缓存未命中，调用 LLM (第 {self.api_calls} 次)")

        start = time.time()
        response = client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": question}]
        )
        elapsed = time.time() - start

        answer = response.choices[0].message.content

        # 估算成本（假设 $0.01/1K tokens）
        tokens = response.usage.total_tokens
        cost = tokens / 1000 * 0.01
        self.total_cost += cost

        print(f"   耗时: {elapsed:.2f}秒, 成本: ${cost:.4f}")

        # 存入缓存
        self.cache.put(question, answer)

        return answer

    def stats(self) -> dict:
        """统计信息"""
        cache_hits = self.cache.hits
        total = self.api_calls + cache_hits

        return {
            "api_calls": self.api_calls,
            "cache_hits": cache_hits,
            "total_requests": total,
            "hit_rate": cache_hits / total if total > 0 else 0.0,
            "total_cost": self.total_cost,
            "cost_saved": cache_hits * 0.01  # 假设每次调用 $0.01
        }

# 测试
if __name__ == "__main__":
    cache = BasicSemanticCache(threshold=0.95)
    llm = LLMWithSemanticCache(cache)

    # 第一次提问
    answer1 = llm.ask("什么是 RAG？")

    # 不同问法，但语义相同
    answer2 = llm.ask("RAG 是什么？")  # ✅ 缓存命中
    answer3 = llm.ask("请解释 RAG")    # ✅ 缓存命中
    answer4 = llm.ask("什么是向量数据库？")  # ❌ 缓存未命中

    # 统计
    stats = llm.stats()
    print(f"\n=== 统计信息 ===")
    print(f"API 调用: {stats['api_calls']} 次")
    print(f"缓存命中: {stats['cache_hits']} 次")
    print(f"命中率: {stats['hit_rate']:.2%}")
    print(f"总成本: ${stats['total_cost']:.4f}")
    print(f"节省成本: ${stats['cost_saved']:.4f}")
```

---

## 批量 Embedding 优化

### 提升性能

```python
from typing import List

class BatchSemanticCache(BasicSemanticCache):
    """支持批量 Embedding 的语义缓存"""

    def _get_embeddings_batch(self, texts: List[str]) -> List[np.ndarray]:
        """批量获取向量"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [np.array(emb.embedding) for emb in response.data]

    def put_batch(self, qa_pairs: List[tuple[str, str]]) -> None:
        """
        批量存入缓存

        Args:
            qa_pairs: [(question, answer), ...]
        """
        questions = [q for q, _ in qa_pairs]
        embeddings = self._get_embeddings_batch(questions)

        for (question, answer), embedding in zip(qa_pairs, embeddings):
            self.cache.append((embedding, question, answer))

        print(f"✅ 批量缓存: {len(qa_pairs)} 个问答对")

# 测试
if __name__ == "__main__":
    cache = BatchSemanticCache(threshold=0.95)

    # 批量缓存
    qa_pairs = [
        ("什么是 RAG？", "RAG 是检索增强生成..."),
        ("什么是向量数据库？", "向量数据库用于存储 Embedding..."),
        ("什么是 LLM？", "LLM 是大语言模型...")
    ]

    cache.put_batch(qa_pairs)

    # 查询
    answer = cache.get("RAG 是什么？")
    print(answer)
```

---

## FAISS 加速

### 使用 FAISS 索引

```python
import faiss
import numpy as np
from typing import Optional

class FAISSSemanticCache:
    """使用 FAISS 加速的语义缓存"""

    def __init__(
        self,
        dimension: int = 1536,
        threshold: float = 0.95
    ):
        """
        初始化

        Args:
            dimension: 向量维度
            threshold: 相似度阈值
        """
        self.dimension = dimension
        self.threshold = threshold
        self.index = faiss.IndexFlatIP(dimension)  # 内积索引
        self.cache: List[tuple[str, str]] = []  # [(question, answer), ...]
        self.hits = 0
        self.misses = 0

    def _get_embedding(self, text: str) -> np.ndarray:
        """获取文本向量"""
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return np.array(response.data[0].embedding)

    def put(self, question: str, answer: str) -> None:
        """
        存入缓存

        Args:
            question: 问题
            answer: 答案
        """
        embedding = self._get_embedding(question)

        # 归一化（用于余弦相似度）
        embedding = embedding / np.linalg.norm(embedding)

        # 添加到 FAISS 索引
        self.index.add(embedding.reshape(1, -1).astype(np.float32))

        # 添加到缓存
        self.cache.append((question, answer))

    def get(self, question: str) -> Optional[str]:
        """
        查询缓存（使用 FAISS 加速）

        Args:
            question: 问题

        Returns:
            答案，如果未命中返回 None
        """
        if self.index.ntotal == 0:
            self.misses += 1
            return None

        query_emb = self._get_embedding(question)
        query_emb = query_emb / np.linalg.norm(query_emb)

        # FAISS 搜索
        similarities, indices = self.index.search(
            query_emb.reshape(1, -1).astype(np.float32),
            k=1
        )

        similarity = similarities[0][0]
        if similarity >= self.threshold:
            idx = indices[0][0]
            self.hits += 1
            print(f"✅ FAISS 缓存命中 (相似度: {similarity:.3f})")
            return self.cache[idx][1]

        self.misses += 1
        return None

    def hit_rate(self) -> float:
        """计算命中率"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

# 测试
if __name__ == "__main__":
    cache = FAISSSemanticCache(dimension=1536, threshold=0.95)

    # 批量添加
    for i in range(1000):
        cache.put(f"问题 {i}", f"答案 {i}")

    # 快速查询
    import time
    start = time.time()
    answer = cache.get("问题 500")
    print(f"查询耗时: {(time.time() - start)*1000:.2f}ms")
```

---

## 缓存预热

### 常见问题预热

```python
def warm_up_cache(
    cache: BasicSemanticCache,
    common_questions: List[str]
) -> None:
    """
    缓存预热

    Args:
        cache: 语义缓存实例
        common_questions: 常见问题列表
    """
    print(f"预热缓存: {len(common_questions)} 个常见问题")

    for question in common_questions:
        # 调用 LLM 生成答案
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": question}]
        )
        answer = response.choices[0].message.content

        # 存入缓存
        cache.put(question, answer)

    print("✅ 缓存预热完成")

# 使用示例
common_questions = [
    "什么是 RAG？",
    "什么是向量数据库？",
    "什么是 LLM？",
    "如何实现 LRU 缓存？",
    "Python 如何异步编程？"
]

cache = BasicSemanticCache(threshold=0.95)
warm_up_cache(cache, common_questions)
```

---

## 完整测试套件

```python
import unittest

class TestSemanticCache(unittest.TestCase):
    """语义缓存测试"""

    def setUp(self):
        """测试前准备"""
        self.cache = BasicSemanticCache(threshold=0.95)

    def test_exact_match(self):
        """测试精确匹配"""
        self.cache.put("什么是 RAG？", "RAG 是检索增强生成")

        answer = self.cache.get("什么是 RAG？")
        self.assertIsNotNone(answer)
        self.assertEqual(answer, "RAG 是检索增强生成")

    def test_semantic_match(self):
        """测试语义匹配"""
        self.cache.put("什么是 RAG？", "RAG 是检索增强生成")

        # 不同问法，但语义相同
        answer = self.cache.get("RAG 是什么？")
        self.assertIsNotNone(answer)

    def test_no_match(self):
        """测试未命中"""
        self.cache.put("什么是 RAG？", "RAG 是检索增强生成")

        # 完全不同的问题
        answer = self.cache.get("什么是向量数据库？")
        self.assertIsNone(answer)

    def test_hit_rate(self):
        """测试命中率"""
        self.cache.put("什么是 RAG？", "RAG 是检索增强生成")

        self.cache.get("RAG 是什么？")  # 命中
        self.cache.get("什么是向量数据库？")  # 未命中

        self.assertAlmostEqual(self.cache.hit_rate(), 0.5)

if __name__ == "__main__":
    unittest.main()
```

---

## 性能基准测试

```python
import time
import random

def benchmark_semantic_cache():
    """性能基准测试"""
    cache = BasicSemanticCache(threshold=0.95)

    # 预热：添加 100 个问答对
    print("预热缓存...")
    for i in range(100):
        cache.put(f"问题 {i}", f"答案 {i}")

    # 测试查询性能（命中）
    start = time.time()
    for i in range(100):
        cache.get(f"问题 {i}")
    hit_time = time.time() - start

    # 测试查询性能（未命中）
    start = time.time()
    for i in range(100):
        cache.get(f"不存在的问题 {i}")
    miss_time = time.time() - start

    print(f"\n=== 性能基准 ===")
    print(f"查询 100 次（命中）: {hit_time:.3f}秒 ({100/hit_time:.0f} ops/s)")
    print(f"查询 100 次（未命中）: {miss_time:.3f}秒 ({100/miss_time:.0f} ops/s)")
    print(f"命中率: {cache.hit_rate():.2%}")

if __name__ == "__main__":
    benchmark_semantic_cache()
```

---

## 总结

### 关键要点

1. **Embedding 生成**：使用 OpenAI API
2. **相似度计算**：余弦相似度
3. **阈值选择**：0.95 平衡准确性和命中率
4. **性能优化**：批量 Embedding、FAISS 索引
5. **集成方案**：ChromaDB、Redis

### 最佳实践

- 使用批量 Embedding 提升性能
- 选择合适的相似度阈值
- 定期清理过期缓存
- 监控命中率和成本节省

---

## 参考资源

- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [Redis Semantic Caching](https://redis.io/blog/build-smarter-ai-agents-manage-short-term-and-long-term-memory-with-redis)
