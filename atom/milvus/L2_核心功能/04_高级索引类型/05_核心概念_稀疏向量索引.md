# 核心概念：稀疏向量索引

> 稀疏向量索引通过倒排索引结构，高效处理稀疏数据，是混合检索的关键技术

---

## 概念定义

**稀疏向量索引**是专门为稀疏向量（大部分元素为0）设计的索引结构，只存储和计算非零元素，效率提升100-1000倍。

**核心特点**：
- 倒排索引：只存储非零维度
- 高效计算：只计算交集
- 混合检索：结合关键词和语义

---

## 一、稀疏向量索引原理

### 1.1 稀疏向量表示

```python
import numpy as np

# 稠密向量（传统表示）
dense_vector = np.array([0, 0, 0.5, 0, 0, 0.8, 0, 0, 0.3, 0])
print(f"稠密向量: {dense_vector}")
print(f"存储: {dense_vector.nbytes} 字节")
print(f"非零元素: {np.count_nonzero(dense_vector)}")

# 稀疏向量（字典表示）
sparse_vector = {2: 0.5, 5: 0.8, 8: 0.3}
print(f"\n稀疏向量: {sparse_vector}")
print(f"存储: {len(sparse_vector) * 12} 字节")  # (index:4字节 + value:8字节)
print(f"压缩比: {dense_vector.nbytes / (len(sparse_vector) * 12):.1f}x")
```

**输出**：
```
稠密向量: [0.  0.  0.5 0.  0.  0.8 0.  0.  0.3 0. ]
存储: 80 字节
非零元素: 3

稀疏向量: {2: 0.5, 5: 0.8, 8: 0.3}
存储: 36 字节
压缩比: 2.2x
```

### 1.2 倒排索引结构

```python
class InvertedIndex:
    """倒排索引实现"""
    
    def __init__(self):
        self.index = {}  # {dimension: [(doc_id, value), ...]}
        
    def add_document(self, doc_id, sparse_vector):
        """添加文档的稀疏向量"""
        for dim, value in sparse_vector.items():
            if dim not in self.index:
                self.index[dim] = []
            self.index[dim].append((doc_id, value))
    
    def search(self, query_vector, top_k=10):
        """搜索最相似的文档"""
        # 1. 收集候选文档（只看query的非零维度）
        candidates = {}
        for dim, query_value in query_vector.items():
            if dim in self.index:
                for doc_id, doc_value in self.index[dim]:
                    if doc_id not in candidates:
                        candidates[doc_id] = 0
                    # 累加内积
                    candidates[doc_id] += query_value * doc_value
        
        # 2. 排序返回Top-K
        sorted_docs = sorted(candidates.items(), key=lambda x: x[1], reverse=True)
        return sorted_docs[:top_k]

# 示例
index = InvertedIndex()

# 添加文档
docs = [
    {2: 0.5, 5: 0.8, 8: 0.3},  # doc_0
    {2: 0.3, 5: 0.6, 10: 0.9}, # doc_1
    {5: 0.7, 8: 0.4, 10: 0.5}, # doc_2
]

for doc_id, doc_vector in enumerate(docs):
    index.add_document(doc_id, doc_vector)

print("倒排索引结构:")
for dim, postings in index.index.items():
    print(f"维度 {dim}: {postings}")

# 搜索
query = {2: 0.4, 5: 0.9}
results = index.search(query, top_k=3)
print(f"\n查询: {query}")
print(f"结果: {results}")
```

**输出**：
```
倒排索引结构:
维度 2: [(0, 0.5), (1, 0.3)]
维度 5: [(0, 0.8), (1, 0.6), (2, 0.7)]
维度 8: [(0, 0.3), (2, 0.4)]
维度 10: [(1, 0.9), (2, 0.5)]

查询: {2: 0.4, 5: 0.9}
结果: [(0, 0.92), (1, 0.66), (2, 0.63)]
```

### 1.3 效率分析

```python
# 稠密向量内积（需要计算所有维度）
def dense_dot_product(v1, v2):
    return np.dot(v1, v2)  # O(D) 时间复杂度

# 稀疏向量内积（只计算非零维度的交集）
def sparse_dot_product(v1, v2):
    result = 0
    for dim in v1.keys():
        if dim in v2:
            result += v1[dim] * v2[dim]
    return result  # O(min(|v1|, |v2|)) 时间复杂度

# 性能对比
D = 100_000  # 词表大小
k = 50       # 平均非零元素数

dense_ops = D
sparse_ops = k

print(f"稠密向量计算: {dense_ops:,} 次乘法")
print(f"稀疏向量计算: {sparse_ops:,} 次乘法")
print(f"加速比: {dense_ops / sparse_ops:.0f}x")
```

**输出**：
```
稠密向量计算: 100,000 次乘法
稀疏向量计算: 50 次乘法
加速比: 2000x
```

---

## 二、RAG应用场景

### 2.1 BM25关键词匹配

```python
from collections import Counter
import math

class BM25:
    """BM25算法实现"""
    
    def __init__(self, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.doc_freqs = []  # 每个文档的词频
        self.idf = {}        # 逆文档频率
        self.avgdl = 0       # 平均文档长度
        
    def fit(self, documents):
        """训练：计算IDF"""
        self.doc_freqs = [Counter(doc.split()) for doc in documents]
        self.avgdl = sum(len(doc.split()) for doc in documents) / len(documents)
        
        # 计算IDF
        N = len(documents)
        df = Counter()
        for doc_freq in self.doc_freqs:
            df.update(doc_freq.keys())
        
        for term, doc_count in df.items():
            self.idf[term] = math.log((N - doc_count + 0.5) / (doc_count + 0.5) + 1)
    
    def get_sparse_vector(self, query):
        """将查询转换为稀疏向量"""
        query_terms = query.split()
        sparse_vector = {}
        
        for term in query_terms:
            if term in self.idf:
                # 使用词的索引作为维度（简化示例）
                dim = hash(term) % 100000
                sparse_vector[dim] = self.idf[term]
        
        return sparse_vector

# 示例
documents = [
    "机器学习 算法 模型 训练",
    "深度学习 神经网络 模型",
    "自然语言处理 文本 分析",
]

bm25 = BM25()
bm25.fit(documents)

query = "机器学习 模型"
sparse_vec = bm25.get_sparse_vector(query)
print(f"查询: {query}")
print(f"稀疏向量: {sparse_vec}")
print(f"非零维度数: {len(sparse_vec)}")
```

### 2.2 混合检索实现

```python
from pymilvus import Collection, AnnSearchRequest, RRFRanker

# 创建混合Collection
collection = Collection("hybrid_search")

# 稀疏向量搜索
sparse_req = AnnSearchRequest(
    data=[{123: 0.8, 456: 0.5}],  # BM25稀疏向量
    anns_field="sparse_embedding",
    param={"metric_type": "IP", "params": {"drop_ratio_search": 0.2}},
    limit=20
)

# 稠密向量搜索
dense_req = AnnSearchRequest(
    data=[[0.1] * 768],  # BERT稠密向量
    anns_field="dense_embedding",
    param={"metric_type": "L2", "params": {"ef": 100}},
    limit=20
)

# RRF融合
results = collection.hybrid_search(
    reqs=[sparse_req, dense_req],
    rerank=RRFRanker(),
    limit=10
)

print(f"混合检索结果: {len(results[0])} 个文档")
```

### 2.3 效果对比

| 方法 | NDCG@10 | 延迟 | 特点 |
|------|---------|------|------|
| BM25（稀疏） | 0.35 | 5ms | 精确关键词匹配 |
| BERT（稠密） | 0.42 | 50ms | 语义理解 |
| Hybrid（RRF） | 0.48 | 30ms | 结合两者优势 |

---

## 三、Milvus稀疏向量索引

### 3.1 SPARSE_INVERTED_INDEX

```python
from pymilvus import Collection, FieldSchema, CollectionSchema, DataType

# 定义Schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="sparse_embedding", dtype=DataType.SPARSE_FLOAT_VECTOR)
]
schema = CollectionSchema(fields)

# 创建Collection
collection = Collection("sparse_demo", schema)

# 创建索引
index_params = {
    "index_type": "SPARSE_INVERTED_INDEX",
    "metric_type": "IP",
    "params": {
        "drop_ratio_build": 0.2  # 构建时丢弃最小的20%值
    }
}
collection.create_index("sparse_embedding", index_params)

# 插入数据
sparse_data = [
    {0: 0.5, 10: 0.3, 100: 0.8},
    {5: 0.6, 20: 0.4, 150: 0.9}
]
collection.insert([sparse_data])
collection.load()

# 搜索
search_params = {"metric_type": "IP", "params": {"drop_ratio_search": 0.2}}
results = collection.search(
    data=[{0: 0.5, 10: 0.3}],
    anns_field="sparse_embedding",
    param=search_params,
    limit=10
)
```

### 3.2 参数说明

| 参数 | 作用 | 推荐值 | 影响 |
|------|------|--------|------|
| drop_ratio_build | 构建时丢弃小值比例 | 0.1-0.3 | 越大索引越小，召回越低 |
| drop_ratio_search | 搜索时丢弃小值比例 | 0.1-0.3 | 越大搜索越快，召回越低 |

---

## 总结

### 稀疏向量索引的核心价值

1. **高效存储**：只存储非零元素，压缩100-1000倍
2. **快速计算**：只计算交集，加速100-1000倍
3. **混合检索**：结合关键词和语义，效果提升15%

### 适用场景

- ✅ BM25关键词检索
- ✅ SPLADE稀疏Embedding
- ✅ 混合检索（稀疏+稠密）
- ❌ 稠密向量（使用HNSW等索引）

### 下一步

- **实战练习**：[11_实战代码_场景3_稀疏向量实战](./11_实战代码_场景3_稀疏向量实战.md)
- **综合应用**：[12_实战代码_场景4_RAG性能优化](./12_实战代码_场景4_RAG性能优化.md)
