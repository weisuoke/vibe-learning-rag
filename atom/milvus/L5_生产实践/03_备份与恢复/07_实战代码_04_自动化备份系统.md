# å®æˆ˜ä»£ç 4ï¼šè‡ªåŠ¨åŒ–å¤‡ä»½ç³»ç»Ÿ

> ç”Ÿäº§ç¯å¢ƒè‡ªåŠ¨åŒ–å¤‡ä»½çš„å®Œæ•´å®ç°

---

## åœºæ™¯æ¦‚è¿°

æœ¬åœºæ™¯æ¼”ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªå®Œæ•´çš„è‡ªåŠ¨åŒ–å¤‡ä»½ç³»ç»Ÿï¼š
- å®šæ—¶è‡ªåŠ¨å¤‡ä»½
- å¤‡ä»½éªŒè¯å’Œç›‘æ§
- å‘Šè­¦é€šçŸ¥
- å¤‡ä»½æ¸…ç†ç­–ç•¥
- Web ç®¡ç†ç•Œé¢

**é€‚ç”¨åœºæ™¯ï¼š**
- ç”Ÿäº§ç¯å¢ƒå®šæœŸå¤‡ä»½
- å¤š Collection ç»Ÿä¸€ç®¡ç†
- å¤‡ä»½çŠ¶æ€ç›‘æ§
- æ•…éšœè‡ªåŠ¨å‘Šè­¦

---

## å®Œæ•´ç¤ºä¾‹ä»£ç 

### ç¤ºä¾‹1ï¼šè‡ªåŠ¨åŒ–å¤‡ä»½è°ƒåº¦å™¨

```python
#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–å¤‡ä»½è°ƒåº¦å™¨
"""

import schedule
import time
import logging
from datetime import datetime, timedelta
from typing import List, Dict
import json
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/milvus-backup.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class AutoBackupScheduler:
    """è‡ªåŠ¨åŒ–å¤‡ä»½è°ƒåº¦å™¨"""

    def __init__(self, config_file: str = "backup_config.json"):
        """åˆå§‹åŒ–"""
        self.config = self._load_config(config_file)
        self.backup_history = []

    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®"""
        with open(config_file, 'r') as f:
            return json.load(f)

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        logger.info("å¯åŠ¨è‡ªåŠ¨å¤‡ä»½è°ƒåº¦å™¨...")

        # é…ç½®å®šæ—¶ä»»åŠ¡
        for job in self.config['jobs']:
            if job['type'] == 'full':
                schedule.every().week.at(job['time']).do(
                    self.full_backup,
                    collections=job['collections']
                )
            elif job['type'] == 'incremental':
                schedule.every().day.at(job['time']).do(
                    self.incremental_backup,
                    collections=job['collections']
                )

        # å¥åº·æ£€æŸ¥
        schedule.every().hour.do(self.health_check)

        # æ¸…ç†æ—§å¤‡ä»½
        schedule.every().day.at("03:00").do(self.cleanup_old_backups)

        logger.info("è°ƒåº¦å™¨å·²å¯åŠ¨")

        # è¿è¡Œå¾ªç¯
        while True:
            schedule.run_pending()
            time.sleep(60)

    def full_backup(self, collections: List[str]):
        """å…¨é‡å¤‡ä»½"""
        logger.info(f"å¼€å§‹å…¨é‡å¤‡ä»½: {collections}")

        backup_name = f"full_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        try:
            # è°ƒç”¨å¤‡ä»½å·¥å…·
            result = self._execute_backup(backup_name, collections, full=True)

            # éªŒè¯å¤‡ä»½
            if self._verify_backup(backup_name):
                logger.info(f"âœ… å…¨é‡å¤‡ä»½æˆåŠŸ: {backup_name}")
                self._record_backup(backup_name, 'full', result)
            else:
                raise Exception("å¤‡ä»½éªŒè¯å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ å…¨é‡å¤‡ä»½å¤±è´¥: {e}")
            self._send_alert(f"å…¨é‡å¤‡ä»½å¤±è´¥: {e}")

    def incremental_backup(self, collections: List[str]):
        """å¢é‡å¤‡ä»½"""
        logger.info(f"å¼€å§‹å¢é‡å¤‡ä»½: {collections}")

        backup_name = f"incr_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        try:
            # è·å–ä¸Šæ¬¡å¤‡ä»½
            last_backup = self._get_last_backup('full')

            # è°ƒç”¨å¤‡ä»½å·¥å…·
            result = self._execute_backup(
                backup_name,
                collections,
                full=False,
                base_backup=last_backup
            )

            # éªŒè¯å¤‡ä»½
            if self._verify_backup(backup_name):
                logger.info(f"âœ… å¢é‡å¤‡ä»½æˆåŠŸ: {backup_name}")
                self._record_backup(backup_name, 'incremental', result)
            else:
                raise Exception("å¤‡ä»½éªŒè¯å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ å¢é‡å¤‡ä»½å¤±è´¥: {e}")
            self._send_alert(f"å¢é‡å¤‡ä»½å¤±è´¥: {e}")

    def _execute_backup(
        self,
        backup_name: str,
        collections: List[str],
        full: bool = True,
        base_backup: str = None
    ) -> Dict:
        """æ‰§è¡Œå¤‡ä»½"""
        import requests

        url = f"{self.config['backup_api']}/backup/create"
        payload = {
            "backup_name": backup_name,
            "collections": collections,
            "compression": True
        }

        if not full and base_backup:
            payload["base_backup"] = base_backup
            payload["incremental"] = True

        response = requests.post(url, json=payload, timeout=3600)
        response.raise_for_status()

        return response.json()['data']

    def _verify_backup(self, backup_name: str) -> bool:
        """éªŒè¯å¤‡ä»½"""
        import requests

        url = f"{self.config['backup_api']}/backup/verify"
        payload = {"backup_name": backup_name}

        response = requests.post(url, json=payload, timeout=600)
        response.raise_for_status()

        return response.json()['data']['is_valid']

    def _record_backup(self, backup_name: str, backup_type: str, result: Dict):
        """è®°å½•å¤‡ä»½"""
        self.backup_history.append({
            "backup_name": backup_name,
            "type": backup_type,
            "timestamp": time.time(),
            "entities": result.get('total_entities', 0),
            "size": result.get('backup_size', 0),
            "duration": result.get('duration', 0)
        })

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open('/var/lib/milvus-backup/history.json', 'w') as f:
            json.dump(self.backup_history, f, indent=2)

    def _get_last_backup(self, backup_type: str) -> str:
        """è·å–æœ€è¿‘çš„å¤‡ä»½"""
        backups = [
            b for b in self.backup_history
            if b['type'] == backup_type
        ]

        if backups:
            return max(backups, key=lambda x: x['timestamp'])['backup_name']

        return None

    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        logger.info("æ‰§è¡Œå¥åº·æ£€æŸ¥...")

        try:
            # æ£€æŸ¥æœ€è¿‘å¤‡ä»½æ—¶é—´
            if self.backup_history:
                latest = max(self.backup_history, key=lambda x: x['timestamp'])
                time_since_last = time.time() - latest['timestamp']

                if time_since_last > 86400:  # 24å°æ—¶
                    self._send_alert(
                        f"æœ€è¿‘å¤‡ä»½æ—¶é—´è¶…è¿‡24å°æ—¶: {time_since_last/3600:.1f}å°æ—¶"
                    )

            # æ£€æŸ¥å¤‡ä»½å­˜å‚¨ç©ºé—´
            self._check_storage_space()

            logger.info("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")

        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

    def _check_storage_space(self):
        """æ£€æŸ¥å­˜å‚¨ç©ºé—´"""
        import shutil

        backup_dir = self.config['backup_dir']
        stat = shutil.disk_usage(backup_dir)

        used_percent = stat.used / stat.total * 100

        if used_percent > 90:
            self._send_alert(f"å¤‡ä»½å­˜å‚¨ç©ºé—´ä¸è¶³: {used_percent:.1f}%")

    def cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        logger.info("æ¸…ç†æ—§å¤‡ä»½...")

        retention_days = self.config.get('retention_days', 7)
        cutoff_time = time.time() - (retention_days * 86400)

        deleted_count = 0
        for backup in self.backup_history[:]:
            if backup['timestamp'] < cutoff_time:
                try:
                    self._delete_backup(backup['backup_name'])
                    self.backup_history.remove(backup)
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"åˆ é™¤å¤‡ä»½å¤±è´¥: {e}")

        logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤ {deleted_count} ä¸ªæ—§å¤‡ä»½")

    def _delete_backup(self, backup_name: str):
        """åˆ é™¤å¤‡ä»½"""
        import requests

        url = f"{self.config['backup_api']}/backup/delete"
        payload = {"backup_name": backup_name}

        response = requests.delete(url, json=payload)
        response.raise_for_status()

    def _send_alert(self, message: str):
        """å‘é€å‘Šè­¦"""
        logger.error(f"ğŸš¨ å‘Šè­¦: {message}")

        # å‘é€é‚®ä»¶
        if self.config.get('email_alerts'):
            self._send_email_alert(message)

        # å‘é€ Slack é€šçŸ¥
        if self.config.get('slack_webhook'):
            self._send_slack_alert(message)

    def _send_email_alert(self, message: str):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        import smtplib
        from email.mime.text import MIMEText

        msg = MIMEText(message)
        msg['Subject'] = 'Milvus å¤‡ä»½å‘Šè­¦'
        msg['From'] = self.config['email_from']
        msg['To'] = self.config['email_to']

        with smtplib.SMTP(self.config['smtp_server']) as server:
            server.send_message(msg)

    def _send_slack_alert(self, message: str):
        """å‘é€ Slack å‘Šè­¦"""
        import requests

        payload = {
            "text": f"ğŸš¨ Milvus å¤‡ä»½å‘Šè­¦\n{message}"
        }

        requests.post(self.config['slack_webhook'], json=payload)


def main():
    """ä¸»å‡½æ•°"""
    scheduler = AutoBackupScheduler(config_file="backup_config.json")
    scheduler.start()


if __name__ == "__main__":
    main()
```

**é…ç½®æ–‡ä»¶ backup_config.jsonï¼š**

```json
{
  "backup_api": "http://localhost:8080/api/v1",
  "backup_dir": "/data/milvus-backup",
  "retention_days": 7,
  "jobs": [
    {
      "type": "full",
      "time": "02:00",
      "collections": ["collection1", "collection2"]
    },
    {
      "type": "incremental",
      "time": "06:00",
      "collections": ["collection1", "collection2"]
    }
  ],
  "email_alerts": true,
  "email_from": "backup@example.com",
  "email_to": "admin@example.com",
  "smtp_server": "smtp.example.com",
  "slack_webhook": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
}
```

---

### ç¤ºä¾‹2ï¼šWeb ç®¡ç†ç•Œé¢

```python
#!/usr/bin/env python3
"""
å¤‡ä»½ç®¡ç† Web ç•Œé¢
"""

from flask import Flask, render_template, jsonify, request
import json
from datetime import datetime

app = Flask(__name__)


@app.route('/')
def index():
    """é¦–é¡µ"""
    return render_template('index.html')


@app.route('/api/backups')
def list_backups():
    """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
    with open('/var/lib/milvus-backup/history.json', 'r') as f:
        backups = json.load(f)

    return jsonify({
        "code": 0,
        "data": {
            "backups": backups,
            "total": len(backups)
        }
    })


@app.route('/api/backup/create', methods=['POST'])
def create_backup():
    """åˆ›å»ºå¤‡ä»½"""
    data = request.json

    # è°ƒç”¨å¤‡ä»½å·¥å…·
    # ...

    return jsonify({
        "code": 0,
        "message": "å¤‡ä»½å·²åˆ›å»º"
    })


@app.route('/api/backup/restore', methods=['POST'])
def restore_backup():
    """æ¢å¤å¤‡ä»½"""
    data = request.json

    # è°ƒç”¨æ¢å¤å·¥å…·
    # ...

    return jsonify({
        "code": 0,
        "message": "å¤‡ä»½å·²æ¢å¤"
    })


@app.route('/api/stats')
def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    with open('/var/lib/milvus-backup/history.json', 'r') as f:
        backups = json.load(f)

    total_size = sum(b['size'] for b in backups)
    total_entities = sum(b['entities'] for b in backups)

    return jsonify({
        "code": 0,
        "data": {
            "total_backups": len(backups),
            "total_size": total_size,
            "total_entities": total_entities,
            "latest_backup": max(backups, key=lambda x: x['timestamp']) if backups else None
        }
    })


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

---

### ç¤ºä¾‹3ï¼šéƒ¨ç½²ä¸ºç³»ç»ŸæœåŠ¡

**systemd æœåŠ¡æ–‡ä»¶ `/etc/systemd/system/milvus-backup.service`ï¼š**

```ini
[Unit]
Description=Milvus Auto Backup Service
After=network.target milvus.service

[Service]
Type=simple
User=milvus
WorkingDirectory=/opt/milvus-backup
ExecStart=/usr/bin/python3 /opt/milvus-backup/auto_backup.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

**éƒ¨ç½²æ­¥éª¤ï¼š**

```bash
# 1. å®‰è£…æœåŠ¡
sudo cp milvus-backup.service /etc/systemd/system/
sudo systemctl daemon-reload

# 2. å¯åŠ¨æœåŠ¡
sudo systemctl enable milvus-backup
sudo systemctl start milvus-backup

# 3. æŸ¥çœ‹çŠ¶æ€
sudo systemctl status milvus-backup

# 4. æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u milvus-backup -f
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **è‡ªåŠ¨åŒ–è°ƒåº¦**ï¼šä½¿ç”¨ schedule åº“å®ç°å®šæ—¶å¤‡ä»½
2. **å¥åº·ç›‘æ§**ï¼šå®šæœŸæ£€æŸ¥å¤‡ä»½çŠ¶æ€å’Œå­˜å‚¨ç©ºé—´
3. **å‘Šè­¦é€šçŸ¥**ï¼šæ”¯æŒé‚®ä»¶å’Œ Slack å‘Šè­¦
4. **Web ç®¡ç†**ï¼šæä¾›å¯è§†åŒ–ç®¡ç†ç•Œé¢
5. **ç³»ç»ŸæœåŠ¡**ï¼šéƒ¨ç½²ä¸º systemd æœåŠ¡ï¼Œå¼€æœºè‡ªå¯

### é€‚ç”¨åœºæ™¯

- âœ… ç”Ÿäº§ç¯å¢ƒè‡ªåŠ¨å¤‡ä»½
- âœ… å¤š Collection ç»Ÿä¸€ç®¡ç†
- âœ… å¤‡ä»½çŠ¶æ€ç›‘æ§
- âœ… æ•…éšœè‡ªåŠ¨å‘Šè­¦

### å®Œæˆå­¦ä¹ 

æ­å–œä½ å®Œæˆäº† Milvus å¤‡ä»½ä¸æ¢å¤çš„å…¨éƒ¨å­¦ä¹ ï¼

**ä¸‹ä¸€æ­¥ï¼š**
- å­¦ä¹  [Kuberneteséƒ¨ç½²](../04_Kuberneteséƒ¨ç½²/)
- æˆ–æ·±å…¥å­¦ä¹  [RAGé›†æˆå®æˆ˜](../../L6_RAGé›†æˆå®æˆ˜/)
