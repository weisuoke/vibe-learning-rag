# 最小可用

掌握以下内容，就能在 RAG 开发中正确使用文本分块。

---

## 4.1 使用 LangChain 的 RecursiveCharacterTextSplitter

**这是 90% 场景的最佳选择，开箱即用。**

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 创建分块器（最常用配置）
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # 每个块的目标大小
    chunk_overlap=50,    # 重叠大小（防止切断上下文）
)

# 分块
text = "你的长文本内容..."
chunks = splitter.split_text(text)

# 查看结果
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}: {len(chunk)} chars")
```

**记住这个配置：**
- `chunk_size=500`：适合大多数场景
- `chunk_overlap=50`：约 10% 的重叠

---

## 4.2 根据场景调整块大小

**不同场景的推荐配置：**

| 场景 | chunk_size | chunk_overlap | 原因 |
|------|------------|---------------|------|
| **问答系统** | 300-500 | 50 | 精准匹配，块小更精确 |
| **文档摘要** | 1000-2000 | 100 | 需要更多上下文 |
| **代码检索** | 500-1000 | 100 | 保持函数完整 |
| **聊天机器人** | 200-400 | 30 | 快速响应，块小更快 |

```python
# 问答系统配置
qa_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50,
)

# 文档摘要配置
summary_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=150,
)
```

---

## 4.3 处理中文文档

**中文需要添加中文标点作为分隔符：**

```python
# 中文文档分块器
chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=[
        "\n\n",      # 段落
        "\n",        # 换行
        "。",        # 句号
        "！",        # 感叹号
        "？",        # 问号
        "；",        # 分号
        "，",        # 逗号
        " ",         # 空格
        "",          # 字符
    ]
)

text = """
第一段内容，介绍了Python的基本概念。Python是一种简单易学的编程语言。

第二段内容，详细讲解了变量和数据类型。变量是存储数据的容器。
"""

chunks = chinese_splitter.split_text(text)
```

---

## 4.4 与文档加载器配合使用

**完整的文档处理流程：**

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. 加载文档
loader = PyPDFLoader("document.pdf")
documents = loader.load()

# 2. 创建分块器
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
)

# 3. 分块（保留元数据）
chunks = splitter.split_documents(documents)

# 4. 查看结果
for chunk in chunks[:3]:
    print(f"内容: {chunk.page_content[:100]}...")
    print(f"元数据: {chunk.metadata}")
    print("---")
```

**注意：** 使用 `split_documents()` 而不是 `split_text()`，可以保留文档的元数据（如页码、来源等）。

---

## 4.5 快速验证分块效果

**分块后检查这三点：**

```python
def validate_chunks(chunks: list, min_size: int = 100, max_size: int = 600):
    """验证分块效果"""

    # 1. 检查块数量
    print(f"总块数: {len(chunks)}")

    # 2. 检查块大小分布
    sizes = [len(c) if isinstance(c, str) else len(c.page_content) for c in chunks]
    print(f"块大小: 最小={min(sizes)}, 最大={max(sizes)}, 平均={sum(sizes)//len(sizes)}")

    # 3. 检查是否有异常小/大的块
    too_small = sum(1 for s in sizes if s < min_size)
    too_large = sum(1 for s in sizes if s > max_size)
    print(f"异常块: {too_small} 个太小, {too_large} 个太大")

    # 4. 抽样查看内容
    print("\n--- 抽样查看 ---")
    sample = chunks[len(chunks)//2]  # 取中间的块
    content = sample if isinstance(sample, str) else sample.page_content
    print(f"示例块: {content[:200]}...")

# 使用
validate_chunks(chunks)
```

**健康的分块应该：**
- ✅ 块大小在目标范围内（±20%）
- ✅ 没有太多异常小的块
- ✅ 内容在语义边界处切分

---

## 这些知识足以：

- ✅ 为任何文档创建合理的分块
- ✅ 根据场景调整分块参数
- ✅ 正确处理中文文档
- ✅ 验证分块效果
- ✅ 与 LangChain 生态无缝集成

---

## 速查表

```python
# 通用配置（复制即用）
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", "。", ".", "！", "!", "？", "?", " ", ""]
)

# 分块文本
chunks = splitter.split_text(text)

# 分块文档（保留元数据）
chunks = splitter.split_documents(documents)
```

---

**下一步：** [05_双重类比](./05_双重类比.md) - 用熟悉的概念理解文本分块
