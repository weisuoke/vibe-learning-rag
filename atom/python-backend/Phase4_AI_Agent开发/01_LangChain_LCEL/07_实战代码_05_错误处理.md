# 实战代码 05 - 错误处理

> 重试、降级策略，保证系统可用性

---

## 场景1：基础重试机制

### 需求

LLM API 可能因为限流、网络波动等原因失败，需要自动重试。

---

### 完整代码

```python
"""
场景1：基础重试机制
演示：with_retry() 自动重试
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ===== 1. 不带重试的链（容易失败） =====
print("=== 不带重试的链 ===")

prompt = ChatPromptTemplate.from_template("回答：{question}")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
parser = StrOutputParser()

chain_without_retry = prompt | llm | parser

# ===== 2. 带重试的链（更可靠） =====
print("\n=== 带重试的链 ===")

llm_with_retry = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
).with_retry(
    stop_after_attempt=3,  # 最多重试3次
    wait_exponential_jitter=True  # 指数退避 + 随机抖动
)

chain_with_retry = prompt | llm_with_retry | parser

# ===== 3. 测试 =====
print("\n=== 测试重试机制 ===")

try:
    answer = chain_with_retry.invoke({"question": "什么是 LCEL？"})
    print(f"回答: {answer[:100]}...")
except Exception as e:
    print(f"失败: {e}")
```

---

## 场景2：降级策略

### 需求

主模型失败时，自动切换到备用模型。

---

### 完整代码

```python
"""
场景2：降级策略
演示：with_fallbacks() 自动降级
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ===== 1. 定义主模型和备用模型 =====
print("=== 定义主模型和备用模型 ===")

# 主模型：GPT-4（可能失败或限流）
primary_llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    request_timeout=5  # 5秒超时
)

# 备用模型1：GPT-3.5-turbo
backup_llm_1 = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7
)

# 备用模型2：GPT-3.5-turbo-16k
backup_llm_2 = ChatOpenAI(
    model="gpt-3.5-turbo-16k",
    temperature=0.7
)

# ===== 2. 添加降级策略 =====
print("\n=== 添加降级策略 ===")

llm_with_fallbacks = primary_llm.with_fallbacks([
    backup_llm_1,
    backup_llm_2
])

print("降级顺序: GPT-4 → GPT-3.5-turbo → GPT-3.5-turbo-16k")

# ===== 3. 构建链 =====
prompt = ChatPromptTemplate.from_template("回答：{question}")
parser = StrOutputParser()

chain = prompt | llm_with_fallbacks | parser

# ===== 4. 测试 =====
print("\n=== 测试降级策略 ===")

answer = chain.invoke({"question": "什么是 LCEL？"})
print(f"回答: {answer[:100]}...")
```

---

## 场景3：组合重试和降级

### 需求

先重试，重试失败后再降级。

---

### 完整代码

```python
"""
场景3：组合重试和降级
演示：先重试，再降级
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ===== 1. 构建多层防护的 LLM =====
print("=== 构建多层防护的 LLM ===")

# 主模型：GPT-4 + 重试
primary_llm = (
    ChatOpenAI(model="gpt-4", temperature=0.7)
    .with_retry(stop_after_attempt=3)
)

# 备用模型1：GPT-3.5-turbo + 重试
backup_llm_1 = (
    ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    .with_retry(stop_after_attempt=2)
)

# 备用模型2：GPT-3.5-turbo-16k（最后的保障）
backup_llm_2 = ChatOpenAI(model="gpt-3.5-turbo-16k", temperature=0.7)

# 组合：先重试，再降级
llm = primary_llm.with_fallbacks([backup_llm_1, backup_llm_2])

print("防护策略:")
print("1. GPT-4（最多重试3次）")
print("2. 失败后降级到 GPT-3.5-turbo（最多重试2次）")
print("3. 失败后降级到 GPT-3.5-turbo-16k")

# ===== 2. 构建链 =====
prompt = ChatPromptTemplate.from_template("回答：{question}")
parser = StrOutputParser()

chain = prompt | llm | parser

# ===== 3. 测试 =====
print("\n=== 测试多层防护 ===")

answer = chain.invoke({"question": "什么是 LCEL？"})
print(f"回答: {answer[:100]}...")
```

---

## 场景4：自定义错误处理

### 需求

捕获特定错误，执行自定义处理逻辑。

---

### 完整代码

```python
"""
场景4：自定义错误处理
演示：try-except + 自定义逻辑
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

load_dotenv()

# ===== 1. 定义错误处理函数 =====
def handle_llm_error(error: Exception) -> str:
    """自定义错误处理"""
    error_type = type(error).__name__
    error_msg = str(error)

    print(f"\n[错误处理] 捕获到错误: {error_type}")
    print(f"[错误处理] 错误信息: {error_msg[:100]}...")

    # 根据错误类型返回不同的降级响应
    if "rate_limit" in error_msg.lower():
        return "抱歉，当前请求过多，请稍后再试。"
    elif "timeout" in error_msg.lower():
        return "抱歉，请求超时，请稍后再试。"
    elif "invalid" in error_msg.lower():
        return "抱歉，请求参数有误，请检查输入。"
    else:
        return "抱歉，服务暂时不可用，请稍后再试。"

# ===== 2. 包装链以捕获错误 =====
def safe_invoke(chain, input_data):
    """安全调用链，捕获错误"""
    try:
        return chain.invoke(input_data)
    except Exception as e:
        return handle_llm_error(e)

# ===== 3. 构建链 =====
prompt = ChatPromptTemplate.from_template("回答：{question}")
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    request_timeout=5
)
parser = StrOutputParser()

chain = prompt | llm | parser

# ===== 4. 测试 =====
print("=== 测试自定义错误处理 ===")

questions = [
    "什么是 LCEL？",
    "什么是 Runnable？"
]

for question in questions:
    print(f"\n问题: {question}")
    answer = safe_invoke(chain, {"question": question})
    print(f"回答: {answer[:100]}...")
```

---

## 场景5：生产级错误处理

### 需求

完整的生产级错误处理：重试 + 降级 + 日志 + 监控。

---

### 完整代码

```python
"""
场景5：生产级错误处理
演示：完整的错误处理策略
"""

import os
import logging
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# ===== 1. 配置日志 =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===== 2. 定义监控指标 =====
class Metrics:
    """简单的监控指标"""
    def __init__(self):
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.fallback_used = 0
        self.retry_count = 0

    def record_request(self):
        self.total_requests += 1

    def record_success(self):
        self.successful_requests += 1

    def record_failure(self):
        self.failed_requests += 1

    def record_fallback(self):
        self.fallback_used += 1

    def record_retry(self):
        self.retry_count += 1

    def get_stats(self):
        return {
            "total": self.total_requests,
            "success": self.successful_requests,
            "failed": self.failed_requests,
            "fallback": self.fallback_used,
            "retry": self.retry_count,
            "success_rate": f"{self.successful_requests / max(self.total_requests, 1) * 100:.1f}%"
        }

metrics = Metrics()

# ===== 3. 构建生产级链 =====
logger.info("构建生产级链...")

# 主模型
primary_llm = (
    ChatOpenAI(model="gpt-4", temperature=0.7, request_timeout=10)
    .with_retry(
        stop_after_attempt=3,
        wait_exponential_jitter=True
    )
)

# 备用模型
backup_llm = (
    ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, request_timeout=10)
    .with_retry(stop_after_attempt=2)
)

# 最后的保障
emergency_llm = ChatOpenAI(model="gpt-3.5-turbo-16k", temperature=0.7)

# 组合
llm = primary_llm.with_fallbacks([backup_llm, emergency_llm])

prompt = ChatPromptTemplate.from_template("回答：{question}")
parser = StrOutputParser()

chain = prompt | llm | parser

# ===== 4. 包装链以添加监控 =====
def monitored_invoke(question: str) -> dict:
    """带监控的调用"""
    metrics.record_request()
    start_time = datetime.now()

    try:
        logger.info(f"处理问题: {question}")
        answer = chain.invoke({"question": question})

        metrics.record_success()
        duration = (datetime.now() - start_time).total_seconds()

        logger.info(f"成功响应，耗时: {duration:.2f}秒")

        return {
            "success": True,
            "answer": answer,
            "duration": duration,
            "error": None
        }

    except Exception as e:
        metrics.record_failure()
        duration = (datetime.now() - start_time).total_seconds()

        logger.error(f"请求失败: {str(e)}")

        return {
            "success": False,
            "answer": "抱歉，服务暂时不可用，请稍后再试。",
            "duration": duration,
            "error": str(e)
        }

# ===== 5. 测试 =====
print("=== 测试生产级错误处理 ===\n")

questions = [
    "什么是 LCEL？",
    "什么是 Runnable？",
    "什么是管道操作符？"
]

for question in questions:
    result = monitored_invoke(question)

    print(f"问题: {question}")
    print(f"成功: {result['success']}")
    print(f"耗时: {result['duration']:.2f}秒")
    print(f"回答: {result['answer'][:100]}...")
    if result['error']:
        print(f"错误: {result['error']}")
    print()

# ===== 6. 输出监控指标 =====
print("\n=== 监控指标 ===")
stats = metrics.get_stats()
for key, value in stats.items():
    print(f"{key}: {value}")
```

**运行输出示例**：
```
=== 测试生产级错误处理 ===

问题: 什么是 LCEL？
成功: True
耗时: 2.34秒
回答: LCEL 是 LangChain Expression Language 的缩写...

问题: 什么是 Runnable？
成功: True
耗时: 1.98秒
回答: Runnable 是 LCEL 的核心接口...

问题: 什么是管道操作符？
成功: True
耗时: 2.12秒
回答: 管道操作符 | 用于连接多个组件...

=== 监控指标 ===
total: 3
success: 3
failed: 0
fallback: 0
retry: 0
success_rate: 100.0%
```

---

## 总结

### 五个场景对比

| 场景 | 核心特性 | 适用场景 |
|------|---------|---------|
| **场景1** | 基础重试 | 临时性错误 |
| **场景2** | 降级策略 | 主服务不可用 |
| **场景3** | 重试+降级 | 生产环境 |
| **场景4** | 自定义处理 | 特殊错误逻辑 |
| **场景5** | 完整方案 | 生产级应用 |

---

### 错误处理最佳实践

**1. 重试策略**：
- 使用指数退避避免雪崩
- 限制最大重试次数（3-5次）
- 只重试临时性错误

**2. 降级策略**：
- 主模型 → 备用模型 → 缓存/默认响应
- 每层都要有重试机制
- 记录降级事件用于监控

**3. 日志和监控**：
- 记录所有错误和降级事件
- 统计成功率、失败率、降级率
- 设置告警阈值

**4. 用户体验**：
- 提供友好的错误提示
- 避免暴露技术细节
- 提供重试选项

---

### 学习检查清单

- [ ] 使用 with_retry() 添加重试机制
- [ ] 使用 with_fallbacks() 添加降级策略
- [ ] 组合重试和降级（先重试，再降级）
- [ ] 实现自定义错误处理逻辑
- [ ] 添加日志和监控
- [ ] 理解指数退避的作用
- [ ] 构建生产级的错误处理方案

---

**版本**: v1.0
**最后更新**: 2026-02-12
